{"kernels": [{"code": "def matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K: tl.constexpr,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n\n    tl.static_assert(\n        K % (4 * BLOCK_SIZE_K) == 0,\n        \"K / 4 must be divisible by BLOCK_SIZE_K => K divisible by 4*BLOCK_SIZE_K\",\n    )\n\n    pid = tl.program_id(axis=0)\n\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n\n    first_pid_m = group_id * GROUP_SIZE_M\n\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n\n    for i in range(4):\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n        for j in range(0, tl.cdiv(K // 4, BLOCK_SIZE_K)):\n            k = i * tl.cdiv(K // 4, BLOCK_SIZE_K) + j\n\n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0)\n\n            b_uint8 = tl.load(b_ptrs, mask=offs_k[:, None] < K, other=0)\n\n            mask = 3 << (2 * i)\n\n            b = (b_uint8 & mask) >> (2 * i)\n\n            tensor_full = tl.full((1,), 1, dtype=tl.int8)\n\n            accumulator += tl.dot(a, (b.to(tl.int8) - tensor_full), out_dtype=tl.int32)\n\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)", "encoded": [25, 280, 176, 180, 87, 181, 87, 182, 87, 183, 87, 184, 87, 185, 51, 6, 87, 186, 87, 187, 87, 188, 87, 189, 87, 190, 87, 191, 87, 192, 51, 6, 87, 193, 51, 6, 87, 194, 51, 6, 87, 195, 51, 6, 133, 51, 27, -1, 67, 176, 185, 162, 176, 281, 177, 194, 133, 61, 282, 87, 283, 133, 27, 196, 145, 129, 176, 197, 145, 282, 133, 27, 198, 145, 53, 176, 183, 87, 192, 133, 27, 199, 145, 53, 176, 184, 87, 193, 133, 27, 200, 145, 195, 177, 199, 27, 201, 145, 196, 42, 200, 27, 202, 145, 201, 177, 195, 27, 203, 145, 34, 176, 198, 4, 202, 87, 195, 133, 27, 204, 145, 202, 59, 196, 162, 200, 162, 203, 27, 205, 145, 196, 162, 200, 42, 203, 27, 206, 145, 176, 204, 177, 192, 59, 60, 176, 282, 87, 192, 133, 133, 162, 183, 27, 207, 145, 176, 205, 177, 193, 59, 60, 176, 282, 87, 193, 133, 133, 162, 184, 27, 208, 145, 60, 176, 282, 87, 194, 133, 27, 209, 145, 180, 59, 176, 206, 165, 51, 87, 150, 23, 177, 186, 59, 208, 165, 150, 87, 51, 23, 177, 187, 133, 27, 210, 145, 181, 59, 176, 208, 165, 51, 87, 150, 23, 177, 188, 59, 207, 165, 150, 87, 51, 23, 177, 189, 133, 27, 211, 145, 134, 176, 176, 192, 87, 193, 133, 87, 73, 145, 179, 133, 27, 107, 212, 120, 5, 176, 281, 133, 51, 27, 210, 145, 181, 59, 176, 208, 165, 51, 87, 150, 23, 177, 188, 59, 207, 165, 150, 87, 51, 23, 177, 189, 133, 27, 107, 213, 120, 5, 176, 282, 87, 53, 176, 185, 42, 281, 87, 194, 133, 133, 51, 27, 214, 145, 212, 177, 53, 176, 185, 42, 281, 87, 194, 133, 59, 213, 27, 215, 145, 48, 176, 209, 87, 216, 145, 208, 165, 150, 87, 51, 23, 1, 185, 4, 214, 177, 194, 87, 217, 145, 282, 133, 27, 218, 145, 48, 176, 210, 87, 216, 145, 208, 165, 51, 87, 150, 23, 1, 185, 87, 217, 145, 282, 133, 27, 216, 145, 284, 108, 285, 177, 212, 27, 219, 145, 176, 218, 131, 216, 133, 102, 285, 177, 212, 27, 220, 145, 171, 176, 176, 286, 87, 133, 87, 286, 87, 73, 145, 110, 133, 27, 211, 130, 14, 176, 215, 87, 219, 66, 221, 176, 110, 133, 4, 220, 87, 222, 145, 179, 133, 27, 209, 130, 194, 177, 187, 27, 210, 130, 194, 177, 188, 27, 62, 27, 62, 27, 223, 145, 211, 27, 224, 145, 204, 177, 192, 59, 60, 176, 282, 87, 192, 133, 27, 225, 145, 205, 177, 193, 59, 60, 176, 282, 87, 193, 133, 27, 226, 145, 182, 59, 190, 177, 224, 165, 51, 87, 150, 23, 59, 191, 177, 225, 165, 150, 87, 51, 23, 27, 227, 145, 176, 224, 165, 51, 87, 150, 23, 1, 183, 133, 131, 176, 225, 165, 150, 87, 51, 23, 1, 184, 133, 27, 10, 176, 226, 87, 223, 87, 216, 145, 227, 133, 27, 3, 27]}, {"code": "def causal_conv1d_fwd_kernel(\n    x,\n    y,\n    weight,\n    bias,\n    residual,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    B: tl.constexpr,\n    D: tl.constexpr,\n    W: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    NB: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n), tl.load(cu_seqlens + i_n + 1)\n        T = eos - bos\n    else:\n        i_n = i_b\n        bos, eos = i_b * T, i_b * T + T\n\n    o_d = i_d * BD + tl.arange(0, BD)\n    o_w = tl.arange(0, W)\n    m_d = o_d < D\n\n    if HAS_WEIGHT:\n\n        b_w = tl.load(weight + o_d[:, None] * W + o_w, mask=m_d[:, None], other=0).to(\n            tl.float32\n        )\n\n    b_y = tl.zeros((BT, BD), dtype=tl.float32)\n    for i_w in tl.static_range(-W + 1, 1):\n        p_yi = tl.make_block_ptr(\n            x + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0)\n        )\n\n        b_yi = tl.load(p_yi, boundary_check=(0, 1))\n        if HAS_WEIGHT:\n            b_yi *= tl.sum(b_w * (o_w == (i_w + W - 1)), 1)\n        b_y += b_yi\n    if HAS_BIAS:\n        b_y += tl.load(bias + o_d, mask=m_d).to(tl.float32)\n\n    if ACTIVATION == \"swish\" or ACTIVATION == \"silu\":\n        b_y = b_y * tl.sigmoid(b_y)\n\n    if HAS_RESIDUAL:\n        p_residual = tl.make_block_ptr(\n            residual + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0)\n        )\n        b_residual = tl.load(p_residual, boundary_check=(0, 1))\n        b_y += b_residual\n\n    p_y = tl.make_block_ptr(\n        y + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0)\n    )\n    tl.store(\n        p_y,\n        tl.cast(b_y, dtype=p_y.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )", "encoded": [25, 286, 182, 186, 87, 187, 87, 188, 87, 189, 87, 190, 87, 191, 87, 192, 87, 193, 87, 194, 51, 6, 87, 195, 51, 6, 87, 196, 51, 6, 87, 197, 51, 6, 87, 198, 51, 6, 87, 199, 51, 6, 87, 200, 51, 6, 87, 201, 51, 6, 87, 202, 51, 6, 87, 203, 51, 6, 87, 204, 51, 6, 134, 51, 27, -1, 205, 87, 206, 87, 207, 146, 182, 130, 182, 287, 134, 87, 130, 182, 288, 134, 87, 130, 182, 289, 134, 134, 27, 139, 204, 51, 27, 208, 87, 206, 146, 182, 46, 182, 192, 59, 206, 183, 289, 134, 66, 209, 182, 185, 134, 87, 46, 182, 192, 59, 206, 183, 289, 59, 288, 134, 66, 209, 182, 185, 134, 134, 27, 210, 87, 211, 146, 182, 46, 182, 191, 59, 208, 134, 87, 46, 182, 191, 59, 208, 59, 288, 134, 134, 27, 193, 146, 211, 4, 210, 27, 141, 27, 26, 51, 27, 208, 146, 207, 27, 210, 87, 211, 146, 182, 207, 183, 193, 87, 207, 183, 193, 59, 193, 134, 27, 47, 27, 212, 146, 205, 183, 198, 59, 60, 182, 287, 87, 198, 134, 27, 213, 146, 60, 182, 287, 87, 196, 134, 27, 214, 146, 212, 1, 195, 27, 139, 201, 51, 27, 215, 146, 46, 182, 188, 59, 212, 171, 51, 87, 151, 23, 183, 196, 59, 213, 87, 216, 146, 214, 171, 51, 87, 151, 23, 87, 217, 146, 287, 134, 66, 209, 182, 114, 134, 27, 141, 27, 218, 146, 135, 182, 182, 197, 87, 198, 134, 87, 73, 146, 114, 134, 27, 107, 219, 121, 157, 182, 4, 196, 59, 288, 87, 288, 134, 51, 27, 220, 146, 164, 182, 186, 59, 210, 183, 195, 87, 182, 193, 87, 195, 134, 87, 182, 195, 87, 288, 134, 87, 182, 206, 183, 197, 59, 219, 87, 205, 183, 198, 134, 87, 182, 197, 87, 198, 134, 87, 182, 288, 87, 287, 134, 134, 27, 221, 146, 46, 182, 220, 87, 222, 146, 182, 287, 87, 288, 134, 134, 27, 139, 201, 51, 27, 221, 20, 165, 182, 215, 183, 182, 213, 61, 219, 59, 196, 4, 288, 134, 87, 288, 134, 27, 141, 27, 218, 131, 221, 27, 62, 27, 139, 202, 51, 27, 218, 131, 46, 182, 189, 59, 212, 87, 216, 146, 214, 134, 66, 209, 182, 114, 134, 27, 141, 27, 139, 200, 61, 290, 111, 200, 61, 291, 51, 27, 218, 146, 218, 183, 154, 182, 218, 134, 27, 141, 27, 139, 203, 51, 27, 223, 146, 164, 182, 190, 59, 210, 183, 195, 87, 182, 193, 87, 195, 134, 87, 182, 195, 87, 288, 134, 87, 182, 206, 183, 197, 87, 205, 183, 198, 134, 87, 182, 197, 87, 198, 134, 87, 182, 288, 87, 287, 134, 134, 27, 224, 146, 46, 182, 223, 87, 222, 146, 182, 287, 87, 288, 134, 134, 27, 218, 131, 224, 27, 141, 27, 225, 146, 164, 182, 187, 59, 210, 183, 195, 87, 182, 193, 87, 195, 134, 87, 182, 195, 87, 288, 134, 87, 182, 206, 183, 197, 87, 205, 183, 198, 134, 87, 182, 197, 87, 198, 134, 87, 182, 288, 87, 287, 134, 134, 27, 10, 182, 225, 87, 166, 182, 218, 87, 73, 146, 225, 66, 73, 66, 92, 87, 127, 146, 292, 134, 87, 222, 146, 182, 287, 87, 288, 134, 134, 27, 3, 27]}, {"code": "def causal_conv1d_bwd_kernel(\n    x,\n    y,\n    weight,\n    bias,\n    residual,\n    dy,\n    dx,\n    dw,\n    db,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    B: tl.constexpr,\n    D: tl.constexpr,\n    W: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    NB: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n), tl.load(cu_seqlens + i_n + 1)\n        T = eos - bos\n    else:\n        i_tg = i_b * tl.num_programs(1) + i_t\n        i_n = i_b\n        bos, eos = i_b * T, i_b * T + T\n\n    o_d = i_d * BD + tl.arange(0, BD)\n    o_w = tl.arange(0, W)\n    m_d = o_d < D\n\n    if HAS_WEIGHT:\n        p_x = tl.make_block_ptr(\n            x + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0)\n        )\n        b_x = tl.load(p_x, boundary_check=(0, 1))\n\n        b_w = tl.load(weight + o_d[:, None] * W + o_w, mask=m_d[:, None], other=0)\n\n    b_dx = tl.zeros((BT, BD), dtype=tl.float32)\n    if HAS_BIAS:\n        b_db = tl.zeros((BD,), dtype=tl.float32)\n    for i_w in tl.static_range(0, W):\n        p_dy = tl.make_block_ptr(\n            dy + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0)\n        )\n\n        b_dy = tl.load(p_dy, boundary_check=(0, 1))\n\n        if ACTIVATION == \"swish\" or ACTIVATION == \"silu\":\n            p_y = tl.make_block_ptr(\n                y + bos * D,\n                (T, D),\n                (D, 1),\n                (i_t * BT + i_w, i_d * BD),\n                (BT, BD),\n                (1, 0),\n            )\n\n            b_y = tl.load(p_y, boundary_check=(0, 1)).to(tl.float32)\n            b_ys = tl.sigmoid(b_y)\n            b_dy = b_dy * b_ys * (1 + b_y * (1 - b_ys))\n\n        b_wdy = b_dy\n        if HAS_WEIGHT:\n\n            b_wdy = b_wdy * tl.sum(b_w * (o_w == (W - i_w - 1)), 1)\n\n            b_dw = tl.sum(b_dy * b_x, 0)\n            tl.store(\n                dw + i_tg * D * W + o_d * W + W - i_w - 1,\n                b_dw.to(dw.dtype.element_ty),\n                mask=m_d,\n            )\n        if HAS_BIAS and i_w == 0:\n            b_db += tl.sum(b_dy, 0)\n        b_dx += b_wdy\n    if HAS_BIAS:\n        b_db = tl.cast(b_db, dtype=db.dtype.element_ty, fp_downcast_rounding=\"rtne\")\n        tl.store(db + i_tg * D + o_d, b_db, mask=m_d)\n\n    p_dx = tl.make_block_ptr(\n        dx + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0)\n    )\n    tl.store(\n        p_dx,\n        tl.cast(b_dx, dtype=p_dx.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )", "encoded": [25, 287, 183, 187, 87, 188, 87, 189, 87, 190, 87, 191, 87, 192, 87, 193, 87, 194, 87, 195, 87, 196, 87, 197, 87, 198, 87, 199, 51, 6, 87, 200, 51, 6, 87, 201, 51, 6, 87, 202, 51, 6, 87, 203, 51, 6, 87, 204, 51, 6, 87, 205, 51, 6, 87, 206, 51, 6, 87, 207, 51, 6, 87, 208, 51, 6, 87, 209, 51, 6, 135, 51, 27, -1, 210, 87, 211, 87, 212, 147, 183, 131, 183, 288, 135, 87, 131, 183, 289, 135, 87, 131, 183, 290, 135, 135, 27, 140, 209, 51, 27, 213, 147, 211, 27, 214, 87, 211, 147, 183, 47, 183, 197, 59, 211, 184, 290, 135, 66, 215, 183, 186, 135, 87, 47, 183, 197, 59, 211, 184, 290, 59, 289, 135, 66, 215, 183, 186, 135, 135, 27, 216, 87, 217, 147, 183, 47, 183, 196, 59, 214, 135, 87, 47, 183, 196, 59, 214, 59, 289, 135, 135, 27, 198, 147, 217, 4, 216, 27, 142, 27, 26, 51, 27, 213, 147, 212, 184, 110, 183, 289, 135, 59, 211, 27, 214, 147, 212, 27, 216, 87, 217, 147, 183, 212, 184, 198, 87, 212, 184, 198, 59, 198, 135, 27, 46, 27, 218, 147, 210, 184, 203, 59, 60, 183, 288, 87, 203, 135, 27, 219, 147, 60, 183, 288, 87, 201, 135, 27, 220, 147, 218, 1, 200, 27, 140, 206, 51, 27, 221, 147, 165, 183, 187, 59, 216, 184, 200, 87, 183, 198, 87, 200, 135, 87, 183, 200, 87, 289, 135, 87, 183, 211, 184, 202, 87, 210, 184, 203, 135, 87, 183, 202, 87, 203, 135, 87, 183, 289, 87, 288, 135, 135, 27, 222, 147, 47, 183, 221, 87, 223, 147, 183, 288, 87, 289, 135, 135, 27, 224, 147, 47, 183, 189, 59, 218, 172, 51, 87, 152, 23, 184, 201, 59, 219, 87, 225, 147, 220, 172, 51, 87, 152, 23, 87, 226, 147, 288, 135, 27, 142, 27, 227, 147, 136, 183, 183, 202, 87, 203, 135, 87, 73, 147, 115, 135, 27, 140, 207, 51, 27, 228, 147, 136, 183, 183, 203, 87, 135, 87, 73, 147, 115, 135, 27, 142, 27, 107, 229, 122, 158, 183, 288, 87, 201, 135, 51, 27, 230, 147, 165, 183, 192, 59, 216, 184, 200, 87, 183, 198, 87, 200, 135, 87, 183, 200, 87, 289, 135, 87, 183, 211, 184, 202, 59, 229, 87, 210, 184, 203, 135, 87, 183, 202, 87, 203, 135, 87, 183, 289, 87, 288, 135, 135, 27, 231, 147, 47, 183, 230, 87, 223, 147, 183, 288, 87, 289, 135, 135, 27, 140, 205, 61, 291, 112, 205, 61, 292, 51, 27, 232, 147, 165, 183, 188, 59, 216, 184, 200, 87, 183, 198, 87, 200, 135, 87, 183, 200, 87, 289, 135, 87, 183, 211, 184, 202, 59, 229, 87, 210, 184, 203, 135, 87, 183, 202, 87, 203, 135, 87, 183, 289, 87, 288, 135, 135, 27, 233, 147, 47, 183, 232, 87, 223, 147, 183, 288, 87, 289, 135, 135, 66, 215, 183, 115, 135, 27, 234, 147, 155, 183, 233, 135, 27, 231, 147, 231, 184, 234, 184, 183, 289, 59, 233, 184, 183, 289, 4, 234, 135, 135, 27, 142, 27, 235, 147, 231, 27, 140, 206, 51, 27, 235, 147, 235, 184, 166, 183, 224, 184, 183, 219, 61, 201, 4, 229, 4, 289, 135, 87, 289, 135, 27, 236, 147, 166, 183, 231, 184, 222, 87, 288, 135, 27, 10, 183, 194, 59, 213, 184, 200, 184, 201, 59, 218, 184, 201, 59, 201, 4, 229, 4, 289, 87, 236, 66, 215, 183, 194, 66, 73, 66, 92, 135, 87, 225, 147, 220, 135, 27, 142, 27, 140, 207, 82, 229, 61, 288, 51, 27, 228, 132, 166, 183, 231, 87, 288, 135, 27, 142, 27, 227, 132, 235, 27, 62, 27, 140, 207, 51, 27, 228, 147, 167, 183, 228, 87, 73, 147, 195, 66, 73, 66, 92, 87, 128, 147, 293, 135, 27, 10, 183, 195, 59, 213, 184, 200, 59, 218, 87, 228, 87, 225, 147, 220, 135, 27, 142, 27, 237, 147, 165, 183, 193, 59, 216, 184, 200, 87, 183, 198, 87, 200, 135, 87, 183, 200, 87, 289, 135, 87, 183, 211, 184, 202, 87, 210, 184, 203, 135, 87, 183, 202, 87, 203, 135, 87, 183, 289, 87, 288, 135, 135, 27, 10, 183, 237, 87, 167, 183, 227, 87, 73, 147, 237, 66, 73, 66, 92, 87, 128, 147, 293, 135, 87, 223, 147, 183, 288, 87, 289, 135, 135, 27, 3, 27]}, {"code": "def causal_conv1d_update_kernel(\n    x,\n    cache,\n    residual,\n    y,\n    weight,\n    bias,\n    D: tl.constexpr,\n    W: tl.constexpr,\n    BD: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n):\n    i_d, i_n = tl.program_id(0), tl.program_id(1)\n\n    o_d = i_d * BD + tl.arange(0, BD)\n    o_w = tl.arange(0, W)\n    m_d = o_d < D\n    m_c = o_w < W - 1\n\n    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=0).to(tl.float32)\n\n    p_cache = tl.make_block_ptr(\n        cache + i_n * D * W, (D, W), (W, 1), (i_d * BD, 1), (BD, W), (1, 0)\n    )\n\n    b_cache = tl.load(p_cache, boundary_check=(0, 1)).to(tl.float32)\n    b_cache = tl.where(m_c[None, :], b_cache, b_x[:, None])\n\n    if HAS_WEIGHT:\n        b_w = tl.load(weight + o_d[:, None] * W + o_w, mask=m_d[:, None], other=0)\n        b_y = tl.sum(b_cache * b_w, 1)\n    else:\n        b_y = tl.sum(b_cache, 1)\n    if HAS_BIAS:\n        b_y += tl.load(bias + o_d, mask=m_d)\n\n    if ACTIVATION == \"swish\" or ACTIVATION == \"silu\":\n        b_y = b_y * tl.sigmoid(b_y)\n\n    if HAS_RESIDUAL:\n        b_y += tl.load(residual + i_n * D + o_d, mask=m_d, other=0)\n\n    tl.store(\n        y + i_n * D + o_d,\n        tl.cast(b_y, dtype=y.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        mask=m_d,\n    )\n\n    b_cache = tl.cast(\n        b_cache, dtype=cache.dtype.element_ty, fp_downcast_rounding=\"rtne\"\n    )\n\n    p_cache = tl.make_block_ptr(\n        cache + i_n * D * W, (D, W), (W, 1), (i_d * BD, 0), (BD, W), (1, 0)\n    )\n    tl.store(p_cache, b_cache, boundary_check=(0, 1))", "encoded": [25, 288, 184, 188, 87, 189, 87, 190, 87, 191, 87, 192, 87, 193, 87, 194, 51, 6, 87, 195, 51, 6, 87, 196, 51, 6, 87, 197, 51, 6, 87, 198, 51, 6, 87, 199, 51, 6, 87, 200, 51, 6, 135, 51, 27, -1, 201, 87, 202, 147, 184, 131, 184, 289, 135, 87, 131, 184, 290, 135, 135, 27, 203, 147, 201, 185, 196, 59, 60, 184, 289, 87, 196, 135, 27, 204, 147, 60, 184, 289, 87, 195, 135, 27, 205, 147, 203, 1, 194, 27, 206, 147, 204, 1, 195, 4, 290, 27, 207, 147, 46, 184, 188, 59, 202, 185, 194, 59, 203, 87, 208, 147, 205, 87, 209, 147, 289, 135, 66, 210, 184, 115, 135, 27, 211, 147, 166, 184, 189, 59, 202, 185, 194, 185, 195, 87, 184, 194, 87, 195, 135, 87, 184, 195, 87, 290, 135, 87, 184, 201, 185, 196, 87, 290, 135, 87, 184, 196, 87, 195, 135, 87, 184, 290, 87, 289, 135, 135, 27, 212, 147, 46, 184, 211, 87, 213, 147, 184, 289, 87, 290, 135, 135, 66, 210, 184, 115, 135, 27, 212, 147, 155, 184, 206, 173, 152, 87, 51, 23, 87, 212, 87, 207, 173, 51, 87, 152, 23, 135, 27, 140, 198, 51, 27, 214, 147, 46, 184, 192, 59, 203, 173, 51, 87, 152, 23, 185, 195, 59, 204, 87, 208, 147, 205, 173, 51, 87, 152, 23, 87, 209, 147, 289, 135, 27, 215, 147, 167, 184, 212, 185, 214, 87, 290, 135, 27, 142, 27, 26, 51, 27, 215, 147, 167, 184, 212, 87, 290, 135, 27, 47, 27, 140, 199, 51, 27, 215, 132, 46, 184, 193, 59, 203, 87, 208, 147, 205, 135, 27, 142, 27, 140, 197, 61, 291, 112, 197, 61, 292, 51, 27, 215, 147, 215, 185, 156, 184, 215, 135, 27, 142, 27, 140, 200, 51, 27, 215, 132, 46, 184, 190, 59, 202, 185, 194, 59, 203, 87, 208, 147, 205, 87, 209, 147, 289, 135, 27, 142, 27, 10, 184, 191, 59, 202, 185, 194, 59, 203, 87, 168, 184, 215, 87, 73, 147, 191, 66, 73, 66, 92, 87, 128, 147, 293, 135, 87, 208, 147, 205, 135, 27, 212, 147, 168, 184, 212, 87, 73, 147, 189, 66, 73, 66, 92, 87, 128, 147, 293, 135, 27, 211, 147, 166, 184, 189, 59, 202, 185, 194, 185, 195, 87, 184, 194, 87, 195, 135, 87, 184, 195, 87, 290, 135, 87, 184, 201, 185, 196, 87, 289, 135, 87, 184, 196, 87, 195, 135, 87, 184, 290, 87, 289, 135, 135, 27, 10, 184, 211, 87, 212, 87, 213, 147, 184, 289, 87, 290, 135, 135, 27, 3, 27]}, {"code": "def layer_norm_fwd_kernel_quant(\n    X,\n    Y,\n    W,\n    B,\n    RESIDUAL,\n    RESIDUAL_OUT,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_y_row,\n    stride_res_row,\n    stride_res_out_row,\n    N,\n    eps,\n    IS_RMS_NORM: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    STORE_RESIDUAL_OUT: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n):\n\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n\n    mask = cols < N\n    if HAS_WEIGHT:\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n\n    y = x_hat * w if HAS_WEIGHT else x_hat\n    if HAS_BIAS:\n        y = y + b\n\n    scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), 1e-5)\n\n    y = tl.extra.cuda.libdevice.round(y * scale)\n    y = tl.maximum(tl.minimum(y, 127), -128) / scale\n\n    tl.store(Y + cols, y, mask=mask)", "encoded": [26, 294, 190, 194, 90, 195, 90, 196, 90, 197, 90, 198, 90, 199, 90, 200, 90, 201, 90, 202, 90, 203, 90, 204, 90, 205, 90, 206, 90, 207, 90, 208, 52, 6, 90, 209, 52, 6, 90, 210, 52, 6, 90, 211, 52, 6, 90, 212, 52, 6, 90, 213, 52, 6, 140, 52, 28, -1, 214, 153, 136, 190, 295, 140, 28, 194, 137, 214, 191, 202, 28, 195, 137, 214, 191, 203, 28, 145, 210, 52, 28, 198, 137, 214, 191, 204, 28, 147, 28, 145, 211, 52, 28, 199, 137, 214, 191, 205, 28, 147, 28, 215, 153, 62, 190, 295, 90, 209, 140, 28, 216, 153, 48, 190, 194, 61, 215, 90, 217, 153, 215, 1, 206, 90, 218, 153, 295, 140, 68, 219, 190, 119, 140, 28, 145, 210, 52, 28, 220, 153, 48, 190, 198, 61, 215, 90, 217, 153, 215, 1, 206, 90, 218, 153, 295, 140, 68, 219, 190, 119, 140, 28, 216, 137, 220, 28, 147, 28, 145, 211, 52, 28, 10, 190, 199, 61, 215, 90, 216, 90, 217, 153, 215, 1, 206, 140, 28, 147, 28, 145, 53, 208, 52, 28, 221, 153, 173, 190, 216, 90, 222, 153, 295, 140, 38, 206, 28, 10, 190, 200, 61, 214, 90, 221, 140, 28, 223, 153, 161, 190, 215, 1, 206, 90, 216, 4, 221, 90, 295, 140, 28, 224, 153, 173, 190, 223, 191, 223, 90, 222, 153, 295, 140, 38, 206, 28, 147, 28, 27, 52, 28, 223, 153, 161, 190, 215, 1, 206, 90, 216, 90, 295, 140, 28, 224, 153, 173, 190, 223, 191, 223, 90, 222, 153, 295, 140, 38, 206, 28, 47, 28, 225, 153, 296, 38, 106, 190, 224, 61, 207, 140, 28, 10, 190, 201, 61, 214, 90, 225, 140, 28, 217, 153, 215, 1, 206, 28, 145, 212, 52, 28, 226, 153, 48, 190, 196, 61, 215, 90, 217, 153, 217, 140, 68, 219, 190, 119, 140, 28, 147, 28, 145, 213, 52, 28, 227, 153, 48, 190, 197, 61, 215, 90, 217, 153, 217, 140, 68, 219, 190, 119, 140, 28, 147, 28, 228, 153, 190, 216, 4, 221, 140, 191, 225, 145, 53, 208, 27, 216, 191, 225, 28, 229, 153, 228, 191, 226, 145, 212, 27, 228, 28, 145, 213, 52, 28, 229, 153, 229, 61, 227, 28, 147, 28, 230, 153, 297, 38, 152, 190, 12, 190, 83, 190, 229, 140, 90, 295, 140, 90, 298, 140, 28, 229, 153, 126, 190, 229, 191, 230, 140, 28, 229, 153, 152, 190, 56, 190, 229, 90, 296, 299, 300, 140, 90, 4, 296, 299, 301, 140, 38, 230, 28, 10, 190, 195, 61, 215, 90, 229, 90, 217, 153, 217, 140, 28, 3, 28]}, {"code": "def layer_norm_bwd_kernel(\n    X,\n    W,\n    B,\n    Y,\n    DY,\n    DX,\n    DW,\n    DB,\n    DRESIDUAL,\n    DRESIDUAL_IN,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_y_row,\n    stride_dy_row,\n    stride_dx_row,\n    stride_dres_row,\n    stride_dres_in_row,\n    M,\n    N,\n    eps,\n    rows_per_program,\n    IS_RMS_NORM: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HAS_DRESIDUAL: tl.constexpr,\n    STORE_DRESIDUAL: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n):\n\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    if HAS_WEIGHT:\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n        dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w if HAS_WEIGHT else xhat\n            if HAS_BIAS:\n                y = y + b\n\n            scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), 1e-5)\n\n            y = tl.extra.cuda.libdevice.round(y * scale)\n            y = tl.maximum(tl.minimum(y, 127), -128) / scale\n\n            tl.store(Y + cols, y, mask=mask)\n        wdy = dy\n        if HAS_WEIGHT:\n            wdy = dy * w\n            dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)\n            dx += dres\n\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    if HAS_WEIGHT:\n        tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)", "encoded": [26, 294, 190, 194, 91, 195, 91, 196, 91, 197, 91, 198, 91, 199, 91, 200, 91, 201, 91, 202, 91, 203, 91, 204, 91, 205, 91, 206, 91, 207, 91, 208, 91, 209, 91, 210, 91, 211, 91, 212, 91, 213, 91, 214, 91, 215, 91, 216, 53, 6, 91, 217, 53, 6, 91, 218, 53, 6, 91, 219, 53, 6, 91, 220, 53, 6, 91, 221, 53, 6, 91, 222, 53, 6, 140, 53, 28, -1, 223, 153, 136, 190, 295, 140, 28, 224, 153, 223, 191, 215, 28, 225, 153, 63, 190, 295, 91, 217, 140, 28, 226, 153, 225, 1, 213, 28, 194, 137, 224, 191, 206, 28, 145, 218, 53, 28, 202, 137, 224, 191, 210, 28, 147, 28, 145, 219, 53, 28, 203, 137, 224, 191, 211, 28, 147, 28, 198, 137, 224, 191, 208, 28, 199, 137, 224, 191, 209, 28, 145, 222, 53, 28, 197, 137, 224, 191, 207, 28, 147, 28, 145, 220, 53, 28, 227, 153, 47, 190, 195, 62, 225, 91, 226, 153, 226, 140, 69, 228, 190, 120, 140, 28, 229, 153, 141, 190, 190, 217, 91, 140, 91, 76, 153, 120, 140, 28, 147, 28, 145, 222, 86, 221, 53, 28, 230, 153, 47, 190, 196, 62, 225, 91, 226, 153, 226, 91, 231, 153, 295, 140, 69, 228, 190, 120, 140, 28, 147, 28, 145, 221, 53, 28, 232, 153, 141, 190, 190, 217, 91, 140, 91, 76, 153, 120, 140, 28, 147, 28, 233, 153, 35, 190, 190, 223, 62, 296, 140, 191, 215, 91, 212, 140, 28, 112, 234, 127, 5, 190, 224, 91, 233, 140, 53, 28, 235, 153, 47, 190, 194, 62, 225, 91, 226, 153, 226, 91, 231, 153, 295, 140, 69, 228, 190, 120, 140, 28, 236, 153, 47, 190, 198, 62, 225, 91, 226, 153, 226, 91, 231, 153, 295, 140, 69, 228, 190, 120, 140, 28, 145, 54, 216, 53, 28, 237, 153, 47, 190, 204, 62, 234, 140, 28, 147, 28, 238, 153, 47, 190, 205, 62, 234, 140, 28, 239, 153, 190, 235, 4, 237, 140, 191, 238, 145, 54, 216, 27, 235, 191, 238, 28, 239, 153, 161, 190, 226, 91, 239, 91, 295, 140, 28, 145, 222, 53, 28, 240, 153, 239, 191, 227, 145, 220, 27, 239, 28, 145, 221, 53, 28, 240, 153, 240, 62, 230, 28, 147, 28, 241, 153, 297, 38, 152, 190, 12, 190, 84, 190, 240, 140, 91, 295, 140, 91, 298, 140, 28, 240, 153, 193, 190, 240, 191, 241, 140, 28, 240, 153, 152, 190, 57, 190, 240, 91, 296, 299, 300, 140, 91, 4, 296, 299, 301, 140, 38, 241, 28, 10, 190, 197, 62, 225, 91, 240, 91, 226, 153, 226, 140, 28, 147, 28, 242, 153, 236, 28, 145, 220, 53, 28, 242, 153, 236, 191, 227, 28, 229, 137, 236, 191, 239, 28, 147, 28, 145, 221, 53, 28, 232, 137, 236, 28, 147, 28, 145, 54, 216, 53, 28, 243, 153, 173, 190, 239, 191, 242, 91, 244, 153, 295, 140, 38, 213, 28, 245, 153, 173, 190, 242, 91, 244, 153, 295, 140, 38, 213, 28, 246, 153, 190, 242, 4, 190, 239, 191, 243, 62, 245, 140, 140, 191, 238, 28, 147, 28, 27, 53, 28, 243, 153, 173, 190, 239, 191, 242, 91, 244, 153, 295, 140, 38, 213, 28, 246, 153, 190, 242, 4, 239, 191, 243, 140, 191, 238, 28, 48, 28, 145, 218, 53, 28, 247, 153, 47, 190, 202, 62, 225, 91, 226, 153, 226, 91, 231, 153, 295, 140, 69, 228, 190, 120, 140, 28, 246, 137, 247, 28, 147, 28, 145, 219, 53, 28, 10, 190, 203, 62, 225, 91, 246, 91, 226, 153, 226, 140, 28, 147, 28, 10, 190, 199, 62, 225, 91, 246, 91, 226, 153, 226, 140, 28, 194, 137, 206, 28, 145, 218, 53, 28, 202, 137, 210, 28, 147, 28, 145, 219, 53, 28, 203, 137, 211, 28, 147, 28, 145, 222, 53, 28, 197, 137, 207, 28, 147, 28, 198, 137, 208, 28, 199, 137, 209, 28, 65, 28, 145, 220, 53, 28, 10, 190, 200, 62, 223, 191, 213, 62, 225, 91, 229, 91, 226, 153, 226, 140, 28, 147, 28, 145, 221, 53, 28, 10, 190, 201, 62, 223, 191, 213, 62, 225, 91, 232, 91, 226, 153, 226, 140, 28, 147, 28, 3, 28]}, {"code": "def cross_entropy_fwd_kernel(\n    loss_ptr,\n    lse_ptr,\n    z_loss_ptr,\n    logits_ptr,\n    labels_ptr,\n    label_smoothing,\n    logit_scale,\n    lse_square_scale,\n    ignore_index,\n    total_classes,\n    class_start_idx,\n    n_cols,\n    n_rows,\n    logits_row_stride,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n    SPLIT: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(\n        logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")\n    )\n    logits = logits.to(tl.float32) * logit_scale\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = log(tl.sum(exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignore_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(\n            n_cols, (col_block_idx + 1) * BLOCK_SIZE\n        ):\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (\n                    (lse if not SPLIT else 0.0)\n                    - label_smoothing * sum_logits / total_classes\n                    - (1 - label_smoothing) * logits_label\n                )\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        else:\n\n            if HAS_SMOOTHING:\n                loss = label_smoothing * (\n                    (lse if not SPLIT else 0.0) - sum_logits / total_classes\n                )\n            else:\n                loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)", "encoded": [26, 295, 191, 195, 90, 196, 90, 197, 90, 198, 90, 199, 90, 200, 90, 201, 90, 202, 90, 203, 90, 204, 90, 205, 90, 206, 90, 207, 90, 208, 90, 209, 52, 6, 90, 210, 52, 6, 90, 211, 52, 6, 140, 52, 28, -1, 212, 154, 136, 191, 296, 140, 28, 213, 154, 136, 191, 297, 140, 28, 198, 154, 198, 61, 212, 192, 208, 68, 214, 191, 145, 140, 28, 215, 154, 213, 192, 209, 61, 62, 191, 296, 90, 209, 140, 28, 216, 154, 48, 191, 199, 61, 212, 140, 28, 217, 154, 48, 191, 198, 61, 215, 90, 218, 154, 215, 1, 206, 90, 219, 154, 4, 220, 191, 298, 140, 140, 28, 217, 154, 217, 68, 214, 191, 119, 140, 192, 201, 28, 221, 154, 12, 191, 217, 90, 296, 140, 28, 146, 210, 52, 28, 222, 154, 174, 191, 162, 191, 215, 1, 206, 90, 217, 90, 296, 140, 90, 296, 140, 28, 148, 28, 223, 154, 11, 191, 174, 191, 164, 191, 217, 4, 221, 140, 90, 296, 140, 140, 61, 221, 28, 10, 191, 196, 61, 213, 192, 207, 61, 212, 90, 223, 140, 28, 146, 216, 63, 203, 52, 28, 224, 154, 296, 28, 225, 154, 296, 28, 148, 28, 27, 52, 28, 216, 2, 205, 28, 146, 216, 118, 213, 192, 209, 85, 216, 1, 35, 191, 206, 90, 191, 213, 61, 297, 140, 192, 209, 140, 52, 28, 226, 154, 48, 191, 198, 61, 216, 140, 192, 201, 28, 146, 210, 52, 28, 224, 154, 191, 223, 146, 53, 211, 27, 296, 140, 4, 200, 192, 222, 38, 204, 4, 191, 297, 4, 200, 140, 192, 226, 28, 148, 28, 27, 52, 28, 224, 154, 191, 223, 146, 53, 211, 27, 296, 140, 4, 226, 28, 47, 28, 55, 28, 33, 210, 52, 28, 224, 154, 200, 192, 191, 191, 223, 146, 53, 211, 27, 296, 140, 4, 222, 38, 204, 140, 28, 148, 28, 27, 52, 28, 224, 154, 296, 28, 47, 28, 146, 53, 211, 52, 28, 225, 154, 202, 192, 223, 192, 223, 28, 224, 137, 225, 28, 148, 28, 27, 52, 28, 225, 154, 296, 28, 47, 28, 47, 28, 10, 191, 195, 61, 213, 192, 207, 61, 212, 90, 224, 140, 28, 146, 53, 211, 52, 28, 10, 191, 197, 61, 213, 192, 207, 61, 212, 90, 225, 140, 28, 148, 28, 3, 28]}, {"code": "def cross_entropy_bwd_kernel(\n    dlogits_ptr,\n    dloss_ptr,\n    logits_ptr,\n    lse_ptr,\n    labels_ptr,\n    label_smoothing,\n    logit_scale,\n    lse_square_scale,\n    ignore_index,\n    total_classes,\n    class_start_idx,\n    n_cols,\n    logits_row_stride,\n    dlogits_row_stride,\n    dloss_row_stride,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignore_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = (\n        tl.load(\n            logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")\n        ).to(tl.float32)\n        * logit_scale\n    )\n    lse = tl.load(lse_ptr + row_idx)\n    probs = exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_negative = label_smoothing / total_classes\n        probs = (\n            tl.where(col_offsets == label_idx, probs - (1 - label_smoothing), probs)\n            - smooth_negative\n        )\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(\n        dlogits_ptr + col_offsets,\n        (dloss * logit_scale) * probs,\n        mask=col_offsets < n_cols,\n    )", "encoded": [26, 295, 191, 195, 91, 196, 91, 197, 91, 198, 91, 199, 91, 200, 91, 201, 91, 202, 91, 203, 91, 204, 91, 205, 91, 206, 91, 207, 91, 208, 91, 209, 91, 210, 53, 6, 91, 211, 53, 6, 140, 53, 28, -1, 212, 154, 136, 191, 296, 140, 28, 213, 154, 136, 191, 297, 140, 28, 197, 154, 197, 62, 212, 192, 207, 69, 214, 191, 145, 140, 28, 195, 154, 195, 62, 212, 192, 208, 69, 214, 191, 145, 140, 28, 215, 154, 213, 192, 210, 62, 63, 191, 296, 91, 210, 140, 28, 216, 154, 47, 191, 199, 62, 212, 140, 28, 146, 216, 149, 203, 53, 28, 217, 154, 47, 191, 196, 62, 212, 192, 209, 140, 28, 148, 28, 27, 53, 28, 217, 154, 296, 28, 48, 28, 218, 154, 47, 191, 197, 62, 215, 91, 219, 154, 215, 1, 206, 91, 220, 154, 4, 221, 191, 298, 140, 140, 69, 214, 191, 120, 140, 192, 201, 28, 222, 154, 47, 191, 198, 62, 212, 140, 28, 223, 154, 164, 191, 218, 4, 222, 140, 28, 223, 137, 299, 192, 202, 192, 222, 192, 223, 28, 216, 2, 205, 28, 146, 211, 53, 28, 224, 154, 200, 38, 204, 28, 223, 154, 162, 191, 215, 64, 216, 91, 223, 4, 191, 297, 4, 200, 140, 91, 223, 140, 4, 224, 28, 148, 28, 27, 53, 28, 223, 154, 162, 191, 215, 64, 216, 91, 223, 4, 297, 91, 223, 140, 28, 48, 28, 10, 191, 195, 62, 215, 91, 217, 192, 201, 192, 223, 91, 219, 154, 215, 1, 206, 140, 28, 3, 28]}, {"code": "def layer_norm_gated_fwd_kernel(\n    x,\n    g,\n    y,\n    w,\n    b,\n    residual,\n    residual_out,\n    mean,\n    rstd,\n    eps,\n    T,\n    D: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    NB: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n    STORE_RESIDUAL_OUT: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n):\n    i_t = tl.program_id(0)\n\n    o_d = tl.arange(0, BD)\n    m_d = o_d < D\n\n    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\n    if HAS_RESIDUAL:\n        p_res = tl.make_block_ptr(\n            residual, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0)\n        )\n        b_x += tl.load(p_res, boundary_check=(0, 1)).to(tl.float32)\n    if STORE_RESIDUAL_OUT:\n        p_res_out = tl.make_block_ptr(\n            residual_out, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0)\n        )\n        tl.store(p_res_out, b_x.to(p_res_out.dtype.element_ty), boundary_check=(0, 1))\n    if not IS_RMS_NORM:\n        b_mean = tl.sum(b_x, axis=1) / D\n        p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        tl.store(p_mean, b_mean.to(p_mean.dtype.element_ty), boundary_check=(0,))\n        b_xbar = tl.where(m_d[None, :], b_x - b_mean[:, None], 0.0)\n        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D\n    else:\n        b_xbar = tl.where(m_d[None, :], b_x, 0.0)\n        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D\n    b_rstd = 1 / tl.sqrt(b_var + eps)\n\n    p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_rstd, b_rstd.to(p_rstd.dtype.element_ty), boundary_check=(0,))\n\n    if HAS_WEIGHT:\n        b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)\n    if HAS_BIAS:\n        b_b = tl.load(b + o_d, mask=m_d).to(tl.float32)\n    b_x_hat = (\n        (b_x - b_mean[:, None]) * b_rstd[:, None]\n        if not IS_RMS_NORM\n        else b_x * b_rstd[:, None]\n    )\n    b_y = b_x_hat * b_w[None, :] if HAS_WEIGHT else b_x_hat\n    if HAS_BIAS:\n        b_y = b_y + b_b[None, :]\n\n    p_g = tl.make_block_ptr(g, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)\n    if ACTIVATION == \"swish\":\n        b_y = b_y * b_g * tl.sigmoid(b_g)\n    elif ACTIVATION == \"silu\":\n        b_y = b_y * b_g * tl.sigmoid(b_g)\n    elif ACTIVATION == \"sigmoid\":\n        b_y = b_y * tl.sigmoid(b_g)\n\n    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))", "encoded": [26, 295, 191, 195, 90, 196, 90, 197, 90, 198, 90, 199, 90, 200, 90, 201, 90, 202, 90, 203, 90, 204, 90, 205, 90, 206, 52, 6, 90, 207, 52, 6, 90, 208, 52, 6, 90, 209, 52, 6, 90, 210, 52, 6, 90, 211, 52, 6, 90, 212, 52, 6, 90, 213, 52, 6, 90, 214, 52, 6, 90, 215, 52, 6, 140, 52, 28, -1, 216, 154, 136, 191, 296, 140, 28, 217, 154, 62, 191, 296, 90, 208, 140, 28, 218, 154, 217, 1, 206, 28, 219, 154, 173, 191, 195, 90, 191, 205, 90, 206, 140, 90, 191, 206, 90, 297, 140, 90, 191, 216, 192, 207, 90, 296, 140, 90, 191, 207, 90, 208, 140, 90, 191, 297, 90, 296, 140, 140, 28, 220, 154, 48, 191, 219, 90, 221, 154, 191, 296, 90, 297, 140, 140, 68, 222, 191, 119, 140, 28, 146, 213, 52, 28, 223, 154, 173, 191, 200, 90, 191, 205, 90, 206, 140, 90, 191, 206, 90, 297, 140, 90, 191, 216, 192, 207, 90, 296, 140, 90, 191, 207, 90, 208, 140, 90, 191, 297, 90, 296, 140, 140, 28, 220, 137, 48, 191, 223, 90, 221, 154, 191, 296, 90, 297, 140, 140, 68, 222, 191, 119, 140, 28, 148, 28, 146, 212, 52, 28, 224, 154, 173, 191, 201, 90, 191, 205, 90, 206, 140, 90, 191, 206, 90, 297, 140, 90, 191, 216, 192, 207, 90, 296, 140, 90, 191, 207, 90, 208, 140, 90, 191, 297, 90, 296, 140, 140, 28, 10, 191, 224, 90, 220, 68, 222, 191, 224, 68, 75, 68, 95, 140, 90, 221, 154, 191, 296, 90, 297, 140, 140, 28, 148, 28, 146, 53, 211, 52, 28, 225, 154, 174, 191, 220, 90, 226, 154, 297, 140, 38, 206, 28, 227, 154, 173, 191, 202, 90, 191, 205, 90, 140, 90, 191, 297, 90, 140, 90, 191, 216, 192, 207, 90, 140, 90, 191, 207, 90, 140, 90, 191, 296, 90, 140, 140, 28, 10, 191, 227, 90, 225, 68, 222, 191, 227, 68, 75, 68, 95, 140, 90, 221, 154, 191, 296, 90, 140, 140, 28, 228, 154, 162, 191, 218, 180, 159, 90, 52, 24, 90, 220, 4, 225, 180, 52, 90, 159, 24, 90, 296, 140, 28, 229, 154, 174, 191, 228, 192, 228, 90, 226, 154, 297, 140, 38, 206, 28, 148, 28, 27, 52, 28, 228, 154, 162, 191, 218, 180, 159, 90, 52, 24, 90, 220, 90, 296, 140, 28, 229, 154, 174, 191, 228, 192, 228, 90, 226, 154, 297, 140, 38, 206, 28, 47, 28, 230, 154, 297, 38, 106, 191, 229, 61, 204, 140, 28, 231, 154, 173, 191, 203, 90, 191, 205, 90, 140, 90, 191, 297, 90, 140, 90, 191, 216, 192, 207, 90, 140, 90, 191, 207, 90, 140, 90, 191, 296, 90, 140, 140, 28, 10, 191, 231, 90, 230, 68, 222, 191, 231, 68, 75, 68, 95, 140, 90, 221, 154, 191, 296, 90, 140, 140, 28, 146, 214, 52, 28, 232, 154, 48, 191, 198, 61, 217, 90, 233, 154, 218, 140, 68, 222, 191, 119, 140, 28, 148, 28, 146, 215, 52, 28, 234, 154, 48, 191, 199, 61, 217, 90, 233, 154, 218, 140, 68, 222, 191, 119, 140, 28, 148, 28, 235, 154, 191, 220, 4, 225, 180, 52, 90, 159, 24, 140, 192, 230, 180, 52, 90, 159, 24, 146, 53, 211, 27, 220, 192, 230, 180, 52, 90, 159, 24, 28, 236, 154, 235, 192, 232, 180, 159, 90, 52, 24, 146, 214, 27, 235, 28, 146, 215, 52, 28, 236, 154, 236, 61, 234, 180, 159, 90, 52, 24, 28, 148, 28, 237, 154, 173, 191, 196, 90, 191, 205, 90, 206, 140, 90, 191, 206, 90, 297, 140, 90, 191, 216, 192, 207, 90, 296, 140, 90, 191, 207, 90, 208, 140, 90, 191, 297, 90, 296, 140, 140, 28, 238, 154, 48, 191, 237, 90, 221, 154, 191, 296, 90, 297, 140, 140, 68, 222, 191, 119, 140, 28, 146, 210, 63, 298, 52, 28, 236, 154, 236, 192, 238, 192, 163, 191, 238, 140, 28, 55, 28, 33, 210, 63, 299, 52, 28, 236, 154, 236, 192, 238, 192, 163, 191, 238, 140, 28, 55, 28, 33, 210, 63, 300, 52, 28, 236, 154, 236, 192, 163, 191, 238, 140, 28, 148, 28, 239, 154, 173, 191, 197, 90, 191, 205, 90, 206, 140, 90, 191, 206, 90, 297, 140, 90, 191, 216, 192, 207, 90, 296, 140, 90, 191, 207, 90, 208, 140, 90, 191, 297, 90, 296, 140, 140, 28, 10, 191, 239, 90, 236, 68, 222, 191, 239, 68, 75, 68, 95, 140, 90, 221, 154, 191, 296, 90, 297, 140, 140, 28, 3, 28]}, {"code": "def layer_norm_gated_fwd_kernel1(\n    x,\n    g,\n    y,\n    w,\n    b,\n    residual,\n    residual_out,\n    mean,\n    rstd,\n    eps,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n    STORE_RESIDUAL_OUT: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n):\n    i_t = tl.program_id(0)\n    x += i_t * D\n    y += i_t * D\n    g += i_t * D\n    if HAS_RESIDUAL:\n        residual += i_t * D\n    if STORE_RESIDUAL_OUT:\n        residual_out += i_t * D\n\n    o_d = tl.arange(0, BD)\n    m_d = o_d < D\n    b_x = tl.load(x + o_d, mask=m_d, other=0.0).to(tl.float32)\n    if HAS_RESIDUAL:\n        b_x += tl.load(residual + o_d, mask=m_d, other=0.0).to(tl.float32)\n    if STORE_RESIDUAL_OUT:\n        tl.store(residual_out + o_d, b_x, mask=m_d)\n    if not IS_RMS_NORM:\n        b_mean = tl.sum(b_x, axis=0) / D\n        tl.store(mean + i_t, b_mean)\n        b_xbar = tl.where(m_d, b_x - b_mean, 0.0)\n        b_var = tl.sum(b_xbar * b_xbar, axis=0) / D\n    else:\n        b_xbar = tl.where(m_d, b_x, 0.0)\n        b_var = tl.sum(b_xbar * b_xbar, axis=0) / D\n    b_rstd = 1 / tl.sqrt(b_var + eps)\n    tl.store(rstd + i_t, b_rstd)\n\n    if HAS_WEIGHT:\n        b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)\n    if HAS_BIAS:\n        b_b = tl.load(b + o_d, mask=m_d).to(tl.float32)\n    b_x_hat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd\n    b_y = b_x_hat * b_w if HAS_WEIGHT else b_x_hat\n    if HAS_BIAS:\n        b_y = b_y + b_b\n\n    b_g = tl.load(g + o_d, mask=m_d, other=0.0).to(tl.float32)\n    if ACTIVATION == \"swish\":\n        b_y = b_y * b_g * tl.sigmoid(b_g)\n    elif ACTIVATION == \"silu\":\n        b_y = b_y * b_g * tl.sigmoid(b_g)\n    elif ACTIVATION == \"sigmoid\":\n        b_y = b_y * tl.sigmoid(b_g)\n\n    tl.store(y + o_d, b_y, mask=m_d)", "encoded": [26, 295, 191, 195, 91, 196, 91, 197, 91, 198, 91, 199, 91, 200, 91, 201, 91, 202, 91, 203, 91, 204, 91, 205, 53, 6, 91, 206, 53, 6, 91, 207, 53, 6, 91, 208, 53, 6, 91, 209, 53, 6, 91, 210, 53, 6, 91, 211, 53, 6, 91, 212, 53, 6, 140, 53, 28, -1, 213, 154, 136, 191, 296, 140, 28, 195, 137, 213, 192, 205, 28, 197, 137, 213, 192, 205, 28, 196, 137, 213, 192, 205, 28, 146, 210, 53, 28, 200, 137, 213, 192, 205, 28, 148, 28, 146, 209, 53, 28, 201, 137, 213, 192, 205, 28, 148, 28, 214, 154, 63, 191, 296, 91, 206, 140, 28, 215, 154, 214, 1, 205, 28, 216, 154, 47, 191, 195, 62, 214, 91, 217, 154, 215, 91, 218, 154, 296, 140, 69, 219, 191, 120, 140, 28, 146, 210, 53, 28, 216, 137, 47, 191, 200, 62, 214, 91, 217, 154, 215, 91, 218, 154, 296, 140, 69, 219, 191, 120, 140, 28, 148, 28, 146, 209, 53, 28, 10, 191, 201, 62, 214, 91, 216, 91, 217, 154, 215, 140, 28, 148, 28, 146, 54, 208, 53, 28, 220, 154, 174, 191, 216, 91, 221, 154, 296, 140, 38, 205, 28, 10, 191, 202, 62, 213, 91, 220, 140, 28, 222, 154, 162, 191, 215, 91, 216, 4, 220, 91, 296, 140, 28, 223, 154, 174, 191, 222, 192, 222, 91, 221, 154, 296, 140, 38, 205, 28, 148, 28, 27, 53, 28, 222, 154, 162, 191, 215, 91, 216, 91, 296, 140, 28, 223, 154, 174, 191, 222, 192, 222, 91, 221, 154, 296, 140, 38, 205, 28, 48, 28, 224, 154, 297, 38, 107, 191, 223, 62, 204, 140, 28, 10, 191, 203, 62, 213, 91, 224, 140, 28, 146, 211, 53, 28, 225, 154, 47, 191, 198, 62, 214, 91, 217, 154, 215, 140, 69, 219, 191, 120, 140, 28, 148, 28, 146, 212, 53, 28, 226, 154, 47, 191, 199, 62, 214, 91, 217, 154, 215, 140, 69, 219, 191, 120, 140, 28, 148, 28, 227, 154, 191, 216, 4, 220, 140, 192, 224, 146, 54, 208, 27, 216, 192, 224, 28, 228, 154, 227, 192, 225, 146, 211, 27, 227, 28, 146, 212, 53, 28, 228, 154, 228, 62, 226, 28, 148, 28, 229, 154, 47, 191, 196, 62, 214, 91, 217, 154, 215, 91, 218, 154, 296, 140, 69, 219, 191, 120, 140, 28, 146, 207, 64, 298, 53, 28, 228, 154, 228, 192, 229, 192, 163, 191, 229, 140, 28, 56, 28, 33, 207, 64, 299, 53, 28, 228, 154, 228, 192, 229, 192, 163, 191, 229, 140, 28, 56, 28, 33, 207, 64, 300, 53, 28, 228, 154, 228, 192, 163, 191, 229, 140, 28, 148, 28, 10, 191, 197, 62, 214, 91, 228, 91, 217, 154, 215, 140, 28, 3, 28]}, {"code": "def layer_norm_gated_bwd_kernel(\n    x,\n    g,\n    w,\n    b,\n    y,\n    dy,\n    dx,\n    dg,\n    dw,\n    db,\n    dresidual,\n    dresidual_in,\n    mean,\n    rstd,\n    T,\n    D: tl.constexpr,\n    BS: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    NB: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n    STORE_DRESIDUAL: tl.constexpr,\n    HAS_DRESIDUAL: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n):\n    i_s = tl.program_id(0)\n    o_d = tl.arange(0, BD)\n    m_d = o_d < D\n    if HAS_WEIGHT:\n        b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)\n        b_dw = tl.zeros((BT, BD), dtype=tl.float32)\n    if HAS_BIAS:\n        b_b = tl.load(b + o_d, mask=m_d, other=0.0).to(tl.float32)\n        b_db = tl.zeros((BT, BD), dtype=tl.float32)\n\n    T = min(i_s * BS + BS, T)\n    for i_t in range(i_s * BS, T, BT):\n        p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n        p_g = tl.make_block_ptr(g, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n        p_dy = tl.make_block_ptr(dy, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n        p_dx = tl.make_block_ptr(dx, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n        p_dg = tl.make_block_ptr(dg, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n\n        b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\n        b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)\n        b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)\n\n        if not IS_RMS_NORM:\n            p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t,), (BT,), (0,))\n            b_mean = tl.load(p_mean, boundary_check=(0,))\n        p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t,), (BT,), (0,))\n        b_rstd = tl.load(p_rstd, boundary_check=(0,))\n\n        b_xhat = (\n            (b_x - b_mean[:, None]) * b_rstd[:, None]\n            if not IS_RMS_NORM\n            else b_x * b_rstd[:, None]\n        )\n        b_xhat = tl.where(m_d[None, :], b_xhat, 0.0)\n\n        b_y = b_xhat * b_w[None, :] if HAS_WEIGHT else b_xhat\n        if HAS_BIAS:\n            b_y = b_y + b_b[None, :]\n        if RECOMPUTE_OUTPUT:\n            p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n            tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))\n\n        b_sigmoid_g = tl.sigmoid(b_g)\n        if ACTIVATION == \"swish\":\n            b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))\n            b_dy = b_dy * b_g * b_sigmoid_g\n        elif ACTIVATION == \"silu\":\n            b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))\n            b_dy = b_dy * b_g * b_sigmoid_g\n        elif ACTIVATION == \"sigmoid\":\n            b_dg = b_dy * b_y * b_sigmoid_g * (1 - b_sigmoid_g)\n            b_dy = b_dy * b_sigmoid_g\n        b_wdy = b_dy\n\n        if HAS_WEIGHT or HAS_BIAS:\n            m_t = (i_t + tl.arange(0, BT)) < T\n        if HAS_WEIGHT:\n            b_wdy = b_dy * b_w\n            b_dw += tl.where(m_t[:, None], b_dy * b_xhat, 0.0)\n        if HAS_BIAS:\n            b_db += tl.where(m_t[:, None], b_dy, 0.0)\n        if not IS_RMS_NORM:\n            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D\n            b_c2 = tl.sum(b_wdy, axis=1) / D\n            b_dx = (b_wdy - (b_xhat * b_c1[:, None] + b_c2[:, None])) * b_rstd[:, None]\n        else:\n            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D\n            b_dx = (b_wdy - b_xhat * b_c1[:, None]) * b_rstd[:, None]\n        if HAS_DRESIDUAL:\n            p_dres = tl.make_block_ptr(\n                dresidual, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0)\n            )\n            b_dres = tl.load(p_dres, boundary_check=(0, 1)).to(tl.float32)\n            b_dx += b_dres\n\n        if STORE_DRESIDUAL:\n            p_dres_in = tl.make_block_ptr(\n                dresidual_in, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0)\n            )\n            tl.store(\n                p_dres_in, b_dx.to(p_dres_in.dtype.element_ty), boundary_check=(0, 1)\n            )\n\n        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))\n\n    if HAS_WEIGHT:\n        tl.store(dw + i_s * D + o_d, tl.sum(b_dw, axis=0), mask=m_d)\n    if HAS_BIAS:\n        tl.store(db + i_s * D + o_d, tl.sum(b_db, axis=0), mask=m_d)", "encoded": [26, 295, 191, 195, 90, 196, 90, 197, 90, 198, 90, 199, 90, 200, 90, 201, 90, 202, 90, 203, 90, 204, 90, 205, 90, 206, 90, 207, 90, 208, 90, 209, 90, 210, 52, 6, 90, 211, 52, 6, 90, 212, 52, 6, 90, 213, 52, 6, 90, 214, 52, 6, 90, 215, 52, 6, 90, 216, 52, 6, 90, 217, 52, 6, 90, 218, 52, 6, 90, 219, 52, 6, 90, 220, 52, 6, 90, 221, 52, 6, 140, 52, 28, -1, 222, 154, 136, 191, 296, 140, 28, 223, 154, 62, 191, 296, 90, 213, 140, 28, 224, 154, 223, 1, 210, 28, 146, 219, 52, 28, 225, 154, 48, 191, 197, 61, 223, 90, 226, 154, 224, 140, 68, 227, 191, 119, 140, 28, 228, 154, 141, 191, 191, 212, 90, 213, 140, 90, 75, 154, 119, 140, 28, 148, 28, 146, 220, 52, 28, 229, 154, 48, 191, 198, 61, 223, 90, 226, 154, 224, 90, 230, 154, 296, 140, 68, 227, 191, 119, 140, 28, 231, 154, 141, 191, 191, 212, 90, 213, 140, 90, 75, 154, 119, 140, 28, 148, 28, 209, 154, 35, 191, 222, 192, 211, 61, 211, 90, 209, 140, 28, 111, 232, 127, 5, 191, 222, 192, 211, 90, 209, 90, 212, 140, 52, 28, 233, 154, 173, 191, 195, 90, 191, 209, 90, 210, 140, 90, 191, 210, 90, 297, 140, 90, 191, 232, 90, 296, 140, 90, 191, 212, 90, 213, 140, 90, 191, 297, 90, 296, 140, 140, 28, 234, 154, 173, 191, 196, 90, 191, 209, 90, 210, 140, 90, 191, 210, 90, 297, 140, 90, 191, 232, 90, 296, 140, 90, 191, 212, 90, 213, 140, 90, 191, 297, 90, 296, 140, 140, 28, 235, 154, 173, 191, 200, 90, 191, 209, 90, 210, 140, 90, 191, 210, 90, 297, 140, 90, 191, 232, 90, 296, 140, 90, 191, 212, 90, 213, 140, 90, 191, 297, 90, 296, 140, 140, 28, 236, 154, 173, 191, 201, 90, 191, 209, 90, 210, 140, 90, 191, 210, 90, 297, 140, 90, 191, 232, 90, 296, 140, 90, 191, 212, 90, 213, 140, 90, 191, 297, 90, 296, 140, 140, 28, 237, 154, 173, 191, 202, 90, 191, 209, 90, 210, 140, 90, 191, 210, 90, 297, 140, 90, 191, 232, 90, 296, 140, 90, 191, 212, 90, 213, 140, 90, 191, 297, 90, 296, 140, 140, 28, 238, 154, 48, 191, 233, 90, 239, 154, 191, 296, 90, 297, 140, 140, 68, 227, 191, 119, 140, 28, 240, 154, 48, 191, 234, 90, 239, 154, 191, 296, 90, 297, 140, 140, 68, 227, 191, 119, 140, 28, 241, 154, 48, 191, 235, 90, 239, 154, 191, 296, 90, 297, 140, 140, 68, 227, 191, 119, 140, 28, 146, 53, 216, 52, 28, 242, 154, 173, 191, 207, 90, 191, 209, 90, 140, 90, 191, 297, 90, 140, 90, 191, 232, 90, 140, 90, 191, 212, 90, 140, 90, 191, 296, 90, 140, 140, 28, 243, 154, 48, 191, 242, 90, 239, 154, 191, 296, 90, 140, 140, 28, 148, 28, 244, 154, 173, 191, 208, 90, 191, 209, 90, 140, 90, 191, 297, 90, 140, 90, 191, 232, 90, 140, 90, 191, 212, 90, 140, 90, 191, 296, 90, 140, 140, 28, 245, 154, 48, 191, 244, 90, 239, 154, 191, 296, 90, 140, 140, 28, 246, 154, 191, 238, 4, 243, 180, 52, 90, 159, 24, 140, 192, 245, 180, 52, 90, 159, 24, 146, 53, 216, 27, 238, 192, 245, 180, 52, 90, 159, 24, 28, 246, 154, 162, 191, 224, 180, 159, 90, 52, 24, 90, 246, 90, 296, 140, 28, 247, 154, 246, 192, 225, 180, 159, 90, 52, 24, 146, 219, 27, 246, 28, 146, 220, 52, 28, 247, 154, 247, 61, 229, 180, 159, 90, 52, 24, 28, 148, 28, 146, 221, 52, 28, 248, 154, 173, 191, 199, 90, 191, 209, 90, 210, 140, 90, 191, 210, 90, 297, 140, 90, 191, 232, 90, 296, 140, 90, 191, 212, 90, 213, 140, 90, 191, 297, 90, 296, 140, 140, 28, 10, 191, 248, 90, 247, 68, 227, 191, 248, 68, 75, 68, 95, 140, 90, 239, 154, 191, 296, 90, 297, 140, 140, 28, 148, 28, 249, 154, 163, 191, 240, 140, 28, 146, 215, 63, 298, 52, 28, 250, 154, 241, 192, 247, 192, 191, 249, 61, 240, 192, 249, 192, 191, 297, 4, 249, 140, 140, 28, 241, 154, 241, 192, 240, 192, 249, 28, 55, 28, 33, 215, 63, 299, 52, 28, 250, 154, 241, 192, 247, 192, 191, 249, 61, 240, 192, 249, 192, 191, 297, 4, 249, 140, 140, 28, 241, 154, 241, 192, 240, 192, 249, 28, 55, 28, 33, 215, 63, 300, 52, 28, 250, 154, 241, 192, 247, 192, 249, 192, 191, 297, 4, 249, 140, 28, 241, 154, 241, 192, 249, 28, 148, 28, 251, 154, 241, 28, 146, 219, 116, 220, 52, 28, 252, 154, 232, 61, 62, 191, 296, 90, 212, 140, 1, 209, 28, 148, 28, 146, 219, 52, 28, 251, 154, 241, 192, 225, 28, 228, 137, 162, 191, 252, 180, 52, 90, 159, 24, 90, 241, 192, 246, 90, 296, 140, 28, 148, 28, 146, 220, 52, 28, 231, 137, 162, 191, 252, 180, 52, 90, 159, 24, 90, 241, 90, 296, 140, 28, 148, 28, 146, 53, 216, 52, 28, 253, 154, 174, 191, 246, 192, 251, 90, 254, 154, 297, 140, 38, 210, 28, 255, 154, 174, 191, 251, 90, 254, 154, 297, 140, 38, 210, 28, 256, 154, 191, 251, 4, 191, 246, 192, 253, 180, 52, 90, 159, 24, 61, 255, 180, 52, 90, 159, 24, 140, 140, 192, 245, 180, 52, 90, 159, 24, 28, 148, 28, 27, 52, 28, 253, 154, 174, 191, 246, 192, 251, 90, 254, 154, 297, 140, 38, 210, 28, 256, 154, 191, 251, 4, 246, 192, 253, 180, 52, 90, 159, 24, 140, 192, 245, 180, 52, 90, 159, 24, 28, 47, 28, 146, 218, 52, 28, 257, 154, 173, 191, 205, 90, 191, 209, 90, 210, 140, 90, 191, 210, 90, 297, 140, 90, 191, 232, 90, 296, 140, 90, 191, 212, 90, 213, 140, 90, 191, 297, 90, 296, 140, 140, 28, 258, 154, 48, 191, 257, 90, 239, 154, 191, 296, 90, 297, 140, 140, 68, 227, 191, 119, 140, 28, 256, 137, 258, 28, 148, 28, 146, 217, 52, 28, 259, 154, 173, 191, 206, 90, 191, 209, 90, 210, 140, 90, 191, 210, 90, 297, 140, 90, 191, 232, 90, 296, 140, 90, 191, 212, 90, 213, 140, 90, 191, 297, 90, 296, 140, 140, 28, 10, 191, 259, 90, 256, 68, 227, 191, 259, 68, 75, 68, 95, 140, 90, 239, 154, 191, 296, 90, 297, 140, 140, 28, 148, 28, 10, 191, 236, 90, 256, 68, 227, 191, 236, 68, 75, 68, 95, 140, 90, 239, 154, 191, 296, 90, 297, 140, 140, 28, 10, 191, 237, 90, 250, 68, 227, 191, 237, 68, 75, 68, 95, 140, 90, 239, 154, 191, 296, 90, 297, 140, 140, 28, 64, 28, 146, 219, 52, 28, 10, 191, 203, 61, 222, 192, 210, 61, 223, 90, 174, 191, 228, 90, 254, 154, 296, 140, 90, 226, 154, 224, 140, 28, 148, 28, 146, 220, 52, 28, 10, 191, 204, 61, 222, 192, 210, 61, 223, 90, 174, 191, 231, 90, 254, 154, 296, 140, 90, 226, 154, 224, 140, 28, 148, 28, 3, 28]}, {"code": "def layer_norm_gated_bwd_kernel1(\n    x,\n    g,\n    w,\n    b,\n    y,\n    dy,\n    dx,\n    dg,\n    dw,\n    db,\n    dresidual,\n    dresidual_in,\n    mean,\n    rstd,\n    T,\n    D: tl.constexpr,\n    BS: tl.constexpr,\n    BD: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n    STORE_DRESIDUAL: tl.constexpr,\n    HAS_DRESIDUAL: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n):\n    i_s = tl.program_id(0)\n    o_d = tl.arange(0, BD)\n    mask = o_d < D\n    x += i_s * BS * D\n    g += i_s * BS * D\n    if HAS_DRESIDUAL:\n        dresidual += i_s * BS * D\n    if STORE_DRESIDUAL:\n        dresidual_in += i_s * BS * D\n    dy += i_s * BS * D\n    dx += i_s * BS * D\n    dg += i_s * BS * D\n    if RECOMPUTE_OUTPUT:\n        y += i_s * BS * D\n    if HAS_WEIGHT:\n        b_w = tl.load(w + o_d, mask=mask).to(tl.float32)\n        b_dw = tl.zeros((BD,), dtype=tl.float32)\n    if HAS_BIAS:\n        b_b = tl.load(b + o_d, mask=mask, other=0.0).to(tl.float32)\n        b_db = tl.zeros((BD,), dtype=tl.float32)\n\n    for i_t in range(i_s * BS, min(i_s * BS + BS, T)):\n\n        b_x = tl.load(x + o_d, mask=mask, other=0).to(tl.float32)\n        b_g = tl.load(g + o_d, mask=mask, other=0).to(tl.float32)\n        b_dy = tl.load(dy + o_d, mask=mask, other=0).to(tl.float32)\n\n        if not IS_RMS_NORM:\n            b_mean = tl.load(mean + i_t)\n        b_rstd = tl.load(rstd + i_t)\n\n        b_xhat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd\n        b_xhat = tl.where(mask, b_xhat, 0.0)\n\n        b_y = b_xhat * b_w if HAS_WEIGHT else b_xhat\n        if HAS_BIAS:\n            b_y = b_y + b_b\n        if RECOMPUTE_OUTPUT:\n            tl.store(y + o_d, b_y, mask=mask)\n\n        b_sigmoid_g = tl.sigmoid(b_g)\n        if ACTIVATION == \"swish\":\n            b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))\n            b_dy = b_dy * b_g * b_sigmoid_g\n        elif ACTIVATION == \"silu\":\n            b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))\n            b_dy = b_dy * b_g * b_sigmoid_g\n        elif ACTIVATION == \"sigmoid\":\n            b_dg = b_dy * b_y * b_sigmoid_g * (1 - b_sigmoid_g)\n            b_dy = b_dy * b_sigmoid_g\n        b_wdy = b_dy\n        if HAS_WEIGHT:\n            b_wdy = b_dy * b_w\n            b_dw += b_dy * b_xhat\n        if HAS_BIAS:\n            b_db += b_dy\n        if not IS_RMS_NORM:\n            b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D\n            b_c2 = tl.sum(b_wdy, axis=0) / D\n            b_dx = (b_wdy - (b_xhat * b_c1 + b_c2)) * b_rstd\n        else:\n            b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D\n            b_dx = (b_wdy - b_xhat * b_c1) * b_rstd\n        if HAS_DRESIDUAL:\n            b_dres = tl.load(dresidual + o_d, mask=mask, other=0).to(tl.float32)\n            b_dx += b_dres\n\n        if STORE_DRESIDUAL:\n            tl.store(dresidual_in + o_d, b_dx, mask=mask)\n        tl.store(dx + o_d, b_dx, mask=mask)\n        tl.store(dg + o_d, b_dg, mask=mask)\n\n        x += D\n        g += D\n        if HAS_DRESIDUAL:\n            dresidual += D\n        if STORE_DRESIDUAL:\n            dresidual_in += D\n        if RECOMPUTE_OUTPUT:\n            y += D\n        dy += D\n        dx += D\n        dg += D\n    if HAS_WEIGHT:\n        tl.store(dw + i_s * D + o_d, b_dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(db + i_s * D + o_d, b_db, mask=mask)", "encoded": [26, 295, 191, 195, 91, 196, 91, 197, 91, 198, 91, 199, 91, 200, 91, 201, 91, 202, 91, 203, 91, 204, 91, 205, 91, 206, 91, 207, 91, 208, 91, 209, 91, 210, 53, 6, 91, 211, 53, 6, 91, 212, 53, 6, 91, 213, 53, 6, 91, 214, 53, 6, 91, 215, 53, 6, 91, 216, 53, 6, 91, 217, 53, 6, 91, 218, 53, 6, 91, 219, 53, 6, 140, 53, 28, -1, 220, 154, 136, 191, 296, 140, 28, 221, 154, 63, 191, 296, 91, 212, 140, 28, 222, 154, 221, 1, 210, 28, 195, 137, 220, 192, 211, 192, 210, 28, 196, 137, 220, 192, 211, 192, 210, 28, 146, 216, 53, 28, 205, 137, 220, 192, 211, 192, 210, 28, 148, 28, 146, 215, 53, 28, 206, 137, 220, 192, 211, 192, 210, 28, 148, 28, 200, 137, 220, 192, 211, 192, 210, 28, 201, 137, 220, 192, 211, 192, 210, 28, 202, 137, 220, 192, 211, 192, 210, 28, 146, 219, 53, 28, 199, 137, 220, 192, 211, 192, 210, 28, 148, 28, 146, 217, 53, 28, 223, 154, 47, 191, 197, 62, 221, 91, 222, 154, 222, 140, 69, 224, 191, 120, 140, 28, 225, 154, 141, 191, 191, 212, 91, 140, 91, 76, 154, 120, 140, 28, 148, 28, 146, 218, 53, 28, 226, 154, 47, 191, 198, 62, 221, 91, 222, 154, 222, 91, 227, 154, 296, 140, 69, 224, 191, 120, 140, 28, 228, 154, 141, 191, 191, 212, 91, 140, 91, 76, 154, 120, 140, 28, 148, 28, 112, 229, 127, 5, 191, 220, 192, 211, 91, 35, 191, 220, 192, 211, 62, 211, 91, 209, 140, 140, 53, 28, 230, 154, 47, 191, 195, 62, 221, 91, 222, 154, 222, 91, 227, 154, 296, 140, 69, 224, 191, 120, 140, 28, 231, 154, 47, 191, 196, 62, 221, 91, 222, 154, 222, 91, 227, 154, 296, 140, 69, 224, 191, 120, 140, 28, 232, 154, 47, 191, 200, 62, 221, 91, 222, 154, 222, 91, 227, 154, 296, 140, 69, 224, 191, 120, 140, 28, 146, 54, 214, 53, 28, 233, 154, 47, 191, 207, 62, 229, 140, 28, 148, 28, 234, 154, 47, 191, 208, 62, 229, 140, 28, 235, 154, 191, 230, 4, 233, 140, 192, 234, 146, 54, 214, 27, 230, 192, 234, 28, 235, 154, 162, 191, 222, 91, 235, 91, 296, 140, 28, 236, 154, 235, 192, 223, 146, 217, 27, 235, 28, 146, 218, 53, 28, 236, 154, 236, 62, 226, 28, 148, 28, 146, 219, 53, 28, 10, 191, 199, 62, 221, 91, 236, 91, 222, 154, 222, 140, 28, 148, 28, 237, 154, 163, 191, 231, 140, 28, 146, 213, 64, 297, 53, 28, 238, 154, 232, 192, 236, 192, 191, 237, 62, 231, 192, 237, 192, 191, 298, 4, 237, 140, 140, 28, 232, 154, 232, 192, 231, 192, 237, 28, 56, 28, 33, 213, 64, 299, 53, 28, 238, 154, 232, 192, 236, 192, 191, 237, 62, 231, 192, 237, 192, 191, 298, 4, 237, 140, 140, 28, 232, 154, 232, 192, 231, 192, 237, 28, 56, 28, 33, 213, 64, 300, 53, 28, 238, 154, 232, 192, 236, 192, 237, 192, 191, 298, 4, 237, 140, 28, 232, 154, 232, 192, 237, 28, 148, 28, 239, 154, 232, 28, 146, 217, 53, 28, 239, 154, 232, 192, 223, 28, 225, 137, 232, 192, 235, 28, 148, 28, 146, 218, 53, 28, 228, 137, 232, 28, 148, 28, 146, 54, 214, 53, 28, 240, 154, 174, 191, 235, 192, 239, 91, 241, 154, 296, 140, 38, 210, 28, 242, 154, 174, 191, 239, 91, 241, 154, 296, 140, 38, 210, 28, 243, 154, 191, 239, 4, 191, 235, 192, 240, 62, 242, 140, 140, 192, 234, 28, 148, 28, 27, 53, 28, 240, 154, 174, 191, 235, 192, 239, 91, 241, 154, 296, 140, 38, 210, 28, 243, 154, 191, 239, 4, 235, 192, 240, 140, 192, 234, 28, 48, 28, 146, 216, 53, 28, 244, 154, 47, 191, 205, 62, 221, 91, 222, 154, 222, 91, 227, 154, 296, 140, 69, 224, 191, 120, 140, 28, 243, 137, 244, 28, 148, 28, 146, 215, 53, 28, 10, 191, 206, 62, 221, 91, 243, 91, 222, 154, 222, 140, 28, 148, 28, 10, 191, 201, 62, 221, 91, 243, 91, 222, 154, 222, 140, 28, 10, 191, 202, 62, 221, 91, 238, 91, 222, 154, 222, 140, 28, 195, 137, 210, 28, 196, 137, 210, 28, 146, 216, 53, 28, 205, 137, 210, 28, 148, 28, 146, 215, 53, 28, 206, 137, 210, 28, 148, 28, 146, 219, 53, 28, 199, 137, 210, 28, 148, 28, 200, 137, 210, 28, 201, 137, 210, 28, 202, 137, 210, 28, 65, 28, 146, 217, 53, 28, 10, 191, 203, 62, 220, 192, 210, 62, 221, 91, 225, 91, 222, 154, 222, 140, 28, 148, 28, 146, 218, 53, 28, 10, 191, 204, 62, 220, 192, 210, 62, 221, 91, 228, 91, 222, 154, 222, 140, 28, 148, 28, 3, 28]}, {"code": "def grpo_fwd_kernel(\n    logits_ptr,\n    ref_logp_ptr,\n    input_ids_ptr,\n    advantages_ptr,\n    completion_mask_ptr,\n    loss_ptr,\n    lse_ptr,\n    beta,\n    save_kl: tl.constexpr,\n    B,\n    M,\n    N,\n    L,\n    start_idx,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n\n    off_b = row_idx // L\n    N = tl.cast(N, tl.int64)\n\n    loss_ptr += row_idx\n\n    completion_mask_ptr += row_idx\n    not_skip = tl.load(completion_mask_ptr).to(tl.int1)\n    if not_skip == 1:\n        ref_logp_ptr += row_idx\n        lse_ptr += row_idx\n        advantages_ptr += off_b\n        logits_ptr += N * (row_idx + off_b)\n        input_ids_ptr += row_idx + (off_b + 1) * start_idx\n        base_cols = tl.arange(0, BLOCK_SIZE)\n\n        m_i = -float(\"inf\")\n        l_i = 0.0\n        for start_n in tl.range(0, N, BLOCK_SIZE):\n            cols = start_n + base_cols\n            mask = cols < N\n            logits = tl.load(logits_ptr + cols, mask=mask, other=-float(\"inf\")).to(\n                tl.float32\n            )\n            m_ij = tl.max(logits)\n            new_m_i = tl.maximum(m_i, m_ij)\n            l_i = l_i * exp(m_i - new_m_i) + tl.sum(exp(logits - new_m_i))\n            m_i = new_m_i\n        lse = log(l_i) + m_i\n\n        idx = tl.load(input_ids_ptr)\n        x = tl.load(logits_ptr + idx).to(tl.float32)\n        advantage = tl.load(advantages_ptr).to(tl.float32)\n        ref_logp = tl.load(ref_logp_ptr)\n        logp = x - lse\n        diff = ref_logp - logp\n        kl = exp(diff) - diff - 1\n        loss = kl * beta - advantage\n\n        tl.store(loss_ptr, loss.to(loss_ptr.dtype.element_ty))\n        tl.store(lse_ptr, lse.to(lse_ptr.dtype.element_ty))\n        if save_kl:\n            tl.store(loss_ptr + M, kl.to(loss_ptr.dtype.element_ty))\n    else:\n\n        tl.store(loss_ptr, 0.0)\n        if save_kl:\n            tl.store(loss_ptr + M, 0.0)", "encoded": [26, 297, 193, 197, 90, 198, 90, 199, 90, 200, 90, 201, 90, 202, 90, 203, 90, 204, 90, 205, 52, 6, 90, 206, 90, 207, 90, 208, 90, 209, 90, 210, 90, 211, 52, 6, 141, 52, 28, -1, 212, 155, 137, 193, 298, 141, 28, 213, 155, 212, 43, 209, 28, 208, 155, 177, 193, 208, 90, 146, 141, 28, 202, 138, 212, 28, 201, 138, 212, 28, 214, 155, 48, 193, 201, 141, 68, 215, 193, 99, 141, 28, 147, 214, 63, 299, 52, 28, 198, 138, 212, 28, 203, 138, 212, 28, 200, 138, 213, 28, 197, 138, 208, 194, 193, 212, 61, 213, 141, 28, 199, 138, 212, 61, 193, 213, 61, 299, 141, 194, 210, 28, 216, 155, 62, 193, 298, 90, 211, 141, 28, 217, 155, 4, 218, 193, 300, 141, 28, 219, 155, 298, 28, 112, 220, 128, 175, 193, 298, 90, 208, 90, 211, 141, 52, 28, 221, 155, 220, 61, 216, 28, 222, 155, 221, 1, 208, 28, 223, 155, 48, 193, 197, 61, 221, 90, 222, 155, 222, 90, 224, 155, 4, 218, 193, 300, 141, 141, 68, 215, 193, 120, 141, 28, 225, 155, 12, 193, 223, 141, 28, 226, 155, 154, 193, 217, 90, 225, 141, 28, 219, 155, 219, 194, 165, 193, 217, 4, 226, 141, 61, 176, 193, 165, 193, 223, 4, 226, 141, 141, 28, 217, 155, 226, 28, 64, 28, 227, 155, 11, 193, 219, 141, 61, 217, 28, 228, 155, 48, 193, 199, 141, 28, 229, 155, 48, 193, 197, 61, 228, 141, 68, 215, 193, 120, 141, 28, 230, 155, 48, 193, 200, 141, 68, 215, 193, 120, 141, 28, 231, 155, 48, 193, 198, 141, 28, 232, 155, 229, 4, 227, 28, 233, 155, 231, 4, 232, 28, 234, 155, 165, 193, 233, 141, 4, 233, 4, 299, 28, 235, 155, 234, 194, 204, 4, 230, 28, 10, 193, 202, 90, 235, 68, 215, 193, 202, 68, 75, 68, 95, 141, 141, 28, 10, 193, 203, 90, 227, 68, 215, 193, 203, 68, 75, 68, 95, 141, 141, 28, 147, 205, 52, 28, 10, 193, 202, 61, 207, 90, 234, 68, 215, 193, 202, 68, 75, 68, 95, 141, 141, 28, 149, 28, 149, 28, 27, 52, 28, 10, 193, 202, 90, 298, 141, 28, 147, 205, 52, 28, 10, 193, 202, 61, 207, 90, 298, 141, 28, 149, 28, 47, 28, 3, 28]}, {"code": "def grpo_bwd_kernel(\n    dloss_ptr,\n    dlogits_ptr,\n    logits_ptr,\n    ref_logp_ptr,\n    input_ids_ptr,\n    advantages_ptr,\n    completion_mask_ptr,\n    lse_ptr,\n    beta,\n    B,\n    N,\n    L,\n    start_idx,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    row_idx = tl.program_id(0)\n    off_b = row_idx // L\n\n    N = tl.cast(N, tl.int64)\n\n    dlogits_ptr += N * (row_idx + off_b)\n    base_cols = tl.arange(0, BLOCK_SIZE)\n    completion_mask_ptr += row_idx\n    not_skip = tl.load(completion_mask_ptr).to(tl.int1)\n\n    if not_skip == 1:\n        lse_ptr += row_idx\n        dloss_ptr += row_idx\n        advantages_ptr += off_b\n        ref_logp_ptr += row_idx\n        logits_ptr += N * (row_idx + off_b)\n        input_ids_ptr += row_idx + (off_b + 1) * start_idx\n        dloss = tl.load(dloss_ptr).to(tl.float32)\n        lse = tl.load(lse_ptr).to(tl.float32)\n        idx = tl.load(input_ids_ptr)\n        x = tl.load(logits_ptr + idx).to(tl.float32)\n        advantage = tl.load(advantages_ptr).to(tl.float32)\n        ref_logp = tl.load(ref_logp_ptr)\n\n        tl.debug_barrier()\n        logp = x - lse\n\n        dlogp = (beta * (-1.0 * exp(ref_logp - logp) + 1) - advantage) * dloss\n\n        for start_n in tl.range(0, N, BLOCK_SIZE):\n            cols = start_n + base_cols\n            mask = cols < N\n            logits = tl.load(logits_ptr + cols, mask=mask, other=-float(\"inf\")).to(\n                tl.float32\n            )\n            probs = exp(logits - lse)\n            dlogits = tl.where(cols == idx, 1 - probs, -probs) * dlogp\n\n            tl.store(\n                dlogits_ptr + cols, dlogits.to(dlogits_ptr.dtype.element_ty), mask=mask\n            )\n    else:\n        dlogits = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        for start_n in tl.range(0, N, BLOCK_SIZE):\n            cols = start_n + base_cols\n            mask = cols < N\n\n            tl.store(\n                dlogits_ptr + cols, dlogits.to(dlogits_ptr.dtype.element_ty), mask=mask\n            )", "encoded": [26, 298, 194, 198, 92, 199, 92, 200, 92, 201, 92, 202, 92, 203, 92, 204, 92, 205, 92, 206, 92, 207, 92, 208, 92, 209, 92, 210, 92, 211, 54, 6, 142, 54, 28, -1, 212, 156, 138, 194, 299, 142, 28, 213, 156, 212, 43, 209, 28, 208, 156, 178, 194, 208, 92, 147, 142, 28, 199, 139, 208, 195, 194, 212, 63, 213, 142, 28, 214, 156, 64, 194, 299, 92, 211, 142, 28, 204, 139, 212, 28, 215, 156, 48, 194, 204, 142, 70, 216, 194, 101, 142, 28, 148, 215, 65, 300, 54, 28, 205, 139, 212, 28, 198, 139, 212, 28, 203, 139, 213, 28, 201, 139, 212, 28, 200, 139, 208, 195, 194, 212, 63, 213, 142, 28, 202, 139, 212, 63, 194, 213, 63, 300, 142, 195, 210, 28, 217, 156, 48, 194, 198, 142, 70, 216, 194, 122, 142, 28, 218, 156, 48, 194, 205, 142, 70, 216, 194, 122, 142, 28, 219, 156, 48, 194, 202, 142, 28, 220, 156, 48, 194, 200, 63, 219, 142, 70, 216, 194, 122, 142, 28, 221, 156, 48, 194, 203, 142, 70, 216, 194, 122, 142, 28, 222, 156, 48, 194, 201, 142, 28, 45, 194, 142, 28, 223, 156, 220, 4, 218, 28, 224, 156, 194, 206, 195, 194, 4, 300, 195, 166, 194, 222, 4, 223, 142, 63, 300, 142, 4, 221, 142, 195, 217, 28, 114, 225, 129, 176, 194, 299, 92, 208, 92, 211, 142, 54, 28, 226, 156, 225, 63, 214, 28, 227, 156, 226, 1, 208, 28, 228, 156, 48, 194, 200, 63, 226, 92, 227, 156, 227, 92, 229, 156, 4, 230, 194, 301, 142, 142, 70, 216, 194, 122, 142, 28, 231, 156, 166, 194, 228, 4, 218, 142, 28, 232, 156, 164, 194, 226, 65, 219, 92, 300, 4, 231, 92, 4, 231, 142, 195, 224, 28, 10, 194, 199, 63, 226, 92, 232, 70, 216, 194, 199, 70, 77, 70, 97, 142, 92, 227, 156, 227, 142, 28, 66, 28, 150, 28, 27, 54, 28, 232, 156, 143, 194, 194, 211, 92, 142, 92, 77, 156, 122, 142, 28, 114, 225, 129, 176, 194, 299, 92, 208, 92, 211, 142, 54, 28, 226, 156, 225, 63, 214, 28, 227, 156, 226, 1, 208, 28, 10, 194, 199, 63, 226, 92, 232, 70, 216, 194, 199, 70, 77, 70, 97, 142, 92, 227, 156, 227, 142, 28, 66, 28, 49, 28, 3, 28]}, {"code": "def l2norm_fwd_kernel1(\n    x,\n    y,\n    D,\n    BD: tl.constexpr,\n    eps,\n):\n    i_t = tl.program_id(0)\n    x += i_t * D\n    y += i_t * D\n\n    cols = tl.arange(0, BD)\n    mask = cols < D\n    b_x = tl.load(x + cols, mask=mask, other=0.0).to(tl.float32)\n    b_var = tl.sum(b_x * b_x, axis=0)\n    b_rstd = 1 / tl.sqrt(b_var + eps)\n\n    b_y = b_x * b_rstd\n    tl.store(y + cols, b_y, mask=mask)", "encoded": [26, 298, 194, 198, 91, 199, 91, 200, 91, 201, 53, 6, 91, 202, 142, 53, 28, -1, 203, 156, 138, 194, 299, 142, 28, 198, 139, 203, 195, 200, 28, 199, 139, 203, 195, 200, 28, 204, 156, 63, 194, 299, 91, 201, 142, 28, 205, 156, 204, 1, 200, 28, 206, 156, 49, 194, 198, 62, 204, 91, 205, 156, 205, 91, 207, 156, 299, 142, 69, 208, 194, 121, 142, 28, 209, 156, 177, 194, 206, 195, 206, 91, 210, 156, 299, 142, 28, 211, 156, 300, 38, 108, 194, 209, 62, 202, 142, 28, 212, 156, 206, 195, 211, 28, 10, 194, 199, 62, 204, 91, 212, 91, 205, 156, 205, 142, 28, 3, 28]}, {"code": "def l2norm_bwd_kernel1(\n    x,\n    dy,\n    dx,\n    eps,\n    D,\n    BD: tl.constexpr,\n):\n    i_t = tl.program_id(0)\n    x += i_t * D\n    dx += i_t * D\n    dy += i_t * D\n\n    cols = tl.arange(0, BD)\n    mask = cols < D\n    b_x = tl.load(x + cols, mask=mask, other=0.0).to(tl.float32)\n    b_var = tl.sum(b_x * b_x)\n    b_rstd = 1 / tl.sqrt(b_var + eps)\n    b_dy = tl.load(dy + cols, mask=mask, other=0.0).to(tl.float32)\n    b_dx = b_dy * b_rstd - tl.sum(b_dy * b_x) * (1 / (b_var + eps)) * b_rstd * b_x\n    tl.store(dx + cols, b_dx, mask=mask)", "encoded": [26, 298, 194, 198, 92, 199, 92, 200, 92, 201, 92, 202, 92, 203, 54, 6, 142, 54, 28, -1, 204, 156, 138, 194, 299, 142, 28, 198, 139, 204, 195, 202, 28, 200, 139, 204, 195, 202, 28, 199, 139, 204, 195, 202, 28, 205, 156, 64, 194, 299, 92, 203, 142, 28, 206, 156, 205, 1, 202, 28, 207, 156, 48, 194, 198, 63, 205, 92, 206, 156, 206, 92, 208, 156, 299, 142, 70, 209, 194, 122, 142, 28, 210, 156, 177, 194, 207, 195, 207, 142, 28, 211, 156, 300, 38, 109, 194, 210, 63, 201, 142, 28, 212, 156, 48, 194, 199, 63, 205, 92, 206, 156, 206, 92, 208, 156, 299, 142, 70, 209, 194, 122, 142, 28, 213, 156, 212, 195, 211, 4, 177, 194, 212, 195, 207, 142, 195, 194, 300, 38, 194, 210, 63, 201, 142, 142, 195, 211, 195, 207, 28, 10, 194, 200, 63, 205, 92, 213, 92, 206, 156, 206, 142, 28, 3, 28]}, {"code": "def l2norm_fwd_kernel(\n    x,\n    y,\n    eps,\n    NB: tl.constexpr,\n    T: tl.constexpr,\n    D: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n):\n    i_t = tl.program_id(0)\n    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\n    b_var = tl.sum(b_x * b_x, axis=1)\n    b_y = b_x / tl.sqrt(b_var + eps)[:, None]\n    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))", "encoded": [26, 298, 194, 198, 91, 199, 91, 200, 91, 201, 53, 6, 91, 202, 53, 6, 91, 203, 53, 6, 91, 204, 53, 6, 91, 205, 53, 6, 142, 53, 28, -1, 206, 156, 138, 194, 299, 142, 28, 207, 156, 175, 194, 198, 91, 194, 202, 91, 203, 142, 91, 194, 203, 91, 300, 142, 91, 194, 206, 195, 204, 91, 299, 142, 91, 194, 204, 91, 205, 142, 91, 194, 300, 91, 299, 142, 142, 28, 208, 156, 49, 194, 207, 91, 209, 156, 194, 299, 91, 300, 142, 142, 69, 210, 194, 121, 142, 28, 211, 156, 177, 194, 208, 195, 208, 91, 212, 156, 300, 142, 28, 213, 156, 208, 38, 108, 194, 211, 62, 200, 142, 183, 53, 91, 161, 24, 28, 214, 156, 175, 194, 199, 91, 194, 202, 91, 203, 142, 91, 194, 203, 91, 300, 142, 91, 194, 206, 195, 204, 91, 299, 142, 91, 194, 204, 91, 205, 142, 91, 194, 300, 91, 299, 142, 142, 28, 10, 194, 214, 91, 213, 69, 210, 194, 214, 69, 76, 69, 96, 142, 91, 209, 156, 194, 299, 91, 300, 142, 142, 28, 3, 28]}, {"code": "def l2norm_bwd_kernel(\n    x,\n    dy,\n    dx,\n    eps,\n    NB: tl.constexpr,\n    T: tl.constexpr,\n    D: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n):\n    i_t = tl.program_id(0)\n    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    p_dy = tl.make_block_ptr(dy, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    p_dx = tl.make_block_ptr(dx, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\n    b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)\n    b_var = tl.sum(b_x * b_x, axis=1)[:, None]\n    b_rstd = 1 / tl.sqrt(b_var + eps)\n    b_dx = (\n        b_dy * b_rstd\n        - tl.sum(b_dy * b_x, axis=1)[:, None] / (b_var + eps) * b_rstd * b_x\n    )\n    tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))", "encoded": [26, 298, 194, 198, 92, 199, 92, 200, 92, 201, 92, 202, 54, 6, 92, 203, 54, 6, 92, 204, 54, 6, 92, 205, 54, 6, 92, 206, 54, 6, 142, 54, 28, -1, 207, 156, 138, 194, 299, 142, 28, 208, 156, 175, 194, 198, 92, 194, 203, 92, 204, 142, 92, 194, 204, 92, 300, 142, 92, 194, 207, 195, 205, 92, 299, 142, 92, 194, 205, 92, 206, 142, 92, 194, 300, 92, 299, 142, 142, 28, 209, 156, 175, 194, 199, 92, 194, 203, 92, 204, 142, 92, 194, 204, 92, 300, 142, 92, 194, 207, 195, 205, 92, 299, 142, 92, 194, 205, 92, 206, 142, 92, 194, 300, 92, 299, 142, 142, 28, 210, 156, 175, 194, 200, 92, 194, 203, 92, 204, 142, 92, 194, 204, 92, 300, 142, 92, 194, 207, 195, 205, 92, 299, 142, 92, 194, 205, 92, 206, 142, 92, 194, 300, 92, 299, 142, 142, 28, 211, 156, 48, 194, 208, 92, 212, 156, 194, 299, 92, 300, 142, 142, 70, 213, 194, 122, 142, 28, 214, 156, 48, 194, 209, 92, 212, 156, 194, 299, 92, 300, 142, 142, 70, 213, 194, 122, 142, 28, 215, 156, 177, 194, 211, 195, 211, 92, 216, 156, 300, 142, 183, 54, 92, 161, 24, 28, 217, 156, 300, 38, 109, 194, 215, 63, 201, 142, 28, 218, 156, 214, 195, 217, 4, 177, 194, 214, 195, 211, 92, 216, 156, 300, 142, 183, 54, 92, 161, 24, 38, 194, 215, 63, 201, 142, 195, 217, 195, 211, 28, 10, 194, 210, 92, 218, 70, 213, 194, 210, 70, 77, 70, 97, 142, 92, 212, 156, 194, 299, 92, 300, 142, 142, 28, 3, 28]}, {"code": "def layer_norm_fwd_kernel(\n    x,\n    y,\n    w,\n    b,\n    res,\n    res_out,\n    mean,\n    rstd,\n    eps,\n    T,\n    G: tl.constexpr,\n    D: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    NB: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    STORE_RESIDUAL_OUT: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n):\n    i_t = tl.program_id(0)\n\n    o_t = i_t * BT + tl.arange(0, BT)\n    o_g = o_t % G\n    o_d = tl.arange(0, BD)\n    m_d = o_d < D\n\n    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\n    if HAS_RESIDUAL:\n        p_res = tl.make_block_ptr(res, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n        b_x += tl.load(p_res, boundary_check=(0, 1)).to(tl.float32)\n    if STORE_RESIDUAL_OUT:\n        p_res_out = tl.make_block_ptr(\n            res_out, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0)\n        )\n        tl.store(p_res_out, b_x.to(p_res_out.dtype.element_ty), boundary_check=(0, 1))\n    if not IS_RMS_NORM:\n        b_mean = tl.sum(b_x, axis=1) / D\n        p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        tl.store(p_mean, b_mean.to(p_mean.dtype.element_ty), boundary_check=(0,))\n        b_xbar = tl.where(m_d[None, :], b_x - b_mean[:, None], 0.0)\n        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D\n    else:\n        b_xbar = tl.where(m_d[None, :], b_x, 0.0)\n        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D\n    b_rstd = 1 / tl.sqrt(b_var + eps)\n\n    p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_rstd, b_rstd.to(p_rstd.dtype.element_ty), boundary_check=(0,))\n\n    if HAS_WEIGHT:\n        b_w = tl.load(w + o_g[:, None] * D + o_d[None, :], mask=m_d[None, :]).to(\n            tl.float32\n        )\n    if HAS_BIAS:\n        b_b = tl.load(b + o_g[:, None] * D + o_d[None, :], mask=m_d[None, :]).to(\n            tl.float32\n        )\n    b_x_hat = (\n        (b_x - b_mean[:, None]) * b_rstd[:, None]\n        if not IS_RMS_NORM\n        else b_x * b_rstd[:, None]\n    )\n    b_y = b_x_hat * b_w if HAS_WEIGHT else b_x_hat\n    if HAS_BIAS:\n        b_y = b_y + b_b\n\n    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))", "encoded": [26, 298, 194, 198, 91, 199, 91, 200, 91, 201, 91, 202, 91, 203, 91, 204, 91, 205, 91, 206, 91, 207, 91, 208, 53, 6, 91, 209, 53, 6, 91, 210, 53, 6, 91, 211, 53, 6, 91, 212, 53, 6, 91, 213, 53, 6, 91, 214, 53, 6, 91, 215, 53, 6, 91, 216, 53, 6, 91, 217, 53, 6, 142, 53, 28, -1, 218, 156, 138, 194, 299, 142, 28, 219, 156, 218, 195, 210, 62, 63, 194, 299, 91, 210, 142, 28, 220, 156, 219, 180, 208, 28, 221, 156, 63, 194, 299, 91, 211, 142, 28, 222, 156, 221, 1, 209, 28, 223, 156, 175, 194, 198, 91, 194, 207, 91, 209, 142, 91, 194, 209, 91, 300, 142, 91, 194, 218, 195, 210, 91, 299, 142, 91, 194, 210, 91, 211, 142, 91, 194, 300, 91, 299, 142, 142, 28, 224, 156, 49, 194, 223, 91, 225, 156, 194, 299, 91, 300, 142, 142, 69, 226, 194, 121, 142, 28, 148, 214, 53, 28, 227, 156, 175, 194, 202, 91, 194, 207, 91, 209, 142, 91, 194, 209, 91, 300, 142, 91, 194, 218, 195, 210, 91, 299, 142, 91, 194, 210, 91, 211, 142, 91, 194, 300, 91, 299, 142, 142, 28, 224, 139, 49, 194, 227, 91, 225, 156, 194, 299, 91, 300, 142, 142, 69, 226, 194, 121, 142, 28, 150, 28, 148, 215, 53, 28, 228, 156, 175, 194, 203, 91, 194, 207, 91, 209, 142, 91, 194, 209, 91, 300, 142, 91, 194, 218, 195, 210, 91, 299, 142, 91, 194, 210, 91, 211, 142, 91, 194, 300, 91, 299, 142, 142, 28, 10, 194, 228, 91, 224, 69, 226, 194, 228, 69, 76, 69, 96, 142, 91, 225, 156, 194, 299, 91, 300, 142, 142, 28, 150, 28, 148, 54, 213, 53, 28, 229, 156, 177, 194, 224, 91, 230, 156, 300, 142, 38, 209, 28, 231, 156, 175, 194, 204, 91, 194, 207, 91, 142, 91, 194, 300, 91, 142, 91, 194, 218, 195, 210, 91, 142, 91, 194, 210, 91, 142, 91, 194, 299, 91, 142, 142, 28, 10, 194, 231, 91, 229, 69, 226, 194, 231, 69, 76, 69, 96, 142, 91, 225, 156, 194, 299, 91, 142, 142, 28, 232, 156, 164, 194, 222, 183, 161, 91, 53, 24, 91, 224, 4, 229, 183, 53, 91, 161, 24, 91, 299, 142, 28, 233, 156, 177, 194, 232, 195, 232, 91, 230, 156, 300, 142, 38, 209, 28, 150, 28, 27, 53, 28, 232, 156, 164, 194, 222, 183, 161, 91, 53, 24, 91, 224, 91, 299, 142, 28, 233, 156, 177, 194, 232, 195, 232, 91, 230, 156, 300, 142, 38, 209, 28, 48, 28, 234, 156, 300, 38, 108, 194, 233, 62, 206, 142, 28, 235, 156, 175, 194, 205, 91, 194, 207, 91, 142, 91, 194, 300, 91, 142, 91, 194, 218, 195, 210, 91, 142, 91, 194, 210, 91, 142, 91, 194, 299, 91, 142, 142, 28, 10, 194, 235, 91, 234, 69, 226, 194, 235, 69, 76, 69, 96, 142, 91, 225, 156, 194, 299, 91, 142, 142, 28, 148, 216, 53, 28, 236, 156, 49, 194, 200, 62, 220, 183, 53, 91, 161, 24, 195, 209, 62, 221, 183, 161, 91, 53, 24, 91, 237, 156, 222, 183, 161, 91, 53, 24, 142, 69, 226, 194, 121, 142, 28, 150, 28, 148, 217, 53, 28, 238, 156, 49, 194, 201, 62, 220, 183, 53, 91, 161, 24, 195, 209, 62, 221, 183, 161, 91, 53, 24, 91, 237, 156, 222, 183, 161, 91, 53, 24, 142, 69, 226, 194, 121, 142, 28, 150, 28, 239, 156, 194, 224, 4, 229, 183, 53, 91, 161, 24, 142, 195, 234, 183, 53, 91, 161, 24, 148, 54, 213, 27, 224, 195, 234, 183, 53, 91, 161, 24, 28, 240, 156, 239, 195, 236, 148, 216, 27, 239, 28, 148, 217, 53, 28, 240, 156, 240, 62, 238, 28, 150, 28, 241, 156, 175, 194, 199, 91, 194, 207, 91, 209, 142, 91, 194, 209, 91, 300, 142, 91, 194, 218, 195, 210, 91, 299, 142, 91, 194, 210, 91, 211, 142, 91, 194, 300, 91, 299, 142, 142, 28, 10, 194, 241, 91, 240, 69, 226, 194, 241, 69, 76, 69, 96, 142, 91, 225, 156, 194, 299, 91, 300, 142, 142, 28, 3, 28]}, {"code": "def layer_norm_fwd_kernel1(\n    x,\n    y,\n    w,\n    b,\n    res,\n    res_out,\n    mean,\n    rstd,\n    eps,\n    G: tl.constexpr,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    STORE_RESIDUAL_OUT: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n):\n    i_t = tl.program_id(0)\n    i_g = i_t % G\n\n    x += i_t * D\n    y += i_t * D\n    if HAS_RESIDUAL:\n        res += i_t * D\n    if STORE_RESIDUAL_OUT:\n        res_out += i_t * D\n\n    o_d = tl.arange(0, BD)\n    m_d = o_d < D\n    b_x = tl.load(x + o_d, mask=m_d, other=0.0).to(tl.float32)\n    if HAS_RESIDUAL:\n        b_x += tl.load(res + o_d, mask=m_d, other=0.0).to(tl.float32)\n    if STORE_RESIDUAL_OUT:\n        tl.store(res_out + o_d, b_x, mask=m_d)\n    if not IS_RMS_NORM:\n        b_mean = tl.sum(b_x, axis=0) / D\n        tl.store(mean + i_t, b_mean)\n        b_xbar = tl.where(m_d, b_x - b_mean, 0.0)\n        b_var = tl.sum(b_xbar * b_xbar, axis=0) / D\n    else:\n        b_xbar = tl.where(m_d, b_x, 0.0)\n        b_var = tl.sum(b_xbar * b_xbar, axis=0) / D\n    b_rstd = 1 / tl.sqrt(b_var + eps)\n    tl.store(rstd + i_t, b_rstd)\n\n    if HAS_WEIGHT:\n        b_w = tl.load(w + i_g * D + o_d, mask=m_d).to(tl.float32)\n    if HAS_BIAS:\n        b_b = tl.load(b + i_g * D + o_d, mask=m_d).to(tl.float32)\n    b_x_hat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd\n    b_y = b_x_hat * b_w if HAS_WEIGHT else b_x_hat\n    if HAS_BIAS:\n        b_y = b_y + b_b\n\n    tl.store(y + o_d, b_y, mask=m_d)", "encoded": [26, 298, 194, 198, 92, 199, 92, 200, 92, 201, 92, 202, 92, 203, 92, 204, 92, 205, 92, 206, 92, 207, 54, 6, 92, 208, 54, 6, 92, 209, 54, 6, 92, 210, 54, 6, 92, 211, 54, 6, 92, 212, 54, 6, 92, 213, 54, 6, 92, 214, 54, 6, 142, 54, 28, -1, 215, 156, 138, 194, 299, 142, 28, 216, 156, 215, 180, 207, 28, 198, 139, 215, 195, 208, 28, 199, 139, 215, 195, 208, 28, 148, 211, 54, 28, 202, 139, 215, 195, 208, 28, 150, 28, 148, 212, 54, 28, 203, 139, 215, 195, 208, 28, 150, 28, 217, 156, 64, 194, 299, 92, 209, 142, 28, 218, 156, 217, 1, 208, 28, 219, 156, 48, 194, 198, 63, 217, 92, 220, 156, 218, 92, 221, 156, 299, 142, 70, 222, 194, 122, 142, 28, 148, 211, 54, 28, 219, 139, 48, 194, 202, 63, 217, 92, 220, 156, 218, 92, 221, 156, 299, 142, 70, 222, 194, 122, 142, 28, 150, 28, 148, 212, 54, 28, 10, 194, 203, 63, 217, 92, 219, 92, 220, 156, 218, 142, 28, 150, 28, 148, 55, 210, 54, 28, 223, 156, 177, 194, 219, 92, 224, 156, 299, 142, 38, 208, 28, 10, 194, 204, 63, 215, 92, 223, 142, 28, 225, 156, 164, 194, 218, 92, 219, 4, 223, 92, 299, 142, 28, 226, 156, 177, 194, 225, 195, 225, 92, 224, 156, 299, 142, 38, 208, 28, 150, 28, 27, 54, 28, 225, 156, 164, 194, 218, 92, 219, 92, 299, 142, 28, 226, 156, 177, 194, 225, 195, 225, 92, 224, 156, 299, 142, 38, 208, 28, 49, 28, 227, 156, 300, 38, 109, 194, 226, 63, 206, 142, 28, 10, 194, 205, 63, 215, 92, 227, 142, 28, 148, 213, 54, 28, 228, 156, 48, 194, 200, 63, 216, 195, 208, 63, 217, 92, 220, 156, 218, 142, 70, 222, 194, 122, 142, 28, 150, 28, 148, 214, 54, 28, 229, 156, 48, 194, 201, 63, 216, 195, 208, 63, 217, 92, 220, 156, 218, 142, 70, 222, 194, 122, 142, 28, 150, 28, 230, 156, 194, 219, 4, 223, 142, 195, 227, 148, 55, 210, 27, 219, 195, 227, 28, 231, 156, 230, 195, 228, 148, 213, 27, 230, 28, 148, 214, 54, 28, 231, 156, 231, 63, 229, 28, 150, 28, 10, 194, 199, 63, 217, 92, 231, 92, 220, 156, 218, 142, 28, 3, 28]}, {"code": "def layer_norm_bwd_kernel(\n    x,\n    w,\n    b,\n    y,\n    dy,\n    dx,\n    dw,\n    db,\n    dres,\n    dres_in,\n    mean,\n    rstd,\n    T,\n    G: tl.constexpr,\n    D: tl.constexpr,\n    BS: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    NB: tl.constexpr,\n    GS: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n    HAS_DRESIDUAL: tl.constexpr,\n    STORE_DRESIDUAL: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n):\n    i_s = tl.program_id(0)\n    i_g, i_sg = i_s // GS, i_s % GS\n\n    o_d = tl.arange(0, BD)\n    m_d = o_d < D\n    if HAS_WEIGHT:\n        b_w = tl.load(w + i_g * D + o_d, mask=m_d).to(tl.float32)\n        b_dw = tl.zeros((BT, BD), dtype=tl.float32)\n    if HAS_BIAS:\n        b_b = tl.load(b + i_g * D + o_d, mask=m_d, other=0.0).to(tl.float32)\n        b_db = tl.zeros((BT, BD), dtype=tl.float32)\n\n    T = min(i_sg * BS + BS, T // G)\n    for i_t in range(i_sg * BS, T, BT):\n        p_x = tl.make_block_ptr(\n            x + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)\n        )\n        p_dy = tl.make_block_ptr(\n            dy + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)\n        )\n        p_dx = tl.make_block_ptr(\n            dx + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)\n        )\n\n        b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\n        b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)\n\n        if not IS_RMS_NORM:\n            p_mean = tl.make_block_ptr(mean + i_g, (T,), (G,), (i_t,), (BT,), (0,))\n            b_mean = tl.load(p_mean, boundary_check=(0,))\n        p_rstd = tl.make_block_ptr(rstd + i_g, (T,), (G,), (i_t,), (BT,), (0,))\n        b_rstd = tl.load(p_rstd, boundary_check=(0,))\n\n        b_xhat = (\n            (b_x - b_mean[:, None]) * b_rstd[:, None]\n            if not IS_RMS_NORM\n            else b_x * b_rstd[:, None]\n        )\n        b_xhat = tl.where(m_d[None, :], b_xhat, 0.0)\n\n        b_y = b_xhat * b_w[None, :] if HAS_WEIGHT else b_xhat\n        if HAS_BIAS:\n            b_y = b_y + b_b[None, :]\n        if RECOMPUTE_OUTPUT:\n            p_y = tl.make_block_ptr(\n                y + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)\n            )\n            tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))\n\n        b_wdy = b_dy\n\n        if HAS_WEIGHT or HAS_BIAS:\n            m_t = (i_t + tl.arange(0, BT)) < T\n        if HAS_WEIGHT:\n            b_wdy = b_dy * b_w\n            b_dw += tl.where(m_t[:, None], b_dy * b_xhat, 0.0)\n        if HAS_BIAS:\n            b_db += tl.where(m_t[:, None], b_dy, 0.0)\n        if not IS_RMS_NORM:\n            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D\n            b_c2 = tl.sum(b_wdy, axis=1) / D\n            b_dx = (b_wdy - (b_xhat * b_c1[:, None] + b_c2[:, None])) * b_rstd[:, None]\n        else:\n            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D\n            b_dx = (b_wdy - b_xhat * b_c1[:, None]) * b_rstd[:, None]\n        if HAS_DRESIDUAL:\n            p_dres = tl.make_block_ptr(\n                dres + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)\n            )\n            b_dres = tl.load(p_dres, boundary_check=(0, 1)).to(tl.float32)\n            b_dx += b_dres\n\n        if STORE_DRESIDUAL:\n            p_dres_in = tl.make_block_ptr(\n                dres_in + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)\n            )\n            tl.store(\n                p_dres_in, b_dx.to(p_dres_in.dtype.element_ty), boundary_check=(0, 1)\n            )\n\n        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))\n\n    if HAS_WEIGHT:\n        tl.store(dw + i_s * D + o_d, tl.sum(b_dw, axis=0), mask=m_d)\n    if HAS_BIAS:\n        tl.store(db + i_s * D + o_d, tl.sum(b_db, axis=0), mask=m_d)", "encoded": [26, 298, 194, 198, 91, 199, 91, 200, 91, 201, 91, 202, 91, 203, 91, 204, 91, 205, 91, 206, 91, 207, 91, 208, 91, 209, 91, 210, 91, 211, 53, 6, 91, 212, 53, 6, 91, 213, 53, 6, 91, 214, 53, 6, 91, 215, 53, 6, 91, 216, 53, 6, 91, 217, 53, 6, 91, 218, 53, 6, 91, 219, 53, 6, 91, 220, 53, 6, 91, 221, 53, 6, 91, 222, 53, 6, 91, 223, 53, 6, 142, 53, 28, -1, 224, 156, 138, 194, 299, 142, 28, 225, 91, 226, 156, 194, 224, 43, 217, 91, 224, 180, 217, 142, 28, 227, 156, 63, 194, 299, 91, 215, 142, 28, 228, 156, 227, 1, 212, 28, 148, 221, 53, 28, 229, 156, 49, 194, 199, 62, 225, 195, 212, 62, 227, 91, 230, 156, 228, 142, 69, 231, 194, 121, 142, 28, 232, 156, 143, 194, 194, 214, 91, 215, 142, 91, 76, 156, 121, 142, 28, 150, 28, 148, 222, 53, 28, 233, 156, 49, 194, 200, 62, 225, 195, 212, 62, 227, 91, 230, 156, 228, 91, 234, 156, 299, 142, 69, 231, 194, 121, 142, 28, 235, 156, 143, 194, 194, 214, 91, 215, 142, 91, 76, 156, 121, 142, 28, 150, 28, 210, 156, 35, 194, 226, 195, 213, 62, 213, 91, 210, 43, 211, 142, 28, 113, 236, 129, 5, 194, 226, 195, 213, 91, 210, 91, 214, 142, 53, 28, 237, 156, 175, 194, 198, 62, 225, 195, 212, 91, 194, 210, 91, 212, 142, 91, 194, 211, 195, 212, 91, 300, 142, 91, 194, 236, 91, 299, 142, 91, 194, 214, 91, 215, 142, 91, 194, 300, 91, 299, 142, 142, 28, 238, 156, 175, 194, 202, 62, 225, 195, 212, 91, 194, 210, 91, 212, 142, 91, 194, 211, 195, 212, 91, 300, 142, 91, 194, 236, 91, 299, 142, 91, 194, 214, 91, 215, 142, 91, 194, 300, 91, 299, 142, 142, 28, 239, 156, 175, 194, 203, 62, 225, 195, 212, 91, 194, 210, 91, 212, 142, 91, 194, 211, 195, 212, 91, 300, 142, 91, 194, 236, 91, 299, 142, 91, 194, 214, 91, 215, 142, 91, 194, 300, 91, 299, 142, 142, 28, 240, 156, 49, 194, 237, 91, 241, 156, 194, 299, 91, 300, 142, 142, 69, 231, 194, 121, 142, 28, 242, 156, 49, 194, 238, 91, 241, 156, 194, 299, 91, 300, 142, 142, 69, 231, 194, 121, 142, 28, 148, 54, 218, 53, 28, 243, 156, 175, 194, 208, 62, 225, 91, 194, 210, 91, 142, 91, 194, 211, 91, 142, 91, 194, 236, 91, 142, 91, 194, 214, 91, 142, 91, 194, 299, 91, 142, 142, 28, 244, 156, 49, 194, 243, 91, 241, 156, 194, 299, 91, 142, 142, 28, 150, 28, 245, 156, 175, 194, 209, 62, 225, 91, 194, 210, 91, 142, 91, 194, 211, 91, 142, 91, 194, 236, 91, 142, 91, 194, 214, 91, 142, 91, 194, 299, 91, 142, 142, 28, 246, 156, 49, 194, 245, 91, 241, 156, 194, 299, 91, 142, 142, 28, 247, 156, 194, 240, 4, 244, 183, 53, 91, 161, 24, 142, 195, 246, 183, 53, 91, 161, 24, 148, 54, 218, 27, 240, 195, 246, 183, 53, 91, 161, 24, 28, 247, 156, 164, 194, 228, 183, 161, 91, 53, 24, 91, 247, 91, 299, 142, 28, 248, 156, 247, 195, 229, 183, 161, 91, 53, 24, 148, 221, 27, 247, 28, 148, 222, 53, 28, 248, 156, 248, 62, 233, 183, 161, 91, 53, 24, 28, 150, 28, 148, 223, 53, 28, 249, 156, 175, 194, 201, 62, 225, 195, 212, 91, 194, 210, 91, 212, 142, 91, 194, 211, 195, 212, 91, 300, 142, 91, 194, 236, 91, 299, 142, 91, 194, 214, 91, 215, 142, 91, 194, 300, 91, 299, 142, 142, 28, 10, 194, 249, 91, 248, 69, 231, 194, 249, 69, 76, 69, 96, 142, 91, 241, 156, 194, 299, 91, 300, 142, 142, 28, 150, 28, 250, 156, 242, 28, 148, 221, 118, 222, 53, 28, 251, 156, 236, 62, 63, 194, 299, 91, 214, 142, 1, 210, 28, 150, 28, 148, 221, 53, 28, 250, 156, 242, 195, 229, 28, 232, 139, 164, 194, 251, 183, 53, 91, 161, 24, 91, 242, 195, 247, 91, 299, 142, 28, 150, 28, 148, 222, 53, 28, 235, 139, 164, 194, 251, 183, 53, 91, 161, 24, 91, 242, 91, 299, 142, 28, 150, 28, 148, 54, 218, 53, 28, 252, 156, 177, 194, 247, 195, 250, 91, 253, 156, 300, 142, 38, 212, 28, 254, 156, 177, 194, 250, 91, 253, 156, 300, 142, 38, 212, 28, 255, 156, 194, 250, 4, 194, 247, 195, 252, 183, 53, 91, 161, 24, 62, 254, 183, 53, 91, 161, 24, 142, 142, 195, 246, 183, 53, 91, 161, 24, 28, 150, 28, 27, 53, 28, 252, 156, 177, 194, 247, 195, 250, 91, 253, 156, 300, 142, 38, 212, 28, 255, 156, 194, 250, 4, 247, 195, 252, 183, 53, 91, 161, 24, 142, 195, 246, 183, 53, 91, 161, 24, 28, 48, 28, 148, 219, 53, 28, 256, 156, 175, 194, 206, 62, 225, 195, 212, 91, 194, 210, 91, 212, 142, 91, 194, 211, 195, 212, 91, 300, 142, 91, 194, 236, 91, 299, 142, 91, 194, 214, 91, 215, 142, 91, 194, 300, 91, 299, 142, 142, 28, 257, 156, 49, 194, 256, 91, 241, 156, 194, 299, 91, 300, 142, 142, 69, 231, 194, 121, 142, 28, 255, 139, 257, 28, 150, 28, 148, 220, 53, 28, 258, 156, 175, 194, 207, 62, 225, 195, 212, 91, 194, 210, 91, 212, 142, 91, 194, 211, 195, 212, 91, 300, 142, 91, 194, 236, 91, 299, 142, 91, 194, 214, 91, 215, 142, 91, 194, 300, 91, 299, 142, 142, 28, 10, 194, 258, 91, 255, 69, 231, 194, 258, 69, 76, 69, 96, 142, 91, 241, 156, 194, 299, 91, 300, 142, 142, 28, 150, 28, 10, 194, 239, 91, 255, 69, 231, 194, 239, 69, 76, 69, 96, 142, 91, 241, 156, 194, 299, 91, 300, 142, 142, 28, 65, 28, 148, 221, 53, 28, 10, 194, 204, 62, 224, 195, 212, 62, 227, 91, 177, 194, 232, 91, 253, 156, 299, 142, 91, 230, 156, 228, 142, 28, 150, 28, 148, 222, 53, 28, 10, 194, 205, 62, 224, 195, 212, 62, 227, 91, 177, 194, 235, 91, 253, 156, 299, 142, 91, 230, 156, 228, 142, 28, 150, 28, 3, 28]}, {"code": "def layer_norm_bwd_kernel1(\n    x,\n    w,\n    b,\n    y,\n    dy,\n    dx,\n    dw,\n    db,\n    dres,\n    dres_in,\n    mean,\n    rstd,\n    T,\n    G: tl.constexpr,\n    D: tl.constexpr,\n    BS: tl.constexpr,\n    BD: tl.constexpr,\n    GS: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n    HAS_DRESIDUAL: tl.constexpr,\n    STORE_DRESIDUAL: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n):\n    i_s = tl.program_id(0)\n    i_g, i_sg = i_s // GS, i_s % GS\n\n    o_d = tl.arange(0, BD)\n    mask = o_d < D\n\n    if HAS_WEIGHT:\n        b_w = tl.load(w + i_g * D + o_d, mask=mask).to(tl.float32)\n        b_dw = tl.zeros((BD,), dtype=tl.float32)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b_b = tl.load(b + i_g * D + o_d, mask=mask, other=0.0).to(tl.float32)\n    if HAS_BIAS:\n        b_db = tl.zeros((BD,), dtype=tl.float32)\n\n    for i_t in range(i_sg * BS * G + i_g, min((i_sg * BS + BS) * G + i_g, T), G):\n        b_x = tl.load(x + i_t * D + o_d, mask=mask, other=0).to(tl.float32)\n        b_dy = tl.load(dy + i_t * D + o_d, mask=mask, other=0).to(tl.float32)\n\n        if not IS_RMS_NORM:\n            b_mean = tl.load(mean + i_t)\n        b_rstd = tl.load(rstd + i_t)\n\n        b_xhat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd\n        b_xhat = tl.where(mask, b_xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            b_y = b_xhat * b_w if HAS_WEIGHT else b_xhat\n            if HAS_BIAS:\n                b_y = b_y + b_b\n            tl.store(y + i_t * D + o_d, b_y, mask=mask)\n        b_wdy = b_dy\n        if HAS_WEIGHT:\n            b_wdy = b_dy * b_w\n            b_dw += b_dy * b_xhat\n        if HAS_BIAS:\n            b_db += b_dy\n        if not IS_RMS_NORM:\n            b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D\n            b_c2 = tl.sum(b_wdy, axis=0) / D\n            b_dx = (b_wdy - (b_xhat * b_c1 + b_c2)) * b_rstd\n        else:\n            b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D\n            b_dx = (b_wdy - b_xhat * b_c1) * b_rstd\n        if HAS_DRESIDUAL:\n            b_dres = tl.load(dres + i_t * D + o_d, mask=mask, other=0).to(tl.float32)\n            b_dx += b_dres\n\n        b_dx = tl.cast(b_dx, dtype=dx.dtype.element_ty, fp_downcast_rounding=\"rtne\")\n        if STORE_DRESIDUAL:\n            tl.store(dres_in + i_t * D + o_d, b_dx, mask=mask)\n        tl.store(dx + i_t * D + o_d, b_dx, mask=mask)\n\n    if HAS_WEIGHT:\n        tl.store(dw + i_s * D + o_d, b_dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(db + i_s * D + o_d, b_db, mask=mask)", "encoded": [26, 298, 194, 198, 92, 199, 92, 200, 92, 201, 92, 202, 92, 203, 92, 204, 92, 205, 92, 206, 92, 207, 92, 208, 92, 209, 92, 210, 92, 211, 54, 6, 92, 212, 54, 6, 92, 213, 54, 6, 92, 214, 54, 6, 92, 215, 54, 6, 92, 216, 54, 6, 92, 217, 54, 6, 92, 218, 54, 6, 92, 219, 54, 6, 92, 220, 54, 6, 92, 221, 54, 6, 142, 54, 28, -1, 222, 156, 138, 194, 299, 142, 28, 223, 92, 224, 156, 194, 222, 43, 215, 92, 222, 180, 215, 142, 28, 225, 156, 64, 194, 299, 92, 214, 142, 28, 226, 156, 225, 1, 212, 28, 148, 219, 54, 28, 227, 156, 48, 194, 199, 63, 223, 195, 212, 63, 225, 92, 226, 156, 226, 142, 70, 228, 194, 122, 142, 28, 229, 156, 143, 194, 194, 214, 92, 142, 92, 77, 156, 122, 142, 28, 150, 28, 148, 221, 87, 220, 54, 28, 230, 156, 48, 194, 200, 63, 223, 195, 212, 63, 225, 92, 226, 156, 226, 92, 231, 156, 299, 142, 70, 228, 194, 122, 142, 28, 150, 28, 148, 220, 54, 28, 232, 156, 143, 194, 194, 214, 92, 142, 92, 77, 156, 122, 142, 28, 150, 28, 114, 233, 129, 5, 194, 224, 195, 213, 195, 211, 63, 223, 92, 35, 194, 194, 224, 195, 213, 63, 213, 142, 195, 211, 63, 223, 92, 210, 142, 92, 211, 142, 54, 28, 234, 156, 48, 194, 198, 63, 233, 195, 212, 63, 225, 92, 226, 156, 226, 92, 231, 156, 299, 142, 70, 228, 194, 122, 142, 28, 235, 156, 48, 194, 202, 63, 233, 195, 212, 63, 225, 92, 226, 156, 226, 92, 231, 156, 299, 142, 70, 228, 194, 122, 142, 28, 148, 55, 216, 54, 28, 236, 156, 48, 194, 208, 63, 233, 142, 28, 150, 28, 237, 156, 48, 194, 209, 63, 233, 142, 28, 238, 156, 194, 234, 4, 236, 142, 195, 237, 148, 55, 216, 27, 234, 195, 237, 28, 238, 156, 164, 194, 226, 92, 238, 92, 299, 142, 28, 148, 221, 54, 28, 239, 156, 238, 195, 227, 148, 219, 27, 238, 28, 148, 220, 54, 28, 239, 156, 239, 63, 230, 28, 150, 28, 10, 194, 201, 63, 233, 195, 212, 63, 225, 92, 239, 92, 226, 156, 226, 142, 28, 150, 28, 240, 156, 235, 28, 148, 219, 54, 28, 240, 156, 235, 195, 227, 28, 229, 139, 235, 195, 238, 28, 150, 28, 148, 220, 54, 28, 232, 139, 235, 28, 150, 28, 148, 55, 216, 54, 28, 241, 156, 177, 194, 238, 195, 240, 92, 242, 156, 299, 142, 38, 212, 28, 243, 156, 177, 194, 240, 92, 242, 156, 299, 142, 38, 212, 28, 244, 156, 194, 240, 4, 194, 238, 195, 241, 63, 243, 142, 142, 195, 237, 28, 150, 28, 27, 54, 28, 241, 156, 177, 194, 238, 195, 240, 92, 242, 156, 299, 142, 38, 212, 28, 244, 156, 194, 240, 4, 238, 195, 241, 142, 195, 237, 28, 49, 28, 148, 217, 54, 28, 245, 156, 48, 194, 206, 63, 233, 195, 212, 63, 225, 92, 226, 156, 226, 92, 231, 156, 299, 142, 70, 228, 194, 122, 142, 28, 244, 139, 245, 28, 150, 28, 244, 156, 178, 194, 244, 92, 77, 156, 203, 70, 77, 70, 97, 92, 135, 156, 300, 142, 28, 148, 218, 54, 28, 10, 194, 207, 63, 233, 195, 212, 63, 225, 92, 244, 92, 226, 156, 226, 142, 28, 150, 28, 10, 194, 203, 63, 233, 195, 212, 63, 225, 92, 244, 92, 226, 156, 226, 142, 28, 66, 28, 148, 219, 54, 28, 10, 194, 204, 63, 222, 195, 212, 63, 225, 92, 229, 92, 226, 156, 226, 142, 28, 150, 28, 148, 220, 54, 28, 10, 194, 205, 63, 222, 195, 212, 63, 225, 92, 232, 92, 226, 156, 226, 142, 28, 150, 28, 3, 28]}, {"code": "def layer_norm_fwd_kernel(\n    X,\n    Y,\n    W,\n    B,\n    Z,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_y_row,\n    stride_z_row,\n    M,\n    N,\n    eps,\n    BLOCK_N: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_Z: tl.constexpr,\n    NORM_BEFORE_GATE: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n):\n\n    row = tl.program_id(0)\n    group = tl.program_id(1)\n    X += row * stride_x_row + group * N\n    Y += row * stride_y_row + group * N\n    if HAS_Z:\n        Z += row * stride_z_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    if HAS_BIAS:\n        B += group * N\n\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_Z and not NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=cols < N).to(tl.float32)\n        x *= z * tl.sigmoid(z)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    if HAS_Z and NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=mask).to(tl.float32)\n        y *= z * tl.sigmoid(z)\n\n    tl.store(Y + cols, y, mask=mask)", "encoded": [26, 298, 194, 198, 91, 199, 91, 200, 91, 201, 91, 202, 91, 203, 91, 204, 91, 205, 91, 206, 91, 207, 91, 208, 91, 209, 91, 210, 91, 211, 53, 6, 91, 212, 53, 6, 91, 213, 53, 6, 91, 214, 53, 6, 91, 215, 53, 6, 142, 53, 28, -1, 216, 156, 138, 194, 299, 142, 28, 217, 156, 138, 194, 300, 142, 28, 198, 139, 216, 195, 205, 62, 217, 195, 209, 28, 199, 139, 216, 195, 206, 62, 217, 195, 209, 28, 148, 213, 53, 28, 202, 139, 216, 195, 207, 62, 217, 195, 209, 28, 150, 28, 148, 54, 215, 53, 28, 203, 139, 217, 195, 208, 28, 150, 28, 204, 139, 217, 195, 208, 28, 200, 139, 217, 195, 209, 28, 148, 212, 53, 28, 201, 139, 217, 195, 209, 28, 150, 28, 218, 156, 63, 194, 299, 91, 211, 142, 28, 219, 156, 49, 194, 198, 62, 218, 91, 220, 156, 218, 1, 209, 91, 221, 156, 299, 142, 69, 222, 194, 121, 142, 28, 148, 213, 86, 194, 54, 214, 142, 53, 28, 223, 156, 49, 194, 202, 62, 218, 91, 220, 156, 218, 1, 209, 142, 69, 222, 194, 121, 142, 28, 219, 21, 223, 195, 165, 194, 223, 142, 28, 150, 28, 148, 54, 215, 53, 28, 224, 156, 177, 194, 219, 91, 225, 156, 299, 142, 38, 209, 28, 10, 194, 203, 62, 216, 91, 224, 142, 28, 226, 156, 164, 194, 218, 1, 209, 91, 219, 4, 224, 91, 299, 142, 28, 227, 156, 177, 194, 226, 195, 226, 91, 225, 156, 299, 142, 38, 209, 28, 150, 28, 27, 53, 28, 226, 156, 164, 194, 218, 1, 209, 91, 219, 91, 299, 142, 28, 227, 156, 177, 194, 226, 195, 226, 91, 225, 156, 299, 142, 38, 209, 28, 48, 28, 228, 156, 300, 38, 108, 194, 227, 62, 210, 142, 28, 10, 194, 204, 62, 216, 91, 228, 142, 28, 220, 156, 218, 1, 209, 28, 229, 156, 49, 194, 200, 62, 218, 91, 220, 156, 220, 142, 69, 222, 194, 121, 142, 28, 148, 212, 53, 28, 230, 156, 49, 194, 201, 62, 218, 91, 220, 156, 220, 142, 69, 222, 194, 121, 142, 28, 150, 28, 231, 156, 194, 219, 4, 224, 142, 195, 228, 148, 54, 215, 27, 219, 195, 228, 28, 232, 156, 231, 195, 229, 62, 230, 148, 212, 27, 231, 195, 229, 28, 148, 213, 86, 214, 53, 28, 223, 156, 49, 194, 202, 62, 218, 91, 220, 156, 220, 142, 69, 222, 194, 121, 142, 28, 232, 21, 223, 195, 165, 194, 223, 142, 28, 150, 28, 10, 194, 199, 62, 218, 91, 232, 91, 220, 156, 220, 142, 28, 3, 28]}, {"code": "def layer_norm_bwd_kernel(\n    X,\n    W,\n    B,\n    Z,\n    Y,\n    DY,\n    DX,\n    DW,\n    DB,\n    DZ,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_z_row,\n    stride_y_row,\n    stride_dy_row,\n    stride_dx_row,\n    stride_dz_row,\n    stride_dw_row,\n    stride_db_row,\n    M,\n    N,\n    eps,\n    rows_per_program,\n    NORM_BEFORE_GATE: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_Z: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n\n    row_block_id = tl.program_id(0)\n    group = tl.program_id(1)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row + group * N\n    if HAS_Z:\n        Z += row_start * stride_z_row + group * N\n        DZ += row_start * stride_dz_row + group * N\n    DY += row_start * stride_dy_row + group * N\n    DX += row_start * stride_dx_row + group * N\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:\n        B += group * N\n        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n            x_og = x\n            x = x_og * z * tl.sigmoid(z)\n        rstd = tl.load(Rstd + row)\n\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if HAS_Z and NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n            z_sigmoid = tl.sigmoid(z)\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            if RECOMPUTE_OUTPUT:\n                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)\n            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dy *= z * z_sigmoid\n        else:\n            if RECOMPUTE_OUTPUT:\n                y = xhat * w + b if HAS_BIAS else xhat * w\n                tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        if not IS_RMS_NORM:\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            dx = (wdy - xhat * c1) * rstd\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z_sigmoid = tl.sigmoid(z)\n            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dx *= z * z_sigmoid\n\n        tl.store(DX + cols, dx, mask=mask)\n\n        X += stride_x_row\n        if HAS_Z:\n            Z += stride_z_row\n            DZ += stride_dz_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db, mask=mask)", "encoded": [26, 298, 194, 198, 92, 199, 92, 200, 92, 201, 92, 202, 92, 203, 92, 204, 92, 205, 92, 206, 92, 207, 92, 208, 92, 209, 92, 210, 92, 211, 92, 212, 92, 213, 92, 214, 92, 215, 92, 216, 92, 217, 92, 218, 92, 219, 92, 220, 92, 221, 92, 222, 54, 6, 92, 223, 54, 6, 92, 224, 54, 6, 92, 225, 54, 6, 92, 226, 54, 6, 92, 227, 54, 6, 142, 54, 28, -1, 228, 156, 138, 194, 299, 142, 28, 229, 156, 138, 194, 300, 142, 28, 230, 156, 228, 195, 221, 28, 231, 156, 64, 194, 299, 92, 227, 142, 28, 232, 156, 231, 1, 219, 28, 198, 139, 230, 195, 210, 63, 229, 195, 219, 28, 148, 225, 54, 28, 201, 139, 230, 195, 211, 63, 229, 195, 219, 28, 207, 139, 230, 195, 215, 63, 229, 195, 219, 28, 150, 28, 203, 139, 230, 195, 213, 63, 229, 195, 219, 28, 204, 139, 230, 195, 214, 63, 229, 195, 219, 28, 148, 226, 54, 28, 202, 139, 230, 195, 212, 63, 229, 195, 219, 28, 150, 28, 148, 55, 223, 54, 28, 208, 139, 229, 195, 218, 28, 150, 28, 209, 139, 229, 195, 218, 28, 199, 139, 229, 195, 219, 28, 233, 156, 48, 194, 199, 63, 231, 92, 232, 156, 232, 142, 70, 234, 194, 122, 142, 28, 148, 194, 226, 119, 225, 142, 87, 224, 54, 28, 200, 139, 229, 195, 219, 28, 235, 156, 48, 194, 200, 63, 231, 92, 232, 156, 232, 92, 236, 156, 299, 142, 70, 234, 194, 122, 142, 28, 150, 28, 237, 156, 143, 194, 194, 227, 92, 142, 92, 77, 156, 122, 142, 28, 148, 224, 54, 28, 238, 156, 143, 194, 194, 227, 92, 142, 92, 77, 156, 122, 142, 28, 150, 28, 239, 156, 35, 194, 194, 228, 63, 300, 142, 195, 221, 92, 218, 142, 28, 114, 240, 129, 5, 194, 230, 92, 239, 142, 54, 28, 241, 156, 48, 194, 198, 63, 231, 92, 232, 156, 232, 92, 236, 156, 299, 142, 70, 234, 194, 122, 142, 28, 242, 156, 48, 194, 203, 63, 231, 92, 232, 156, 232, 92, 236, 156, 299, 142, 70, 234, 194, 122, 142, 28, 148, 55, 223, 54, 28, 243, 156, 48, 194, 208, 63, 240, 142, 28, 150, 28, 148, 225, 87, 194, 55, 222, 142, 54, 28, 244, 156, 48, 194, 201, 63, 231, 92, 232, 156, 232, 92, 236, 156, 299, 142, 70, 234, 194, 122, 142, 28, 245, 156, 241, 28, 241, 156, 245, 195, 244, 195, 165, 194, 244, 142, 28, 150, 28, 246, 156, 48, 194, 209, 63, 240, 142, 28, 247, 156, 194, 241, 4, 243, 142, 195, 246, 148, 55, 223, 27, 241, 195, 246, 28, 247, 156, 164, 194, 232, 92, 247, 92, 299, 142, 28, 148, 225, 87, 222, 54, 28, 244, 156, 48, 194, 201, 63, 231, 92, 232, 156, 232, 92, 236, 156, 299, 142, 70, 234, 194, 122, 142, 28, 248, 156, 165, 194, 244, 142, 28, 249, 156, 247, 195, 233, 63, 235, 148, 224, 27, 247, 195, 233, 28, 148, 226, 54, 28, 10, 194, 202, 63, 231, 92, 249, 195, 244, 195, 248, 92, 232, 156, 232, 142, 28, 150, 28, 250, 156, 242, 195, 249, 195, 248, 195, 194, 300, 63, 244, 195, 194, 300, 4, 248, 142, 142, 28, 10, 194, 207, 63, 231, 92, 250, 92, 232, 156, 232, 142, 28, 242, 21, 244, 195, 248, 28, 57, 28, 33, 226, 54, 28, 249, 156, 247, 195, 233, 63, 235, 148, 224, 27, 247, 195, 233, 28, 10, 194, 202, 63, 231, 92, 249, 92, 232, 156, 232, 142, 28, 150, 28, 251, 156, 233, 195, 242, 28, 252, 156, 177, 194, 247, 195, 251, 92, 253, 156, 299, 142, 38, 219, 28, 148, 55, 223, 54, 28, 254, 156, 177, 194, 251, 92, 253, 156, 299, 142, 38, 219, 28, 255, 156, 194, 251, 4, 194, 247, 195, 252, 63, 254, 142, 142, 195, 246, 28, 150, 28, 27, 54, 28, 255, 156, 194, 251, 4, 247, 195, 252, 142, 195, 246, 28, 49, 28, 237, 139, 242, 195, 247, 28, 148, 224, 54, 28, 238, 139, 242, 28, 150, 28, 148, 225, 87, 194, 55, 222, 142, 54, 28, 248, 156, 165, 194, 244, 142, 28, 250, 156, 255, 195, 245, 195, 248, 195, 194, 300, 63, 244, 195, 194, 300, 4, 248, 142, 142, 28, 10, 194, 207, 63, 231, 92, 250, 92, 232, 156, 232, 142, 28, 255, 21, 244, 195, 248, 28, 150, 28, 10, 194, 204, 63, 231, 92, 255, 92, 232, 156, 232, 142, 28, 198, 139, 210, 28, 148, 225, 54, 28, 201, 139, 211, 28, 207, 139, 215, 28, 150, 28, 148, 226, 54, 28, 202, 139, 212, 28, 150, 28, 203, 139, 213, 28, 204, 139, 214, 28, 66, 28, 10, 194, 205, 63, 228, 195, 216, 63, 229, 195, 219, 63, 231, 92, 237, 92, 232, 156, 232, 142, 28, 148, 224, 54, 28, 10, 194, 206, 63, 228, 195, 217, 63, 229, 195, 219, 63, 231, 92, 238, 92, 232, 156, 232, 142, 28, 150, 28, 3, 28]}, {"code": "def rotary_embedding_kernel(\n    x,\n    cos,\n    sin,\n    y,\n    cu_seqlens,\n    chunk_indices,\n    seq_offsets,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    D: tl.constexpr,\n    R: tl.constexpr,\n    TR: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    INTERLEAVED: tl.constexpr,\n    CONJUGATE: tl.constexpr,\n):\n    i_t, i_b, i_h = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n), tl.load(cu_seqlens + i_n + 1)\n        T = eos - bos\n        x = x + bos * H * D + i_h * D\n        y = y + bos * H * D + i_h * D\n    else:\n        i_n = i_b\n        x = x + i_n * T * H * D + i_h * D\n        y = y + i_n * T * H * D + i_h * D\n\n    if i_t * BT >= T:\n        return\n\n    o_t = i_t * BT + tl.arange(0, BT)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        o_cs = o_t + seq_offsets\n    else:\n        o_cs = o_t + tl.load(seq_offsets + i_n)\n    m_t = (o_t >= 0) & (o_t < T) & (o_cs >= 0) & (o_cs < TR)\n\n    if not INTERLEAVED:\n\n        o_r = tl.arange(0, BD // 2)\n        p_x = x + o_t[:, None] * H * D + o_r[None, :]\n        p_cos = cos + (o_cs[:, None] * R + o_r[None, :])\n        p_sin = sin + (o_cs[:, None] * R + o_r[None, :])\n        mask = m_t[:, None] & (o_r < R)[None, :]\n\n        b_cos = tl.load(p_cos, mask=mask, other=1.0).to(tl.float32)\n        b_sin = tl.load(p_sin, mask=mask, other=0.0).to(tl.float32)\n        b_x0 = tl.load(p_x, mask=mask, other=0.0).to(tl.float32)\n        b_x1 = tl.load(p_x + R, mask=mask, other=0.0).to(tl.float32)\n        if CONJUGATE:\n            b_sin = -b_sin\n        b_o0 = b_x0 * b_cos - b_x1 * b_sin\n        b_o1 = b_x0 * b_sin + b_x1 * b_cos\n\n        p_y = y + (o_t[:, None] * H * D + o_r[None, :])\n        tl.store(p_y, b_o0, mask=mask)\n        tl.store(p_y + R, b_o1, mask=mask)\n    else:\n\n        o_d = tl.arange(0, BD)\n        o_d_swap = o_d + ((o_d + 1) % 2) * 2 - 1\n        o_d_repeat = tl.arange(0, BD) // 2\n        p_x0 = x + o_t[:, None] * H * D + o_d[None, :]\n        p_x1 = x + o_t[:, None] * H * D + o_d_swap[None, :]\n        p_cos = cos + (o_cs[:, None] * R + o_d_repeat[None, :])\n        p_sin = sin + (o_cs[:, None] * R + o_d_repeat[None, :])\n        mask = m_t[:, None] & (o_d_repeat < R)[None, :]\n\n        b_cos = tl.load(p_cos, mask=mask, other=1.0).to(tl.float32)\n        b_sin = tl.load(p_sin, mask=mask, other=0.0).to(tl.float32)\n        b_x0 = tl.load(p_x0, mask=mask, other=0.0).to(tl.float32)\n        b_x1 = tl.load(p_x1, mask=mask, other=0.0).to(tl.float32)\n        if CONJUGATE:\n            b_sin = -b_sin\n        b_o0 = b_x0 * b_cos\n        b_o1 = b_x1 * b_sin\n        b_y = tl.where(o_d[None, :] % 2 == 0, b_o0 - b_o1, b_o0 + b_o1)\n        p_y = y + (o_t[:, None] * H * D + o_d[None, :])\n        tl.store(p_y, b_y, mask=mask)", "encoded": [26, 298, 194, 198, 91, 199, 91, 200, 91, 201, 91, 202, 91, 203, 91, 204, 91, 205, 91, 206, 53, 6, 91, 207, 53, 6, 91, 208, 53, 6, 91, 209, 53, 6, 91, 210, 53, 6, 91, 211, 53, 6, 91, 212, 53, 6, 91, 213, 53, 6, 91, 214, 53, 6, 91, 215, 53, 6, 91, 216, 53, 6, 142, 53, 28, -1, 217, 91, 218, 91, 219, 156, 194, 138, 194, 299, 142, 91, 138, 194, 300, 142, 91, 138, 194, 301, 142, 142, 28, 148, 214, 53, 28, 220, 91, 217, 156, 194, 49, 194, 203, 62, 217, 195, 301, 142, 69, 221, 194, 197, 142, 91, 49, 194, 203, 62, 217, 195, 301, 62, 300, 142, 69, 221, 194, 197, 142, 142, 28, 222, 91, 223, 156, 194, 49, 194, 202, 62, 220, 142, 91, 49, 194, 202, 62, 220, 62, 300, 142, 142, 28, 205, 156, 223, 4, 222, 28, 198, 156, 198, 62, 222, 195, 207, 195, 208, 62, 219, 195, 208, 28, 201, 156, 201, 62, 222, 195, 207, 195, 208, 62, 219, 195, 208, 28, 150, 28, 27, 53, 28, 220, 156, 218, 28, 198, 156, 198, 62, 220, 195, 205, 195, 207, 195, 208, 62, 219, 195, 208, 28, 201, 156, 201, 62, 220, 195, 205, 195, 207, 195, 208, 62, 219, 195, 208, 28, 48, 28, 148, 217, 195, 211, 120, 205, 53, 28, 184, 28, 150, 28, 224, 156, 217, 195, 211, 62, 63, 194, 299, 91, 211, 142, 28, 148, 54, 213, 53, 28, 225, 156, 224, 62, 204, 28, 150, 28, 27, 53, 28, 225, 156, 224, 62, 49, 194, 204, 62, 220, 142, 28, 48, 28, 226, 156, 194, 224, 120, 299, 142, 140, 194, 224, 1, 205, 142, 140, 194, 225, 120, 299, 142, 140, 194, 225, 1, 210, 142, 28, 148, 54, 215, 53, 28, 227, 156, 63, 194, 299, 91, 212, 43, 301, 142, 28, 228, 156, 198, 62, 224, 183, 53, 91, 161, 24, 195, 207, 195, 208, 62, 227, 183, 161, 91, 53, 24, 28, 229, 156, 199, 62, 194, 225, 183, 53, 91, 161, 24, 195, 209, 62, 227, 183, 161, 91, 53, 24, 142, 28, 230, 156, 200, 62, 194, 225, 183, 53, 91, 161, 24, 195, 209, 62, 227, 183, 161, 91, 53, 24, 142, 28, 231, 156, 226, 183, 53, 91, 161, 24, 140, 194, 227, 1, 209, 142, 183, 161, 91, 53, 24, 28, 232, 156, 49, 194, 229, 91, 231, 156, 231, 91, 233, 156, 300, 142, 69, 221, 194, 121, 142, 28, 234, 156, 49, 194, 230, 91, 231, 156, 231, 91, 233, 156, 299, 142, 69, 221, 194, 121, 142, 28, 235, 156, 49, 194, 228, 91, 231, 156, 231, 91, 233, 156, 299, 142, 69, 221, 194, 121, 142, 28, 236, 156, 49, 194, 228, 62, 209, 91, 231, 156, 231, 91, 233, 156, 299, 142, 69, 221, 194, 121, 142, 28, 148, 216, 53, 28, 234, 156, 4, 234, 28, 150, 28, 237, 156, 235, 195, 232, 4, 236, 195, 234, 28, 238, 156, 235, 195, 234, 62, 236, 195, 232, 28, 239, 156, 201, 62, 194, 224, 183, 53, 91, 161, 24, 195, 207, 195, 208, 62, 227, 183, 161, 91, 53, 24, 142, 28, 10, 194, 239, 91, 237, 91, 231, 156, 231, 142, 28, 10, 194, 239, 62, 209, 91, 238, 91, 231, 156, 231, 142, 28, 150, 28, 27, 53, 28, 240, 156, 63, 194, 299, 91, 212, 142, 28, 241, 156, 240, 62, 194, 240, 62, 300, 142, 180, 301, 195, 301, 4, 300, 28, 242, 156, 63, 194, 299, 91, 212, 142, 43, 301, 28, 243, 156, 198, 62, 224, 183, 53, 91, 161, 24, 195, 207, 195, 208, 62, 240, 183, 161, 91, 53, 24, 28, 244, 156, 198, 62, 224, 183, 53, 91, 161, 24, 195, 207, 195, 208, 62, 241, 183, 161, 91, 53, 24, 28, 229, 156, 199, 62, 194, 225, 183, 53, 91, 161, 24, 195, 209, 62, 242, 183, 161, 91, 53, 24, 142, 28, 230, 156, 200, 62, 194, 225, 183, 53, 91, 161, 24, 195, 209, 62, 242, 183, 161, 91, 53, 24, 142, 28, 231, 156, 226, 183, 53, 91, 161, 24, 140, 194, 242, 1, 209, 142, 183, 161, 91, 53, 24, 28, 232, 156, 49, 194, 229, 91, 231, 156, 231, 91, 233, 156, 300, 142, 69, 221, 194, 121, 142, 28, 234, 156, 49, 194, 230, 91, 231, 156, 231, 91, 233, 156, 299, 142, 69, 221, 194, 121, 142, 28, 235, 156, 49, 194, 243, 91, 231, 156, 231, 91, 233, 156, 299, 142, 69, 221, 194, 121, 142, 28, 236, 156, 49, 194, 244, 91, 231, 156, 231, 91, 233, 156, 299, 142, 69, 221, 194, 121, 142, 28, 148, 216, 53, 28, 234, 156, 4, 234, 28, 150, 28, 237, 156, 235, 195, 232, 28, 238, 156, 236, 195, 234, 28, 245, 156, 164, 194, 240, 183, 161, 91, 53, 24, 180, 301, 64, 299, 91, 237, 4, 238, 91, 237, 62, 238, 142, 28, 239, 156, 201, 62, 194, 224, 183, 53, 91, 161, 24, 195, 207, 195, 208, 62, 240, 183, 161, 91, 53, 24, 142, 28, 10, 194, 239, 91, 245, 91, 231, 156, 231, 142, 28, 48, 28, 3, 28]}, {"code": "def token_shift_fwd_kernel(\n    x,\n    y,\n    cu_seqlens,\n    T,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_b, i_t = tl.program_id(0), tl.program_id(1)\n\n    if IS_VARLEN:\n        i_n = i_b\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        g_t = i_t + bos\n\n        if g_t >= eos:\n            return\n\n        is_first_pos = i_t == 0\n    else:\n        g_t = i_t\n        is_first_pos = g_t == 0\n\n    o_d = tl.arange(0, BD)\n    m_d = o_d < D\n\n    if IS_VARLEN:\n        base_offset = g_t * D + o_d\n    else:\n        base_offset = i_b * T * D + g_t * D + o_d\n\n    b_x = tl.load(x + base_offset, mask=m_d)\n\n    if is_first_pos:\n\n        tl.store(y + base_offset, -b_x, mask=m_d)\n    else:\n\n        if IS_VARLEN:\n            prev_offset = (g_t - 1) * D + o_d\n        else:\n            prev_offset = i_b * T * D + (g_t - 1) * D + o_d\n\n        prev_values = tl.load(x + prev_offset, mask=m_d)\n        delta = prev_values - b_x\n        tl.store(y + base_offset, delta, mask=m_d)", "encoded": [26, 298, 194, 198, 92, 199, 92, 200, 92, 201, 92, 202, 54, 6, 92, 203, 54, 6, 92, 204, 54, 6, 142, 54, 28, -1, 205, 92, 206, 156, 194, 138, 194, 299, 142, 92, 138, 194, 300, 142, 142, 28, 148, 204, 54, 28, 207, 156, 205, 28, 208, 92, 209, 156, 194, 48, 194, 200, 63, 207, 142, 70, 210, 194, 53, 142, 92, 48, 194, 200, 63, 207, 63, 300, 142, 70, 210, 194, 53, 142, 142, 28, 211, 156, 206, 63, 208, 28, 148, 211, 121, 209, 54, 28, 184, 28, 150, 28, 212, 156, 206, 65, 299, 28, 150, 28, 27, 54, 28, 211, 156, 206, 28, 212, 156, 211, 65, 299, 28, 49, 28, 213, 156, 64, 194, 299, 92, 203, 142, 28, 214, 156, 213, 1, 202, 28, 148, 204, 54, 28, 215, 156, 211, 195, 202, 63, 213, 28, 150, 28, 27, 54, 28, 215, 156, 205, 195, 201, 195, 202, 63, 211, 195, 202, 63, 213, 28, 49, 28, 216, 156, 48, 194, 198, 63, 215, 92, 217, 156, 214, 142, 28, 148, 212, 54, 28, 10, 194, 199, 63, 215, 92, 4, 216, 92, 217, 156, 214, 142, 28, 57, 28, 27, 54, 28, 148, 204, 54, 28, 218, 156, 194, 211, 4, 300, 142, 195, 202, 63, 213, 28, 150, 28, 27, 54, 28, 218, 156, 205, 195, 201, 195, 202, 63, 194, 211, 4, 300, 142, 195, 202, 63, 213, 28, 49, 28, 219, 156, 48, 194, 198, 63, 218, 92, 217, 156, 214, 142, 28, 220, 156, 219, 4, 216, 28, 10, 194, 199, 63, 215, 92, 220, 92, 217, 156, 214, 142, 28, 3, 28]}, {"code": "def token_shift_bwd_kernel(\n    dx,\n    dy,\n    cu_seqlens,\n    T,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_b, i_t = tl.program_id(0), tl.program_id(1)\n    if IS_VARLEN:\n        i_n = i_b\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        g_t = i_t + bos\n        if g_t >= eos:\n            return\n\n        is_last_pos = g_t == eos - 1\n    else:\n        g_t = i_t\n        is_last_pos = g_t == T - 1\n\n    o_d = tl.arange(0, BD)\n    m_d = o_d < D\n\n    if IS_VARLEN:\n        base_offset = g_t * D + o_d\n    else:\n        base_offset = i_b * T * D + g_t * D + o_d\n\n    b_dy = tl.load(dy + base_offset, mask=m_d)\n\n    if is_last_pos:\n\n        b_dx = -b_dy\n    else:\n\n        if IS_VARLEN:\n            next_offset = (g_t + 1) * D + o_d\n        else:\n            next_offset = i_b * T * D + (g_t + 1) * D + o_d\n\n        b_dx = -b_dy + tl.load(dy + next_offset, mask=m_d)\n\n    tl.store(dx + base_offset, b_dx, mask=m_d)", "encoded": [26, 298, 194, 198, 91, 199, 91, 200, 91, 201, 91, 202, 53, 6, 91, 203, 53, 6, 91, 204, 53, 6, 142, 53, 28, -1, 205, 91, 206, 156, 194, 138, 194, 299, 142, 91, 138, 194, 300, 142, 142, 28, 148, 204, 53, 28, 207, 156, 205, 28, 208, 91, 209, 156, 194, 49, 194, 200, 62, 207, 142, 69, 210, 194, 197, 142, 91, 49, 194, 200, 62, 207, 62, 300, 142, 69, 210, 194, 197, 142, 142, 28, 211, 156, 206, 62, 208, 28, 148, 211, 120, 209, 53, 28, 184, 28, 150, 28, 212, 156, 211, 64, 209, 4, 300, 28, 150, 28, 27, 53, 28, 211, 156, 206, 28, 212, 156, 211, 64, 201, 4, 300, 28, 48, 28, 213, 156, 63, 194, 299, 91, 203, 142, 28, 214, 156, 213, 1, 202, 28, 148, 204, 53, 28, 215, 156, 211, 195, 202, 62, 213, 28, 150, 28, 27, 53, 28, 215, 156, 205, 195, 201, 195, 202, 62, 211, 195, 202, 62, 213, 28, 48, 28, 216, 156, 49, 194, 199, 62, 215, 91, 217, 156, 214, 142, 28, 148, 212, 53, 28, 218, 156, 4, 216, 28, 56, 28, 27, 53, 28, 148, 204, 53, 28, 219, 156, 194, 211, 62, 300, 142, 195, 202, 62, 213, 28, 150, 28, 27, 53, 28, 219, 156, 205, 195, 201, 195, 202, 62, 194, 211, 62, 300, 142, 195, 202, 62, 213, 28, 48, 28, 218, 156, 4, 216, 62, 49, 194, 199, 62, 219, 91, 217, 156, 214, 142, 28, 10, 194, 198, 62, 215, 91, 218, 91, 217, 156, 214, 142, 28, 3, 28]}, {"code": "def chunk_abc_fwd_kernel_h(\n    k,\n    v,\n    z,\n    h,\n    h0,\n    ht,\n    T,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n    NORMK: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(\n            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n    if NORMK:\n        p_z0 = tl.make_block_ptr(\n            z + i_bh * T * K, (T * K,), (1,), (i_k * BK,), (BK,), (0,)\n        )\n    else:\n        p_z0 = tl.make_block_ptr(\n            z + i_bh * T * V, (T * V,), (1,), (i_v * BV,), (BV,), (0,)\n        )\n    b_zp = tl.load(p_z0).to(tl.float32)\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(\n            k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_h = tl.make_block_ptr(\n            h + i_bh * NT * K * V + i_t * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if NORMK:\n            p_zc = tl.make_block_ptr(\n                z + i_bh * T * K,\n                (T * K,),\n                (1,),\n                ((i_t * BT + BT - 1) * K + i_k * BK,),\n                (BK,),\n                (0,),\n            )\n\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = exp(b_zp - b_zc), b_zc\n\n            b_h = b_h * b_r[:, None]\n            b_k = exp(b_k - b_zc[:, None]).to(b_k.dtype)\n        else:\n            p_zc = tl.make_block_ptr(\n                z + i_bh * T * V,\n                (T * V,),\n                (1,),\n                ((i_t * BT + BT - 1) * V + i_v * BV,),\n                (BV,),\n                (0,),\n            )\n\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = exp(b_zp - b_zc), b_zc\n\n            b_h = b_h * b_r[None, :]\n            b_v = exp(b_v - b_zc[None, :]).to(b_v.dtype)\n\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n\n    if STORE_FINAL_STATE:\n        p_h = tl.make_block_ptr(\n            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))", "encoded": [26, 298, 194, 198, 92, 199, 92, 200, 92, 201, 92, 202, 92, 203, 92, 204, 92, 205, 54, 6, 92, 206, 54, 6, 92, 207, 54, 6, 92, 208, 54, 6, 92, 209, 54, 6, 92, 210, 54, 6, 92, 211, 54, 6, 92, 212, 54, 6, 92, 213, 54, 6, 142, 54, 28, -1, 214, 92, 215, 92, 216, 156, 194, 138, 194, 299, 142, 92, 138, 194, 300, 142, 92, 138, 194, 301, 142, 142, 28, 217, 156, 143, 194, 183, 208, 92, 209, 24, 92, 77, 156, 122, 142, 28, 148, 212, 54, 28, 218, 156, 175, 194, 202, 63, 216, 195, 205, 195, 206, 92, 194, 205, 92, 206, 142, 92, 194, 206, 92, 300, 142, 92, 194, 215, 195, 208, 92, 214, 195, 209, 142, 92, 194, 208, 92, 209, 142, 92, 194, 300, 92, 299, 142, 142, 28, 217, 139, 48, 194, 218, 92, 219, 156, 194, 299, 92, 300, 142, 142, 70, 220, 194, 122, 142, 28, 150, 28, 148, 211, 54, 28, 221, 156, 175, 194, 200, 63, 216, 195, 204, 195, 205, 92, 194, 204, 195, 205, 92, 142, 92, 194, 300, 92, 142, 92, 194, 215, 195, 208, 92, 142, 92, 194, 208, 92, 142, 92, 194, 299, 92, 142, 142, 28, 150, 28, 27, 54, 28, 221, 156, 175, 194, 200, 63, 216, 195, 204, 195, 206, 92, 194, 204, 195, 206, 92, 142, 92, 194, 300, 92, 142, 92, 194, 214, 195, 209, 92, 142, 92, 194, 209, 92, 142, 92, 194, 299, 92, 142, 142, 28, 49, 28, 222, 156, 48, 194, 221, 142, 70, 220, 194, 122, 142, 28, 114, 223, 129, 5, 194, 210, 142, 54, 28, 224, 156, 175, 194, 198, 63, 216, 195, 204, 195, 205, 92, 194, 205, 92, 204, 142, 92, 194, 300, 92, 205, 142, 92, 194, 215, 195, 208, 92, 223, 195, 207, 142, 92, 194, 208, 92, 207, 142, 92, 194, 299, 92, 300, 142, 142, 28, 225, 156, 175, 194, 199, 63, 216, 195, 204, 195, 206, 92, 194, 204, 92, 206, 142, 92, 194, 206, 92, 300, 142, 92, 194, 223, 195, 207, 92, 214, 195, 209, 142, 92, 194, 207, 92, 209, 142, 92, 194, 300, 92, 299, 142, 142, 28, 218, 156, 175, 194, 201, 63, 216, 195, 210, 195, 205, 195, 206, 63, 223, 195, 205, 195, 206, 92, 194, 205, 92, 206, 142, 92, 194, 206, 92, 300, 142, 92, 194, 215, 195, 208, 92, 214, 195, 209, 142, 92, 194, 208, 92, 209, 142, 92, 194, 300, 92, 299, 142, 142, 28, 10, 194, 218, 92, 217, 70, 220, 194, 218, 70, 77, 70, 97, 142, 92, 219, 156, 194, 299, 92, 300, 142, 142, 28, 226, 156, 48, 194, 224, 92, 219, 156, 194, 299, 92, 300, 142, 142, 28, 227, 156, 48, 194, 225, 92, 219, 156, 194, 299, 92, 300, 142, 142, 28, 148, 211, 54, 28, 228, 156, 175, 194, 200, 63, 216, 195, 204, 195, 205, 92, 194, 204, 195, 205, 92, 142, 92, 194, 300, 92, 142, 92, 194, 194, 223, 195, 207, 63, 207, 4, 300, 142, 195, 205, 63, 215, 195, 208, 92, 142, 92, 194, 208, 92, 142, 92, 194, 299, 92, 142, 142, 28, 229, 156, 48, 194, 228, 92, 219, 156, 194, 299, 92, 142, 142, 28, 230, 92, 222, 156, 194, 166, 194, 222, 4, 229, 142, 92, 229, 142, 28, 217, 156, 217, 195, 230, 183, 54, 92, 161, 24, 28, 226, 156, 166, 194, 226, 4, 229, 183, 54, 92, 161, 24, 142, 70, 220, 194, 226, 70, 77, 142, 28, 150, 28, 27, 54, 28, 228, 156, 175, 194, 200, 63, 216, 195, 204, 195, 206, 92, 194, 204, 195, 206, 92, 142, 92, 194, 300, 92, 142, 92, 194, 194, 223, 195, 207, 63, 207, 4, 300, 142, 195, 206, 63, 214, 195, 209, 92, 142, 92, 194, 209, 92, 142, 92, 194, 299, 92, 142, 142, 28, 229, 156, 48, 194, 228, 92, 219, 156, 194, 299, 92, 142, 142, 28, 230, 92, 222, 156, 194, 166, 194, 222, 4, 229, 142, 92, 229, 142, 28, 217, 156, 217, 195, 230, 183, 161, 92, 54, 24, 28, 227, 156, 166, 194, 227, 4, 229, 183, 161, 92, 54, 24, 142, 70, 220, 194, 227, 70, 77, 142, 28, 49, 28, 217, 139, 15, 194, 226, 92, 227, 92, 231, 156, 51, 142, 28, 66, 28, 148, 213, 54, 28, 218, 156, 175, 194, 203, 63, 216, 195, 205, 195, 206, 92, 194, 205, 92, 206, 142, 92, 194, 206, 92, 300, 142, 92, 194, 215, 195, 208, 92, 214, 195, 209, 142, 92, 194, 208, 92, 209, 142, 92, 194, 300, 92, 299, 142, 142, 28, 10, 194, 218, 92, 217, 70, 220, 194, 218, 70, 77, 70, 97, 142, 92, 219, 156, 194, 299, 92, 300, 142, 142, 28, 150, 28, 3, 28]}, {"code": "def chunk_abc_fwd_kernel_intra_K(\n    v,\n    z,\n    o,\n    A,\n    T,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BV: tl.constexpr,\n    NC: tl.constexpr,\n):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n\n    p_z = tl.make_block_ptr(\n        z + i_bh * T * V,\n        (T, V),\n        (V, 1),\n        (i_t * BT + i_i * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n    p_zn = tl.make_block_ptr(\n        z + i_bh * T * V,\n        (T * V,),\n        (1,),\n        ((i_t * BT + i_i * BC) * V + i_v * BV,),\n        (BV,),\n        (0,),\n    )\n\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n\n    b_o = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_A = tl.make_block_ptr(\n            A + i_bh * T * BT,\n            (T, BT),\n            (BT, 1),\n            (i_t * BT + i_i * BC, i_j * BC),\n            (BC, BC),\n            (1, 0),\n        )\n        p_v = tl.make_block_ptr(\n            v + i_bh * T * V,\n            (T, V),\n            (V, 1),\n            (i_t * BT + i_j * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_o += tl.dot(b_A, exp(b_v - b_zn[None, :]).to(b_v.dtype), allow_tf32=False)\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_o *= exp(b_zn[None, :] - b_z)\n\n    o_i = tl.arange(0, BC)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_i * BC\n    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n    for j in range(0, BC):\n        p_v = tl.make_block_ptr(\n            v + i_bh * T * V,\n            (T * V,),\n            (1,),\n            ((i_t * BT + i_i * BC + j) * V + i_v * BV,),\n            (BV,),\n            (0,),\n        )\n\n        b_A = tl.load(A + o_A + j, mask=m_A, other=0)\n\n        b_v = tl.load(p_v, boundary_check=(0,)).to(tl.float32)\n\n        m_i = o_i[:, None] >= j\n        b_o += tl.where(m_i, b_A[:, None] * exp(b_v[None, :] - b_z), 0)\n    p_o = tl.make_block_ptr(\n        o + i_bh * T * V,\n        (T, V),\n        (V, 1),\n        (i_t * BT + i_i * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [26, 298, 194, 198, 91, 199, 91, 200, 91, 201, 91, 202, 91, 203, 53, 6, 91, 204, 53, 6, 91, 205, 53, 6, 91, 206, 53, 6, 91, 207, 53, 6, 142, 53, 28, -1, 208, 91, 209, 91, 210, 156, 194, 138, 194, 299, 142, 91, 138, 194, 300, 142, 91, 138, 194, 301, 142, 142, 28, 211, 91, 212, 156, 194, 209, 43, 207, 91, 209, 180, 207, 142, 28, 213, 156, 175, 194, 199, 62, 210, 195, 202, 195, 203, 91, 194, 202, 91, 203, 142, 91, 194, 203, 91, 300, 142, 91, 194, 211, 195, 204, 62, 212, 195, 205, 91, 208, 195, 206, 142, 91, 194, 205, 91, 206, 142, 91, 194, 300, 91, 299, 142, 142, 28, 214, 156, 175, 194, 199, 62, 210, 195, 202, 195, 203, 91, 194, 202, 195, 203, 91, 142, 91, 194, 300, 91, 142, 91, 194, 194, 211, 195, 204, 62, 212, 195, 205, 142, 195, 203, 62, 208, 195, 206, 91, 142, 91, 194, 206, 91, 142, 91, 194, 299, 91, 142, 142, 28, 215, 156, 49, 194, 214, 91, 216, 156, 194, 299, 91, 142, 142, 28, 217, 156, 143, 194, 183, 205, 91, 206, 24, 91, 76, 156, 121, 142, 28, 113, 218, 129, 5, 194, 299, 91, 212, 142, 53, 28, 219, 156, 175, 194, 201, 62, 210, 195, 202, 195, 204, 91, 194, 202, 91, 204, 142, 91, 194, 204, 91, 300, 142, 91, 194, 211, 195, 204, 62, 212, 195, 205, 91, 218, 195, 205, 142, 91, 194, 205, 91, 205, 142, 91, 194, 300, 91, 299, 142, 142, 28, 220, 156, 175, 194, 198, 62, 210, 195, 202, 195, 203, 91, 194, 202, 91, 203, 142, 91, 194, 203, 91, 300, 142, 91, 194, 211, 195, 204, 62, 218, 195, 205, 91, 208, 195, 206, 142, 91, 194, 205, 91, 206, 142, 91, 194, 300, 91, 299, 142, 142, 28, 221, 156, 49, 194, 220, 91, 216, 156, 194, 299, 91, 300, 142, 142, 28, 222, 156, 49, 194, 219, 91, 216, 156, 194, 299, 91, 300, 142, 142, 28, 217, 139, 15, 194, 222, 91, 166, 194, 221, 4, 215, 183, 161, 91, 53, 24, 142, 69, 223, 194, 221, 69, 76, 142, 91, 224, 156, 51, 142, 28, 65, 28, 225, 156, 49, 194, 213, 91, 216, 156, 194, 299, 91, 300, 142, 142, 28, 217, 21, 166, 194, 215, 183, 161, 91, 53, 24, 4, 225, 142, 28, 226, 156, 63, 194, 299, 91, 205, 142, 28, 227, 156, 210, 195, 202, 195, 204, 62, 194, 211, 195, 204, 62, 212, 195, 205, 62, 63, 194, 299, 91, 205, 142, 142, 195, 204, 62, 212, 195, 205, 28, 228, 156, 211, 195, 204, 62, 212, 195, 205, 62, 63, 194, 299, 91, 205, 142, 1, 202, 28, 113, 229, 129, 5, 194, 299, 91, 205, 142, 53, 28, 220, 156, 175, 194, 198, 62, 210, 195, 202, 195, 203, 91, 194, 202, 195, 203, 91, 142, 91, 194, 300, 91, 142, 91, 194, 194, 211, 195, 204, 62, 212, 195, 205, 62, 229, 142, 195, 203, 62, 208, 195, 206, 91, 142, 91, 194, 206, 91, 142, 91, 194, 299, 91, 142, 142, 28, 222, 156, 49, 194, 201, 62, 227, 62, 229, 91, 230, 156, 228, 91, 231, 156, 299, 142, 28, 221, 156, 49, 194, 220, 91, 216, 156, 194, 299, 91, 142, 142, 69, 223, 194, 121, 142, 28, 232, 156, 226, 183, 53, 91, 161, 24, 120, 229, 28, 217, 139, 164, 194, 232, 91, 222, 183, 53, 91, 161, 24, 195, 166, 194, 221, 183, 161, 91, 53, 24, 4, 225, 142, 91, 299, 142, 28, 65, 28, 233, 156, 175, 194, 200, 62, 210, 195, 202, 195, 203, 91, 194, 202, 91, 203, 142, 91, 194, 203, 91, 300, 142, 91, 194, 211, 195, 204, 62, 212, 195, 205, 91, 208, 195, 206, 142, 91, 194, 205, 91, 206, 142, 91, 194, 300, 91, 299, 142, 142, 28, 10, 194, 233, 91, 217, 69, 223, 194, 233, 69, 76, 69, 96, 142, 91, 216, 156, 194, 299, 91, 300, 142, 142, 28, 3, 28]}, {"code": "def chunk_abc_fwd_kernel_K(\n    q,\n    k,\n    z,\n    h,\n    o,\n    A,\n    scale,\n    T,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(\n            q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_k = tl.make_block_ptr(\n            k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)\n        )\n        p_h = tl.make_block_ptr(\n            h + i_bh * NT * K * V + i_t * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n\n        b_A += tl.dot(b_q, b_k, allow_tf32=False)\n    p_z = tl.make_block_ptr(\n        z + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_o = tl.make_block_ptr(\n        o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n\n    p_zp = tl.make_block_ptr(\n        z + i_bh * T * V, (T * V,), (1,), (i_p * V + i_v * BV,), (BV,), (0,)\n    )\n    b_zp = tl.load(p_zp, boundary_check=(0,))\n    b_o = b_o * exp(b_zp[None, :] - b_z)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n    p_A = tl.make_block_ptr(\n        A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n\n    b_A = tl.where(m_s, b_A, 0.0)\n    if i_v == 0:\n        tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))", "encoded": [26, 298, 194, 198, 92, 199, 92, 200, 92, 201, 92, 202, 92, 203, 92, 204, 92, 205, 92, 206, 54, 6, 92, 207, 54, 6, 92, 208, 54, 6, 92, 209, 54, 6, 92, 210, 54, 6, 92, 211, 54, 6, 142, 54, 28, -1, 212, 92, 213, 92, 214, 156, 194, 138, 194, 299, 142, 92, 138, 194, 300, 142, 92, 138, 194, 301, 142, 142, 28, 215, 156, 155, 194, 213, 195, 208, 4, 300, 92, 299, 142, 28, 216, 156, 64, 194, 299, 92, 208, 142, 28, 217, 156, 216, 183, 54, 92, 161, 24, 121, 216, 183, 161, 92, 54, 24, 28, 218, 156, 143, 194, 183, 208, 92, 210, 24, 92, 77, 156, 122, 142, 28, 219, 156, 143, 194, 183, 208, 92, 208, 24, 92, 77, 156, 122, 142, 28, 114, 220, 129, 5, 194, 56, 194, 206, 92, 209, 142, 142, 54, 28, 221, 156, 175, 194, 198, 63, 214, 195, 205, 195, 206, 92, 194, 205, 92, 206, 142, 92, 194, 206, 92, 300, 142, 92, 194, 213, 195, 208, 92, 220, 195, 209, 142, 92, 194, 208, 92, 209, 142, 92, 194, 300, 92, 299, 142, 142, 28, 222, 156, 175, 194, 199, 63, 214, 195, 205, 195, 206, 92, 194, 206, 92, 205, 142, 92, 194, 300, 92, 206, 142, 92, 194, 220, 195, 209, 92, 213, 195, 208, 142, 92, 194, 209, 92, 208, 142, 92, 194, 299, 92, 300, 142, 142, 28, 223, 156, 175, 194, 201, 63, 214, 195, 211, 195, 206, 195, 207, 63, 213, 195, 206, 195, 207, 92, 194, 206, 92, 207, 142, 92, 194, 207, 92, 300, 142, 92, 194, 220, 195, 209, 92, 212, 195, 210, 142, 92, 194, 209, 92, 210, 142, 92, 194, 300, 92, 299, 142, 142, 28, 224, 156, 48, 194, 221, 92, 225, 156, 194, 299, 92, 300, 142, 142, 28, 224, 156, 194, 224, 195, 204, 142, 70, 226, 194, 224, 70, 77, 142, 28, 227, 156, 48, 194, 222, 92, 225, 156, 194, 299, 92, 300, 142, 142, 28, 228, 156, 48, 194, 223, 92, 225, 156, 194, 299, 92, 300, 142, 142, 28, 218, 139, 15, 194, 224, 92, 228, 92, 229, 156, 51, 142, 28, 219, 139, 15, 194, 224, 92, 227, 92, 229, 156, 51, 142, 28, 66, 28, 230, 156, 175, 194, 200, 63, 214, 195, 205, 195, 207, 92, 194, 205, 92, 207, 142, 92, 194, 207, 92, 300, 142, 92, 194, 213, 195, 208, 92, 212, 195, 210, 142, 92, 194, 208, 92, 210, 142, 92, 194, 300, 92, 299, 142, 142, 28, 231, 156, 175, 194, 202, 63, 214, 195, 205, 195, 207, 92, 194, 205, 92, 207, 142, 92, 194, 207, 92, 300, 142, 92, 194, 213, 195, 208, 92, 212, 195, 210, 142, 92, 194, 208, 92, 210, 142, 92, 194, 300, 92, 299, 142, 142, 28, 232, 156, 48, 194, 230, 92, 225, 156, 194, 299, 92, 300, 142, 142, 28, 233, 156, 175, 194, 200, 63, 214, 195, 205, 195, 207, 92, 194, 205, 195, 207, 92, 142, 92, 194, 300, 92, 142, 92, 194, 215, 195, 207, 63, 212, 195, 210, 92, 142, 92, 194, 210, 92, 142, 92, 194, 299, 92, 142, 142, 28, 234, 156, 48, 194, 233, 92, 225, 156, 194, 299, 92, 142, 142, 28, 218, 156, 218, 195, 166, 194, 234, 183, 161, 92, 54, 24, 4, 232, 142, 28, 10, 194, 231, 92, 218, 70, 226, 194, 231, 70, 77, 70, 97, 142, 92, 225, 156, 194, 299, 92, 300, 142, 142, 28, 235, 156, 175, 194, 203, 63, 214, 195, 205, 195, 208, 92, 194, 205, 92, 208, 142, 92, 194, 208, 92, 300, 142, 92, 194, 213, 195, 208, 92, 299, 142, 92, 194, 208, 92, 208, 142, 92, 194, 300, 92, 299, 142, 142, 28, 219, 156, 164, 194, 217, 92, 219, 92, 299, 142, 28, 148, 212, 65, 299, 54, 28, 10, 194, 235, 92, 219, 70, 226, 194, 235, 70, 77, 70, 97, 142, 92, 225, 156, 194, 299, 92, 300, 142, 142, 28, 150, 28, 3, 28]}, {"code": "def chunk_abc_fwd_kernel_intra_V(\n    q,\n    k,\n    z,\n    A,\n    scale,\n    T,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr,\n):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i, i_j = i_c // (NC * NC), (i_c % (NC * NC)) // NC, (i_c % (NC * NC)) % NC\n    n_bh = tl.num_programs(2)\n\n    if i_i > i_j:\n        p_q = tl.make_block_ptr(\n            q + i_bh * T * K,\n            (T, K),\n            (K, 1),\n            (i_t * BT + i_i * BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_k = tl.make_block_ptr(\n            k + i_bh * T * K,\n            (K, T),\n            (1, K),\n            (i_k * BK, i_t * BT + i_j * BC),\n            (BK, BC),\n            (0, 1),\n        )\n        p_z = tl.make_block_ptr(\n            z + i_bh * T * K,\n            (T, K),\n            (K, 1),\n            (i_t * BT + i_i * BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_A = tl.make_block_ptr(\n            A + (i_k * n_bh + i_bh) * T * BT,\n            (T, BT),\n            (BT, 1),\n            (i_t * BT + i_i * BC, i_j * BC),\n            (BC, BC),\n            (1, 0),\n        )\n        p_zn = tl.make_block_ptr(\n            z + i_bh * T * K,\n            (T * K,),\n            (1,),\n            ((i_t * BT + i_i * BC) * K + i_k * BK,),\n            (BK,),\n            (0,),\n        )\n\n        b_zn = tl.load(p_zn, boundary_check=(0,))\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_q = (b_q * exp(b_zn[None, :] - b_z) * scale).to(b_q.dtype)\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k = exp(b_k - b_zn[:, None]).to(b_k.dtype)\n\n        b_A = tl.dot(b_q, b_k, allow_tf32=False)\n        tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))\n    elif i_i == i_j:\n        p_q = tl.make_block_ptr(\n            q + i_bh * T * K,\n            (T, K),\n            (K, 1),\n            (i_t * BT + i_i * BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_k = tl.make_block_ptr(\n            k + i_bh * T * K,\n            (T * K,),\n            (1,),\n            ((i_t * BT + i_j * BC) * K + i_k * BK,),\n            (BK,),\n            (0,),\n        )\n        p_z = tl.make_block_ptr(\n            z + i_bh * T * K,\n            (T, K),\n            (K, 1),\n            (i_t * BT + i_i * BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n\n        o_i = tl.arange(0, BC)\n        o_A = (\n            (i_bh + i_k * n_bh) * T * BT\n            + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT\n            + i_j * BC\n        )\n        m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n        for j in range(0, BC):\n\n            b_k = tl.load(p_k, boundary_check=(0,)).to(tl.float32)\n\n            b_A = tl.sum(b_q * exp(b_k[None, :] - b_z) * scale, 1)\n            b_A = tl.where(o_i >= j, b_A, 0.0)\n            tl.store(A + o_A + j, b_A.to(b_q.dtype), mask=m_A)\n\n            p_k = tl.advance(p_k, (K,))", "encoded": [26, 299, 195, 199, 91, 200, 91, 201, 91, 202, 91, 203, 91, 204, 91, 205, 53, 6, 91, 206, 53, 6, 91, 207, 53, 6, 91, 208, 53, 6, 91, 209, 53, 6, 143, 53, 28, -1, 210, 91, 211, 91, 212, 157, 195, 139, 195, 300, 143, 91, 139, 195, 301, 143, 91, 139, 195, 302, 143, 143, 28, 213, 91, 214, 91, 215, 157, 195, 211, 43, 195, 209, 196, 209, 143, 91, 211, 181, 195, 209, 196, 209, 143, 43, 209, 91, 211, 181, 195, 209, 196, 209, 143, 181, 209, 143, 28, 216, 157, 116, 195, 302, 143, 28, 149, 214, 104, 215, 53, 28, 217, 157, 176, 195, 199, 62, 212, 196, 204, 196, 205, 91, 195, 204, 91, 205, 143, 91, 195, 205, 91, 301, 143, 91, 195, 213, 196, 206, 62, 214, 196, 207, 91, 210, 196, 208, 143, 91, 195, 207, 91, 208, 143, 91, 195, 301, 91, 300, 143, 143, 28, 218, 157, 176, 195, 200, 62, 212, 196, 204, 196, 205, 91, 195, 205, 91, 204, 143, 91, 195, 301, 91, 205, 143, 91, 195, 210, 196, 208, 91, 213, 196, 206, 62, 215, 196, 207, 143, 91, 195, 208, 91, 207, 143, 91, 195, 300, 91, 301, 143, 143, 28, 219, 157, 176, 195, 201, 62, 212, 196, 204, 196, 205, 91, 195, 204, 91, 205, 143, 91, 195, 205, 91, 301, 143, 91, 195, 213, 196, 206, 62, 214, 196, 207, 91, 210, 196, 208, 143, 91, 195, 207, 91, 208, 143, 91, 195, 301, 91, 300, 143, 143, 28, 220, 157, 176, 195, 202, 62, 195, 210, 196, 216, 62, 212, 143, 196, 204, 196, 206, 91, 195, 204, 91, 206, 143, 91, 195, 206, 91, 301, 143, 91, 195, 213, 196, 206, 62, 214, 196, 207, 91, 215, 196, 207, 143, 91, 195, 207, 91, 207, 143, 91, 195, 301, 91, 300, 143, 143, 28, 221, 157, 176, 195, 201, 62, 212, 196, 204, 196, 205, 91, 195, 204, 196, 205, 91, 143, 91, 195, 301, 91, 143, 91, 195, 195, 213, 196, 206, 62, 214, 196, 207, 143, 196, 205, 62, 210, 196, 208, 91, 143, 91, 195, 208, 91, 143, 91, 195, 300, 91, 143, 143, 28, 222, 157, 49, 195, 221, 91, 223, 157, 195, 300, 91, 143, 143, 28, 224, 157, 49, 195, 217, 91, 223, 157, 195, 300, 91, 301, 143, 143, 28, 225, 157, 49, 195, 219, 91, 223, 157, 195, 300, 91, 301, 143, 143, 28, 224, 157, 195, 224, 196, 167, 195, 222, 184, 162, 91, 53, 24, 4, 225, 143, 196, 203, 143, 69, 226, 195, 224, 69, 76, 143, 28, 227, 157, 49, 195, 218, 91, 223, 157, 195, 300, 91, 301, 143, 143, 28, 227, 157, 167, 195, 227, 4, 222, 184, 53, 91, 162, 24, 143, 69, 226, 195, 227, 69, 76, 143, 28, 228, 157, 15, 195, 224, 91, 227, 91, 229, 157, 51, 143, 28, 10, 195, 220, 91, 228, 69, 226, 195, 202, 69, 76, 69, 96, 143, 91, 223, 157, 195, 300, 91, 301, 143, 143, 28, 56, 28, 33, 214, 64, 215, 53, 28, 217, 157, 176, 195, 199, 62, 212, 196, 204, 196, 205, 91, 195, 204, 91, 205, 143, 91, 195, 205, 91, 301, 143, 91, 195, 213, 196, 206, 62, 214, 196, 207, 91, 210, 196, 208, 143, 91, 195, 207, 91, 208, 143, 91, 195, 301, 91, 300, 143, 143, 28, 218, 157, 176, 195, 200, 62, 212, 196, 204, 196, 205, 91, 195, 204, 196, 205, 91, 143, 91, 195, 301, 91, 143, 91, 195, 195, 213, 196, 206, 62, 215, 196, 207, 143, 196, 205, 62, 210, 196, 208, 91, 143, 91, 195, 208, 91, 143, 91, 195, 300, 91, 143, 143, 28, 219, 157, 176, 195, 201, 62, 212, 196, 204, 196, 205, 91, 195, 204, 91, 205, 143, 91, 195, 205, 91, 301, 143, 91, 195, 213, 196, 206, 62, 214, 196, 207, 91, 210, 196, 208, 143, 91, 195, 207, 91, 208, 143, 91, 195, 301, 91, 300, 143, 143, 28, 224, 157, 49, 195, 217, 91, 223, 157, 195, 300, 91, 301, 143, 143, 28, 225, 157, 49, 195, 219, 91, 223, 157, 195, 300, 91, 301, 143, 143, 28, 230, 157, 63, 195, 300, 91, 207, 143, 28, 231, 157, 195, 212, 62, 210, 196, 216, 143, 196, 204, 196, 206, 62, 195, 213, 196, 206, 62, 214, 196, 207, 62, 63, 195, 300, 91, 207, 143, 143, 196, 206, 62, 215, 196, 207, 28, 232, 157, 213, 196, 206, 62, 214, 196, 207, 62, 63, 195, 300, 91, 207, 143, 1, 204, 28, 113, 233, 130, 5, 195, 300, 91, 207, 143, 53, 28, 227, 157, 49, 195, 218, 91, 223, 157, 195, 300, 91, 143, 143, 69, 226, 195, 122, 143, 28, 228, 157, 178, 195, 224, 196, 167, 195, 227, 184, 162, 91, 53, 24, 4, 225, 143, 196, 203, 91, 301, 143, 28, 228, 157, 165, 195, 230, 121, 233, 91, 228, 91, 300, 143, 28, 10, 195, 202, 62, 231, 62, 233, 91, 228, 69, 226, 195, 224, 69, 76, 143, 91, 234, 157, 232, 143, 28, 218, 157, 120, 195, 218, 91, 195, 205, 91, 143, 143, 28, 65, 28, 151, 28, 3, 28]}, {"code": "def chunk_abc_fwd_kernel_V(\n    q,\n    v,\n    z,\n    h,\n    o,\n    A,\n    scale,\n    T,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(\n            q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_z = tl.make_block_ptr(\n            z + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_h = tl.make_block_ptr(\n            h + i_bh * NT * K * V + i_t * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        p_zp = tl.make_block_ptr(\n            z + i_bh * T * K, (T * K,), (1,), (i_p * K + i_k * BK,), (BK,), (0,)\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n\n        b_zp = tl.load(p_zp, boundary_check=(0,))\n        b_q = (b_q * exp(b_zp[None, :] - b_z)).to(b_q.dtype)\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n\n        if i_k >= 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n    p_v = tl.make_block_ptr(\n        v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_o = tl.make_block_ptr(\n        o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_A = tl.make_block_ptr(\n        A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_o += tl.dot(b_A.to(b_v.dtype), b_v, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [26, 299, 195, 199, 92, 200, 92, 201, 92, 202, 92, 203, 92, 204, 92, 205, 92, 206, 92, 207, 54, 6, 92, 208, 54, 6, 92, 209, 54, 6, 92, 210, 54, 6, 92, 211, 54, 6, 92, 212, 54, 6, 143, 54, 28, -1, 213, 92, 214, 92, 215, 157, 195, 139, 195, 300, 143, 92, 139, 195, 301, 143, 92, 139, 195, 302, 143, 143, 28, 216, 157, 156, 195, 214, 196, 209, 4, 301, 92, 300, 143, 28, 217, 157, 144, 195, 184, 209, 92, 211, 24, 92, 77, 157, 123, 143, 28, 114, 218, 130, 5, 195, 56, 195, 207, 92, 210, 143, 143, 54, 28, 219, 157, 176, 195, 199, 63, 215, 196, 206, 196, 207, 92, 195, 206, 92, 207, 143, 92, 195, 207, 92, 301, 143, 92, 195, 214, 196, 209, 92, 218, 196, 210, 143, 92, 195, 209, 92, 210, 143, 92, 195, 301, 92, 300, 143, 143, 28, 220, 157, 176, 195, 201, 63, 215, 196, 206, 196, 207, 92, 195, 206, 92, 207, 143, 92, 195, 207, 92, 301, 143, 92, 195, 214, 196, 209, 92, 218, 196, 210, 143, 92, 195, 209, 92, 210, 143, 92, 195, 301, 92, 300, 143, 143, 28, 221, 157, 176, 195, 202, 63, 215, 196, 212, 196, 207, 196, 208, 63, 214, 196, 207, 196, 208, 92, 195, 207, 92, 208, 143, 92, 195, 208, 92, 301, 143, 92, 195, 218, 196, 210, 92, 213, 196, 211, 143, 92, 195, 210, 92, 211, 143, 92, 195, 301, 92, 300, 143, 143, 28, 222, 157, 176, 195, 201, 63, 215, 196, 206, 196, 207, 92, 195, 206, 196, 207, 92, 143, 92, 195, 301, 92, 143, 92, 195, 216, 196, 207, 63, 218, 196, 210, 92, 143, 92, 195, 210, 92, 143, 92, 195, 300, 92, 143, 143, 28, 223, 157, 48, 195, 219, 92, 224, 157, 195, 300, 92, 301, 143, 143, 28, 223, 157, 195, 223, 196, 205, 143, 70, 225, 195, 223, 70, 77, 143, 28, 226, 157, 48, 195, 220, 92, 224, 157, 195, 300, 92, 301, 143, 143, 28, 227, 157, 48, 195, 222, 92, 224, 157, 195, 300, 92, 143, 143, 28, 223, 157, 195, 223, 196, 167, 195, 227, 184, 162, 92, 54, 24, 4, 226, 143, 143, 70, 225, 195, 223, 70, 77, 143, 28, 228, 157, 48, 195, 221, 92, 224, 157, 195, 300, 92, 301, 143, 143, 28, 149, 218, 122, 300, 54, 28, 217, 140, 15, 195, 223, 92, 228, 92, 229, 157, 51, 143, 28, 151, 28, 66, 28, 230, 157, 176, 195, 200, 63, 215, 196, 206, 196, 208, 92, 195, 206, 92, 208, 143, 92, 195, 208, 92, 301, 143, 92, 195, 214, 196, 209, 92, 213, 196, 211, 143, 92, 195, 209, 92, 211, 143, 92, 195, 301, 92, 300, 143, 143, 28, 231, 157, 176, 195, 203, 63, 215, 196, 206, 196, 208, 92, 195, 206, 92, 208, 143, 92, 195, 208, 92, 301, 143, 92, 195, 214, 196, 209, 92, 213, 196, 211, 143, 92, 195, 209, 92, 211, 143, 92, 195, 301, 92, 300, 143, 143, 28, 232, 157, 176, 195, 204, 63, 215, 196, 206, 196, 209, 92, 195, 206, 92, 209, 143, 92, 195, 209, 92, 301, 143, 92, 195, 214, 196, 209, 92, 300, 143, 92, 195, 209, 92, 209, 143, 92, 195, 301, 92, 300, 143, 143, 28, 233, 157, 48, 195, 230, 92, 224, 157, 195, 300, 92, 301, 143, 143, 28, 234, 157, 48, 195, 232, 92, 224, 157, 195, 300, 92, 301, 143, 143, 28, 217, 140, 15, 195, 234, 70, 225, 195, 233, 70, 77, 143, 92, 233, 92, 229, 157, 51, 143, 28, 10, 195, 231, 92, 217, 70, 225, 195, 231, 70, 77, 70, 97, 143, 92, 224, 157, 195, 300, 92, 301, 143, 143, 28, 3, 28]}, {"code": "def chunk_abc_bwd_kernel_dh(\n    q,\n    z,\n    do,\n    dh,\n    scale,\n    T,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n    NORMK: tl.constexpr,\n):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    b_zp = tl.full([BK if NORMK else BV], float(\"inf\"), dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        i_p = tl.maximum(i_t * BT - 1, 0)\n        p_q = tl.make_block_ptr(\n            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_dh = tl.make_block_ptr(\n            dh + i_bh * NT * K * V + i_t * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        if NORMK:\n            p_z = tl.make_block_ptr(\n                z + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)\n            )\n            p_zc = tl.make_block_ptr(\n                z + i_bh * T * K, (T * K,), (1,), (i_p * K + i_k * BK,), (BK,), (0,)\n            )\n\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = exp(b_zc - b_zp), b_zc\n\n            b_z = tl.load(p_z, boundary_check=(0, 1))\n            b_q = (b_q * exp(b_zc[:, None] - b_z)).to(b_q.dtype)\n\n            b_dh = b_dh * b_r[:, None]\n        else:\n            p_z = tl.make_block_ptr(\n                z + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n            )\n            p_zc = tl.make_block_ptr(\n                z + i_bh * T * V, (T * V,), (1,), (i_p * V + i_v * BV,), (BV,), (0,)\n            )\n\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = exp(b_zc - b_zp), b_zc\n\n            b_z = tl.load(p_z, boundary_check=(0,))\n            b_do = (b_do * exp(b_zc[None, :] - b_z)).to(b_do.dtype)\n\n            b_dh = b_dh * b_r[None, :]\n\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)", "encoded": [26, 299, 195, 199, 91, 200, 91, 201, 91, 202, 91, 203, 91, 204, 91, 205, 53, 6, 91, 206, 53, 6, 91, 207, 53, 6, 91, 208, 53, 6, 91, 209, 53, 6, 91, 210, 53, 6, 91, 211, 53, 6, 143, 53, 28, -1, 212, 91, 213, 91, 214, 157, 195, 139, 195, 300, 143, 91, 139, 195, 301, 143, 91, 139, 195, 302, 143, 143, 28, 215, 157, 144, 195, 184, 208, 91, 209, 24, 91, 76, 157, 122, 143, 28, 216, 157, 190, 195, 184, 208, 149, 211, 27, 209, 24, 91, 217, 195, 303, 143, 91, 76, 157, 122, 143, 28, 113, 218, 130, 5, 195, 210, 4, 301, 91, 4, 301, 91, 4, 301, 143, 53, 28, 219, 157, 156, 195, 218, 196, 207, 4, 301, 91, 300, 143, 28, 220, 157, 176, 195, 199, 62, 214, 196, 204, 196, 205, 91, 195, 205, 91, 204, 143, 91, 195, 301, 91, 205, 143, 91, 195, 212, 196, 208, 91, 218, 196, 207, 143, 91, 195, 208, 91, 207, 143, 91, 195, 300, 91, 301, 143, 143, 28, 221, 157, 176, 195, 201, 62, 214, 196, 204, 196, 206, 91, 195, 204, 91, 206, 143, 91, 195, 206, 91, 301, 143, 91, 195, 218, 196, 207, 91, 213, 196, 209, 143, 91, 195, 207, 91, 209, 143, 91, 195, 301, 91, 300, 143, 143, 28, 222, 157, 176, 195, 202, 62, 214, 196, 210, 196, 205, 196, 206, 62, 218, 196, 205, 196, 206, 91, 195, 205, 91, 206, 143, 91, 195, 206, 91, 301, 143, 91, 195, 212, 196, 208, 91, 213, 196, 209, 143, 91, 195, 208, 91, 209, 143, 91, 195, 301, 91, 300, 143, 143, 28, 223, 157, 49, 195, 220, 91, 224, 157, 195, 300, 91, 301, 143, 143, 28, 223, 157, 195, 223, 196, 203, 143, 69, 225, 195, 223, 69, 76, 143, 28, 226, 157, 49, 195, 221, 91, 224, 157, 195, 300, 91, 301, 143, 143, 28, 10, 195, 222, 91, 215, 69, 225, 195, 222, 69, 76, 69, 96, 143, 91, 224, 157, 195, 300, 91, 301, 143, 143, 28, 149, 211, 53, 28, 227, 157, 176, 195, 200, 62, 214, 196, 204, 196, 205, 91, 195, 205, 91, 204, 143, 91, 195, 301, 91, 205, 143, 91, 195, 212, 196, 208, 91, 218, 196, 207, 143, 91, 195, 208, 91, 207, 143, 91, 195, 300, 91, 301, 143, 143, 28, 228, 157, 176, 195, 200, 62, 214, 196, 204, 196, 205, 91, 195, 204, 196, 205, 91, 143, 91, 195, 301, 91, 143, 91, 195, 219, 196, 205, 62, 212, 196, 208, 91, 143, 91, 195, 208, 91, 143, 91, 195, 300, 91, 143, 143, 28, 229, 157, 49, 195, 228, 91, 224, 157, 195, 300, 91, 143, 143, 28, 230, 91, 216, 157, 195, 167, 195, 229, 4, 216, 143, 91, 229, 143, 28, 231, 157, 49, 195, 227, 91, 224, 157, 195, 300, 91, 301, 143, 143, 28, 223, 157, 195, 223, 196, 167, 195, 229, 184, 53, 91, 162, 24, 4, 231, 143, 143, 69, 225, 195, 223, 69, 76, 143, 28, 215, 157, 215, 196, 230, 184, 53, 91, 162, 24, 28, 151, 28, 27, 53, 28, 227, 157, 176, 195, 200, 62, 214, 196, 204, 196, 206, 91, 195, 204, 91, 206, 143, 91, 195, 206, 91, 301, 143, 91, 195, 218, 196, 207, 91, 213, 196, 209, 143, 91, 195, 207, 91, 209, 143, 91, 195, 301, 91, 300, 143, 143, 28, 228, 157, 176, 195, 200, 62, 214, 196, 204, 196, 206, 91, 195, 204, 196, 206, 91, 143, 91, 195, 301, 91, 143, 91, 195, 219, 196, 206, 62, 213, 196, 209, 91, 143, 91, 195, 209, 91, 143, 91, 195, 300, 91, 143, 143, 28, 229, 157, 49, 195, 228, 91, 224, 157, 195, 300, 91, 143, 143, 28, 230, 91, 216, 157, 195, 167, 195, 229, 4, 216, 143, 91, 229, 143, 28, 231, 157, 49, 195, 227, 91, 224, 157, 195, 300, 91, 143, 143, 28, 226, 157, 195, 226, 196, 167, 195, 229, 184, 162, 91, 53, 24, 4, 231, 143, 143, 69, 225, 195, 226, 69, 76, 143, 28, 215, 157, 215, 196, 230, 184, 162, 91, 53, 24, 28, 48, 28, 215, 140, 15, 195, 223, 91, 226, 91, 232, 157, 51, 143, 28, 65, 28, 3, 28]}, {"code": "def chunk_abc_bwd_kernel_V(\n    k,\n    v,\n    z,\n    h,\n    A,\n    do,\n    dh,\n    dq,\n    dk,\n    dv,\n    dA,\n    scale,\n    T,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    n_bh = tl.num_programs(2)\n\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_zc = tl.make_block_ptr(\n        z + i_bh * T * K,\n        (T * K,),\n        (1,),\n        ((i_t * BT + BT - 1) * K + i_k * BK,),\n        (BK,),\n        (0,),\n    )\n    p_A = tl.make_block_ptr(\n        A + i_bh * T * BT, (BT, T), (1, BT), (0, i_t * BT), (BT, BT), (0, 1)\n    )\n\n    b_zc = tl.load(p_zc, boundary_check=(0,))\n\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_k = exp(b_k - b_zc[None, :]).to(b_k.dtype)\n\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_h = tl.make_block_ptr(\n            h + i_bh * NT * K * V + i_t * V * K,\n            (V, K),\n            (1, V),\n            (i_v * BV, i_k * BK),\n            (BV, BK),\n            (0, 1),\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_dh = tl.make_block_ptr(\n            dh + i_bh * NT * K * V + i_t * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        p_dv = tl.make_block_ptr(\n            dv + (i_k * n_bh + i_bh) * T * V,\n            (T, V),\n            (V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False)\n        if i_k == 0:\n            b_dv += tl.dot(b_A.to(b_do.dtype), b_do, allow_tf32=False)\n        b_do = (b_do * scale).to(b_do.dtype)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n\n        b_dA += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n    p_z = tl.make_block_ptr(\n        z + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_zp = tl.make_block_ptr(\n        z + i_bh * T * K, (T * K,), (1,), (i_p * K + i_k * BK,), (BK,), (0,)\n    )\n\n    b_zp = tl.load(p_zp, boundary_check=(0,))\n\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_z = exp(b_zp[None, :] - b_z)\n\n    b_dq = b_dq * b_z\n    b_dk = b_dk * b_k\n\n    p_dq = tl.make_block_ptr(\n        dq + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_dk = tl.make_block_ptr(\n        dk + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_dA = tl.make_block_ptr(\n        dA + i_bh * T * BT,\n        (\n            T,\n            BT,\n        ),\n        (BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n\n    b_dA = tl.where(m_s, b_dA, 0.0).to(b_k.dtype)\n    if i_k == 0:\n        tl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))", "encoded": [26, 300, 196, 200, 93, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 93, 211, 93, 212, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 93, 218, 54, 6, 144, 54, 28, -1, 219, 93, 220, 93, 221, 158, 196, 140, 196, 301, 144, 93, 140, 196, 302, 144, 93, 140, 196, 303, 144, 144, 28, 222, 158, 157, 196, 220, 197, 215, 4, 302, 93, 301, 144, 28, 223, 158, 118, 196, 303, 144, 28, 224, 158, 177, 196, 200, 64, 221, 197, 212, 197, 213, 93, 196, 212, 93, 213, 144, 93, 196, 213, 93, 302, 144, 93, 196, 220, 197, 215, 93, 219, 197, 216, 144, 93, 196, 215, 93, 216, 144, 93, 196, 302, 93, 301, 144, 144, 28, 225, 158, 177, 196, 202, 64, 221, 197, 212, 197, 213, 93, 196, 212, 197, 213, 93, 144, 93, 196, 302, 93, 144, 93, 196, 196, 220, 197, 215, 64, 215, 4, 302, 144, 197, 213, 64, 219, 197, 216, 93, 144, 93, 196, 216, 93, 144, 93, 196, 301, 93, 144, 144, 28, 226, 158, 177, 196, 204, 64, 221, 197, 212, 197, 215, 93, 196, 215, 93, 212, 144, 93, 196, 302, 93, 215, 144, 93, 196, 301, 93, 220, 197, 215, 144, 93, 196, 215, 93, 215, 144, 93, 196, 301, 93, 302, 144, 144, 28, 227, 158, 48, 196, 225, 93, 228, 158, 196, 301, 93, 144, 144, 28, 229, 158, 48, 196, 224, 93, 228, 158, 196, 301, 93, 302, 144, 144, 28, 229, 158, 168, 196, 229, 4, 227, 185, 163, 93, 54, 24, 144, 71, 230, 196, 229, 71, 78, 144, 28, 231, 158, 48, 196, 226, 93, 228, 158, 196, 301, 93, 302, 144, 144, 28, 232, 158, 145, 196, 185, 215, 93, 216, 24, 93, 78, 158, 124, 144, 28, 233, 158, 145, 196, 185, 215, 93, 216, 24, 93, 78, 158, 124, 144, 28, 234, 158, 145, 196, 185, 215, 93, 215, 24, 93, 78, 158, 124, 144, 28, 115, 235, 131, 5, 196, 56, 196, 214, 93, 217, 144, 144, 54, 28, 236, 158, 177, 196, 201, 64, 221, 197, 212, 197, 214, 93, 196, 212, 93, 214, 144, 93, 196, 214, 93, 302, 144, 93, 196, 220, 197, 215, 93, 235, 197, 217, 144, 93, 196, 215, 93, 217, 144, 93, 196, 302, 93, 301, 144, 144, 28, 237, 158, 177, 196, 203, 64, 221, 197, 218, 197, 213, 197, 214, 64, 220, 197, 214, 197, 213, 93, 196, 214, 93, 213, 144, 93, 196, 302, 93, 214, 144, 93, 196, 235, 197, 217, 93, 219, 197, 216, 144, 93, 196, 217, 93, 216, 144, 93, 196, 301, 93, 302, 144, 144, 28, 238, 158, 177, 196, 205, 64, 221, 197, 212, 197, 214, 93, 196, 212, 93, 214, 144, 93, 196, 214, 93, 302, 144, 93, 196, 220, 197, 215, 93, 235, 197, 217, 144, 93, 196, 215, 93, 217, 144, 93, 196, 302, 93, 301, 144, 144, 28, 239, 158, 177, 196, 206, 64, 221, 197, 218, 197, 213, 197, 214, 64, 220, 197, 213, 197, 214, 93, 196, 213, 93, 214, 144, 93, 196, 214, 93, 302, 144, 93, 196, 219, 197, 216, 93, 235, 197, 217, 144, 93, 196, 216, 93, 217, 144, 93, 196, 302, 93, 301, 144, 144, 28, 240, 158, 177, 196, 209, 64, 196, 219, 197, 223, 64, 221, 144, 197, 212, 197, 214, 93, 196, 212, 93, 214, 144, 93, 196, 214, 93, 302, 144, 93, 196, 220, 197, 215, 93, 235, 197, 217, 144, 93, 196, 215, 93, 217, 144, 93, 196, 302, 93, 301, 144, 144, 28, 241, 158, 48, 196, 236, 93, 228, 158, 196, 301, 93, 302, 144, 144, 28, 242, 158, 48, 196, 237, 93, 228, 158, 196, 301, 93, 302, 144, 144, 28, 243, 158, 48, 196, 238, 93, 228, 158, 196, 301, 93, 302, 144, 144, 28, 244, 158, 48, 196, 239, 93, 228, 158, 196, 301, 93, 302, 144, 144, 28, 245, 158, 15, 196, 229, 93, 244, 93, 246, 158, 51, 144, 28, 150, 219, 66, 301, 54, 28, 245, 141, 15, 196, 231, 71, 230, 196, 243, 71, 78, 144, 93, 243, 93, 246, 158, 51, 144, 28, 152, 28, 243, 158, 196, 243, 197, 211, 144, 71, 230, 196, 243, 71, 78, 144, 28, 10, 196, 240, 93, 245, 71, 230, 196, 240, 71, 78, 71, 98, 144, 93, 228, 158, 196, 301, 93, 302, 144, 144, 28, 234, 141, 15, 196, 243, 93, 62, 196, 241, 144, 93, 246, 158, 51, 144, 28, 232, 141, 15, 196, 243, 93, 242, 93, 246, 158, 51, 144, 28, 233, 141, 15, 196, 241, 93, 62, 196, 244, 144, 93, 246, 158, 51, 144, 28, 67, 28, 247, 158, 177, 196, 202, 64, 221, 197, 212, 197, 213, 93, 196, 212, 93, 213, 144, 93, 196, 213, 93, 302, 144, 93, 196, 220, 197, 215, 93, 219, 197, 216, 144, 93, 196, 215, 93, 216, 144, 93, 196, 302, 93, 301, 144, 144, 28, 248, 158, 177, 196, 202, 64, 221, 197, 212, 197, 213, 93, 196, 212, 197, 213, 93, 144, 93, 196, 302, 93, 144, 93, 196, 222, 197, 213, 64, 219, 197, 216, 93, 144, 93, 196, 216, 93, 144, 93, 196, 301, 93, 144, 144, 28, 249, 158, 48, 196, 248, 93, 228, 158, 196, 301, 93, 144, 144, 28, 250, 158, 48, 196, 247, 93, 228, 158, 196, 301, 93, 302, 144, 144, 28, 250, 158, 168, 196, 249, 185, 163, 93, 54, 24, 4, 250, 144, 28, 232, 158, 232, 197, 250, 28, 233, 158, 233, 197, 229, 28, 251, 158, 177, 196, 207, 64, 221, 197, 212, 197, 213, 93, 196, 212, 93, 213, 144, 93, 196, 213, 93, 302, 144, 93, 196, 220, 197, 215, 93, 219, 197, 216, 144, 93, 196, 215, 93, 216, 144, 93, 196, 302, 93, 301, 144, 144, 28, 252, 158, 177, 196, 208, 64, 221, 197, 212, 197, 213, 93, 196, 212, 93, 213, 144, 93, 196, 213, 93, 302, 144, 93, 196, 220, 197, 215, 93, 219, 197, 216, 144, 93, 196, 215, 93, 216, 144, 93, 196, 302, 93, 301, 144, 144, 28, 253, 158, 177, 196, 210, 64, 221, 197, 212, 197, 215, 93, 196, 212, 93, 215, 144, 93, 196, 215, 93, 302, 144, 93, 196, 220, 197, 215, 93, 301, 144, 93, 196, 215, 93, 215, 144, 93, 196, 302, 93, 301, 144, 144, 28, 10, 196, 251, 93, 232, 71, 230, 196, 251, 71, 78, 71, 98, 144, 93, 228, 158, 196, 301, 93, 302, 144, 144, 28, 10, 196, 252, 93, 233, 71, 230, 196, 252, 71, 78, 71, 98, 144, 93, 228, 158, 196, 301, 93, 302, 144, 144, 28, 254, 158, 65, 196, 301, 93, 215, 144, 28, 255, 158, 254, 185, 54, 93, 163, 24, 123, 254, 185, 163, 93, 54, 24, 28, 234, 158, 166, 196, 255, 93, 234, 93, 301, 144, 71, 230, 196, 229, 71, 78, 144, 28, 150, 219, 66, 301, 54, 28, 10, 196, 253, 93, 234, 71, 230, 196, 253, 71, 78, 71, 98, 144, 93, 228, 158, 196, 301, 93, 302, 144, 144, 28, 152, 28, 3, 28]}, {"code": "def chunk_abc_bwd_kernel_intra_V(\n    q,\n    k,\n    z,\n    dA,\n    dq,\n    dk,\n    T,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr,\n):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n\n    p_z = tl.make_block_ptr(\n        z + i_bh * T * K,\n        (T, K),\n        (K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    p_zn = tl.make_block_ptr(\n        z + i_bh * T * K,\n        (T * K,),\n        (1,),\n        ((i_t * BT + i_i * BC) * K + i_k * BK,),\n        (BK,),\n        (0,),\n    )\n\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_zq = exp(b_zn[None, :] - b_z)\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_k = tl.make_block_ptr(\n            k + i_bh * T * K,\n            (T, K),\n            (K, 1),\n            (i_t * BT + i_j * BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_dA = tl.make_block_ptr(\n            dA + i_bh * T * BT,\n            (T, BT),\n            (BT, 1),\n            (i_t * BT + i_i * BC, i_j * BC),\n            (BC, BC),\n            (1, 0),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kz = exp(b_k - b_zn[None, :]).to(b_k.dtype)\n\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n\n        b_dq += tl.dot(b_dA, b_kz, allow_tf32=False)\n    b_dq *= b_zq\n\n    o_i = tl.arange(0, BC)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_i * BC\n    m_dA = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n    for j in range(0, BC):\n        p_kj = tl.make_block_ptr(\n            k + i_bh * T * K,\n            (T * K,),\n            (1,),\n            ((i_t * BT + i_i * BC + j) * K + i_k * BK,),\n            (BK,),\n            (0,),\n        )\n\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n\n        b_kj = tl.load(p_kj, boundary_check=(0,)).to(tl.float32)\n\n        m_i = o_i[:, None] >= j\n\n        b_dq += tl.where(m_i, b_dA[:, None] * exp(b_kj[None, :] - b_z), 0.0)\n    p_dq = tl.make_block_ptr(\n        dq + i_bh * T * K,\n        (T, K),\n        (K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n\n    tl.debug_barrier()\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K,\n        (T, K),\n        (K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    p_zn = tl.make_block_ptr(\n        z + i_bh * T * K,\n        (T * K,),\n        (1,),\n        ((i_t * BT + i_i * BC + BC - 1) * K + i_k * BK,),\n        (BK,),\n        (0,),\n    )\n\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_kz = exp(b_k - b_zn[None, :])\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_q = tl.make_block_ptr(\n            q + i_bh * T * K,\n            (T, K),\n            (K, 1),\n            (i_t * BT + i_j * BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_z = tl.make_block_ptr(\n            z + i_bh * T * K,\n            (T, K),\n            (K, 1),\n            (i_t * BT + i_j * BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_dA = tl.make_block_ptr(\n            dA + i_bh * T * BT,\n            (T, BT),\n            (BT, 1),\n            (i_t * BT + i_j * BC, i_i * BC),\n            (BC, BC),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_qz = (b_q * exp(b_zn[None, :] - b_z)).to(b_q.dtype)\n\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n\n        b_dk += tl.dot(tl.trans(b_dA), b_qz, allow_tf32=False)\n    b_dk *= b_kz\n\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(0, BC)\n    for j in range(0, BC):\n        p_qj = tl.make_block_ptr(\n            q + i_bh * T * K,\n            (T * K,),\n            (1,),\n            ((i_t * BT + i_i * BC + j) * K + i_k * BK,),\n            (BK,),\n            (0,),\n        )\n        p_zj = tl.make_block_ptr(\n            z + i_bh * T * K,\n            (T * K,),\n            (1,),\n            ((i_t * BT + i_i * BC + j) * K + i_k * BK,),\n            (BK,),\n            (0,),\n        )\n\n        b_dA = tl.load(dA + o_dA + j * BT, mask=(i_t * BT + i_i * BC + j < T), other=0)\n\n        b_qj = tl.load(p_qj, boundary_check=(0,)).to(tl.float32)\n        b_zj = tl.load(p_zj, boundary_check=(0,)).to(tl.float32)\n\n        m_i = o_i[:, None] <= j\n        b_dk += tl.where(\n            m_i, b_dA[:, None] * b_qj[None, :] * exp(b_k - b_zj[None, :]), 0.0\n        )\n    p_dk = tl.make_block_ptr(\n        dk + i_bh * T * K,\n        (T, K),\n        (K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))", "encoded": [26, 300, 196, 200, 92, 201, 92, 202, 92, 203, 92, 204, 92, 205, 92, 206, 92, 207, 53, 6, 92, 208, 53, 6, 92, 209, 53, 6, 92, 210, 53, 6, 92, 211, 53, 6, 144, 53, 28, -1, 212, 92, 213, 92, 214, 158, 196, 140, 196, 301, 144, 92, 140, 196, 302, 144, 92, 140, 196, 303, 144, 144, 28, 215, 92, 216, 158, 196, 213, 43, 211, 92, 213, 182, 211, 144, 28, 217, 158, 177, 196, 202, 63, 214, 197, 206, 197, 207, 92, 196, 206, 92, 207, 144, 92, 196, 207, 92, 302, 144, 92, 196, 215, 197, 208, 63, 216, 197, 209, 92, 212, 197, 210, 144, 92, 196, 209, 92, 210, 144, 92, 196, 302, 92, 301, 144, 144, 28, 218, 158, 177, 196, 202, 63, 214, 197, 206, 197, 207, 92, 196, 206, 197, 207, 92, 144, 92, 196, 302, 92, 144, 92, 196, 196, 215, 197, 208, 63, 216, 197, 209, 144, 197, 207, 63, 212, 197, 210, 92, 144, 92, 196, 210, 92, 144, 92, 196, 301, 92, 144, 144, 28, 219, 158, 49, 196, 218, 92, 220, 158, 196, 301, 92, 144, 144, 28, 221, 158, 49, 196, 217, 92, 220, 158, 196, 301, 92, 302, 144, 144, 28, 222, 158, 168, 196, 219, 185, 163, 92, 53, 24, 4, 221, 144, 28, 223, 158, 145, 196, 185, 209, 92, 210, 24, 92, 77, 158, 123, 144, 28, 114, 224, 131, 5, 196, 301, 92, 216, 144, 53, 28, 225, 158, 177, 196, 201, 63, 214, 197, 206, 197, 207, 92, 196, 206, 92, 207, 144, 92, 196, 207, 92, 302, 144, 92, 196, 215, 197, 208, 63, 224, 197, 209, 92, 212, 197, 210, 144, 92, 196, 209, 92, 210, 144, 92, 196, 302, 92, 301, 144, 144, 28, 226, 158, 177, 196, 203, 63, 214, 197, 206, 197, 208, 92, 196, 206, 92, 208, 144, 92, 196, 208, 92, 302, 144, 92, 196, 215, 197, 208, 63, 216, 197, 209, 92, 224, 197, 209, 144, 92, 196, 209, 92, 209, 144, 92, 196, 302, 92, 301, 144, 144, 28, 227, 158, 49, 196, 225, 92, 220, 158, 196, 301, 92, 302, 144, 144, 28, 228, 158, 168, 196, 227, 4, 219, 185, 163, 92, 53, 24, 144, 70, 229, 196, 227, 70, 77, 144, 28, 230, 158, 49, 196, 226, 92, 220, 158, 196, 301, 92, 302, 144, 144, 28, 223, 141, 15, 196, 230, 92, 228, 92, 231, 158, 51, 144, 28, 66, 28, 223, 21, 222, 28, 232, 158, 64, 196, 301, 92, 209, 144, 28, 233, 158, 214, 197, 206, 197, 208, 63, 196, 215, 197, 208, 63, 216, 197, 209, 63, 64, 196, 301, 92, 209, 144, 144, 197, 208, 63, 216, 197, 209, 28, 234, 158, 215, 197, 208, 63, 216, 197, 209, 63, 64, 196, 301, 92, 209, 144, 1, 206, 28, 114, 235, 131, 5, 196, 301, 92, 209, 144, 53, 28, 236, 158, 177, 196, 201, 63, 214, 197, 206, 197, 207, 92, 196, 206, 197, 207, 92, 144, 92, 196, 302, 92, 144, 92, 196, 196, 215, 197, 208, 63, 216, 197, 209, 63, 235, 144, 197, 207, 63, 212, 197, 210, 92, 144, 92, 196, 210, 92, 144, 92, 196, 301, 92, 144, 144, 28, 230, 158, 49, 196, 203, 63, 233, 63, 235, 92, 237, 158, 234, 92, 238, 158, 301, 144, 28, 239, 158, 49, 196, 236, 92, 220, 158, 196, 301, 92, 144, 144, 70, 229, 196, 123, 144, 28, 240, 158, 232, 185, 53, 92, 163, 24, 122, 235, 28, 223, 141, 166, 196, 240, 92, 230, 185, 53, 92, 163, 24, 197, 168, 196, 239, 185, 163, 92, 53, 24, 4, 221, 144, 92, 301, 144, 28, 66, 28, 241, 158, 177, 196, 204, 63, 214, 197, 206, 197, 207, 92, 196, 206, 92, 207, 144, 92, 196, 207, 92, 302, 144, 92, 196, 215, 197, 208, 63, 216, 197, 209, 92, 212, 197, 210, 144, 92, 196, 209, 92, 210, 144, 92, 196, 302, 92, 301, 144, 144, 28, 10, 196, 241, 92, 223, 70, 229, 196, 241, 70, 77, 70, 97, 144, 92, 220, 158, 196, 301, 92, 302, 144, 144, 28, 45, 196, 144, 28, 225, 158, 177, 196, 201, 63, 214, 197, 206, 197, 207, 92, 196, 206, 92, 207, 144, 92, 196, 207, 92, 302, 144, 92, 196, 215, 197, 208, 63, 216, 197, 209, 92, 212, 197, 210, 144, 92, 196, 209, 92, 210, 144, 92, 196, 302, 92, 301, 144, 144, 28, 218, 158, 177, 196, 202, 63, 214, 197, 206, 197, 207, 92, 196, 206, 197, 207, 92, 144, 92, 196, 302, 92, 144, 92, 196, 196, 215, 197, 208, 63, 216, 197, 209, 63, 209, 4, 302, 144, 197, 207, 63, 212, 197, 210, 92, 144, 92, 196, 210, 92, 144, 92, 196, 301, 92, 144, 144, 28, 219, 158, 49, 196, 218, 92, 220, 158, 196, 301, 92, 144, 144, 28, 227, 158, 49, 196, 225, 92, 220, 158, 196, 301, 92, 302, 144, 144, 28, 228, 158, 168, 196, 227, 4, 219, 185, 163, 92, 53, 24, 144, 28, 242, 158, 145, 196, 185, 209, 92, 210, 24, 92, 77, 158, 123, 144, 28, 114, 224, 131, 5, 196, 216, 63, 302, 92, 211, 144, 53, 28, 243, 158, 177, 196, 200, 63, 214, 197, 206, 197, 207, 92, 196, 206, 92, 207, 144, 92, 196, 207, 92, 302, 144, 92, 196, 215, 197, 208, 63, 224, 197, 209, 92, 212, 197, 210, 144, 92, 196, 209, 92, 210, 144, 92, 196, 302, 92, 301, 144, 144, 28, 217, 158, 177, 196, 202, 63, 214, 197, 206, 197, 207, 92, 196, 206, 92, 207, 144, 92, 196, 207, 92, 302, 144, 92, 196, 215, 197, 208, 63, 224, 197, 209, 92, 212, 197, 210, 144, 92, 196, 209, 92, 210, 144, 92, 196, 302, 92, 301, 144, 144, 28, 226, 158, 177, 196, 203, 63, 214, 197, 206, 197, 208, 92, 196, 206, 92, 208, 144, 92, 196, 208, 92, 302, 144, 92, 196, 215, 197, 208, 63, 224, 197, 209, 92, 216, 197, 209, 144, 92, 196, 209, 92, 209, 144, 92, 196, 302, 92, 301, 144, 144, 28, 244, 158, 49, 196, 243, 92, 220, 158, 196, 301, 92, 302, 144, 144, 28, 221, 158, 49, 196, 217, 92, 220, 158, 196, 301, 92, 302, 144, 144, 28, 245, 158, 196, 244, 197, 168, 196, 219, 185, 163, 92, 53, 24, 4, 221, 144, 144, 70, 229, 196, 244, 70, 77, 144, 28, 230, 158, 49, 196, 226, 92, 220, 158, 196, 301, 92, 302, 144, 144, 28, 242, 141, 15, 196, 61, 196, 230, 144, 92, 245, 92, 231, 158, 51, 144, 28, 66, 28, 242, 21, 228, 28, 233, 158, 214, 197, 206, 197, 208, 63, 196, 215, 197, 208, 63, 216, 197, 209, 144, 197, 208, 63, 216, 197, 209, 63, 64, 196, 301, 92, 209, 144, 28, 114, 235, 131, 5, 196, 301, 92, 209, 144, 53, 28, 246, 158, 177, 196, 200, 63, 214, 197, 206, 197, 207, 92, 196, 206, 197, 207, 92, 144, 92, 196, 302, 92, 144, 92, 196, 196, 215, 197, 208, 63, 216, 197, 209, 63, 235, 144, 197, 207, 63, 212, 197, 210, 92, 144, 92, 196, 210, 92, 144, 92, 196, 301, 92, 144, 144, 28, 247, 158, 177, 196, 202, 63, 214, 197, 206, 197, 207, 92, 196, 206, 197, 207, 92, 144, 92, 196, 302, 92, 144, 92, 196, 196, 215, 197, 208, 63, 216, 197, 209, 63, 235, 144, 197, 207, 63, 212, 197, 210, 92, 144, 92, 196, 210, 92, 144, 92, 196, 301, 92, 144, 144, 28, 230, 158, 49, 196, 203, 63, 233, 63, 235, 197, 208, 92, 237, 158, 215, 197, 208, 63, 216, 197, 209, 63, 235, 1, 206, 92, 238, 158, 301, 144, 28, 248, 158, 49, 196, 246, 92, 220, 158, 196, 301, 92, 144, 144, 70, 229, 196, 123, 144, 28, 249, 158, 49, 196, 247, 92, 220, 158, 196, 301, 92, 144, 144, 70, 229, 196, 123, 144, 28, 240, 158, 232, 185, 53, 92, 163, 24, 176, 235, 28, 242, 141, 166, 196, 240, 92, 230, 185, 53, 92, 163, 24, 197, 248, 185, 163, 92, 53, 24, 197, 168, 196, 227, 4, 249, 185, 163, 92, 53, 24, 144, 92, 301, 144, 28, 66, 28, 250, 158, 177, 196, 205, 63, 214, 197, 206, 197, 207, 92, 196, 206, 92, 207, 144, 92, 196, 207, 92, 302, 144, 92, 196, 215, 197, 208, 63, 216, 197, 209, 92, 212, 197, 210, 144, 92, 196, 209, 92, 210, 144, 92, 196, 302, 92, 301, 144, 144, 28, 10, 196, 250, 92, 242, 70, 229, 196, 250, 70, 77, 70, 97, 144, 92, 220, 158, 196, 301, 92, 302, 144, 144, 28, 3, 28]}, {"code": "def chunk_abc_bwd_kernel_intra_K(\n    v,\n    z,\n    do,\n    dA,\n    scale,\n    T,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BV: tl.constexpr,\n    NC: tl.constexpr,\n):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i, i_j = i_c // (NC * NC), (i_c % (NC * NC)) // NC, (i_c % (NC * NC)) % NC\n    n_bh = tl.num_programs(2)\n\n    if i_i > i_j:\n        p_v = tl.make_block_ptr(\n            v + i_bh * T * V,\n            (V, T),\n            (1, V),\n            (i_v * BV, i_t * BT + i_j * BC),\n            (BV, BC),\n            (0, 1),\n        )\n        p_z = tl.make_block_ptr(\n            z + i_bh * T * V,\n            (T, V),\n            (V, 1),\n            (i_t * BT + i_i * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n        p_zn = tl.make_block_ptr(\n            z + i_bh * T * V,\n            (T * V,),\n            (1,),\n            ((i_t * BT + i_i * BC) * V + i_v * BV,),\n            (BV,),\n            (0,),\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * T * V,\n            (T, V),\n            (V, 1),\n            (i_t * BT + i_i * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n        p_dA = tl.make_block_ptr(\n            dA + (i_bh + i_v * n_bh) * T * BT,\n            (T, BT),\n            (BT, 1),\n            (i_t * BT + i_i * BC, i_j * BC),\n            (BC, BC),\n            (1, 0),\n        )\n\n        b_zn = tl.load(p_zn, boundary_check=(0,))\n\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = (b_do * exp(b_zn[None, :] - b_z) * scale).to(b_do.dtype)\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v = exp(b_v - b_zn[:, None]).to(b_v.dtype)\n\n        b_dA = tl.dot(b_do, b_v, allow_tf32=False)\n        tl.store(p_dA, b_dA.to(dA.dtype.element_ty), boundary_check=(0, 1))\n    elif i_i == i_j:\n        p_v = tl.make_block_ptr(\n            v + i_bh * T * V,\n            (T * V,),\n            (1,),\n            ((i_t * BT + i_j * BC) * V + i_v * BV,),\n            (BV,),\n            (0,),\n        )\n        p_z = tl.make_block_ptr(\n            z + i_bh * T * V,\n            (T, V),\n            (V, 1),\n            (i_t * BT + i_i * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * T * V,\n            (T, V),\n            (V, 1),\n            (i_t * BT + i_i * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)) * scale\n\n        o_i = tl.arange(0, BC)\n        o_A = (\n            (i_bh + i_v * n_bh) * T * BT\n            + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT\n            + i_j * BC\n        )\n        m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n        for j in range(0, BC):\n\n            b_v = tl.load(p_v, boundary_check=(0,)).to(tl.float32)\n\n            b_dA = tl.sum(b_do * exp(b_v[None, :] - b_z), 1)\n            b_dA = tl.where(o_i >= j, b_dA, 0)\n            tl.store(dA + o_A + j, b_dA.to(b_do.dtype), mask=m_A)\n\n            p_v = tl.advance(p_v, (V,))", "encoded": [26, 300, 196, 200, 93, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 54, 6, 93, 207, 54, 6, 93, 208, 54, 6, 93, 209, 54, 6, 93, 210, 54, 6, 144, 54, 28, -1, 211, 93, 212, 93, 213, 158, 196, 140, 196, 301, 144, 93, 140, 196, 302, 144, 93, 140, 196, 303, 144, 144, 28, 214, 93, 215, 93, 216, 158, 196, 212, 43, 196, 210, 197, 210, 144, 93, 212, 182, 196, 210, 197, 210, 144, 43, 210, 93, 212, 182, 196, 210, 197, 210, 144, 182, 210, 144, 28, 217, 158, 118, 196, 303, 144, 28, 150, 215, 106, 216, 54, 28, 218, 158, 177, 196, 200, 64, 213, 197, 205, 197, 206, 93, 196, 206, 93, 205, 144, 93, 196, 302, 93, 206, 144, 93, 196, 211, 197, 209, 93, 214, 197, 207, 64, 216, 197, 208, 144, 93, 196, 209, 93, 208, 144, 93, 196, 301, 93, 302, 144, 144, 28, 219, 158, 177, 196, 201, 64, 213, 197, 205, 197, 206, 93, 196, 205, 93, 206, 144, 93, 196, 206, 93, 302, 144, 93, 196, 214, 197, 207, 64, 215, 197, 208, 93, 211, 197, 209, 144, 93, 196, 208, 93, 209, 144, 93, 196, 302, 93, 301, 144, 144, 28, 220, 158, 177, 196, 201, 64, 213, 197, 205, 197, 206, 93, 196, 205, 197, 206, 93, 144, 93, 196, 302, 93, 144, 93, 196, 196, 214, 197, 207, 64, 215, 197, 208, 144, 197, 206, 64, 211, 197, 209, 93, 144, 93, 196, 209, 93, 144, 93, 196, 301, 93, 144, 144, 28, 221, 158, 177, 196, 202, 64, 213, 197, 205, 197, 206, 93, 196, 205, 93, 206, 144, 93, 196, 206, 93, 302, 144, 93, 196, 214, 197, 207, 64, 215, 197, 208, 93, 211, 197, 209, 144, 93, 196, 208, 93, 209, 144, 93, 196, 302, 93, 301, 144, 144, 28, 222, 158, 177, 196, 203, 64, 196, 213, 64, 211, 197, 217, 144, 197, 205, 197, 207, 93, 196, 205, 93, 207, 144, 93, 196, 207, 93, 302, 144, 93, 196, 214, 197, 207, 64, 215, 197, 208, 93, 216, 197, 208, 144, 93, 196, 208, 93, 208, 144, 93, 196, 302, 93, 301, 144, 144, 28, 223, 158, 48, 196, 220, 93, 224, 158, 196, 301, 93, 144, 144, 28, 225, 158, 48, 196, 219, 93, 224, 158, 196, 301, 93, 302, 144, 144, 28, 226, 158, 48, 196, 221, 93, 224, 158, 196, 301, 93, 302, 144, 144, 28, 226, 158, 196, 226, 197, 168, 196, 223, 185, 163, 93, 54, 24, 4, 225, 144, 197, 204, 144, 71, 227, 196, 226, 71, 78, 144, 28, 228, 158, 48, 196, 218, 93, 224, 158, 196, 301, 93, 302, 144, 144, 28, 228, 158, 168, 196, 228, 4, 223, 185, 54, 93, 163, 24, 144, 71, 227, 196, 228, 71, 78, 144, 28, 229, 158, 15, 196, 226, 93, 228, 93, 230, 158, 51, 144, 28, 10, 196, 222, 93, 229, 71, 227, 196, 203, 71, 78, 71, 98, 144, 93, 224, 158, 196, 301, 93, 302, 144, 144, 28, 57, 28, 33, 215, 66, 216, 54, 28, 218, 158, 177, 196, 200, 64, 213, 197, 205, 197, 206, 93, 196, 205, 197, 206, 93, 144, 93, 196, 302, 93, 144, 93, 196, 196, 214, 197, 207, 64, 216, 197, 208, 144, 197, 206, 64, 211, 197, 209, 93, 144, 93, 196, 209, 93, 144, 93, 196, 301, 93, 144, 144, 28, 219, 158, 177, 196, 201, 64, 213, 197, 205, 197, 206, 93, 196, 205, 93, 206, 144, 93, 196, 206, 93, 302, 144, 93, 196, 214, 197, 207, 64, 215, 197, 208, 93, 211, 197, 209, 144, 93, 196, 208, 93, 209, 144, 93, 196, 302, 93, 301, 144, 144, 28, 221, 158, 177, 196, 202, 64, 213, 197, 205, 197, 206, 93, 196, 205, 93, 206, 144, 93, 196, 206, 93, 302, 144, 93, 196, 214, 197, 207, 64, 215, 197, 208, 93, 211, 197, 209, 144, 93, 196, 208, 93, 209, 144, 93, 196, 302, 93, 301, 144, 144, 28, 225, 158, 48, 196, 219, 93, 224, 158, 196, 301, 93, 302, 144, 144, 28, 226, 158, 48, 196, 221, 93, 224, 158, 196, 301, 93, 302, 144, 144, 197, 204, 28, 231, 158, 65, 196, 301, 93, 208, 144, 28, 232, 158, 196, 213, 64, 211, 197, 217, 144, 197, 205, 197, 207, 64, 196, 214, 197, 207, 64, 215, 197, 208, 64, 65, 196, 301, 93, 208, 144, 144, 197, 207, 64, 216, 197, 208, 28, 233, 158, 214, 197, 207, 64, 215, 197, 208, 64, 65, 196, 301, 93, 208, 144, 1, 205, 28, 115, 234, 131, 5, 196, 301, 93, 208, 144, 54, 28, 228, 158, 48, 196, 218, 93, 224, 158, 196, 301, 93, 144, 144, 71, 227, 196, 124, 144, 28, 229, 158, 179, 196, 226, 197, 168, 196, 228, 185, 163, 93, 54, 24, 4, 225, 144, 93, 302, 144, 28, 229, 158, 166, 196, 231, 123, 234, 93, 229, 93, 301, 144, 28, 10, 196, 203, 64, 232, 64, 234, 93, 229, 71, 227, 196, 226, 71, 78, 144, 93, 235, 158, 233, 144, 28, 218, 158, 122, 196, 218, 93, 196, 206, 93, 144, 144, 28, 67, 28, 152, 28, 3, 28]}, {"code": "def chunk_abc_bwd_kernel_K(\n    q,\n    k,\n    v,\n    z,\n    h,\n    A,\n    do,\n    dh,\n    dq,\n    dk,\n    dv,\n    dA,\n    scale,\n    T,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    n_bh = tl.num_programs(2)\n\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n\n    p_q = tl.make_block_ptr(\n        q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_A = tl.make_block_ptr(\n        A + (i_k * n_bh + i_bh) * T * BT,\n        (\n            T,\n            BT,\n        ),\n        (BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n\n    b_A = tl.dot((b_q * scale).to(b_q.dtype), tl.trans(b_k), allow_tf32=False)\n    b_A = tl.where(m_s, b_A, 0.0)\n    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_z = tl.make_block_ptr(\n            z + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_zp = tl.make_block_ptr(\n            z + i_bh * T * V, (T * V,), (1,), (i_p * V + i_v * BV,), (BV,), (0,)\n        )\n        p_zc = tl.make_block_ptr(\n            z + i_bh * T * V,\n            (T * V,),\n            (1,),\n            ((i_t * BT + BT - 1) * V + i_v * BV,),\n            (BV,),\n            (0,),\n        )\n        p_h = tl.make_block_ptr(\n            h + i_bh * NT * K * V + i_t * K * V,\n            (V, K),\n            (1, V),\n            (i_v * BV, i_k * BK),\n            (BV, BK),\n            (0, 1),\n        )\n\n        p_do = tl.make_block_ptr(\n            do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_dh = tl.make_block_ptr(\n            dh + i_bh * NT * K * V + i_t * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        p_dv = tl.make_block_ptr(\n            dv + (i_k * n_bh + i_bh) * T * V,\n            (T, V),\n            (V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n\n        b_zp = tl.load(p_zp, boundary_check=(0,))\n        b_zc = tl.load(p_zc, boundary_check=(0,))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v = exp(b_v - b_zc[None, :]).to(b_v.dtype)\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_z = exp(b_zp[None, :] - b_z)\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = (b_do * b_z * scale).to(b_do.dtype)\n\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n\n        b_dv = b_v * tl.dot(b_k, b_dh, allow_tf32=False)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    p_dA = tl.make_block_ptr(\n        dA + i_bh * T * BT,\n        (\n            T,\n            BT,\n        ),\n        (BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n\n    b_dA = tl.load(p_dA, boundary_check=(0, 1))\n\n    b_dq += tl.dot(b_dA, b_k, allow_tf32=False)\n    b_dk += tl.dot(tl.trans(b_dA).to(b_k.dtype), b_q, allow_tf32=False)\n\n    p_dq = tl.make_block_ptr(\n        dq + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_dk = tl.make_block_ptr(\n        dk + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))", "encoded": [26, 300, 196, 200, 92, 201, 92, 202, 92, 203, 92, 204, 92, 205, 92, 206, 92, 207, 92, 208, 92, 209, 92, 210, 92, 211, 92, 212, 92, 213, 92, 214, 53, 6, 92, 215, 53, 6, 92, 216, 53, 6, 92, 217, 53, 6, 92, 218, 53, 6, 92, 219, 53, 6, 144, 53, 28, -1, 220, 92, 221, 92, 222, 158, 196, 140, 196, 301, 144, 92, 140, 196, 302, 144, 92, 140, 196, 303, 144, 144, 28, 223, 158, 157, 196, 221, 197, 216, 4, 302, 92, 301, 144, 28, 224, 158, 117, 196, 303, 144, 28, 225, 158, 64, 196, 301, 92, 216, 144, 28, 226, 158, 225, 185, 53, 92, 163, 24, 122, 225, 185, 163, 92, 53, 24, 28, 227, 158, 177, 196, 200, 63, 222, 197, 213, 197, 214, 92, 196, 213, 92, 214, 144, 92, 196, 214, 92, 302, 144, 92, 196, 221, 197, 216, 92, 220, 197, 217, 144, 92, 196, 216, 92, 217, 144, 92, 196, 302, 92, 301, 144, 144, 28, 228, 158, 177, 196, 201, 63, 222, 197, 213, 197, 214, 92, 196, 213, 92, 214, 144, 92, 196, 214, 92, 302, 144, 92, 196, 221, 197, 216, 92, 220, 197, 217, 144, 92, 196, 216, 92, 217, 144, 92, 196, 302, 92, 301, 144, 144, 28, 229, 158, 177, 196, 205, 63, 196, 220, 197, 224, 63, 222, 144, 197, 213, 197, 216, 92, 196, 213, 92, 216, 144, 92, 196, 216, 92, 302, 144, 92, 196, 221, 197, 216, 92, 301, 144, 92, 196, 216, 92, 216, 144, 92, 196, 302, 92, 301, 144, 144, 28, 230, 158, 49, 196, 227, 92, 231, 158, 196, 301, 92, 302, 144, 144, 28, 232, 158, 49, 196, 228, 92, 231, 158, 196, 301, 92, 302, 144, 144, 28, 233, 158, 15, 196, 196, 230, 197, 212, 144, 70, 234, 196, 230, 70, 77, 144, 92, 61, 196, 232, 144, 92, 235, 158, 51, 144, 28, 233, 158, 166, 196, 226, 92, 233, 92, 301, 144, 28, 10, 196, 229, 92, 233, 70, 234, 196, 229, 70, 77, 70, 97, 144, 92, 231, 158, 196, 301, 92, 302, 144, 144, 28, 236, 158, 145, 196, 185, 216, 92, 217, 24, 92, 77, 158, 123, 144, 28, 237, 158, 145, 196, 185, 216, 92, 217, 24, 92, 77, 158, 123, 144, 28, 114, 238, 131, 5, 196, 55, 196, 215, 92, 218, 144, 144, 53, 28, 239, 158, 177, 196, 202, 63, 222, 197, 213, 197, 215, 92, 196, 213, 92, 215, 144, 92, 196, 215, 92, 302, 144, 92, 196, 221, 197, 216, 92, 238, 197, 218, 144, 92, 196, 216, 92, 218, 144, 92, 196, 302, 92, 301, 144, 144, 28, 240, 158, 177, 196, 203, 63, 222, 197, 213, 197, 215, 92, 196, 213, 92, 215, 144, 92, 196, 215, 92, 302, 144, 92, 196, 221, 197, 216, 92, 238, 197, 218, 144, 92, 196, 216, 92, 218, 144, 92, 196, 302, 92, 301, 144, 144, 28, 241, 158, 177, 196, 203, 63, 222, 197, 213, 197, 215, 92, 196, 213, 197, 215, 92, 144, 92, 196, 302, 92, 144, 92, 196, 223, 197, 215, 63, 238, 197, 218, 92, 144, 92, 196, 218, 92, 144, 92, 196, 301, 92, 144, 144, 28, 242, 158, 177, 196, 203, 63, 222, 197, 213, 197, 215, 92, 196, 213, 197, 215, 92, 144, 92, 196, 302, 92, 144, 92, 196, 196, 221, 197, 216, 63, 216, 4, 302, 144, 197, 215, 63, 238, 197, 218, 92, 144, 92, 196, 218, 92, 144, 92, 196, 301, 92, 144, 144, 28, 243, 158, 177, 196, 204, 63, 222, 197, 219, 197, 214, 197, 215, 63, 221, 197, 214, 197, 215, 92, 196, 215, 92, 214, 144, 92, 196, 302, 92, 215, 144, 92, 196, 238, 197, 218, 92, 220, 197, 217, 144, 92, 196, 218, 92, 217, 144, 92, 196, 301, 92, 302, 144, 144, 28, 244, 158, 177, 196, 206, 63, 222, 197, 213, 197, 215, 92, 196, 213, 92, 215, 144, 92, 196, 215, 92, 302, 144, 92, 196, 221, 197, 216, 92, 238, 197, 218, 144, 92, 196, 216, 92, 218, 144, 92, 196, 302, 92, 301, 144, 144, 28, 245, 158, 177, 196, 207, 63, 222, 197, 219, 197, 214, 197, 215, 63, 221, 197, 214, 197, 215, 92, 196, 214, 92, 215, 144, 92, 196, 215, 92, 302, 144, 92, 196, 220, 197, 217, 92, 238, 197, 218, 144, 92, 196, 217, 92, 218, 144, 92, 196, 302, 92, 301, 144, 144, 28, 246, 158, 177, 196, 210, 63, 196, 220, 197, 224, 63, 222, 144, 197, 213, 197, 215, 92, 196, 213, 92, 215, 144, 92, 196, 215, 92, 302, 144, 92, 196, 221, 197, 216, 92, 238, 197, 218, 144, 92, 196, 216, 92, 218, 144, 92, 196, 302, 92, 301, 144, 144, 28, 247, 158, 49, 196, 241, 92, 231, 158, 196, 301, 92, 144, 144, 28, 248, 158, 49, 196, 242, 92, 231, 158, 196, 301, 92, 144, 144, 28, 249, 158, 49, 196, 239, 92, 231, 158, 196, 301, 92, 302, 144, 144, 28, 249, 158, 168, 196, 249, 4, 248, 185, 163, 92, 53, 24, 144, 70, 234, 196, 249, 70, 77, 144, 28, 250, 158, 49, 196, 240, 92, 231, 158, 196, 301, 92, 302, 144, 144, 28, 250, 158, 168, 196, 247, 185, 163, 92, 53, 24, 4, 250, 144, 28, 251, 158, 49, 196, 243, 92, 231, 158, 196, 301, 92, 302, 144, 144, 28, 252, 158, 49, 196, 244, 92, 231, 158, 196, 301, 92, 302, 144, 144, 28, 252, 158, 196, 252, 197, 250, 197, 212, 144, 70, 234, 196, 252, 70, 77, 144, 28, 253, 158, 49, 196, 245, 92, 231, 158, 196, 301, 92, 302, 144, 144, 28, 236, 141, 15, 196, 252, 92, 251, 92, 235, 158, 51, 144, 28, 237, 141, 15, 196, 249, 92, 61, 196, 253, 144, 92, 235, 158, 51, 144, 28, 254, 158, 249, 197, 15, 196, 232, 92, 253, 92, 235, 158, 51, 144, 28, 10, 196, 246, 92, 254, 70, 234, 196, 246, 70, 77, 70, 97, 144, 92, 231, 158, 196, 301, 92, 302, 144, 144, 28, 66, 28, 255, 158, 177, 196, 211, 63, 222, 197, 213, 197, 216, 92, 196, 213, 92, 216, 144, 92, 196, 216, 92, 302, 144, 92, 196, 221, 197, 216, 92, 301, 144, 92, 196, 216, 92, 216, 144, 92, 196, 302, 92, 301, 144, 144, 28, 256, 158, 49, 196, 255, 92, 231, 158, 196, 301, 92, 302, 144, 144, 28, 236, 141, 15, 196, 256, 92, 232, 92, 235, 158, 51, 144, 28, 237, 141, 15, 196, 61, 196, 256, 144, 70, 234, 196, 232, 70, 77, 144, 92, 230, 92, 235, 158, 51, 144, 28, 257, 158, 177, 196, 208, 63, 222, 197, 213, 197, 214, 92, 196, 213, 92, 214, 144, 92, 196, 214, 92, 302, 144, 92, 196, 221, 197, 216, 92, 220, 197, 217, 144, 92, 196, 216, 92, 217, 144, 92, 196, 302, 92, 301, 144, 144, 28, 258, 158, 177, 196, 209, 63, 222, 197, 213, 197, 214, 92, 196, 213, 92, 214, 144, 92, 196, 214, 92, 302, 144, 92, 196, 221, 197, 216, 92, 220, 197, 217, 144, 92, 196, 216, 92, 217, 144, 92, 196, 302, 92, 301, 144, 144, 28, 10, 196, 257, 92, 236, 70, 234, 196, 257, 70, 77, 70, 97, 144, 92, 231, 158, 196, 301, 92, 302, 144, 144, 28, 10, 196, 258, 92, 237, 70, 234, 196, 258, 70, 77, 70, 97, 144, 92, 231, 158, 196, 301, 92, 302, 144, 144, 28, 3, 28]}, {"code": "def chunk_abc_bwd_kernel_intra_KV(\n    v,\n    z,\n    A,\n    do,\n    dv,\n    T,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BV: tl.constexpr,\n    NC: tl.constexpr,\n):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n\n    p_v = tl.make_block_ptr(\n        v + i_bh * T * V,\n        (T, V),\n        (V, 1),\n        (i_t * BT + i_i * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n    p_zn = tl.make_block_ptr(\n        z + i_bh * T * V,\n        (T * V,),\n        (1,),\n        ((i_t * BT + i_i * BC + BC - 1) * V + i_v * BV,),\n        (BV,),\n        (0,),\n    )\n\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_z = tl.make_block_ptr(\n            z + i_bh * T * V,\n            (T, V),\n            (V, 1),\n            (i_t * BT + i_j * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n        p_A = tl.make_block_ptr(\n            A + i_bh * T * BT,\n            (BT, T),\n            (1, BT),\n            (i_i * BC, i_t * BT + i_j * BC),\n            (BC, BC),\n            (0, 1),\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * T * V,\n            (T, V),\n            (V, 1),\n            (i_t * BT + i_j * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = (b_do * exp(b_zn[None, :] - b_z)).to(b_do.dtype)\n\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_dv += tl.dot(b_A, b_do, allow_tf32=False)\n    b_dv *= exp(b_v - b_zn[None, :])\n\n    o_i = tl.arange(0, BC)\n    for j in range(0, BC):\n        p_z = tl.make_block_ptr(\n            z + i_bh * T * V,\n            (T * V,),\n            (1,),\n            ((i_t * BT + i_i * BC + j) * V + i_v * BV,),\n            (BV,),\n            (0,),\n        )\n        p_A = tl.make_block_ptr(\n            A + i_bh * T * BT,\n            (T * BT,),\n            (1,),\n            ((i_t * BT + i_i * BC + j) * BT + i_i * BC,),\n            (BC,),\n            (0,),\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * T * V,\n            (T * V,),\n            (1,),\n            ((i_t * BT + i_i * BC + j) * V + i_v * BV,),\n            (BV,),\n            (0,),\n        )\n\n        b_A = tl.load(p_A, boundary_check=(0,))\n\n        b_z = tl.load(p_z, boundary_check=(0,))\n        b_do = tl.load(p_do, boundary_check=(0,))\n\n        m_i = o_i[:, None] <= j\n        b_dv += tl.where(\n            m_i, exp(b_v - b_z[None, :]) * b_A[:, None] * b_do[None, :], 0.0\n        )\n    p_dv = tl.make_block_ptr(\n        dv + i_bh * T * V,\n        (T, V),\n        (V, 1),\n        (i_t * BT + i_i * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [26, 300, 196, 200, 93, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 54, 6, 93, 207, 54, 6, 93, 208, 54, 6, 93, 209, 54, 6, 93, 210, 54, 6, 144, 54, 28, -1, 211, 93, 212, 93, 213, 158, 196, 140, 196, 301, 144, 93, 140, 196, 302, 144, 93, 140, 196, 303, 144, 144, 28, 214, 93, 215, 158, 196, 212, 43, 210, 93, 212, 182, 210, 144, 28, 216, 158, 177, 196, 200, 64, 213, 197, 205, 197, 206, 93, 196, 205, 93, 206, 144, 93, 196, 206, 93, 302, 144, 93, 196, 214, 197, 207, 64, 215, 197, 208, 93, 211, 197, 209, 144, 93, 196, 208, 93, 209, 144, 93, 196, 302, 93, 301, 144, 144, 28, 217, 158, 177, 196, 201, 64, 213, 197, 205, 197, 206, 93, 196, 205, 197, 206, 93, 144, 93, 196, 302, 93, 144, 93, 196, 196, 214, 197, 207, 64, 215, 197, 208, 64, 208, 4, 302, 144, 197, 206, 64, 211, 197, 209, 93, 144, 93, 196, 209, 93, 144, 93, 196, 301, 93, 144, 144, 28, 218, 158, 48, 196, 217, 93, 219, 158, 196, 301, 93, 144, 144, 28, 220, 158, 48, 196, 216, 93, 219, 158, 196, 301, 93, 302, 144, 144, 28, 221, 158, 145, 196, 185, 208, 93, 209, 24, 93, 78, 158, 124, 144, 28, 115, 222, 131, 5, 196, 215, 64, 302, 93, 210, 144, 54, 28, 223, 158, 177, 196, 201, 64, 213, 197, 205, 197, 206, 93, 196, 205, 93, 206, 144, 93, 196, 206, 93, 302, 144, 93, 196, 214, 197, 207, 64, 222, 197, 208, 93, 211, 197, 209, 144, 93, 196, 208, 93, 209, 144, 93, 196, 302, 93, 301, 144, 144, 28, 224, 158, 177, 196, 202, 64, 213, 197, 205, 197, 207, 93, 196, 207, 93, 205, 144, 93, 196, 302, 93, 207, 144, 93, 196, 215, 197, 208, 93, 214, 197, 207, 64, 222, 197, 208, 144, 93, 196, 208, 93, 208, 144, 93, 196, 301, 93, 302, 144, 144, 28, 225, 158, 177, 196, 203, 64, 213, 197, 205, 197, 206, 93, 196, 205, 93, 206, 144, 93, 196, 206, 93, 302, 144, 93, 196, 214, 197, 207, 64, 222, 197, 208, 93, 211, 197, 209, 144, 93, 196, 208, 93, 209, 144, 93, 196, 302, 93, 301, 144, 144, 28, 226, 158, 48, 196, 223, 93, 219, 158, 196, 301, 93, 302, 144, 144, 28, 227, 158, 48, 196, 225, 93, 219, 158, 196, 301, 93, 302, 144, 144, 28, 227, 158, 196, 227, 197, 168, 196, 218, 185, 163, 93, 54, 24, 4, 226, 144, 144, 71, 228, 196, 227, 71, 78, 144, 28, 229, 158, 48, 196, 224, 93, 219, 158, 196, 301, 93, 302, 144, 144, 28, 221, 141, 15, 196, 229, 93, 227, 93, 230, 158, 51, 144, 28, 67, 28, 221, 21, 168, 196, 220, 4, 218, 185, 163, 93, 54, 24, 144, 28, 231, 158, 65, 196, 301, 93, 208, 144, 28, 115, 232, 131, 5, 196, 301, 93, 208, 144, 54, 28, 223, 158, 177, 196, 201, 64, 213, 197, 205, 197, 206, 93, 196, 205, 197, 206, 93, 144, 93, 196, 302, 93, 144, 93, 196, 196, 214, 197, 207, 64, 215, 197, 208, 64, 232, 144, 197, 206, 64, 211, 197, 209, 93, 144, 93, 196, 209, 93, 144, 93, 196, 301, 93, 144, 144, 28, 224, 158, 177, 196, 202, 64, 213, 197, 205, 197, 207, 93, 196, 205, 197, 207, 93, 144, 93, 196, 302, 93, 144, 93, 196, 196, 214, 197, 207, 64, 215, 197, 208, 64, 232, 144, 197, 207, 64, 215, 197, 208, 93, 144, 93, 196, 208, 93, 144, 93, 196, 301, 93, 144, 144, 28, 225, 158, 177, 196, 203, 64, 213, 197, 205, 197, 206, 93, 196, 205, 197, 206, 93, 144, 93, 196, 302, 93, 144, 93, 196, 196, 214, 197, 207, 64, 215, 197, 208, 64, 232, 144, 197, 206, 64, 211, 197, 209, 93, 144, 93, 196, 209, 93, 144, 93, 196, 301, 93, 144, 144, 28, 229, 158, 48, 196, 224, 93, 219, 158, 196, 301, 93, 144, 144, 28, 226, 158, 48, 196, 223, 93, 219, 158, 196, 301, 93, 144, 144, 28, 227, 158, 48, 196, 225, 93, 219, 158, 196, 301, 93, 144, 144, 28, 233, 158, 231, 185, 54, 93, 163, 24, 176, 232, 28, 221, 141, 166, 196, 233, 93, 168, 196, 220, 4, 226, 185, 163, 93, 54, 24, 144, 197, 229, 185, 54, 93, 163, 24, 197, 227, 185, 163, 93, 54, 24, 93, 301, 144, 28, 67, 28, 234, 158, 177, 196, 204, 64, 213, 197, 205, 197, 206, 93, 196, 205, 93, 206, 144, 93, 196, 206, 93, 302, 144, 93, 196, 214, 197, 207, 64, 215, 197, 208, 93, 211, 197, 209, 144, 93, 196, 208, 93, 209, 144, 93, 196, 302, 93, 301, 144, 144, 28, 10, 196, 234, 93, 221, 71, 228, 196, 234, 71, 78, 71, 98, 144, 93, 219, 158, 196, 301, 93, 302, 144, 144, 28, 3, 28]}, {"code": "def chunk_abc_bwd_kernel_rcum_inter(\n    s,\n    z,\n    ss,\n    doo,\n    T,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    NT: tl.constexpr,\n):\n    i_m, i_bh = tl.program_id(0), tl.program_id(1)\n\n    b_sp = tl.zeros(\n        [\n            BS,\n        ],\n        dtype=tl.float32,\n    )\n    b_zp = tl.full(\n        [\n            BS,\n        ],\n        float(\"inf\"),\n        dtype=tl.float32,\n    )\n    for i_t in range(NT - 1, -1, -1):\n        p_s = tl.make_block_ptr(\n            s + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0)\n        )\n        p_z = tl.make_block_ptr(\n            z + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0)\n        )\n        p_zc = tl.make_block_ptr(\n            z + i_bh * T * S, (T * S,), (1,), ((i_t * BT) * S + i_m * BS,), (BS,), (0,)\n        )\n        p_ss = tl.make_block_ptr(\n            ss + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0)\n        )\n        p_doo = tl.make_block_ptr(\n            doo + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0)\n        )\n\n        b_zc = tl.load(p_zc, boundary_check=(0,))\n\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_ss = tl.load(p_ss, boundary_check=(0, 1))\n\n        b_doo = exp(b_s - b_zp[None, :]) * b_sp[None, :]\n        tl.store(p_doo, b_doo.to(p_doo.dtype.element_ty), boundary_check=(0, 1))\n\n        b_sp = b_sp * exp(b_zc - b_zp) + tl.sum(b_ss * exp(b_zc[None, :] - b_z), 0)\n        b_zp = b_zc", "encoded": [26, 300, 196, 200, 92, 201, 92, 202, 92, 203, 92, 204, 92, 205, 53, 6, 92, 206, 53, 6, 92, 207, 53, 6, 92, 208, 53, 6, 144, 53, 28, -1, 209, 92, 210, 158, 196, 140, 196, 301, 144, 92, 140, 196, 302, 144, 144, 28, 211, 158, 145, 196, 185, 207, 24, 92, 77, 158, 123, 144, 28, 212, 158, 191, 196, 185, 207, 24, 92, 213, 196, 303, 144, 92, 77, 158, 123, 144, 28, 114, 214, 131, 5, 196, 208, 4, 302, 92, 4, 302, 92, 4, 302, 144, 53, 28, 215, 158, 177, 196, 200, 63, 210, 197, 204, 197, 205, 92, 196, 204, 92, 205, 144, 92, 196, 205, 92, 302, 144, 92, 196, 214, 197, 206, 92, 209, 197, 207, 144, 92, 196, 206, 92, 207, 144, 92, 196, 302, 92, 301, 144, 144, 28, 216, 158, 177, 196, 201, 63, 210, 197, 204, 197, 205, 92, 196, 204, 92, 205, 144, 92, 196, 205, 92, 302, 144, 92, 196, 214, 197, 206, 92, 209, 197, 207, 144, 92, 196, 206, 92, 207, 144, 92, 196, 302, 92, 301, 144, 144, 28, 217, 158, 177, 196, 201, 63, 210, 197, 204, 197, 205, 92, 196, 204, 197, 205, 92, 144, 92, 196, 302, 92, 144, 92, 196, 214, 197, 206, 197, 205, 63, 209, 197, 207, 92, 144, 92, 196, 207, 92, 144, 92, 196, 301, 92, 144, 144, 28, 218, 158, 177, 196, 202, 63, 210, 197, 204, 197, 205, 92, 196, 204, 92, 205, 144, 92, 196, 205, 92, 302, 144, 92, 196, 214, 197, 206, 92, 209, 197, 207, 144, 92, 196, 206, 92, 207, 144, 92, 196, 302, 92, 301, 144, 144, 28, 219, 158, 177, 196, 203, 63, 210, 197, 204, 197, 205, 92, 196, 204, 92, 205, 144, 92, 196, 205, 92, 302, 144, 92, 196, 214, 197, 206, 92, 209, 197, 207, 144, 92, 196, 206, 92, 207, 144, 92, 196, 302, 92, 301, 144, 144, 28, 220, 158, 49, 196, 217, 92, 221, 158, 196, 301, 92, 144, 144, 28, 222, 158, 49, 196, 215, 92, 221, 158, 196, 301, 92, 302, 144, 144, 28, 223, 158, 49, 196, 216, 92, 221, 158, 196, 301, 92, 302, 144, 144, 28, 224, 158, 49, 196, 218, 92, 221, 158, 196, 301, 92, 302, 144, 144, 28, 225, 158, 168, 196, 222, 4, 212, 185, 163, 92, 53, 24, 144, 197, 211, 185, 163, 92, 53, 24, 28, 10, 196, 219, 92, 225, 70, 226, 196, 219, 70, 77, 70, 97, 144, 92, 221, 158, 196, 301, 92, 302, 144, 144, 28, 211, 158, 211, 197, 168, 196, 220, 4, 212, 144, 63, 179, 196, 224, 197, 168, 196, 220, 185, 163, 92, 53, 24, 4, 223, 144, 92, 301, 144, 28, 212, 158, 220, 28, 66, 28, 3, 28]}, {"code": "def chunk_abc_bwd_kernel_rcum_intra(\n    s,\n    z,\n    ss,\n    doo,\n    T,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BS: tl.constexpr,\n    NC: tl.constexpr,\n):\n    i_s, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n\n    o_i = tl.arange(0, BC)\n    m_o = tl.full([BC, BC], 1.0, dtype=tl.float32)\n\n    p_s = tl.make_block_ptr(\n        s + i_bh * T * S,\n        (T, S),\n        (S, 1),\n        (i_t * BT + i_i * BC, i_s * BS),\n        (BC, BS),\n        (1, 0),\n    )\n    p_zn = tl.make_block_ptr(\n        z + i_bh * T * S,\n        (T * S,),\n        (1,),\n        ((i_t * BT + i_i * BC + BC - 1) * S + i_s * BS,),\n        (BS,),\n        (0,),\n    )\n    p_doo = tl.make_block_ptr(\n        doo + i_bh * T * S,\n        (T, S),\n        (S, 1),\n        (i_t * BT + i_i * BC, i_s * BS),\n        (BC, BS),\n        (1, 0),\n    )\n\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n\n    b_doo = tl.zeros([BC, BS], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_z = tl.make_block_ptr(\n            z + i_bh * T * S,\n            (T, S),\n            (S, 1),\n            (i_t * BT + i_j * BC, i_s * BS),\n            (BC, BS),\n            (1, 0),\n        )\n        p_ss = tl.make_block_ptr(\n            ss + i_bh * T * S,\n            (T, S),\n            (S, 1),\n            (i_t * BT + i_j * BC, i_s * BS),\n            (BC, BS),\n            (1, 0),\n        )\n\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_ss = tl.load(p_ss, boundary_check=(0, 1))\n\n        b_doo += b_ss * exp(b_zn[None, :] - b_z)\n    b_doo = exp(b_s - b_zn[None, :]) * tl.dot(\n        m_o.to(b_s.dtype), b_doo.to(b_s.dtype), allow_tf32=False\n    )\n\n    for j in range(0, BC):\n        p_z = tl.make_block_ptr(\n            z + i_bh * T * S,\n            (T * S,),\n            (1,),\n            ((i_t * BT + i_i * BC + j) * S + i_s * BS,),\n            (BS,),\n            (0,),\n        )\n        p_ss = tl.make_block_ptr(\n            ss + i_bh * T * S,\n            (T * S,),\n            (1,),\n            ((i_t * BT + i_i * BC + j) * S + i_s * BS,),\n            (BS,),\n            (0,),\n        )\n\n        b_z = tl.load(p_z, boundary_check=(0,))\n        b_ss = tl.load(p_ss, boundary_check=(0,))\n\n        m_i = o_i[:, None] <= j\n        b_doo += tl.where(m_i, exp(b_s - b_z[None, :]) * b_ss[None, :], 0.0)\n    b_doo += tl.load(p_doo, boundary_check=(0, 1))\n    tl.store(p_doo, b_doo.to(p_doo.dtype.element_ty), boundary_check=(0, 1))", "encoded": [26, 300, 196, 200, 93, 201, 93, 202, 93, 203, 93, 204, 93, 205, 54, 6, 93, 206, 54, 6, 93, 207, 54, 6, 93, 208, 54, 6, 93, 209, 54, 6, 144, 54, 28, -1, 210, 93, 211, 93, 212, 158, 196, 140, 196, 301, 144, 93, 140, 196, 302, 144, 93, 140, 196, 303, 144, 144, 28, 213, 93, 214, 158, 196, 211, 43, 209, 93, 211, 182, 209, 144, 28, 215, 158, 65, 196, 301, 93, 207, 144, 28, 216, 158, 191, 196, 185, 207, 93, 207, 24, 93, 302, 93, 78, 158, 124, 144, 28, 217, 158, 177, 196, 200, 64, 212, 197, 204, 197, 205, 93, 196, 204, 93, 205, 144, 93, 196, 205, 93, 302, 144, 93, 196, 213, 197, 206, 64, 214, 197, 207, 93, 210, 197, 208, 144, 93, 196, 207, 93, 208, 144, 93, 196, 302, 93, 301, 144, 144, 28, 218, 158, 177, 196, 201, 64, 212, 197, 204, 197, 205, 93, 196, 204, 197, 205, 93, 144, 93, 196, 302, 93, 144, 93, 196, 196, 213, 197, 206, 64, 214, 197, 207, 64, 207, 4, 302, 144, 197, 205, 64, 210, 197, 208, 93, 144, 93, 196, 208, 93, 144, 93, 196, 301, 93, 144, 144, 28, 219, 158, 177, 196, 203, 64, 212, 197, 204, 197, 205, 93, 196, 204, 93, 205, 144, 93, 196, 205, 93, 302, 144, 93, 196, 213, 197, 206, 64, 214, 197, 207, 93, 210, 197, 208, 144, 93, 196, 207, 93, 208, 144, 93, 196, 302, 93, 301, 144, 144, 28, 220, 158, 48, 196, 217, 93, 221, 158, 196, 301, 93, 302, 144, 144, 28, 222, 158, 48, 196, 218, 93, 221, 158, 196, 301, 93, 144, 144, 28, 223, 158, 145, 196, 185, 207, 93, 208, 24, 93, 78, 158, 124, 144, 28, 115, 224, 131, 5, 196, 214, 64, 302, 93, 209, 144, 54, 28, 225, 158, 177, 196, 201, 64, 212, 197, 204, 197, 205, 93, 196, 204, 93, 205, 144, 93, 196, 205, 93, 302, 144, 93, 196, 213, 197, 206, 64, 224, 197, 207, 93, 210, 197, 208, 144, 93, 196, 207, 93, 208, 144, 93, 196, 302, 93, 301, 144, 144, 28, 226, 158, 177, 196, 202, 64, 212, 197, 204, 197, 205, 93, 196, 204, 93, 205, 144, 93, 196, 205, 93, 302, 144, 93, 196, 213, 197, 206, 64, 224, 197, 207, 93, 210, 197, 208, 144, 93, 196, 207, 93, 208, 144, 93, 196, 302, 93, 301, 144, 144, 28, 227, 158, 48, 196, 225, 93, 221, 158, 196, 301, 93, 302, 144, 144, 28, 228, 158, 48, 196, 226, 93, 221, 158, 196, 301, 93, 302, 144, 144, 28, 223, 141, 228, 197, 168, 196, 222, 185, 163, 93, 54, 24, 4, 227, 144, 28, 67, 28, 223, 158, 168, 196, 220, 4, 222, 185, 163, 93, 54, 24, 144, 197, 15, 196, 216, 71, 229, 196, 220, 71, 78, 144, 93, 223, 71, 229, 196, 220, 71, 78, 144, 93, 230, 158, 51, 144, 28, 115, 231, 131, 5, 196, 301, 93, 207, 144, 54, 28, 225, 158, 177, 196, 201, 64, 212, 197, 204, 197, 205, 93, 196, 204, 197, 205, 93, 144, 93, 196, 302, 93, 144, 93, 196, 196, 213, 197, 206, 64, 214, 197, 207, 64, 231, 144, 197, 205, 64, 210, 197, 208, 93, 144, 93, 196, 208, 93, 144, 93, 196, 301, 93, 144, 144, 28, 226, 158, 177, 196, 202, 64, 212, 197, 204, 197, 205, 93, 196, 204, 197, 205, 93, 144, 93, 196, 302, 93, 144, 93, 196, 196, 213, 197, 206, 64, 214, 197, 207, 64, 231, 144, 197, 205, 64, 210, 197, 208, 93, 144, 93, 196, 208, 93, 144, 93, 196, 301, 93, 144, 144, 28, 227, 158, 48, 196, 225, 93, 221, 158, 196, 301, 93, 144, 144, 28, 228, 158, 48, 196, 226, 93, 221, 158, 196, 301, 93, 144, 144, 28, 232, 158, 215, 185, 54, 93, 163, 24, 176, 231, 28, 223, 141, 166, 196, 232, 93, 168, 196, 220, 4, 227, 185, 163, 93, 54, 24, 144, 197, 228, 185, 163, 93, 54, 24, 93, 301, 144, 28, 67, 28, 223, 141, 48, 196, 219, 93, 221, 158, 196, 301, 93, 302, 144, 144, 28, 10, 196, 219, 93, 223, 71, 229, 196, 219, 71, 78, 71, 98, 144, 93, 221, 158, 196, 301, 93, 302, 144, 144, 28, 3, 28]}, {"code": "def naive_attn_decoding_kernel(\n    q,\n    k,\n    v,\n    o,\n    g_cumsum,\n    scale,\n    gate_scale,\n    cu_seqlens,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n):\n    i_v, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // G\n\n    bos, eos = tl.load(cu_seqlens + i_b).to(tl.int32), tl.load(cu_seqlens + i_b + 1).to(\n        tl.int32\n    )\n    T = eos - bos\n\n    p_q = tl.make_block_ptr(q + i_bh * K, (K,), (1,), (0,), (BK,), (0,))\n    p_o = tl.make_block_ptr(o + i_bh * V, (V,), (1,), (0,), (BV,), (0,))\n\n    b_q = tl.load(p_q, boundary_check=(0,))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    b_o = tl.zeros(\n        [\n            BV,\n        ],\n        dtype=tl.float32,\n    )\n\n    b_m = tl.full(\n        [\n            1,\n        ],\n        float(\"-inf\"),\n        dtype=tl.float32,\n    )\n    b_acc = tl.zeros(\n        [\n            1,\n        ],\n        dtype=tl.float32,\n    )\n\n    if USE_G:\n        p_g = tl.make_block_ptr(\n            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (T - 1,), (1,), (0,)\n        )\n        b_gq = tl.load(p_g, boundary_check=(0,)).to(tl.float32)\n    else:\n        b_gq = None\n\n    for i_s in range(0, T, BS):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_s, 0), (BS, BK), (1, 0)\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_s, i_v * BV),\n            (BS, BV),\n            (1, 0),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_s = tl.sum(b_q[None, :] * b_k, 1)\n\n        mask = i_s + tl.arange(0, BS) < T\n        b_s = tl.where(mask, b_s, float(\"-inf\"))\n\n        if USE_G:\n            p_gk = tl.make_block_ptr(\n                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)\n            )\n            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n            b_s += (b_gq - b_gk) * gate_scale\n\n        b_m, b_mp = tl.maximum(b_m, tl.max(b_s)), b_m\n        b_r = exp(b_mp - b_m)\n\n        b_p = exp(b_s - b_m)\n\n        b_acc = b_acc * b_r + tl.sum(b_p, 0)\n\n        b_o = b_o * b_r + tl.sum(b_p[:, None] * b_v, 0)\n        b_mp = b_m\n    b_o = b_o / b_acc\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))", "encoded": [26, 300, 196, 200, 92, 201, 92, 202, 92, 203, 92, 204, 92, 205, 92, 206, 92, 207, 92, 208, 92, 209, 53, 6, 92, 210, 53, 6, 92, 211, 53, 6, 92, 212, 53, 6, 92, 213, 53, 6, 92, 214, 53, 6, 92, 215, 53, 6, 92, 216, 53, 6, 92, 217, 53, 6, 92, 218, 53, 6, 144, 53, 28, -1, 219, 92, 220, 158, 196, 140, 196, 301, 144, 92, 140, 196, 302, 144, 144, 28, 221, 92, 222, 158, 196, 220, 43, 211, 92, 220, 182, 211, 144, 28, 223, 158, 222, 43, 212, 28, 224, 92, 225, 158, 196, 49, 196, 207, 63, 221, 144, 70, 226, 196, 199, 144, 92, 49, 196, 207, 63, 221, 63, 302, 144, 70, 226, 196, 199, 144, 144, 28, 208, 158, 225, 4, 224, 28, 227, 158, 177, 196, 200, 63, 220, 197, 213, 92, 196, 213, 92, 144, 92, 196, 302, 92, 144, 92, 196, 301, 92, 144, 92, 196, 216, 92, 144, 92, 196, 301, 92, 144, 144, 28, 228, 158, 177, 196, 203, 63, 220, 197, 214, 92, 196, 214, 92, 144, 92, 196, 302, 92, 144, 92, 196, 301, 92, 144, 92, 196, 217, 92, 144, 92, 196, 301, 92, 144, 144, 28, 229, 158, 49, 196, 227, 92, 230, 158, 196, 301, 92, 144, 144, 28, 229, 158, 196, 229, 197, 205, 144, 70, 226, 196, 229, 70, 77, 144, 28, 231, 158, 145, 196, 185, 217, 24, 92, 77, 158, 123, 144, 28, 232, 158, 191, 196, 185, 302, 24, 92, 233, 196, 303, 144, 92, 77, 158, 123, 144, 28, 234, 158, 145, 196, 185, 302, 24, 92, 77, 158, 123, 144, 28, 150, 218, 53, 28, 235, 158, 177, 196, 204, 63, 224, 197, 211, 63, 222, 92, 196, 208, 92, 144, 92, 196, 211, 92, 144, 92, 196, 208, 4, 302, 92, 144, 92, 196, 302, 92, 144, 92, 196, 301, 92, 144, 144, 28, 236, 158, 49, 196, 235, 92, 230, 158, 196, 301, 92, 144, 144, 70, 226, 196, 123, 144, 28, 152, 28, 27, 53, 28, 236, 158, 163, 28, 48, 28, 114, 237, 131, 5, 196, 301, 92, 208, 92, 215, 144, 53, 28, 238, 158, 177, 196, 201, 63, 196, 224, 197, 210, 63, 223, 144, 197, 213, 92, 196, 208, 92, 213, 144, 92, 196, 210, 197, 213, 92, 302, 144, 92, 196, 237, 92, 301, 144, 92, 196, 215, 92, 216, 144, 92, 196, 302, 92, 301, 144, 144, 28, 239, 158, 177, 196, 202, 63, 196, 224, 197, 210, 63, 223, 144, 197, 214, 92, 196, 208, 92, 214, 144, 92, 196, 210, 197, 214, 92, 302, 144, 92, 196, 237, 92, 219, 197, 217, 144, 92, 196, 215, 92, 217, 144, 92, 196, 302, 92, 301, 144, 144, 28, 240, 158, 49, 196, 238, 92, 230, 158, 196, 301, 92, 302, 144, 144, 28, 241, 158, 49, 196, 239, 92, 230, 158, 196, 301, 92, 302, 144, 144, 28, 242, 158, 179, 196, 229, 185, 163, 92, 53, 24, 197, 240, 92, 302, 144, 28, 243, 158, 237, 63, 64, 196, 301, 92, 215, 144, 1, 208, 28, 242, 158, 166, 196, 243, 92, 242, 92, 233, 196, 303, 144, 144, 28, 150, 218, 53, 28, 244, 158, 177, 196, 204, 63, 224, 197, 211, 63, 222, 92, 196, 208, 92, 144, 92, 196, 211, 92, 144, 92, 196, 237, 92, 144, 92, 196, 215, 92, 144, 92, 196, 301, 92, 144, 144, 28, 245, 158, 49, 196, 244, 92, 230, 158, 196, 301, 92, 144, 144, 70, 226, 196, 123, 144, 28, 242, 141, 196, 236, 4, 245, 144, 197, 206, 28, 152, 28, 232, 92, 246, 158, 196, 157, 196, 232, 92, 12, 196, 242, 144, 144, 92, 232, 144, 28, 247, 158, 168, 196, 246, 4, 232, 144, 28, 248, 158, 168, 196, 242, 4, 232, 144, 28, 234, 158, 234, 197, 247, 63, 179, 196, 248, 92, 301, 144, 28, 231, 158, 231, 197, 247, 63, 179, 196, 248, 185, 53, 92, 163, 24, 197, 241, 92, 301, 144, 28, 246, 158, 232, 28, 66, 28, 231, 158, 231, 38, 234, 28, 10, 196, 228, 92, 231, 70, 226, 196, 228, 70, 77, 70, 97, 144, 92, 230, 158, 196, 301, 92, 144, 144, 28, 3, 28]}, {"code": "def parallel_attn_fwd_kernel(\n    q,\n    k,\n    v,\n    o,\n    g_cumsum,\n    lse,\n    scale,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // G\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        i_n = i_b\n        bos, eos = i_n * T, i_n * T + T\n\n    p_q = tl.make_block_ptr(\n        q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    p_o = tl.make_block_ptr(\n        o + (bos * HQ + i_hq) * V,\n        (T, V),\n        (HQ * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    p_lse = tl.make_block_ptr(\n        lse + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n\n    b_m = tl.full([BT], float(\"-inf\"), dtype=tl.float32)\n    b_acc = tl.zeros([BT], dtype=tl.float32)\n\n    if USE_G:\n        p_g = tl.make_block_ptr(\n            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)\n        )\n        b_gq = tl.load(p_g, boundary_check=(0,)).to(tl.float32)\n    else:\n        b_gq = None\n\n    for i_s in range(0, i_t * BT, BS):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_s, i_v * BV),\n            (BS, BV),\n            (1, 0),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, b_k)\n\n        if USE_G:\n            p_gk = tl.make_block_ptr(\n                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)\n            )\n            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n            b_s += b_gq[:, None] - b_gk[None, :]\n\n        b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m\n        b_r = exp(b_mp - b_m)\n\n        b_p = safe_exp(b_s - b_m[:, None])\n\n        b_acc = b_acc * b_r + tl.sum(b_p, 1)\n\n        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)\n\n        b_mp = b_m\n\n    o_q = i_t * BT + tl.arange(0, BT)\n    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_s, i_v * BV),\n            (BS, BV),\n            (1, 0),\n        )\n\n        o_k = i_s + tl.arange(0, BS)\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, b_k)\n        b_s = tl.where(o_q[:, None] >= o_k[None, :], b_s, float(\"-inf\"))\n\n        if USE_G:\n            p_gk = tl.make_block_ptr(\n                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)\n            )\n            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n            b_s += b_gq[:, None] - b_gk[None, :]\n\n        b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m\n        b_r = exp(b_mp - b_m)\n\n        b_p = safe_exp(b_s - b_m[:, None])\n\n        b_acc = b_acc * b_r + tl.sum(b_p, 1)\n\n        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)\n        b_mp = b_m\n\n    b_o = b_o / b_acc[:, None]\n    b_m += log(b_acc)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_lse, b_m.to(p_lse.dtype.element_ty), boundary_check=(0,))", "encoded": [26, 300, 196, 200, 93, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 54, 6, 93, 211, 54, 6, 93, 212, 54, 6, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 93, 218, 54, 6, 93, 219, 54, 6, 93, 220, 54, 6, 93, 221, 54, 6, 144, 54, 28, -1, 222, 93, 223, 93, 224, 158, 196, 140, 196, 301, 144, 93, 140, 196, 302, 144, 93, 140, 196, 303, 144, 144, 28, 225, 93, 226, 158, 196, 224, 43, 212, 93, 224, 182, 212, 144, 28, 227, 158, 226, 43, 213, 28, 150, 221, 54, 28, 228, 93, 223, 158, 196, 48, 196, 208, 64, 223, 197, 303, 144, 71, 229, 196, 53, 144, 93, 48, 196, 208, 64, 223, 197, 303, 64, 302, 144, 71, 229, 196, 53, 144, 144, 28, 230, 93, 231, 158, 196, 48, 196, 207, 64, 228, 144, 71, 229, 196, 53, 144, 93, 48, 196, 207, 64, 228, 64, 302, 144, 71, 229, 196, 53, 144, 144, 28, 209, 158, 231, 4, 230, 28, 152, 28, 27, 54, 28, 228, 158, 225, 28, 230, 93, 231, 158, 196, 228, 197, 209, 93, 228, 197, 209, 64, 209, 144, 28, 49, 28, 232, 158, 177, 196, 200, 64, 196, 230, 197, 212, 64, 226, 144, 197, 214, 93, 196, 209, 93, 214, 144, 93, 196, 212, 197, 214, 93, 302, 144, 93, 196, 223, 197, 216, 93, 301, 144, 93, 196, 216, 93, 218, 144, 93, 196, 302, 93, 301, 144, 144, 28, 233, 158, 177, 196, 203, 64, 196, 230, 197, 212, 64, 226, 144, 197, 215, 93, 196, 209, 93, 215, 144, 93, 196, 212, 197, 215, 93, 302, 144, 93, 196, 223, 197, 216, 93, 222, 197, 219, 144, 93, 196, 216, 93, 219, 144, 93, 196, 302, 93, 301, 144, 144, 28, 234, 158, 177, 196, 205, 64, 230, 197, 212, 64, 226, 93, 196, 209, 93, 144, 93, 196, 212, 93, 144, 93, 196, 223, 197, 216, 93, 144, 93, 196, 216, 93, 144, 93, 196, 301, 93, 144, 144, 28, 235, 158, 48, 196, 232, 93, 236, 158, 196, 301, 93, 302, 144, 144, 28, 235, 158, 196, 235, 197, 206, 144, 71, 229, 196, 235, 71, 78, 144, 28, 237, 158, 145, 196, 185, 216, 93, 219, 24, 93, 78, 158, 124, 144, 28, 238, 158, 191, 196, 185, 216, 24, 93, 239, 196, 304, 144, 93, 78, 158, 124, 144, 28, 240, 158, 145, 196, 185, 216, 24, 93, 78, 158, 124, 144, 28, 150, 220, 54, 28, 241, 158, 177, 196, 204, 64, 230, 197, 212, 64, 226, 93, 196, 209, 93, 144, 93, 196, 212, 93, 144, 93, 196, 223, 197, 216, 93, 144, 93, 196, 216, 93, 144, 93, 196, 301, 93, 144, 144, 28, 242, 158, 48, 196, 241, 93, 236, 158, 196, 301, 93, 144, 144, 71, 229, 196, 124, 144, 28, 152, 28, 27, 54, 28, 242, 158, 163, 28, 49, 28, 115, 243, 131, 5, 196, 301, 93, 223, 197, 216, 93, 217, 144, 54, 28, 244, 158, 177, 196, 201, 64, 196, 230, 197, 211, 64, 227, 144, 197, 214, 93, 196, 214, 93, 209, 144, 93, 196, 302, 93, 211, 197, 214, 144, 93, 196, 301, 93, 243, 144, 93, 196, 218, 93, 217, 144, 93, 196, 301, 93, 302, 144, 144, 28, 245, 158, 177, 196, 202, 64, 196, 230, 197, 211, 64, 227, 144, 197, 215, 93, 196, 209, 93, 215, 144, 93, 196, 211, 197, 215, 93, 302, 144, 93, 196, 243, 93, 222, 197, 219, 144, 93, 196, 217, 93, 219, 144, 93, 196, 302, 93, 301, 144, 144, 28, 246, 158, 48, 196, 244, 93, 236, 158, 196, 301, 93, 302, 144, 144, 28, 247, 158, 48, 196, 245, 93, 236, 158, 196, 301, 93, 302, 144, 144, 28, 248, 158, 15, 196, 235, 93, 246, 144, 28, 150, 220, 54, 28, 249, 158, 177, 196, 204, 64, 230, 197, 212, 64, 226, 93, 196, 209, 93, 144, 93, 196, 212, 93, 144, 93, 196, 243, 93, 144, 93, 196, 217, 93, 144, 93, 196, 301, 93, 144, 144, 28, 250, 158, 48, 196, 249, 93, 236, 158, 196, 301, 93, 144, 144, 71, 229, 196, 124, 144, 28, 248, 141, 242, 185, 54, 93, 163, 24, 4, 250, 185, 163, 93, 54, 24, 28, 152, 28, 238, 93, 251, 158, 196, 157, 196, 238, 93, 12, 196, 248, 93, 302, 144, 144, 93, 238, 144, 28, 252, 158, 168, 196, 251, 4, 238, 144, 28, 253, 158, 254, 196, 248, 4, 238, 185, 54, 93, 163, 24, 144, 28, 240, 158, 240, 197, 252, 64, 179, 196, 253, 93, 302, 144, 28, 237, 158, 237, 197, 252, 185, 54, 93, 163, 24, 64, 15, 196, 253, 71, 229, 196, 235, 71, 78, 144, 93, 247, 144, 28, 251, 158, 238, 28, 67, 28, 255, 158, 223, 197, 216, 64, 65, 196, 301, 93, 216, 144, 28, 115, 243, 131, 5, 196, 223, 197, 216, 93, 35, 196, 196, 223, 64, 302, 144, 197, 216, 93, 209, 144, 93, 217, 144, 54, 28, 244, 158, 177, 196, 201, 64, 196, 230, 197, 211, 64, 227, 144, 197, 214, 93, 196, 214, 93, 209, 144, 93, 196, 302, 93, 211, 197, 214, 144, 93, 196, 301, 93, 243, 144, 93, 196, 218, 93, 217, 144, 93, 196, 301, 93, 302, 144, 144, 28, 245, 158, 177, 196, 202, 64, 196, 230, 197, 211, 64, 227, 144, 197, 215, 93, 196, 209, 93, 215, 144, 93, 196, 211, 197, 215, 93, 302, 144, 93, 196, 243, 93, 222, 197, 219, 144, 93, 196, 217, 93, 219, 144, 93, 196, 302, 93, 301, 144, 144, 28, 256, 158, 243, 64, 65, 196, 301, 93, 217, 144, 28, 246, 158, 48, 196, 244, 93, 236, 158, 196, 301, 93, 302, 144, 144, 28, 247, 158, 48, 196, 245, 93, 236, 158, 196, 301, 93, 302, 144, 144, 28, 248, 158, 15, 196, 235, 93, 246, 144, 28, 248, 158, 166, 196, 255, 185, 54, 93, 163, 24, 123, 256, 185, 163, 93, 54, 24, 93, 248, 93, 239, 196, 304, 144, 144, 28, 150, 220, 54, 28, 249, 158, 177, 196, 204, 64, 230, 197, 212, 64, 226, 93, 196, 209, 93, 144, 93, 196, 212, 93, 144, 93, 196, 243, 93, 144, 93, 196, 217, 93, 144, 93, 196, 301, 93, 144, 144, 28, 250, 158, 48, 196, 249, 93, 236, 158, 196, 301, 93, 144, 144, 71, 229, 196, 124, 144, 28, 248, 141, 242, 185, 54, 93, 163, 24, 4, 250, 185, 163, 93, 54, 24, 28, 152, 28, 238, 93, 251, 158, 196, 157, 196, 238, 93, 12, 196, 248, 93, 302, 144, 144, 93, 238, 144, 28, 252, 158, 168, 196, 251, 4, 238, 144, 28, 253, 158, 254, 196, 248, 4, 238, 185, 54, 93, 163, 24, 144, 28, 240, 158, 240, 197, 252, 64, 179, 196, 253, 93, 302, 144, 28, 237, 158, 237, 197, 252, 185, 54, 93, 163, 24, 64, 15, 196, 253, 71, 229, 196, 235, 71, 78, 144, 93, 247, 144, 28, 251, 158, 238, 28, 67, 28, 237, 158, 237, 38, 240, 185, 54, 93, 163, 24, 28, 238, 141, 11, 196, 240, 144, 28, 10, 196, 233, 93, 237, 71, 229, 196, 233, 71, 78, 71, 98, 144, 93, 236, 158, 196, 301, 93, 302, 144, 144, 28, 10, 196, 234, 93, 238, 71, 229, 196, 234, 71, 78, 71, 98, 144, 93, 236, 158, 196, 301, 93, 144, 144, 28, 3, 28]}, {"code": "def parallel_attn_bwd_kernel_dq(\n    q,\n    k,\n    v,\n    lse,\n    delta,\n    do,\n    dq,\n    dg_cumsum,\n    g_cumsum,\n    scale,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    USE_G: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // G\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        i_n = i_b\n        bos, eos = i_n * T, i_n * T + T\n\n    p_q = tl.make_block_ptr(\n        q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    p_dq = tl.make_block_ptr(\n        dq + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    p_do = tl.make_block_ptr(\n        do + (bos * HQ + i_hq) * V,\n        (T, V),\n        (HQ * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    p_lse = tl.make_block_ptr(\n        lse + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)\n    )\n    p_delta = tl.make_block_ptr(\n        delta + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n\n    b_lse = tl.load(p_lse, boundary_check=(0,))\n    b_delta = tl.load(p_delta, boundary_check=(0,))\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    if USE_G:\n        b_dg = tl.zeros(\n            [\n                BT,\n            ],\n            dtype=tl.float32,\n        )\n        p_gq = tl.make_block_ptr(\n            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)\n        )\n        b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)\n    else:\n        b_gq = None\n        b_dg = None\n\n    for i_s in range(0, i_t * BT, BS):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (V, T),\n            (1, H * V),\n            (i_v * BV, i_s),\n            (BV, BS),\n            (0, 1),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, b_k)\n        if USE_G:\n            p_gk = tl.make_block_ptr(\n                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)\n            )\n            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n            b_s += b_gq[:, None] - b_gk[None, :]\n\n        b_p = safe_exp(b_s - b_lse[:, None])\n\n        b_dp = tl.dot(b_do, b_v)\n        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])\n\n        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))\n        if USE_G:\n            b_dg += tl.sum(b_ds, 1)\n\n    o_q = i_t * BT + tl.arange(0, BT)\n    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (V, T),\n            (1, H * V),\n            (i_v * BV, i_s),\n            (BV, BS),\n            (0, 1),\n        )\n\n        o_k = i_s + tl.arange(0, BS)\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, b_k)\n\n        if USE_G:\n            p_gk = tl.make_block_ptr(\n                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)\n            )\n            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n            b_s += b_gq[:, None] - b_gk[None, :]\n            b_s = tl.where(o_q[:, None] >= o_k[None, :], b_s, -float(\"inf\"))\n\n        b_p = safe_exp(b_s - b_lse[:, None])\n        b_p = tl.where(o_q[:, None] >= o_k[None, :], b_p, 0)\n\n        b_dp = tl.dot(b_do, b_v)\n        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])\n\n        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))\n        if USE_G:\n            b_dg += tl.sum(b_ds, 1)\n\n    b_dq *= scale\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    if USE_G:\n        p_dg = tl.make_block_ptr(\n            dg_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)\n        )\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))", "encoded": [26, 300, 196, 200, 92, 201, 92, 202, 92, 203, 92, 204, 92, 205, 92, 206, 92, 207, 92, 208, 92, 209, 92, 210, 92, 211, 92, 212, 92, 213, 53, 6, 92, 214, 53, 6, 92, 215, 53, 6, 92, 216, 53, 6, 92, 217, 53, 6, 92, 218, 53, 6, 92, 219, 53, 6, 92, 220, 53, 6, 92, 221, 53, 6, 92, 222, 53, 6, 92, 223, 53, 6, 92, 224, 53, 6, 144, 53, 28, -1, 225, 92, 226, 92, 227, 158, 196, 140, 196, 301, 144, 92, 140, 196, 302, 144, 92, 140, 196, 303, 144, 144, 28, 228, 92, 229, 158, 196, 227, 43, 215, 92, 227, 182, 215, 144, 28, 230, 158, 229, 43, 216, 28, 150, 223, 53, 28, 231, 92, 226, 158, 196, 49, 196, 211, 63, 226, 197, 303, 144, 70, 232, 196, 199, 144, 92, 49, 196, 211, 63, 226, 197, 303, 63, 302, 144, 70, 232, 196, 199, 144, 144, 28, 233, 92, 234, 158, 196, 49, 196, 210, 63, 231, 144, 70, 232, 196, 199, 144, 92, 49, 196, 210, 63, 231, 63, 302, 144, 70, 232, 196, 199, 144, 144, 28, 212, 158, 234, 4, 233, 28, 152, 28, 27, 53, 28, 231, 158, 228, 28, 233, 92, 234, 158, 196, 231, 197, 212, 92, 231, 197, 212, 63, 212, 144, 28, 48, 28, 235, 158, 177, 196, 200, 63, 196, 233, 197, 215, 63, 229, 144, 197, 217, 92, 196, 212, 92, 217, 144, 92, 196, 215, 197, 217, 92, 302, 144, 92, 196, 226, 197, 219, 92, 301, 144, 92, 196, 219, 92, 221, 144, 92, 196, 302, 92, 301, 144, 144, 28, 236, 158, 177, 196, 206, 63, 196, 233, 197, 215, 63, 229, 144, 197, 217, 92, 196, 212, 92, 217, 144, 92, 196, 215, 197, 217, 92, 302, 144, 92, 196, 226, 197, 219, 92, 301, 144, 92, 196, 219, 92, 221, 144, 92, 196, 302, 92, 301, 144, 144, 28, 237, 158, 177, 196, 205, 63, 196, 233, 197, 215, 63, 229, 144, 197, 218, 92, 196, 212, 92, 218, 144, 92, 196, 215, 197, 218, 92, 302, 144, 92, 196, 226, 197, 219, 92, 225, 197, 222, 144, 92, 196, 219, 92, 222, 144, 92, 196, 302, 92, 301, 144, 144, 28, 238, 158, 177, 196, 203, 63, 233, 197, 215, 63, 229, 92, 196, 212, 92, 144, 92, 196, 215, 92, 144, 92, 196, 226, 197, 219, 92, 144, 92, 196, 219, 92, 144, 92, 196, 301, 92, 144, 144, 28, 239, 158, 177, 196, 204, 63, 233, 197, 215, 63, 229, 92, 196, 212, 92, 144, 92, 196, 215, 92, 144, 92, 196, 226, 197, 219, 92, 144, 92, 196, 219, 92, 144, 92, 196, 301, 92, 144, 144, 28, 240, 158, 49, 196, 235, 92, 241, 158, 196, 301, 92, 302, 144, 144, 28, 240, 158, 196, 240, 197, 209, 144, 70, 232, 196, 240, 70, 77, 144, 28, 242, 158, 49, 196, 237, 92, 241, 158, 196, 301, 92, 302, 144, 144, 28, 243, 158, 49, 196, 238, 92, 241, 158, 196, 301, 92, 144, 144, 28, 244, 158, 49, 196, 239, 92, 241, 158, 196, 301, 92, 144, 144, 28, 245, 158, 145, 196, 185, 219, 92, 221, 24, 92, 77, 158, 123, 144, 28, 150, 224, 53, 28, 246, 158, 145, 196, 185, 219, 24, 92, 77, 158, 123, 144, 28, 247, 158, 177, 196, 208, 63, 233, 197, 215, 63, 229, 92, 196, 212, 92, 144, 92, 196, 215, 92, 144, 92, 196, 226, 197, 219, 92, 144, 92, 196, 219, 92, 144, 92, 196, 301, 92, 144, 144, 28, 248, 158, 49, 196, 247, 92, 241, 158, 196, 301, 92, 144, 144, 70, 232, 196, 123, 144, 28, 152, 28, 27, 53, 28, 248, 158, 163, 28, 246, 158, 163, 28, 48, 28, 114, 249, 131, 5, 196, 301, 92, 226, 197, 219, 92, 220, 144, 53, 28, 250, 158, 177, 196, 201, 63, 196, 233, 197, 214, 63, 230, 144, 197, 217, 92, 196, 217, 92, 212, 144, 92, 196, 302, 92, 214, 197, 217, 144, 92, 196, 301, 92, 249, 144, 92, 196, 221, 92, 220, 144, 92, 196, 301, 92, 302, 144, 144, 28, 251, 158, 177, 196, 202, 63, 196, 233, 197, 214, 63, 230, 144, 197, 218, 92, 196, 218, 92, 212, 144, 92, 196, 302, 92, 214, 197, 218, 144, 92, 196, 225, 197, 222, 92, 249, 144, 92, 196, 222, 92, 220, 144, 92, 196, 301, 92, 302, 144, 144, 28, 252, 158, 49, 196, 250, 92, 241, 158, 196, 301, 92, 302, 144, 144, 28, 253, 158, 49, 196, 251, 92, 241, 158, 196, 301, 92, 302, 144, 144, 28, 254, 158, 15, 196, 240, 92, 252, 144, 28, 150, 224, 53, 28, 255, 158, 177, 196, 208, 63, 233, 197, 215, 63, 229, 92, 196, 212, 92, 144, 92, 196, 215, 92, 144, 92, 196, 249, 92, 144, 92, 196, 220, 92, 144, 92, 196, 301, 92, 144, 144, 28, 256, 158, 49, 196, 255, 92, 241, 158, 196, 301, 92, 144, 144, 70, 232, 196, 123, 144, 28, 254, 141, 248, 185, 53, 92, 163, 24, 4, 256, 185, 163, 92, 53, 24, 28, 152, 28, 257, 158, 258, 196, 254, 4, 243, 185, 53, 92, 163, 24, 144, 28, 259, 158, 15, 196, 242, 92, 253, 144, 28, 260, 158, 257, 197, 196, 259, 70, 232, 196, 123, 144, 4, 244, 185, 53, 92, 163, 24, 144, 28, 245, 141, 15, 196, 260, 70, 232, 196, 252, 70, 77, 144, 92, 61, 196, 252, 144, 144, 28, 150, 224, 53, 28, 246, 141, 179, 196, 260, 92, 302, 144, 28, 152, 28, 66, 28, 261, 158, 226, 197, 219, 63, 64, 196, 301, 92, 219, 144, 28, 114, 249, 131, 5, 196, 226, 197, 219, 92, 35, 196, 196, 226, 63, 302, 144, 197, 219, 92, 212, 144, 92, 220, 144, 53, 28, 250, 158, 177, 196, 201, 63, 196, 233, 197, 214, 63, 230, 144, 197, 217, 92, 196, 217, 92, 212, 144, 92, 196, 302, 92, 214, 197, 217, 144, 92, 196, 301, 92, 249, 144, 92, 196, 221, 92, 220, 144, 92, 196, 301, 92, 302, 144, 144, 28, 251, 158, 177, 196, 202, 63, 196, 233, 197, 214, 63, 230, 144, 197, 218, 92, 196, 218, 92, 212, 144, 92, 196, 302, 92, 214, 197, 218, 144, 92, 196, 225, 197, 222, 92, 249, 144, 92, 196, 222, 92, 220, 144, 92, 196, 301, 92, 302, 144, 144, 28, 262, 158, 249, 63, 64, 196, 301, 92, 220, 144, 28, 252, 158, 49, 196, 250, 92, 241, 158, 196, 301, 92, 302, 144, 144, 28, 253, 158, 49, 196, 251, 92, 241, 158, 196, 301, 92, 302, 144, 144, 28, 254, 158, 15, 196, 240, 92, 252, 144, 28, 150, 224, 53, 28, 255, 158, 177, 196, 208, 63, 233, 197, 215, 63, 229, 92, 196, 212, 92, 144, 92, 196, 215, 92, 144, 92, 196, 249, 92, 144, 92, 196, 220, 92, 144, 92, 196, 301, 92, 144, 144, 28, 256, 158, 49, 196, 255, 92, 241, 158, 196, 301, 92, 144, 144, 70, 232, 196, 123, 144, 28, 254, 141, 248, 185, 53, 92, 163, 24, 4, 256, 185, 163, 92, 53, 24, 28, 254, 158, 166, 196, 261, 185, 53, 92, 163, 24, 122, 262, 185, 163, 92, 53, 24, 92, 254, 92, 4, 263, 196, 304, 144, 144, 28, 152, 28, 257, 158, 258, 196, 254, 4, 243, 185, 53, 92, 163, 24, 144, 28, 257, 158, 166, 196, 261, 185, 53, 92, 163, 24, 122, 262, 185, 163, 92, 53, 24, 92, 257, 92, 301, 144, 28, 259, 158, 15, 196, 242, 92, 253, 144, 28, 260, 158, 257, 197, 196, 259, 70, 232, 196, 123, 144, 4, 244, 185, 53, 92, 163, 24, 144, 28, 245, 141, 15, 196, 260, 70, 232, 196, 252, 70, 77, 144, 92, 61, 196, 252, 144, 144, 28, 150, 224, 53, 28, 246, 141, 179, 196, 260, 92, 302, 144, 28, 152, 28, 66, 28, 245, 21, 209, 28, 10, 196, 236, 92, 245, 70, 232, 196, 236, 70, 77, 70, 97, 144, 92, 241, 158, 196, 301, 92, 302, 144, 144, 28, 150, 224, 53, 28, 264, 158, 177, 196, 207, 63, 233, 197, 215, 63, 229, 92, 196, 212, 92, 144, 92, 196, 215, 92, 144, 92, 196, 226, 197, 219, 92, 144, 92, 196, 219, 92, 144, 92, 196, 301, 92, 144, 144, 28, 10, 196, 264, 92, 246, 70, 232, 196, 264, 70, 77, 70, 97, 144, 92, 241, 158, 196, 301, 92, 144, 144, 28, 152, 28, 3, 28]}, {"code": "def parallel_attn_bwd_kernel_dkv(\n    q,\n    k,\n    v,\n    g_cumsum,\n    lse,\n    delta,\n    do,\n    dk,\n    dv,\n    dg_cumsum,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // G\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        i_n = i_b\n        bos, eos = i_n * T, i_n * T + T\n\n    p_k = tl.make_block_ptr(\n        k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    p_v = tl.make_block_ptr(\n        v + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    p_dk = tl.make_block_ptr(\n        dk + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    p_dv = tl.make_block_ptr(\n        dv + (bos * HQ + i_hq) * V,\n        (T, V),\n        (HQ * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n\n    o_k = i_t * BT + tl.arange(0, BT)\n\n    if USE_G:\n        p_gk = tl.make_block_ptr(\n            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)\n        )\n        b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n        b_dg = tl.zeros(\n            [\n                BT,\n            ],\n            dtype=tl.float32,\n        )\n    else:\n        b_gk = None\n        b_dg = None\n\n    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n        p_q = tl.make_block_ptr(\n            q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_s, 0), (BS, BK), (1, 0)\n        )\n        p_do = tl.make_block_ptr(\n            do + (bos * HQ + i_hq) * V,\n            (T, V),\n            (HQ * V, 1),\n            (i_s, i_v * BV),\n            (BS, BV),\n            (1, 0),\n        )\n        p_lse = tl.make_block_ptr(\n            lse + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)\n        )\n        p_delta = tl.make_block_ptr(\n            delta + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)\n        )\n\n        o_q = i_s + tl.arange(0, BS)\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_lse = tl.load(p_lse, boundary_check=(0,))\n        b_delta = tl.load(p_delta, boundary_check=(0,))\n\n        b_s = tl.dot(b_k, tl.trans(b_q))\n        if USE_G:\n            p_gq = tl.make_block_ptr(\n                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)\n            )\n            b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)\n            b_s += b_gq[None, :] - b_gk[:, None]\n            b_s = tl.where(o_k[:, None] <= o_q[None, :], b_s, -float(\"inf\"))\n        b_p = safe_exp(b_s - b_lse[None, :])\n        b_p = tl.where(o_k[:, None] <= o_q[None, :], b_p, 0)\n\n        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)\n\n        b_dp = tl.dot(b_v, tl.trans(b_do))\n\n        b_ds = b_p * (b_dp - b_delta[None, :])\n\n        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\n        if USE_G:\n            b_dg -= tl.sum(b_ds, 1)\n\n    for i_s in range((i_t + 1) * BT, tl.cdiv(T, BS) * BS, BS):\n        p_q = tl.make_block_ptr(\n            q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_s, 0), (BS, BK), (1, 0)\n        )\n        p_do = tl.make_block_ptr(\n            do + (bos * HQ + i_hq) * V,\n            (T, V),\n            (HQ * V, 1),\n            (i_s, i_v * BV),\n            (BS, BV),\n            (1, 0),\n        )\n        p_lse = tl.make_block_ptr(\n            lse + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)\n        )\n        p_delta = tl.make_block_ptr(\n            delta + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)\n        )\n\n        o_q = i_s + tl.arange(0, BS)\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_lse = tl.load(p_lse, boundary_check=(0,))\n        b_delta = tl.load(p_delta, boundary_check=(0,))\n\n        b_s = tl.dot(b_k, tl.trans(b_q))\n        if USE_G:\n            p_gq = tl.make_block_ptr(\n                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)\n            )\n            b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)\n            b_s += b_gq[None, :] - b_gk[:, None]\n        b_p = safe_exp(b_s - b_lse[None, :])\n\n        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)\n\n        b_dp = tl.dot(b_v, tl.trans(b_do))\n\n        b_ds = b_p * (b_dp - b_delta[None, :])\n\n        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\n        if USE_G:\n            b_dg -= tl.sum(b_ds, 1)\n\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    if USE_G:\n        p_dg = tl.make_block_ptr(\n            dg_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)\n        )\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))", "encoded": [26, 300, 196, 200, 93, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 93, 211, 93, 212, 93, 213, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 93, 218, 54, 6, 93, 219, 54, 6, 93, 220, 54, 6, 93, 221, 54, 6, 93, 222, 54, 6, 93, 223, 54, 6, 93, 224, 54, 6, 93, 225, 54, 6, 144, 54, 28, -1, 226, 93, 227, 93, 228, 158, 196, 140, 196, 301, 144, 93, 140, 196, 302, 144, 93, 140, 196, 303, 144, 144, 28, 229, 93, 230, 158, 196, 228, 43, 216, 93, 228, 182, 216, 144, 28, 231, 158, 230, 43, 217, 28, 150, 225, 54, 28, 232, 93, 227, 158, 196, 48, 196, 211, 64, 227, 197, 303, 144, 71, 233, 196, 53, 144, 93, 48, 196, 211, 64, 227, 197, 303, 64, 302, 144, 71, 233, 196, 53, 144, 144, 28, 234, 93, 235, 158, 196, 48, 196, 210, 64, 232, 144, 71, 233, 196, 53, 144, 93, 48, 196, 210, 64, 232, 64, 302, 144, 71, 233, 196, 53, 144, 144, 28, 213, 158, 235, 4, 234, 28, 152, 28, 27, 54, 28, 232, 158, 229, 28, 234, 93, 235, 158, 196, 232, 197, 213, 93, 232, 197, 213, 64, 213, 144, 28, 49, 28, 236, 158, 177, 196, 201, 64, 196, 234, 197, 215, 64, 231, 144, 197, 218, 93, 196, 213, 93, 218, 144, 93, 196, 215, 197, 218, 93, 302, 144, 93, 196, 227, 197, 220, 93, 301, 144, 93, 196, 220, 93, 222, 144, 93, 196, 302, 93, 301, 144, 144, 28, 237, 158, 177, 196, 202, 64, 196, 234, 197, 215, 64, 231, 144, 197, 219, 93, 196, 213, 93, 219, 144, 93, 196, 215, 197, 219, 93, 302, 144, 93, 196, 227, 197, 220, 93, 226, 197, 223, 144, 93, 196, 220, 93, 223, 144, 93, 196, 302, 93, 301, 144, 144, 28, 238, 158, 177, 196, 207, 64, 196, 234, 197, 216, 64, 230, 144, 197, 218, 93, 196, 213, 93, 218, 144, 93, 196, 216, 197, 218, 93, 302, 144, 93, 196, 227, 197, 220, 93, 301, 144, 93, 196, 220, 93, 222, 144, 93, 196, 302, 93, 301, 144, 144, 28, 239, 158, 177, 196, 208, 64, 196, 234, 197, 216, 64, 230, 144, 197, 219, 93, 196, 213, 93, 219, 144, 93, 196, 216, 197, 219, 93, 302, 144, 93, 196, 227, 197, 220, 93, 226, 197, 223, 144, 93, 196, 220, 93, 223, 144, 93, 196, 302, 93, 301, 144, 144, 28, 240, 158, 48, 196, 236, 93, 241, 158, 196, 301, 93, 302, 144, 144, 28, 242, 158, 145, 196, 185, 220, 93, 222, 24, 93, 78, 158, 124, 144, 28, 243, 158, 48, 196, 237, 93, 241, 158, 196, 301, 93, 302, 144, 144, 28, 244, 158, 145, 196, 185, 220, 93, 223, 24, 93, 78, 158, 124, 144, 28, 245, 158, 227, 197, 220, 64, 65, 196, 301, 93, 220, 144, 28, 150, 224, 54, 28, 246, 158, 177, 196, 203, 64, 234, 197, 216, 64, 230, 93, 196, 213, 93, 144, 93, 196, 216, 93, 144, 93, 196, 227, 197, 220, 93, 144, 93, 196, 220, 93, 144, 93, 196, 301, 93, 144, 144, 28, 247, 158, 48, 196, 246, 93, 241, 158, 196, 301, 93, 144, 144, 71, 233, 196, 124, 144, 28, 248, 158, 145, 196, 185, 220, 24, 93, 78, 158, 124, 144, 28, 152, 28, 27, 54, 28, 247, 158, 163, 28, 248, 158, 163, 28, 49, 28, 115, 249, 131, 5, 196, 227, 197, 220, 93, 35, 196, 196, 227, 64, 302, 144, 197, 220, 93, 213, 144, 93, 221, 144, 54, 28, 250, 158, 177, 196, 200, 64, 196, 234, 197, 216, 64, 230, 144, 197, 218, 93, 196, 213, 93, 218, 144, 93, 196, 216, 197, 218, 93, 302, 144, 93, 196, 249, 93, 301, 144, 93, 196, 221, 93, 222, 144, 93, 196, 302, 93, 301, 144, 144, 28, 251, 158, 177, 196, 206, 64, 196, 234, 197, 216, 64, 230, 144, 197, 219, 93, 196, 213, 93, 219, 144, 93, 196, 216, 197, 219, 93, 302, 144, 93, 196, 249, 93, 226, 197, 223, 144, 93, 196, 221, 93, 223, 144, 93, 196, 302, 93, 301, 144, 144, 28, 252, 158, 177, 196, 204, 64, 234, 197, 216, 64, 230, 93, 196, 213, 93, 144, 93, 196, 216, 93, 144, 93, 196, 249, 93, 144, 93, 196, 221, 93, 144, 93, 196, 301, 93, 144, 144, 28, 253, 158, 177, 196, 205, 64, 234, 197, 216, 64, 230, 93, 196, 213, 93, 144, 93, 196, 216, 93, 144, 93, 196, 249, 93, 144, 93, 196, 221, 93, 144, 93, 196, 301, 93, 144, 144, 28, 254, 158, 249, 64, 65, 196, 301, 93, 221, 144, 28, 255, 158, 48, 196, 250, 93, 241, 158, 196, 301, 93, 302, 144, 144, 28, 255, 158, 196, 255, 197, 212, 144, 71, 233, 196, 255, 71, 78, 144, 28, 256, 158, 48, 196, 251, 93, 241, 158, 196, 301, 93, 302, 144, 144, 28, 257, 158, 48, 196, 252, 93, 241, 158, 196, 301, 93, 144, 144, 28, 258, 158, 48, 196, 253, 93, 241, 158, 196, 301, 93, 144, 144, 28, 259, 158, 15, 196, 240, 93, 62, 196, 255, 144, 144, 28, 150, 224, 54, 28, 260, 158, 177, 196, 203, 64, 234, 197, 216, 64, 230, 93, 196, 213, 93, 144, 93, 196, 216, 93, 144, 93, 196, 249, 93, 144, 93, 196, 221, 93, 144, 93, 196, 301, 93, 144, 144, 28, 261, 158, 48, 196, 260, 93, 241, 158, 196, 301, 93, 144, 144, 71, 233, 196, 124, 144, 28, 259, 141, 261, 185, 163, 93, 54, 24, 4, 247, 185, 54, 93, 163, 24, 28, 259, 158, 166, 196, 245, 185, 54, 93, 163, 24, 176, 254, 185, 163, 93, 54, 24, 93, 259, 93, 4, 262, 196, 304, 144, 144, 28, 152, 28, 263, 158, 264, 196, 259, 4, 257, 185, 163, 93, 54, 24, 144, 28, 263, 158, 166, 196, 245, 185, 54, 93, 163, 24, 176, 254, 185, 163, 93, 54, 24, 93, 263, 93, 301, 144, 28, 244, 141, 15, 196, 263, 71, 233, 196, 256, 71, 78, 144, 93, 256, 144, 28, 265, 158, 15, 196, 243, 93, 62, 196, 256, 144, 144, 28, 266, 158, 263, 197, 196, 265, 4, 258, 185, 163, 93, 54, 24, 144, 28, 242, 141, 15, 196, 266, 71, 233, 196, 255, 71, 78, 144, 93, 255, 144, 28, 150, 224, 54, 28, 248, 2, 179, 196, 266, 93, 302, 144, 28, 152, 28, 67, 28, 115, 249, 131, 5, 196, 196, 227, 64, 302, 144, 197, 220, 93, 56, 196, 213, 93, 221, 144, 197, 221, 93, 221, 144, 54, 28, 250, 158, 177, 196, 200, 64, 196, 234, 197, 216, 64, 230, 144, 197, 218, 93, 196, 213, 93, 218, 144, 93, 196, 216, 197, 218, 93, 302, 144, 93, 196, 249, 93, 301, 144, 93, 196, 221, 93, 222, 144, 93, 196, 302, 93, 301, 144, 144, 28, 251, 158, 177, 196, 206, 64, 196, 234, 197, 216, 64, 230, 144, 197, 219, 93, 196, 213, 93, 219, 144, 93, 196, 216, 197, 219, 93, 302, 144, 93, 196, 249, 93, 226, 197, 223, 144, 93, 196, 221, 93, 223, 144, 93, 196, 302, 93, 301, 144, 144, 28, 252, 158, 177, 196, 204, 64, 234, 197, 216, 64, 230, 93, 196, 213, 93, 144, 93, 196, 216, 93, 144, 93, 196, 249, 93, 144, 93, 196, 221, 93, 144, 93, 196, 301, 93, 144, 144, 28, 253, 158, 177, 196, 205, 64, 234, 197, 216, 64, 230, 93, 196, 213, 93, 144, 93, 196, 216, 93, 144, 93, 196, 249, 93, 144, 93, 196, 221, 93, 144, 93, 196, 301, 93, 144, 144, 28, 254, 158, 249, 64, 65, 196, 301, 93, 221, 144, 28, 255, 158, 48, 196, 250, 93, 241, 158, 196, 301, 93, 302, 144, 144, 28, 255, 158, 196, 255, 197, 212, 144, 71, 233, 196, 255, 71, 78, 144, 28, 256, 158, 48, 196, 251, 93, 241, 158, 196, 301, 93, 302, 144, 144, 28, 257, 158, 48, 196, 252, 93, 241, 158, 196, 301, 93, 144, 144, 28, 258, 158, 48, 196, 253, 93, 241, 158, 196, 301, 93, 144, 144, 28, 259, 158, 15, 196, 240, 93, 62, 196, 255, 144, 144, 28, 150, 224, 54, 28, 260, 158, 177, 196, 203, 64, 234, 197, 216, 64, 230, 93, 196, 213, 93, 144, 93, 196, 216, 93, 144, 93, 196, 249, 93, 144, 93, 196, 221, 93, 144, 93, 196, 301, 93, 144, 144, 28, 261, 158, 48, 196, 260, 93, 241, 158, 196, 301, 93, 144, 144, 71, 233, 196, 124, 144, 28, 259, 141, 261, 185, 163, 93, 54, 24, 4, 247, 185, 54, 93, 163, 24, 28, 152, 28, 263, 158, 264, 196, 259, 4, 257, 185, 163, 93, 54, 24, 144, 28, 244, 141, 15, 196, 263, 71, 233, 196, 256, 71, 78, 144, 93, 256, 144, 28, 265, 158, 15, 196, 243, 93, 62, 196, 256, 144, 144, 28, 266, 158, 263, 197, 196, 265, 4, 258, 185, 163, 93, 54, 24, 144, 28, 242, 141, 15, 196, 266, 71, 233, 196, 255, 71, 78, 144, 93, 255, 144, 28, 150, 224, 54, 28, 248, 2, 179, 196, 266, 93, 302, 144, 28, 152, 28, 67, 28, 10, 196, 238, 93, 242, 71, 233, 196, 238, 71, 78, 71, 98, 144, 93, 241, 158, 196, 301, 93, 302, 144, 144, 28, 10, 196, 239, 93, 244, 71, 233, 196, 239, 71, 78, 71, 98, 144, 93, 241, 158, 196, 301, 93, 302, 144, 144, 28, 150, 224, 54, 28, 267, 158, 177, 196, 209, 64, 234, 197, 216, 64, 230, 93, 196, 213, 93, 144, 93, 196, 216, 93, 144, 93, 196, 227, 197, 220, 93, 144, 93, 196, 220, 93, 144, 93, 196, 301, 93, 144, 144, 28, 10, 196, 267, 93, 248, 71, 233, 196, 267, 71, 78, 71, 98, 144, 93, 241, 158, 196, 301, 93, 144, 144, 28, 152, 28, 3, 28]}, {"code": "def fused_chunk_based_fwd_kernel(\n    q,\n    k,\n    v,\n    o,\n    z,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    o_i = tl.arange(0, BT)\n\n    m_s = o_i[:, None] >= o_i[None, :]\n\n    b_h_0o = tl.zeros([BV], dtype=tl.float32)\n\n    b_h_1o = tl.zeros([BK, BV], dtype=tl.float32)\n\n    b_h_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\n\n    p_q = tl.make_block_ptr(\n        q + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BT), (0, 1)\n    )\n    p_v = tl.make_block_ptr(\n        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_o = tl.make_block_ptr(\n        o + (i_bh + i_k * B * H) * T * V,\n        (T, V),\n        (V, 1),\n        (0, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n\n    p_z = z + (i_bh + i_k * B * H) * T + tl.arange(0, BT)\n    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\n    k_1o = tl.zeros([1, BK], dtype=tl.float32)\n    k_0o = 0\n\n    for i in range(0, tl.cdiv(T, BT)):\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_k_2o = b_k[:, None, :] * b_k[None, :, :]\n        b_k_2o = tl.reshape(b_k_2o, [BK * BK, BT]).to(b_k.dtype)\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_q = (tl.load(p_q, boundary_check=(0, 1)) * scale).to(b_k.dtype)\n        b_o = tl.zeros([BT, BV], dtype=tl.float32)\n        b_z = tl.zeros([BT], dtype=tl.float32)\n\n        b_o += b_h_0o\n        b_z += k_0o\n\n        b_o += tl.dot(b_q, b_h_1o.to(b_q.dtype), allow_tf32=False)\n        b_z += tl.sum(b_q * k_1o, axis=1)\n\n        b_q_2o = b_q[:, :, None] * b_q[:, None, :]\n        b_q_2o = tl.reshape(b_q_2o, [BT, BK * BK]).to(b_k.dtype)\n        b_o += tl.dot(b_q_2o, b_h_2o.to(b_q_2o.dtype), allow_tf32=False) * 0.5\n        b_z += tl.sum(b_q_2o * k_2o, axis=1) * 0.5\n\n        k_1o += tl.sum(b_k, axis=1)[None, :]\n        k_2o += tl.sum(b_k_2o, axis=1)[None, :]\n        k_0o += BT\n\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(\n            p_z, b_z.to(p_z.dtype.element_ty), mask=(i * BT + tl.arange(0, BT)) < T\n        )\n\n        b_h_2o = b_h_2o + tl.dot(b_k_2o.to(b_v.dtype), b_v, allow_tf32=False)\n        b_h_1o = b_h_1o + tl.dot(b_k, b_v, allow_tf32=False)\n        b_h_0o = b_h_0o + tl.sum(b_v, axis=0)\n\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_z += BT", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 54, 6, 93, 209, 54, 6, 93, 210, 54, 6, 93, 211, 54, 6, 93, 212, 54, 6, 93, 213, 54, 6, 93, 214, 54, 6, 145, 54, 29, -1, 215, 93, 216, 93, 217, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 93, 141, 197, 304, 145, 145, 29, 218, 159, 65, 197, 302, 93, 212, 145, 29, 219, 159, 218, 186, 54, 93, 164, 25, 123, 218, 186, 164, 93, 54, 25, 29, 220, 159, 146, 197, 186, 214, 25, 93, 78, 159, 124, 145, 29, 221, 159, 146, 197, 186, 213, 93, 214, 25, 93, 78, 159, 124, 145, 29, 222, 159, 146, 197, 186, 213, 198, 213, 93, 214, 25, 93, 78, 159, 124, 145, 29, 223, 159, 178, 197, 201, 64, 217, 198, 207, 198, 210, 93, 197, 207, 93, 210, 145, 93, 197, 210, 93, 303, 145, 93, 197, 302, 93, 216, 198, 213, 145, 93, 197, 212, 93, 213, 145, 93, 197, 303, 93, 302, 145, 145, 29, 224, 159, 178, 197, 202, 64, 217, 198, 207, 198, 210, 93, 197, 210, 93, 207, 145, 93, 197, 303, 93, 210, 145, 93, 197, 216, 198, 213, 93, 302, 145, 93, 197, 213, 93, 212, 145, 93, 197, 302, 93, 303, 145, 145, 29, 225, 159, 178, 197, 203, 64, 217, 198, 207, 198, 211, 93, 197, 207, 93, 211, 145, 93, 197, 211, 93, 303, 145, 93, 197, 302, 93, 215, 198, 214, 145, 93, 197, 212, 93, 214, 145, 93, 197, 303, 93, 302, 145, 145, 29, 226, 159, 178, 197, 204, 64, 197, 217, 64, 216, 198, 208, 198, 209, 145, 198, 207, 198, 211, 93, 197, 207, 93, 211, 145, 93, 197, 211, 93, 303, 145, 93, 197, 302, 93, 215, 198, 214, 145, 93, 197, 212, 93, 214, 145, 93, 197, 303, 93, 302, 145, 145, 29, 227, 159, 205, 64, 197, 217, 64, 216, 198, 208, 198, 209, 145, 198, 207, 64, 65, 197, 302, 93, 212, 145, 29, 228, 159, 146, 197, 186, 303, 93, 213, 198, 213, 25, 93, 78, 159, 124, 145, 29, 229, 159, 146, 197, 186, 303, 93, 213, 25, 93, 78, 159, 124, 145, 29, 230, 159, 302, 29, 115, 231, 132, 5, 197, 302, 93, 56, 197, 207, 93, 212, 145, 145, 54, 29, 232, 159, 50, 197, 224, 93, 233, 159, 197, 302, 93, 303, 145, 145, 29, 234, 159, 232, 186, 54, 93, 164, 93, 54, 25, 198, 232, 186, 164, 93, 54, 93, 54, 25, 29, 234, 159, 21, 197, 234, 93, 186, 213, 198, 213, 93, 212, 25, 145, 71, 235, 197, 232, 71, 78, 145, 29, 236, 159, 50, 197, 225, 93, 233, 159, 197, 302, 93, 303, 145, 145, 29, 237, 159, 197, 50, 197, 223, 93, 233, 159, 197, 302, 93, 303, 145, 145, 198, 206, 145, 71, 235, 197, 232, 71, 78, 145, 29, 238, 159, 146, 197, 186, 212, 93, 214, 25, 93, 78, 159, 124, 145, 29, 239, 159, 146, 197, 186, 212, 25, 93, 78, 159, 124, 145, 29, 238, 142, 220, 29, 239, 142, 230, 29, 238, 142, 15, 197, 237, 93, 221, 71, 235, 197, 237, 71, 78, 145, 93, 240, 159, 52, 145, 29, 239, 142, 180, 197, 237, 198, 229, 93, 241, 159, 303, 145, 29, 242, 159, 237, 186, 54, 93, 54, 93, 164, 25, 198, 237, 186, 54, 93, 164, 93, 54, 25, 29, 242, 159, 21, 197, 242, 93, 186, 212, 93, 213, 198, 213, 25, 145, 71, 235, 197, 232, 71, 78, 145, 29, 238, 142, 15, 197, 242, 93, 222, 71, 235, 197, 242, 71, 78, 145, 93, 240, 159, 52, 145, 198, 305, 29, 239, 142, 180, 197, 242, 198, 228, 93, 241, 159, 303, 145, 198, 305, 29, 229, 142, 180, 197, 232, 93, 241, 159, 303, 145, 186, 164, 93, 54, 25, 29, 228, 142, 180, 197, 234, 93, 241, 159, 303, 145, 186, 164, 93, 54, 25, 29, 230, 142, 212, 29, 243, 159, 15, 197, 237, 93, 232, 93, 240, 159, 52, 145, 29, 243, 159, 303, 64, 243, 64, 305, 198, 243, 198, 243, 29, 243, 159, 167, 197, 219, 93, 243, 93, 302, 145, 29, 239, 142, 180, 197, 243, 93, 241, 159, 303, 145, 29, 238, 142, 15, 197, 243, 71, 235, 197, 237, 71, 78, 145, 93, 236, 93, 240, 159, 52, 145, 29, 10, 197, 226, 93, 238, 71, 235, 197, 226, 71, 78, 71, 98, 145, 93, 233, 159, 197, 302, 93, 303, 145, 145, 29, 10, 197, 227, 93, 239, 71, 235, 197, 227, 71, 78, 71, 98, 145, 93, 244, 159, 231, 198, 212, 64, 65, 197, 302, 93, 212, 145, 1, 207, 145, 29, 222, 159, 222, 64, 15, 197, 234, 71, 235, 197, 236, 71, 78, 145, 93, 236, 93, 240, 159, 52, 145, 29, 221, 159, 221, 64, 15, 197, 232, 93, 236, 93, 240, 159, 52, 145, 29, 220, 159, 220, 64, 180, 197, 236, 93, 241, 159, 302, 145, 29, 223, 159, 122, 197, 223, 93, 197, 212, 93, 302, 145, 145, 29, 224, 159, 122, 197, 224, 93, 197, 302, 93, 212, 145, 145, 29, 225, 159, 122, 197, 225, 93, 197, 212, 93, 302, 145, 145, 29, 226, 159, 122, 197, 226, 93, 197, 212, 93, 302, 145, 145, 29, 227, 142, 212, 29, 67, 29, 3, 29]}, {"code": "def parallel_based_fwd_kernel(\n    q,\n    k,\n    v,\n    o,\n    z,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BTL: tl.constexpr,\n    BTS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // (NV)\n    i_v = i_kv % (NV)\n\n    p_q = tl.make_block_ptr(\n        q + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0)\n    )\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BTS), (0, 1)\n    )\n    p_v = tl.make_block_ptr(\n        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BTS, BV), (1, 0)\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n\n    for _ in range(0, i_c * BTL, BTS):\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, (b_k), allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n\n        b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_c * BTL), (BK, BTS), (0, 1)\n    )\n    p_v = tl.make_block_ptr(\n        v + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTS, BV), (1, 0)\n    )\n\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n\n        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n\n    p_o = tl.make_block_ptr(\n        o + (i_bh + B * H * i_k) * T * V,\n        (T, V),\n        (V, 1),\n        (i_c * BTL, i_v * BV),\n        (BTL, BV),\n        (1, 0),\n    )\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(\n        p_z, b_z.to(p_z.dtype.element_ty), mask=((i_c * BTL + tl.arange(0, BTL)) < T)\n    )", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 55, 6, 94, 209, 55, 6, 94, 210, 55, 6, 94, 211, 55, 6, 94, 212, 55, 6, 94, 213, 55, 6, 94, 214, 55, 6, 94, 215, 55, 6, 145, 55, 29, -1, 216, 94, 217, 94, 218, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 94, 141, 197, 304, 145, 145, 29, 219, 159, 57, 197, 211, 94, 215, 145, 29, 220, 159, 216, 44, 219, 29, 221, 159, 216, 183, 219, 29, 222, 159, 178, 197, 201, 65, 218, 198, 207, 198, 210, 94, 197, 207, 94, 210, 145, 94, 197, 210, 94, 303, 145, 94, 197, 217, 198, 212, 94, 220, 198, 214, 145, 94, 197, 212, 94, 214, 145, 94, 197, 303, 94, 302, 145, 145, 29, 223, 159, 178, 197, 202, 65, 218, 198, 207, 198, 210, 94, 197, 210, 94, 207, 145, 94, 197, 303, 94, 210, 145, 94, 197, 220, 198, 214, 94, 302, 145, 94, 197, 214, 94, 213, 145, 94, 197, 302, 94, 303, 145, 145, 29, 224, 159, 178, 197, 203, 65, 218, 198, 207, 198, 211, 94, 197, 207, 94, 211, 145, 94, 197, 211, 94, 303, 145, 94, 197, 302, 94, 221, 198, 215, 145, 94, 197, 213, 94, 215, 145, 94, 197, 303, 94, 302, 145, 145, 29, 225, 159, 49, 197, 222, 94, 226, 159, 197, 302, 94, 303, 145, 145, 29, 225, 159, 197, 225, 198, 206, 145, 72, 227, 197, 225, 72, 79, 145, 29, 228, 159, 146, 197, 186, 212, 94, 215, 25, 94, 79, 159, 125, 145, 29, 229, 159, 146, 197, 186, 212, 25, 94, 79, 159, 125, 145, 29, 116, 230, 132, 5, 197, 302, 94, 217, 198, 212, 94, 213, 145, 55, 29, 231, 159, 49, 197, 223, 94, 226, 159, 197, 302, 94, 303, 145, 145, 29, 232, 159, 49, 197, 224, 94, 226, 159, 197, 302, 94, 303, 145, 145, 29, 233, 159, 15, 197, 225, 94, 231, 94, 234, 159, 52, 145, 29, 233, 159, 303, 65, 233, 65, 305, 198, 233, 198, 233, 29, 229, 142, 180, 197, 233, 94, 235, 159, 303, 145, 29, 228, 159, 228, 65, 15, 197, 233, 72, 227, 197, 232, 72, 79, 145, 94, 232, 94, 234, 159, 52, 145, 29, 223, 159, 123, 197, 223, 94, 197, 302, 94, 213, 145, 145, 29, 224, 159, 123, 197, 224, 94, 197, 213, 94, 302, 145, 145, 29, 68, 29, 46, 197, 145, 29, 236, 159, 66, 197, 302, 94, 212, 145, 29, 237, 159, 66, 197, 302, 94, 213, 145, 29, 223, 159, 178, 197, 202, 65, 218, 198, 207, 198, 210, 94, 197, 210, 94, 207, 145, 94, 197, 303, 94, 210, 145, 94, 197, 220, 198, 214, 94, 217, 198, 212, 145, 94, 197, 214, 94, 213, 145, 94, 197, 302, 94, 303, 145, 145, 29, 224, 159, 178, 197, 203, 65, 218, 198, 207, 198, 211, 94, 197, 207, 94, 211, 145, 94, 197, 211, 94, 303, 145, 94, 197, 217, 198, 212, 94, 221, 198, 215, 145, 94, 197, 213, 94, 215, 145, 94, 197, 303, 94, 302, 145, 145, 29, 116, 230, 132, 5, 197, 217, 198, 212, 94, 197, 217, 65, 303, 145, 198, 212, 94, 213, 145, 55, 29, 231, 159, 49, 197, 223, 94, 226, 159, 197, 302, 94, 303, 145, 145, 29, 232, 159, 49, 197, 224, 94, 226, 159, 197, 302, 94, 303, 145, 145, 29, 238, 159, 236, 186, 55, 94, 164, 25, 124, 237, 186, 164, 94, 55, 25, 29, 233, 159, 15, 197, 225, 94, 231, 94, 234, 159, 52, 145, 29, 233, 159, 303, 65, 233, 65, 305, 198, 233, 198, 233, 29, 233, 159, 167, 197, 238, 94, 233, 94, 302, 145, 29, 229, 142, 180, 197, 233, 94, 235, 159, 303, 145, 29, 228, 142, 15, 197, 233, 72, 227, 197, 225, 72, 79, 145, 94, 232, 94, 234, 159, 52, 145, 29, 223, 159, 123, 197, 223, 94, 197, 302, 94, 213, 145, 145, 29, 224, 159, 123, 197, 224, 94, 197, 213, 94, 302, 145, 145, 29, 237, 142, 213, 29, 68, 29, 239, 159, 178, 197, 204, 65, 197, 218, 65, 208, 198, 209, 198, 220, 145, 198, 207, 198, 211, 94, 197, 207, 94, 211, 145, 94, 197, 211, 94, 303, 145, 94, 197, 217, 198, 212, 94, 221, 198, 215, 145, 94, 197, 212, 94, 215, 145, 94, 197, 303, 94, 302, 145, 145, 29, 240, 159, 205, 65, 197, 218, 65, 208, 198, 209, 198, 220, 145, 198, 207, 65, 217, 198, 212, 65, 66, 197, 302, 94, 212, 145, 29, 10, 197, 239, 94, 228, 72, 227, 197, 239, 72, 79, 72, 99, 145, 94, 226, 159, 197, 302, 94, 303, 145, 145, 29, 10, 197, 240, 94, 229, 72, 227, 197, 240, 72, 79, 72, 99, 145, 94, 241, 159, 217, 198, 212, 65, 66, 197, 302, 94, 212, 145, 1, 207, 145, 29, 3, 29]}, {"code": "def parallel_based_bwd_kernel(\n    q,\n    k,\n    v,\n    do,\n    dz,\n    dq,\n    dk,\n    dv,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BTL: tl.constexpr,\n    BTS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // (NV)\n    i_v = i_kv % NV\n    _parallel_based_bwd_dq(\n        i_bh, i_c, i_k, i_v, q, k, v, do, dz, dq, scale, T, B, H, BTL, BTS, BK, BV, K, V\n    )\n    tl.debug_barrier()\n    _parallel_based_bwd_dkv(\n        i_bh,\n        i_c,\n        i_k,\n        i_v,\n        q,\n        k,\n        v,\n        do,\n        dz,\n        dk,\n        dv,\n        scale,\n        T,\n        B,\n        H,\n        BTL,\n        BTS,\n        BK,\n        BV,\n        K,\n        V,\n    )", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 93, 211, 54, 6, 93, 212, 54, 6, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 93, 218, 54, 6, 145, 54, 29, -1, 219, 93, 220, 93, 221, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 93, 141, 197, 304, 145, 145, 29, 222, 159, 56, 197, 214, 93, 218, 145, 29, 223, 159, 219, 44, 222, 29, 224, 159, 219, 183, 222, 29, 225, 197, 221, 93, 220, 93, 223, 93, 224, 93, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 209, 93, 210, 93, 211, 93, 212, 93, 215, 93, 216, 93, 217, 93, 218, 93, 213, 93, 214, 145, 29, 46, 197, 145, 29, 226, 197, 221, 93, 220, 93, 223, 93, 224, 93, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 207, 93, 208, 93, 209, 93, 210, 93, 211, 93, 212, 93, 215, 93, 216, 93, 217, 93, 218, 93, 213, 93, 214, 145, 29, 3, 29]}, {"code": "def chunk_gated_delta_rule_fwd_kernel_h_blockdim64(\n    k,\n    v,\n    w,\n    v_new,\n    g,\n    h,\n    h0,\n    ht,\n    cu_seqlens,\n    chunk_offsets,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    SAVE_NEW_VALUE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_nh = tl.program_id(0), tl.program_id(1)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n\n    b_h1 = tl.zeros([64, BV], dtype=tl.float32)\n    if K > 64:\n        b_h2 = tl.zeros([64, BV], dtype=tl.float32)\n    if K > 128:\n        b_h3 = tl.zeros([64, BV], dtype=tl.float32)\n    if K > 192:\n        b_h4 = tl.zeros([64, BV], dtype=tl.float32)\n\n    h += (boh * H + i_h) * K * V\n    v += (bos * H + i_h) * V\n    k += (bos * H + i_h) * K\n    w += (bos * H + i_h) * K\n    if SAVE_NEW_VALUE:\n        v_new += (bos * H + i_h) * V\n    stride_v = H * V\n    stride_h = H * K * V\n    stride_k = H * K\n    if USE_INITIAL_STATE:\n        h0 = h0 + i_nh * K * V\n    if STORE_FINAL_STATE:\n        ht = ht + i_nh * K * V\n\n    if USE_INITIAL_STATE:\n        p_h0_1 = tl.make_block_ptr(h0, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))\n        b_h1 += tl.load(p_h0_1, boundary_check=(0, 1)).to(tl.float32)\n        if K > 64:\n            p_h0_2 = tl.make_block_ptr(\n                h0, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)\n            )\n            b_h2 += tl.load(p_h0_2, boundary_check=(0, 1)).to(tl.float32)\n        if K > 128:\n            p_h0_3 = tl.make_block_ptr(\n                h0, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)\n            )\n            b_h3 += tl.load(p_h0_3, boundary_check=(0, 1)).to(tl.float32)\n        if K > 192:\n            p_h0_4 = tl.make_block_ptr(\n                h0, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)\n            )\n            b_h4 += tl.load(p_h0_4, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT):\n        p_h1 = tl.make_block_ptr(\n            h + i_t * stride_h, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0)\n        )\n        tl.store(p_h1, b_h1.to(p_h1.dtype.element_ty), boundary_check=(0, 1))\n        if K > 64:\n            p_h2 = tl.make_block_ptr(\n                h + i_t * stride_h, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)\n            )\n            tl.store(p_h2, b_h2.to(p_h2.dtype.element_ty), boundary_check=(0, 1))\n        if K > 128:\n            p_h3 = tl.make_block_ptr(\n                h + i_t * stride_h, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)\n            )\n            tl.store(p_h3, b_h3.to(p_h3.dtype.element_ty), boundary_check=(0, 1))\n        if K > 192:\n            p_h4 = tl.make_block_ptr(\n                h + i_t * stride_h, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)\n            )\n            tl.store(p_h4, b_h4.to(p_h4.dtype.element_ty), boundary_check=(0, 1))\n\n        p_v = tl.make_block_ptr(\n            v, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_v_new = (\n            tl.make_block_ptr(\n                v_new, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n            )\n            if SAVE_NEW_VALUE\n            else None\n        )\n        b_v_new = tl.zeros([BT, BV], dtype=tl.float32)\n        p_w = tl.make_block_ptr(\n            w, (T, K), (stride_k, 1), (i_t * BT, 0), (BT, 64), (1, 0)\n        )\n        b_w = tl.load(p_w, boundary_check=(0, 1))\n        b_v_new += tl.dot(b_w, b_h1.to(b_w.dtype))\n        if K > 64:\n            p_w = tl.make_block_ptr(\n                w, (T, K), (stride_k, 1), (i_t * BT, 64), (BT, 64), (1, 0)\n            )\n            b_w = tl.load(p_w, boundary_check=(0, 1))\n            b_v_new += tl.dot(b_w, b_h2.to(b_w.dtype))\n        if K > 128:\n            p_w = tl.make_block_ptr(\n                w, (T, K), (stride_k, 1), (i_t * BT, 128), (BT, 64), (1, 0)\n            )\n            b_w = tl.load(p_w, boundary_check=(0, 1))\n            b_v_new += tl.dot(b_w, b_h3.to(b_w.dtype))\n        if K > 192:\n            p_w = tl.make_block_ptr(\n                w, (T, K), (stride_k, 1), (i_t * BT, 192), (BT, 64), (1, 0)\n            )\n            b_w = tl.load(p_w, boundary_check=(0, 1))\n            b_v_new += tl.dot(b_w, b_h4.to(b_w.dtype))\n        b_v_new = -b_v_new + tl.load(p_v, boundary_check=(0, 1))\n\n        if SAVE_NEW_VALUE:\n            p_v_new = tl.make_block_ptr(\n                v_new, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n            )\n            tl.store(\n                p_v_new, b_v_new.to(p_v_new.dtype.element_ty), boundary_check=(0, 1)\n            )\n\n        if USE_G:\n            last_idx = min((i_t + 1) * BT, T) - 1\n            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)\n            p_g = tl.make_block_ptr(\n                g + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)\n            )\n            b_g = tl.load(p_g, boundary_check=(0,))\n            b_v_new = b_v_new * safe_exp(b_g_last - b_g)[:, None]\n            b_g_last = exp(b_g_last)\n            b_h1 = b_h1 * b_g_last\n            if K > 64:\n                b_h2 = b_h2 * b_g_last\n            if K > 128:\n                b_h3 = b_h3 * b_g_last\n            if K > 192:\n                b_h4 = b_h4 * b_g_last\n        b_v_new = b_v_new.to(k.dtype.element_ty)\n        p_k = tl.make_block_ptr(\n            k, (K, T), (1, stride_k), (0, i_t * BT), (64, BT), (0, 1)\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h1 += tl.dot(b_k, b_v_new)\n        if K > 64:\n            p_k = tl.make_block_ptr(\n                k, (K, T), (1, stride_k), (64, i_t * BT), (64, BT), (0, 1)\n            )\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_h2 += tl.dot(b_k, b_v_new)\n        if K > 128:\n            p_k = tl.make_block_ptr(\n                k, (K, T), (1, stride_k), (128, i_t * BT), (64, BT), (0, 1)\n            )\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_h3 += tl.dot(b_k, b_v_new)\n        if K > 192:\n            p_k = tl.make_block_ptr(\n                k, (K, T), (1, stride_k), (192, i_t * BT), (64, BT), (0, 1)\n            )\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_h4 += tl.dot(b_k, b_v_new)\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_ht, b_h1.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n        if K > 64:\n            p_ht = tl.make_block_ptr(\n                ht, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)\n            )\n            tl.store(p_ht, b_h2.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n        if K > 128:\n            p_ht = tl.make_block_ptr(\n                ht, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)\n            )\n            tl.store(p_ht, b_h3.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n        if K > 192:\n            p_ht = tl.make_block_ptr(\n                ht, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)\n            )\n            tl.store(p_ht, b_h4.to(p_ht.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 94, 209, 94, 210, 94, 211, 94, 212, 55, 6, 94, 213, 55, 6, 94, 214, 55, 6, 94, 215, 55, 6, 94, 216, 55, 6, 94, 217, 55, 6, 94, 218, 55, 6, 94, 219, 55, 6, 94, 220, 55, 6, 94, 221, 55, 6, 145, 55, 29, -1, 222, 94, 223, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 145, 29, 224, 94, 225, 159, 197, 223, 44, 212, 94, 223, 183, 212, 145, 29, 151, 221, 55, 29, 226, 94, 227, 159, 197, 49, 197, 209, 65, 224, 145, 72, 228, 197, 54, 145, 94, 49, 197, 209, 65, 224, 65, 303, 145, 72, 228, 197, 54, 145, 145, 29, 211, 159, 227, 4, 226, 29, 229, 159, 57, 197, 211, 94, 215, 145, 29, 230, 159, 49, 197, 210, 65, 224, 145, 72, 228, 197, 54, 145, 29, 153, 29, 28, 55, 29, 226, 94, 227, 159, 197, 224, 198, 211, 94, 224, 198, 211, 65, 211, 145, 29, 229, 159, 57, 197, 211, 94, 215, 145, 29, 230, 159, 224, 198, 229, 29, 50, 29, 231, 159, 146, 197, 186, 304, 305, 94, 216, 25, 94, 79, 159, 125, 145, 29, 151, 213, 107, 304, 305, 55, 29, 232, 159, 146, 197, 186, 304, 305, 94, 216, 25, 94, 79, 159, 125, 145, 29, 153, 29, 151, 213, 107, 303, 306, 307, 55, 29, 233, 159, 146, 197, 186, 304, 305, 94, 216, 25, 94, 79, 159, 125, 145, 29, 153, 29, 151, 213, 107, 303, 308, 306, 55, 29, 234, 159, 146, 197, 186, 304, 305, 94, 216, 25, 94, 79, 159, 125, 145, 29, 153, 29, 206, 142, 197, 230, 198, 212, 65, 225, 145, 198, 213, 198, 214, 29, 202, 142, 197, 226, 198, 212, 65, 225, 145, 198, 214, 29, 201, 142, 197, 226, 198, 212, 65, 225, 145, 198, 213, 29, 203, 142, 197, 226, 198, 212, 65, 225, 145, 198, 213, 29, 151, 220, 55, 29, 204, 142, 197, 226, 198, 212, 65, 225, 145, 198, 214, 29, 153, 29, 235, 159, 212, 198, 214, 29, 236, 159, 212, 198, 213, 198, 214, 29, 237, 159, 212, 198, 213, 29, 151, 218, 55, 29, 207, 159, 207, 65, 223, 198, 213, 198, 214, 29, 153, 29, 151, 219, 55, 29, 208, 159, 208, 65, 223, 198, 213, 198, 214, 29, 153, 29, 151, 218, 55, 29, 238, 159, 178, 197, 207, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 302, 94, 222, 198, 216, 145, 94, 197, 304, 305, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 231, 142, 49, 197, 238, 94, 239, 159, 197, 302, 94, 303, 145, 145, 72, 228, 197, 125, 145, 29, 151, 213, 107, 304, 305, 55, 29, 240, 159, 178, 197, 207, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 304, 305, 94, 222, 198, 216, 145, 94, 197, 304, 305, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 232, 142, 49, 197, 240, 94, 239, 159, 197, 302, 94, 303, 145, 145, 72, 228, 197, 125, 145, 29, 153, 29, 151, 213, 107, 303, 306, 307, 55, 29, 241, 159, 178, 197, 207, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 303, 306, 307, 94, 222, 198, 216, 145, 94, 197, 304, 305, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 233, 142, 49, 197, 241, 94, 239, 159, 197, 302, 94, 303, 145, 145, 72, 228, 197, 125, 145, 29, 153, 29, 151, 213, 107, 303, 308, 306, 55, 29, 242, 159, 178, 197, 207, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 303, 308, 306, 94, 222, 198, 216, 145, 94, 197, 304, 305, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 234, 142, 49, 197, 242, 94, 239, 159, 197, 302, 94, 303, 145, 145, 72, 228, 197, 125, 145, 29, 153, 29, 153, 29, 116, 243, 132, 5, 197, 229, 145, 55, 29, 244, 159, 178, 197, 206, 65, 243, 198, 236, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 302, 94, 222, 198, 216, 145, 94, 197, 304, 305, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 244, 94, 231, 72, 228, 197, 244, 72, 79, 72, 99, 145, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 151, 213, 107, 304, 305, 55, 29, 245, 159, 178, 197, 206, 65, 243, 198, 236, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 304, 305, 94, 222, 198, 216, 145, 94, 197, 304, 305, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 245, 94, 232, 72, 228, 197, 245, 72, 79, 72, 99, 145, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 151, 213, 107, 303, 306, 307, 55, 29, 246, 159, 178, 197, 206, 65, 243, 198, 236, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 303, 306, 307, 94, 222, 198, 216, 145, 94, 197, 304, 305, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 246, 94, 233, 72, 228, 197, 246, 72, 79, 72, 99, 145, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 151, 213, 107, 303, 308, 306, 55, 29, 247, 159, 178, 197, 206, 65, 243, 198, 236, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 303, 308, 306, 94, 222, 198, 216, 145, 94, 197, 304, 305, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 247, 94, 234, 72, 228, 197, 247, 72, 79, 72, 99, 145, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 248, 159, 178, 197, 202, 94, 197, 211, 94, 214, 145, 94, 197, 235, 94, 303, 145, 94, 197, 243, 198, 215, 94, 222, 198, 216, 145, 94, 197, 215, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 249, 159, 178, 197, 204, 94, 197, 211, 94, 214, 145, 94, 197, 235, 94, 303, 145, 94, 197, 243, 198, 215, 94, 222, 198, 216, 145, 94, 197, 215, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 151, 220, 28, 164, 29, 250, 159, 146, 197, 186, 215, 94, 216, 25, 94, 79, 159, 125, 145, 29, 251, 159, 178, 197, 203, 94, 197, 211, 94, 213, 145, 94, 197, 237, 94, 303, 145, 94, 197, 243, 198, 215, 94, 302, 145, 94, 197, 215, 94, 304, 305, 145, 94, 197, 303, 94, 302, 145, 145, 29, 252, 159, 49, 197, 251, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 250, 142, 15, 197, 252, 94, 231, 72, 228, 197, 252, 72, 79, 145, 145, 29, 151, 213, 107, 304, 305, 55, 29, 251, 159, 178, 197, 203, 94, 197, 211, 94, 213, 145, 94, 197, 237, 94, 303, 145, 94, 197, 243, 198, 215, 94, 304, 305, 145, 94, 197, 215, 94, 304, 305, 145, 94, 197, 303, 94, 302, 145, 145, 29, 252, 159, 49, 197, 251, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 250, 142, 15, 197, 252, 94, 232, 72, 228, 197, 252, 72, 79, 145, 145, 29, 153, 29, 151, 213, 107, 303, 306, 307, 55, 29, 251, 159, 178, 197, 203, 94, 197, 211, 94, 213, 145, 94, 197, 237, 94, 303, 145, 94, 197, 243, 198, 215, 94, 303, 306, 307, 145, 94, 197, 215, 94, 304, 305, 145, 94, 197, 303, 94, 302, 145, 145, 29, 252, 159, 49, 197, 251, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 250, 142, 15, 197, 252, 94, 233, 72, 228, 197, 252, 72, 79, 145, 145, 29, 153, 29, 151, 213, 107, 303, 308, 306, 55, 29, 251, 159, 178, 197, 203, 94, 197, 211, 94, 213, 145, 94, 197, 237, 94, 303, 145, 94, 197, 243, 198, 215, 94, 303, 308, 306, 145, 94, 197, 215, 94, 304, 305, 145, 94, 197, 303, 94, 302, 145, 145, 29, 252, 159, 49, 197, 251, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 250, 142, 15, 197, 252, 94, 234, 72, 228, 197, 252, 72, 79, 145, 145, 29, 153, 29, 250, 159, 4, 250, 65, 49, 197, 248, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 151, 220, 55, 29, 249, 159, 178, 197, 204, 94, 197, 211, 94, 214, 145, 94, 197, 235, 94, 303, 145, 94, 197, 243, 198, 215, 94, 222, 198, 216, 145, 94, 197, 215, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 249, 94, 250, 72, 228, 197, 249, 72, 79, 72, 99, 145, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 151, 217, 55, 29, 253, 159, 36, 197, 197, 243, 65, 303, 145, 198, 215, 94, 211, 145, 4, 303, 29, 254, 159, 49, 197, 205, 65, 226, 198, 212, 65, 253, 198, 212, 65, 225, 145, 29, 255, 159, 178, 197, 205, 65, 226, 198, 212, 65, 225, 94, 197, 211, 94, 145, 94, 197, 212, 94, 145, 94, 197, 243, 198, 215, 94, 145, 94, 197, 215, 94, 145, 94, 197, 302, 94, 145, 145, 29, 256, 159, 49, 197, 255, 94, 239, 159, 197, 302, 94, 145, 145, 29, 250, 159, 250, 198, 257, 197, 254, 4, 256, 145, 186, 55, 94, 164, 25, 29, 254, 159, 169, 197, 254, 145, 29, 231, 159, 231, 198, 254, 29, 151, 213, 107, 304, 305, 55, 29, 232, 159, 232, 198, 254, 29, 153, 29, 151, 213, 107, 303, 306, 307, 55, 29, 233, 159, 233, 198, 254, 29, 153, 29, 151, 213, 107, 303, 308, 306, 55, 29, 234, 159, 234, 198, 254, 29, 153, 29, 153, 29, 250, 159, 250, 72, 228, 197, 201, 72, 79, 72, 99, 145, 29, 258, 159, 178, 197, 201, 94, 197, 213, 94, 211, 145, 94, 197, 303, 94, 237, 145, 94, 197, 302, 94, 243, 198, 215, 145, 94, 197, 304, 305, 94, 215, 145, 94, 197, 302, 94, 303, 145, 145, 29, 259, 159, 49, 197, 258, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 231, 142, 15, 197, 259, 94, 250, 145, 29, 151, 213, 107, 304, 305, 55, 29, 258, 159, 178, 197, 201, 94, 197, 213, 94, 211, 145, 94, 197, 303, 94, 237, 145, 94, 197, 304, 305, 94, 243, 198, 215, 145, 94, 197, 304, 305, 94, 215, 145, 94, 197, 302, 94, 303, 145, 145, 29, 259, 159, 49, 197, 258, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 232, 142, 15, 197, 259, 94, 250, 145, 29, 153, 29, 151, 213, 107, 303, 306, 307, 55, 29, 258, 159, 178, 197, 201, 94, 197, 213, 94, 211, 145, 94, 197, 303, 94, 237, 145, 94, 197, 303, 306, 307, 94, 243, 198, 215, 145, 94, 197, 304, 305, 94, 215, 145, 94, 197, 302, 94, 303, 145, 145, 29, 259, 159, 49, 197, 258, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 233, 142, 15, 197, 259, 94, 250, 145, 29, 153, 29, 151, 213, 107, 303, 308, 306, 55, 29, 258, 159, 178, 197, 201, 94, 197, 213, 94, 211, 145, 94, 197, 303, 94, 237, 145, 94, 197, 303, 308, 306, 94, 243, 198, 215, 145, 94, 197, 304, 305, 94, 215, 145, 94, 197, 302, 94, 303, 145, 145, 29, 259, 159, 49, 197, 258, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 234, 142, 15, 197, 259, 94, 250, 145, 29, 153, 29, 68, 29, 151, 219, 55, 29, 260, 159, 178, 197, 208, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 302, 94, 222, 198, 216, 145, 94, 197, 304, 305, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 260, 94, 231, 72, 228, 197, 260, 72, 79, 72, 99, 145, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 151, 213, 107, 304, 305, 55, 29, 260, 159, 178, 197, 208, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 304, 305, 94, 222, 198, 216, 145, 94, 197, 304, 305, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 260, 94, 232, 72, 228, 197, 260, 72, 79, 72, 99, 145, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 151, 213, 107, 303, 306, 307, 55, 29, 260, 159, 178, 197, 208, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 303, 306, 307, 94, 222, 198, 216, 145, 94, 197, 304, 305, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 260, 94, 233, 72, 228, 197, 260, 72, 79, 72, 99, 145, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 151, 213, 107, 303, 308, 306, 55, 29, 260, 159, 178, 197, 208, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 303, 308, 306, 94, 222, 198, 216, 145, 94, 197, 304, 305, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 260, 94, 234, 72, 228, 197, 260, 72, 79, 72, 99, 145, 94, 239, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 153, 29, 3, 29]}, {"code": "def chunk_gated_delta_rule_bwd_kernel_dhu_blockdim64(\n    q,\n    k,\n    w,\n    g,\n    dht,\n    dh0,\n    do,\n    dh,\n    dv,\n    dv2,\n    cu_seqlens,\n    chunk_offsets,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_nh = tl.program_id(0), tl.program_id(1)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n\n    b_dh1 = tl.zeros([64, BV], dtype=tl.float32)\n    if K > 64:\n        b_dh2 = tl.zeros([64, BV], dtype=tl.float32)\n    if K > 128:\n        b_dh3 = tl.zeros([64, BV], dtype=tl.float32)\n    if K > 192:\n        b_dh4 = tl.zeros([64, BV], dtype=tl.float32)\n\n    dh += (boh * H + i_h) * K * V\n    dv += (bos * H + i_h) * V\n    dv2 += (bos * H + i_h) * V\n    q += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    w += (bos * H + i_h) * K\n    do += (bos * H + i_h) * V\n    stride_v = H * V\n    stride_h = H * K * V\n    stride_k = H * K\n    if USE_INITIAL_STATE:\n        dh0 += i_nh * K * V\n    if USE_FINAL_STATE_GRADIENT:\n        dht += i_nh * K * V\n\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht1 = tl.make_block_ptr(dht, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))\n        b_dh1 += tl.load(p_dht1, boundary_check=(0, 1))\n        if K > 64:\n            p_dht2 = tl.make_block_ptr(\n                dht, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)\n            )\n            b_dh2 += tl.load(p_dht2, boundary_check=(0, 1))\n        if K > 128:\n            p_dht3 = tl.make_block_ptr(\n                dht, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)\n            )\n            b_dh3 += tl.load(p_dht3, boundary_check=(0, 1))\n        if K > 192:\n            p_dht4 = tl.make_block_ptr(\n                dht, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)\n            )\n            b_dh4 += tl.load(p_dht4, boundary_check=(0, 1))\n\n    for i_t in range(NT - 1, -1, -1):\n        p_dh1 = tl.make_block_ptr(\n            dh + i_t * stride_h, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0)\n        )\n        tl.store(p_dh1, b_dh1.to(p_dh1.dtype.element_ty), boundary_check=(0, 1))\n        if K > 64:\n            p_dh2 = tl.make_block_ptr(\n                dh + i_t * stride_h, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)\n            )\n            tl.store(p_dh2, b_dh2.to(p_dh2.dtype.element_ty), boundary_check=(0, 1))\n        if K > 128:\n            p_dh3 = tl.make_block_ptr(\n                dh + i_t * stride_h, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)\n            )\n            tl.store(p_dh3, b_dh3.to(p_dh3.dtype.element_ty), boundary_check=(0, 1))\n        if K > 192:\n            p_dh4 = tl.make_block_ptr(\n                dh + i_t * stride_h, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)\n            )\n            tl.store(p_dh4, b_dh4.to(p_dh4.dtype.element_ty), boundary_check=(0, 1))\n\n        if USE_G:\n            last_idx = min((i_t + 1) * BT, T) - 1\n            bg_last = tl.load(g + (bos + last_idx) * H + i_h)\n            bg_last_exp = exp(bg_last)\n            p_g = tl.make_block_ptr(\n                g + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)\n            )\n            b_g = tl.load(p_g, boundary_check=(0,))\n            b_g_exp = exp(b_g)\n        else:\n            bg_last = None\n            last_idx = None\n            b_g = None\n            b_g_exp = None\n\n        p_dv = tl.make_block_ptr(\n            dv, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_wo = tl.make_block_ptr(\n            do, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_dv2 = tl.make_block_ptr(\n            dv2, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n\n        b_wo = tl.load(p_wo, boundary_check=(0, 1))\n        b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n\n        p_k = tl.make_block_ptr(\n            k, (T, K), (stride_k, 1), (i_t * BT, 0), (BT, 64), (1, 0)\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_dv += tl.dot(b_k, b_dh1.to(b_k.dtype))\n\n        if K > 64:\n            p_k = tl.make_block_ptr(\n                k, (T, K), (stride_k, 1), (i_t * BT, 64), (BT, 64), (1, 0)\n            )\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_dv += tl.dot(b_k, b_dh2.to(b_k.dtype))\n\n        if K > 128:\n            p_k = tl.make_block_ptr(\n                k, (T, K), (stride_k, 1), (i_t * BT, 128), (BT, 64), (1, 0)\n            )\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_dv += tl.dot(b_k, b_dh3.to(b_k.dtype))\n\n        if K > 192:\n            p_k = tl.make_block_ptr(\n                k, (T, K), (stride_k, 1), (i_t * BT, 192), (BT, 64), (1, 0)\n            )\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_dv += tl.dot(b_k, b_dh4.to(b_k.dtype))\n\n        if USE_G:\n            b_dv *= safe_exp(bg_last - b_g)[:, None]\n        b_dv += tl.load(p_dv, boundary_check=(0, 1))\n\n        tl.store(p_dv2, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n\n        p_w = tl.make_block_ptr(\n            w, (K, T), (1, stride_k), (0, i_t * BT), (64, BT), (0, 1)\n        )\n        p_q = tl.make_block_ptr(\n            q, (K, T), (1, stride_k), (0, i_t * BT), (64, BT), (0, 1)\n        )\n        b_w = tl.load(p_w, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        if USE_G:\n            b_dh1 *= bg_last_exp\n            b_q = b_q * b_g_exp[None, :]\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_dh1 += tl.dot(b_q, b_wo.to(b_q.dtype)) - tl.dot(b_w, b_dv.to(b_w.dtype))\n        if K > 64:\n            p_q = tl.make_block_ptr(\n                q, (K, T), (1, stride_k), (64, i_t * BT), (64, BT), (0, 1)\n            )\n            p_w = tl.make_block_ptr(\n                w, (K, T), (1, stride_k), (64, i_t * BT), (64, BT), (0, 1)\n            )\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_w = tl.load(p_w, boundary_check=(0, 1))\n            if USE_G:\n                b_dh2 *= bg_last_exp\n                b_q = b_q * b_g_exp[None, :]\n            b_q = (b_q * scale).to(b_q.dtype)\n            b_dh2 += tl.dot(b_q, b_wo.to(b_q.dtype)) - tl.dot(b_w, b_dv.to(b_w.dtype))\n        if K > 128:\n            p_q = tl.make_block_ptr(\n                q, (K, T), (1, stride_k), (128, i_t * BT), (64, BT), (0, 1)\n            )\n            p_w = tl.make_block_ptr(\n                w, (K, T), (1, stride_k), (128, i_t * BT), (64, BT), (0, 1)\n            )\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_w = tl.load(p_w, boundary_check=(0, 1))\n            if USE_G:\n                b_dh3 *= bg_last_exp\n                b_q = b_q * b_g_exp[None, :]\n            b_q = (b_q * scale).to(b_q.dtype)\n            b_dh3 += tl.dot(b_q, b_wo.to(b_q.dtype)) - tl.dot(b_w, b_dv.to(b_w.dtype))\n        if K > 192:\n            p_q = tl.make_block_ptr(\n                q, (K, T), (1, stride_k), (192, i_t * BT), (64, BT), (0, 1)\n            )\n            p_w = tl.make_block_ptr(\n                w, (K, T), (1, stride_k), (192, i_t * BT), (64, BT), (0, 1)\n            )\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_w = tl.load(p_w, boundary_check=(0, 1))\n            if USE_G:\n                b_dh4 *= bg_last_exp\n                b_q = b_q * b_g_exp[None, :]\n            b_q = (b_q * scale).to(b_q.dtype)\n            b_dh4 += tl.dot(b_q, b_wo.to(b_q.dtype)) - tl.dot(b_w, b_dv.to(b_w.dtype))\n\n    if USE_INITIAL_STATE:\n        p_dh0 = tl.make_block_ptr(dh0, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_dh0, b_dh1.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))\n        if K > 64:\n            p_dh1 = tl.make_block_ptr(\n                dh0, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)\n            )\n            tl.store(p_dh1, b_dh2.to(p_dh1.dtype.element_ty), boundary_check=(0, 1))\n        if K > 128:\n            p_dh2 = tl.make_block_ptr(\n                dh0, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)\n            )\n            tl.store(p_dh2, b_dh3.to(p_dh2.dtype.element_ty), boundary_check=(0, 1))\n        if K > 192:\n            p_dh3 = tl.make_block_ptr(\n                dh0, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)\n            )\n            tl.store(p_dh3, b_dh4.to(p_dh3.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 93, 211, 93, 212, 93, 213, 93, 214, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 93, 218, 54, 6, 93, 219, 54, 6, 93, 220, 54, 6, 93, 221, 54, 6, 93, 222, 54, 6, 93, 223, 54, 6, 145, 54, 29, -1, 224, 93, 225, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 145, 29, 226, 93, 227, 159, 197, 225, 44, 215, 93, 225, 183, 215, 145, 29, 151, 223, 54, 29, 228, 93, 229, 159, 197, 50, 197, 211, 64, 226, 145, 71, 230, 197, 200, 145, 93, 50, 197, 211, 64, 226, 64, 303, 145, 71, 230, 197, 200, 145, 145, 29, 214, 159, 229, 4, 228, 29, 231, 159, 56, 197, 214, 93, 218, 145, 29, 232, 159, 50, 197, 212, 64, 226, 145, 71, 230, 197, 200, 145, 29, 153, 29, 28, 54, 29, 228, 93, 229, 159, 197, 226, 198, 214, 93, 226, 198, 214, 64, 214, 145, 29, 231, 159, 56, 197, 214, 93, 218, 145, 29, 232, 159, 226, 198, 231, 29, 49, 29, 233, 159, 146, 197, 186, 304, 305, 93, 219, 25, 93, 78, 159, 124, 145, 29, 151, 216, 106, 304, 305, 54, 29, 234, 159, 146, 197, 186, 304, 305, 93, 219, 25, 93, 78, 159, 124, 145, 29, 153, 29, 151, 216, 106, 303, 306, 307, 54, 29, 235, 159, 146, 197, 186, 304, 305, 93, 219, 25, 93, 78, 159, 124, 145, 29, 153, 29, 151, 216, 106, 303, 308, 306, 54, 29, 236, 159, 146, 197, 186, 304, 305, 93, 219, 25, 93, 78, 159, 124, 145, 29, 153, 29, 208, 142, 197, 232, 198, 215, 64, 227, 145, 198, 216, 198, 217, 29, 209, 142, 197, 228, 198, 215, 64, 227, 145, 198, 217, 29, 210, 142, 197, 228, 198, 215, 64, 227, 145, 198, 217, 29, 201, 142, 197, 228, 198, 215, 64, 227, 145, 198, 216, 29, 202, 142, 197, 228, 198, 215, 64, 227, 145, 198, 216, 29, 203, 142, 197, 228, 198, 215, 64, 227, 145, 198, 216, 29, 207, 142, 197, 228, 198, 215, 64, 227, 145, 198, 217, 29, 237, 159, 215, 198, 217, 29, 238, 159, 215, 198, 216, 198, 217, 29, 239, 159, 215, 198, 216, 29, 151, 221, 54, 29, 206, 142, 225, 198, 216, 198, 217, 29, 153, 29, 151, 222, 54, 29, 205, 142, 225, 198, 216, 198, 217, 29, 153, 29, 151, 222, 54, 29, 240, 159, 178, 197, 205, 93, 197, 216, 93, 217, 145, 93, 197, 217, 93, 303, 145, 93, 197, 302, 93, 224, 198, 219, 145, 93, 197, 304, 305, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 233, 142, 50, 197, 240, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 151, 216, 106, 304, 305, 54, 29, 242, 159, 178, 197, 205, 93, 197, 216, 93, 217, 145, 93, 197, 217, 93, 303, 145, 93, 197, 304, 305, 93, 224, 198, 219, 145, 93, 197, 304, 305, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 234, 142, 50, 197, 242, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 151, 216, 106, 303, 306, 307, 54, 29, 243, 159, 178, 197, 205, 93, 197, 216, 93, 217, 145, 93, 197, 217, 93, 303, 145, 93, 197, 303, 306, 307, 93, 224, 198, 219, 145, 93, 197, 304, 305, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 235, 142, 50, 197, 243, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 151, 216, 106, 303, 308, 306, 54, 29, 244, 159, 178, 197, 205, 93, 197, 216, 93, 217, 145, 93, 197, 217, 93, 303, 145, 93, 197, 303, 308, 306, 93, 224, 198, 219, 145, 93, 197, 304, 305, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 236, 142, 50, 197, 244, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 153, 29, 115, 245, 132, 5, 197, 231, 4, 303, 93, 4, 303, 93, 4, 303, 145, 54, 29, 246, 159, 178, 197, 208, 64, 245, 198, 238, 93, 197, 216, 93, 217, 145, 93, 197, 217, 93, 303, 145, 93, 197, 302, 93, 224, 198, 219, 145, 93, 197, 304, 305, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 246, 93, 233, 71, 230, 197, 246, 71, 78, 71, 98, 145, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 151, 216, 106, 304, 305, 54, 29, 247, 159, 178, 197, 208, 64, 245, 198, 238, 93, 197, 216, 93, 217, 145, 93, 197, 217, 93, 303, 145, 93, 197, 304, 305, 93, 224, 198, 219, 145, 93, 197, 304, 305, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 247, 93, 234, 71, 230, 197, 247, 71, 78, 71, 98, 145, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 151, 216, 106, 303, 306, 307, 54, 29, 248, 159, 178, 197, 208, 64, 245, 198, 238, 93, 197, 216, 93, 217, 145, 93, 197, 217, 93, 303, 145, 93, 197, 303, 306, 307, 93, 224, 198, 219, 145, 93, 197, 304, 305, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 248, 93, 235, 71, 230, 197, 248, 71, 78, 71, 98, 145, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 151, 216, 106, 303, 308, 306, 54, 29, 249, 159, 178, 197, 208, 64, 245, 198, 238, 93, 197, 216, 93, 217, 145, 93, 197, 217, 93, 303, 145, 93, 197, 303, 308, 306, 93, 224, 198, 219, 145, 93, 197, 304, 305, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 249, 93, 236, 71, 230, 197, 249, 71, 78, 71, 98, 145, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 151, 220, 54, 29, 250, 159, 36, 197, 197, 245, 64, 303, 145, 198, 218, 93, 214, 145, 4, 303, 29, 251, 159, 50, 197, 204, 64, 197, 228, 64, 250, 145, 198, 215, 64, 227, 145, 29, 252, 159, 169, 197, 251, 145, 29, 253, 159, 178, 197, 204, 64, 228, 198, 215, 64, 227, 93, 197, 214, 93, 145, 93, 197, 215, 93, 145, 93, 197, 245, 198, 218, 93, 145, 93, 197, 218, 93, 145, 93, 197, 302, 93, 145, 145, 29, 254, 159, 50, 197, 253, 93, 241, 159, 197, 302, 93, 145, 145, 29, 255, 159, 169, 197, 254, 145, 29, 153, 29, 28, 54, 29, 251, 159, 164, 29, 250, 159, 164, 29, 254, 159, 164, 29, 255, 159, 164, 29, 49, 29, 256, 159, 178, 197, 209, 93, 197, 214, 93, 217, 145, 93, 197, 237, 93, 303, 145, 93, 197, 245, 198, 218, 93, 224, 198, 219, 145, 93, 197, 218, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 257, 159, 178, 197, 207, 93, 197, 214, 93, 217, 145, 93, 197, 237, 93, 303, 145, 93, 197, 245, 198, 218, 93, 224, 198, 219, 145, 93, 197, 218, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 258, 159, 178, 197, 210, 93, 197, 214, 93, 217, 145, 93, 197, 237, 93, 303, 145, 93, 197, 245, 198, 218, 93, 224, 198, 219, 145, 93, 197, 218, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 259, 159, 50, 197, 257, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 260, 159, 146, 197, 186, 218, 93, 219, 25, 93, 78, 159, 124, 145, 29, 261, 159, 178, 197, 202, 93, 197, 214, 93, 216, 145, 93, 197, 239, 93, 303, 145, 93, 197, 245, 198, 218, 93, 302, 145, 93, 197, 218, 93, 304, 305, 145, 93, 197, 303, 93, 302, 145, 145, 29, 262, 159, 50, 197, 261, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 260, 142, 15, 197, 262, 93, 233, 71, 230, 197, 262, 71, 78, 145, 145, 29, 151, 216, 106, 304, 305, 54, 29, 261, 159, 178, 197, 202, 93, 197, 214, 93, 216, 145, 93, 197, 239, 93, 303, 145, 93, 197, 245, 198, 218, 93, 304, 305, 145, 93, 197, 218, 93, 304, 305, 145, 93, 197, 303, 93, 302, 145, 145, 29, 262, 159, 50, 197, 261, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 260, 142, 15, 197, 262, 93, 234, 71, 230, 197, 262, 71, 78, 145, 145, 29, 153, 29, 151, 216, 106, 303, 306, 307, 54, 29, 261, 159, 178, 197, 202, 93, 197, 214, 93, 216, 145, 93, 197, 239, 93, 303, 145, 93, 197, 245, 198, 218, 93, 303, 306, 307, 145, 93, 197, 218, 93, 304, 305, 145, 93, 197, 303, 93, 302, 145, 145, 29, 262, 159, 50, 197, 261, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 260, 142, 15, 197, 262, 93, 235, 71, 230, 197, 262, 71, 78, 145, 145, 29, 153, 29, 151, 216, 106, 303, 308, 306, 54, 29, 261, 159, 178, 197, 202, 93, 197, 214, 93, 216, 145, 93, 197, 239, 93, 303, 145, 93, 197, 245, 198, 218, 93, 303, 308, 306, 145, 93, 197, 218, 93, 304, 305, 145, 93, 197, 303, 93, 302, 145, 145, 29, 262, 159, 50, 197, 261, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 260, 142, 15, 197, 262, 93, 236, 71, 230, 197, 262, 71, 78, 145, 145, 29, 153, 29, 151, 220, 54, 29, 260, 22, 263, 197, 251, 4, 254, 145, 186, 54, 93, 164, 25, 29, 153, 29, 260, 142, 50, 197, 256, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 10, 197, 258, 93, 260, 71, 230, 197, 256, 71, 78, 71, 98, 145, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 264, 159, 178, 197, 203, 93, 197, 216, 93, 214, 145, 93, 197, 303, 93, 239, 145, 93, 197, 302, 93, 245, 198, 218, 145, 93, 197, 304, 305, 93, 218, 145, 93, 197, 302, 93, 303, 145, 145, 29, 265, 159, 178, 197, 201, 93, 197, 216, 93, 214, 145, 93, 197, 303, 93, 239, 145, 93, 197, 302, 93, 245, 198, 218, 145, 93, 197, 304, 305, 93, 218, 145, 93, 197, 302, 93, 303, 145, 145, 29, 266, 159, 50, 197, 264, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 267, 159, 50, 197, 265, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 151, 220, 54, 29, 233, 22, 252, 29, 267, 159, 267, 198, 255, 186, 164, 93, 54, 25, 29, 153, 29, 267, 159, 197, 267, 198, 213, 145, 71, 230, 197, 267, 71, 78, 145, 29, 233, 142, 15, 197, 267, 93, 259, 71, 230, 197, 267, 71, 78, 145, 145, 4, 15, 197, 266, 93, 260, 71, 230, 197, 266, 71, 78, 145, 145, 29, 151, 216, 106, 304, 305, 54, 29, 265, 159, 178, 197, 201, 93, 197, 216, 93, 214, 145, 93, 197, 303, 93, 239, 145, 93, 197, 304, 305, 93, 245, 198, 218, 145, 93, 197, 304, 305, 93, 218, 145, 93, 197, 302, 93, 303, 145, 145, 29, 264, 159, 178, 197, 203, 93, 197, 216, 93, 214, 145, 93, 197, 303, 93, 239, 145, 93, 197, 304, 305, 93, 245, 198, 218, 145, 93, 197, 304, 305, 93, 218, 145, 93, 197, 302, 93, 303, 145, 145, 29, 267, 159, 50, 197, 265, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 266, 159, 50, 197, 264, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 151, 220, 54, 29, 234, 22, 252, 29, 267, 159, 267, 198, 255, 186, 164, 93, 54, 25, 29, 153, 29, 267, 159, 197, 267, 198, 213, 145, 71, 230, 197, 267, 71, 78, 145, 29, 234, 142, 15, 197, 267, 93, 259, 71, 230, 197, 267, 71, 78, 145, 145, 4, 15, 197, 266, 93, 260, 71, 230, 197, 266, 71, 78, 145, 145, 29, 153, 29, 151, 216, 106, 303, 306, 307, 54, 29, 265, 159, 178, 197, 201, 93, 197, 216, 93, 214, 145, 93, 197, 303, 93, 239, 145, 93, 197, 303, 306, 307, 93, 245, 198, 218, 145, 93, 197, 304, 305, 93, 218, 145, 93, 197, 302, 93, 303, 145, 145, 29, 264, 159, 178, 197, 203, 93, 197, 216, 93, 214, 145, 93, 197, 303, 93, 239, 145, 93, 197, 303, 306, 307, 93, 245, 198, 218, 145, 93, 197, 304, 305, 93, 218, 145, 93, 197, 302, 93, 303, 145, 145, 29, 267, 159, 50, 197, 265, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 266, 159, 50, 197, 264, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 151, 220, 54, 29, 235, 22, 252, 29, 267, 159, 267, 198, 255, 186, 164, 93, 54, 25, 29, 153, 29, 267, 159, 197, 267, 198, 213, 145, 71, 230, 197, 267, 71, 78, 145, 29, 235, 142, 15, 197, 267, 93, 259, 71, 230, 197, 267, 71, 78, 145, 145, 4, 15, 197, 266, 93, 260, 71, 230, 197, 266, 71, 78, 145, 145, 29, 153, 29, 151, 216, 106, 303, 308, 306, 54, 29, 265, 159, 178, 197, 201, 93, 197, 216, 93, 214, 145, 93, 197, 303, 93, 239, 145, 93, 197, 303, 308, 306, 93, 245, 198, 218, 145, 93, 197, 304, 305, 93, 218, 145, 93, 197, 302, 93, 303, 145, 145, 29, 264, 159, 178, 197, 203, 93, 197, 216, 93, 214, 145, 93, 197, 303, 93, 239, 145, 93, 197, 303, 308, 306, 93, 245, 198, 218, 145, 93, 197, 304, 305, 93, 218, 145, 93, 197, 302, 93, 303, 145, 145, 29, 267, 159, 50, 197, 265, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 266, 159, 50, 197, 264, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 151, 220, 54, 29, 236, 22, 252, 29, 267, 159, 267, 198, 255, 186, 164, 93, 54, 25, 29, 153, 29, 267, 159, 197, 267, 198, 213, 145, 71, 230, 197, 267, 71, 78, 145, 29, 236, 142, 15, 197, 267, 93, 259, 71, 230, 197, 267, 71, 78, 145, 145, 4, 15, 197, 266, 93, 260, 71, 230, 197, 266, 71, 78, 145, 145, 29, 153, 29, 67, 29, 151, 221, 54, 29, 268, 159, 178, 197, 206, 93, 197, 216, 93, 217, 145, 93, 197, 217, 93, 303, 145, 93, 197, 302, 93, 224, 198, 219, 145, 93, 197, 304, 305, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 268, 93, 233, 71, 230, 197, 268, 71, 78, 71, 98, 145, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 151, 216, 106, 304, 305, 54, 29, 246, 159, 178, 197, 206, 93, 197, 216, 93, 217, 145, 93, 197, 217, 93, 303, 145, 93, 197, 304, 305, 93, 224, 198, 219, 145, 93, 197, 304, 305, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 246, 93, 234, 71, 230, 197, 246, 71, 78, 71, 98, 145, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 151, 216, 106, 303, 306, 307, 54, 29, 247, 159, 178, 197, 206, 93, 197, 216, 93, 217, 145, 93, 197, 217, 93, 303, 145, 93, 197, 303, 306, 307, 93, 224, 198, 219, 145, 93, 197, 304, 305, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 247, 93, 235, 71, 230, 197, 247, 71, 78, 71, 98, 145, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 151, 216, 106, 303, 308, 306, 54, 29, 248, 159, 178, 197, 206, 93, 197, 216, 93, 217, 145, 93, 197, 217, 93, 303, 145, 93, 197, 303, 308, 306, 93, 224, 198, 219, 145, 93, 197, 304, 305, 93, 219, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 248, 93, 236, 71, 230, 197, 248, 71, 78, 71, 98, 145, 93, 241, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 153, 29, 3, 29]}, {"code": "def chunk_fwd_kernel_h(\n    k,\n    v,\n    h,\n    g,\n    gk,\n    gv,\n    h0,\n    ht,\n    cu_seqlens,\n    split_offsets,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_GK: tl.constexpr,\n    USE_GV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        NS = tl.cdiv(T, BS)\n        boh = tl.load(split_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        NS = tl.cdiv(T, BS)\n        boh = i_n * NS\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(\n            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT):\n        i_s = i_t // (BS // BT)\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n\n        o_h = ((boh + i_s) * H + i_h).to(tl.int64) * K * V\n        p_h = tl.make_block_ptr(\n            h + o_h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n\n        if i_t % (BS // BT) == 0:\n            tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        last_idx = min((i_t + 1) * BT, T) - 1\n\n        if USE_G:\n            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)\n            p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h\n            b_h *= exp(b_g_last)\n            b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)\n            b_v = (b_v * exp(b_g_last - b_g)[:, None]).to(b_v.dtype)\n\n        if USE_GK:\n            p_gk = tl.make_block_ptr(\n                gk + (bos * H + i_h) * K,\n                (K, T),\n                (1, H * K),\n                (i_k * BK, i_t * BT),\n                (BK, BT),\n                (0, 1),\n            )\n            p_gk_last = (\n                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n            )\n\n            b_gk_last = tl.load(\n                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0\n            )\n            b_h *= exp(b_gk_last)[:, None]\n\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_k = (b_k * exp(b_gk_last[:, None] - b_gk)).to(b_k.dtype)\n\n        if USE_GV:\n            p_gv = tl.make_block_ptr(\n                gv + (bos * H + i_h) * V,\n                (T, V),\n                (H * V, 1),\n                (i_t * BT, i_v * BV),\n                (BT, BV),\n                (1, 0),\n            )\n            p_gv_last = (\n                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n            )\n\n            b_gv_last = tl.load(\n                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0\n            )\n            b_h *= exp(b_gv_last)[None, :]\n\n            b_gv = tl.load(p_gv, boundary_check=(0, 1))\n            b_v = (b_v * exp(b_gv_last[None, :] - b_gv)).to(b_v.dtype)\n\n        b_h += tl.dot(b_k, b_v)\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(\n            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 94, 209, 94, 210, 94, 211, 94, 212, 55, 6, 94, 213, 55, 6, 94, 214, 55, 6, 94, 215, 55, 6, 94, 216, 55, 6, 94, 217, 55, 6, 94, 218, 55, 6, 94, 219, 55, 6, 94, 220, 55, 6, 94, 221, 55, 6, 94, 222, 55, 6, 94, 223, 55, 6, 94, 224, 55, 6, 145, 55, 29, -1, 225, 94, 226, 94, 227, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 94, 141, 197, 304, 145, 145, 29, 228, 94, 229, 159, 197, 227, 44, 212, 94, 227, 183, 212, 145, 29, 151, 224, 55, 29, 230, 94, 231, 159, 197, 49, 197, 209, 65, 228, 145, 72, 232, 197, 54, 145, 94, 49, 197, 209, 65, 228, 65, 303, 145, 72, 232, 197, 54, 145, 145, 29, 211, 159, 231, 4, 230, 29, 233, 159, 57, 197, 211, 94, 215, 145, 29, 234, 159, 57, 197, 211, 94, 216, 145, 29, 235, 159, 49, 197, 210, 65, 228, 145, 72, 232, 197, 54, 145, 29, 153, 29, 28, 55, 29, 230, 94, 231, 159, 197, 228, 198, 211, 94, 228, 198, 211, 65, 211, 145, 29, 233, 159, 57, 197, 211, 94, 215, 145, 29, 234, 159, 57, 197, 211, 94, 216, 145, 29, 235, 159, 228, 198, 234, 29, 50, 29, 236, 159, 146, 197, 186, 217, 94, 218, 25, 94, 79, 159, 125, 145, 29, 151, 222, 55, 29, 237, 159, 178, 197, 207, 65, 227, 198, 213, 198, 214, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 225, 198, 217, 94, 226, 198, 218, 145, 94, 197, 217, 94, 218, 145, 94, 197, 303, 94, 302, 145, 145, 29, 236, 159, 49, 197, 237, 94, 238, 159, 197, 302, 94, 303, 145, 145, 72, 232, 197, 125, 145, 29, 153, 29, 116, 239, 132, 5, 197, 233, 145, 55, 29, 240, 159, 239, 44, 197, 216, 44, 215, 145, 29, 241, 159, 178, 197, 201, 65, 197, 230, 198, 212, 65, 229, 145, 198, 213, 94, 197, 213, 94, 211, 145, 94, 197, 303, 94, 212, 198, 213, 145, 94, 197, 225, 198, 217, 94, 239, 198, 215, 145, 94, 197, 217, 94, 215, 145, 94, 197, 302, 94, 303, 145, 145, 29, 242, 159, 178, 197, 202, 65, 197, 230, 198, 212, 65, 229, 145, 198, 214, 94, 197, 211, 94, 214, 145, 94, 197, 212, 198, 214, 94, 303, 145, 94, 197, 239, 198, 215, 94, 226, 198, 218, 145, 94, 197, 215, 94, 218, 145, 94, 197, 303, 94, 302, 145, 145, 29, 243, 159, 197, 197, 235, 65, 240, 145, 198, 212, 65, 229, 145, 72, 232, 197, 150, 145, 198, 213, 198, 214, 29, 244, 159, 178, 197, 203, 65, 243, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 225, 198, 217, 94, 226, 198, 218, 145, 94, 197, 217, 94, 218, 145, 94, 197, 303, 94, 302, 145, 145, 29, 151, 239, 183, 197, 216, 44, 215, 145, 67, 302, 55, 29, 10, 197, 244, 94, 236, 72, 232, 197, 244, 72, 79, 72, 99, 145, 94, 238, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 245, 159, 49, 197, 241, 94, 238, 159, 197, 302, 94, 303, 145, 145, 29, 246, 159, 49, 197, 242, 94, 238, 159, 197, 302, 94, 303, 145, 145, 29, 247, 159, 36, 197, 197, 239, 65, 303, 145, 198, 215, 94, 211, 145, 4, 303, 29, 151, 219, 55, 29, 248, 159, 49, 197, 204, 65, 230, 198, 212, 65, 247, 198, 212, 65, 229, 145, 29, 249, 159, 204, 65, 230, 198, 212, 65, 197, 239, 198, 215, 65, 66, 197, 302, 94, 215, 145, 145, 198, 212, 65, 229, 29, 236, 22, 169, 197, 248, 145, 29, 250, 159, 49, 197, 249, 94, 251, 159, 239, 198, 215, 65, 66, 197, 302, 94, 215, 145, 1, 211, 94, 252, 159, 302, 145, 29, 246, 159, 197, 246, 198, 169, 197, 248, 4, 250, 145, 186, 55, 94, 164, 25, 145, 72, 232, 197, 246, 72, 79, 145, 29, 153, 29, 151, 220, 55, 29, 253, 159, 178, 197, 205, 65, 197, 230, 198, 212, 65, 229, 145, 198, 213, 94, 197, 213, 94, 211, 145, 94, 197, 303, 94, 212, 198, 213, 145, 94, 197, 225, 198, 217, 94, 239, 198, 215, 145, 94, 197, 217, 94, 215, 145, 94, 197, 302, 94, 303, 145, 145, 29, 254, 159, 205, 65, 197, 230, 65, 247, 145, 198, 212, 198, 213, 65, 229, 198, 213, 65, 225, 198, 217, 65, 66, 197, 302, 94, 217, 145, 29, 255, 159, 49, 197, 254, 94, 251, 159, 225, 198, 217, 65, 66, 197, 302, 94, 217, 145, 1, 213, 94, 252, 159, 302, 145, 29, 236, 22, 169, 197, 255, 145, 186, 55, 94, 164, 25, 29, 256, 159, 49, 197, 253, 94, 238, 159, 197, 302, 94, 303, 145, 145, 29, 245, 159, 197, 245, 198, 169, 197, 255, 186, 55, 94, 164, 25, 4, 256, 145, 145, 72, 232, 197, 245, 72, 79, 145, 29, 153, 29, 151, 221, 55, 29, 257, 159, 178, 197, 206, 65, 197, 230, 198, 212, 65, 229, 145, 198, 214, 94, 197, 211, 94, 214, 145, 94, 197, 212, 198, 214, 94, 303, 145, 94, 197, 239, 198, 215, 94, 226, 198, 218, 145, 94, 197, 215, 94, 218, 145, 94, 197, 303, 94, 302, 145, 145, 29, 258, 159, 206, 65, 197, 230, 65, 247, 145, 198, 212, 198, 214, 65, 229, 198, 214, 65, 226, 198, 218, 65, 66, 197, 302, 94, 218, 145, 29, 259, 159, 49, 197, 258, 94, 251, 159, 226, 198, 218, 65, 66, 197, 302, 94, 218, 145, 1, 214, 94, 252, 159, 302, 145, 29, 236, 22, 169, 197, 259, 145, 186, 164, 94, 55, 25, 29, 260, 159, 49, 197, 257, 94, 238, 159, 197, 302, 94, 303, 145, 145, 29, 246, 159, 197, 246, 198, 169, 197, 259, 186, 164, 94, 55, 25, 4, 260, 145, 145, 72, 232, 197, 246, 72, 79, 145, 29, 153, 29, 236, 142, 15, 197, 245, 94, 246, 145, 29, 68, 29, 151, 223, 55, 29, 261, 159, 178, 197, 208, 65, 227, 198, 213, 198, 214, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 225, 198, 217, 94, 226, 198, 218, 145, 94, 197, 217, 94, 218, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 261, 94, 236, 72, 232, 197, 261, 72, 79, 72, 99, 145, 94, 238, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 3, 29]}, {"code": "def chunk_bwd_kernel_dh(\n    q,\n    g,\n    gk,\n    gv,\n    do,\n    dh,\n    dht,\n    dh0,\n    cu_seqlens,\n    split_offsets,\n    scale,\n    T,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NG: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_GK: tl.constexpr,\n    USE_GV: tl.constexpr,\n    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_hq = i_nh // HQ, i_nh % HQ\n    i_h = i_hq // NG\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        NS = tl.cdiv(T, BS)\n        boh = tl.load(split_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        NS = tl.cdiv(T, BS)\n        boh = i_n * NS\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(\n            dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT - 1, -1, -1):\n        i_s = i_t // (BS // BT)\n        o_dh = ((boh + i_s) * H + i_h).to(tl.int64) * K * V\n        p_dh = tl.make_block_ptr(\n            dh + o_dh, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n\n        if i_t % (BS // BT) == 0:\n            tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        last_idx = min(i_t * BT + BT, T) - 1\n\n        p_q = tl.make_block_ptr(\n            q + (bos * HQ + i_hq) * K,\n            (K, T),\n            (1, HQ * K),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_do = tl.make_block_ptr(\n            do + (bos * HQ + i_hq) * V,\n            (T, V),\n            (HQ * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        if USE_G:\n            p_g = g + (bos + i_t * BT + tl.arange(0, BT)) * H + i_h\n            b_g_last = tl.load(g + (bos + last_idx) * H + i_h)\n            b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)\n            b_q = (b_q * exp(b_g)[None, :]).to(b_q.dtype)\n\n            b_dh *= exp(b_g_last)\n\n        if USE_GK:\n            p_gk = tl.make_block_ptr(\n                gk + (bos * H + i_h) * K,\n                (K, T),\n                (1, H * K),\n                (i_k * BK, i_t * BT),\n                (BK, BT),\n                (0, 1),\n            )\n            p_gk_last = (\n                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n            )\n\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_q = (b_q * exp(b_gk)).to(b_q.dtype)\n            b_gk_last = tl.load(\n                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0\n            )\n            b_dh *= exp(b_gk_last)[:, None]\n\n        if USE_GV:\n            p_gv = tl.make_block_ptr(\n                gv + (bos * H + i_h) * V,\n                (T, V),\n                (H * V, 1),\n                (i_t * BT, i_v * BV),\n                (BT, BV),\n                (1, 0),\n            )\n            p_gv_last = (\n                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n            )\n\n            b_gv = tl.load(p_gv, boundary_check=(0, 1))\n            b_do = (b_do * exp(b_gv)).to(b_do.dtype)\n\n            b_gv_last = tl.load(\n                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0\n            )\n            b_dh *= exp(b_gv_last)[None, :]\n\n        b_dh += tl.dot(b_q, b_do)\n\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = tl.make_block_ptr(\n            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 93, 211, 93, 212, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 93, 218, 54, 6, 93, 219, 54, 6, 93, 220, 54, 6, 93, 221, 54, 6, 93, 222, 54, 6, 93, 223, 54, 6, 93, 224, 54, 6, 93, 225, 54, 6, 93, 226, 54, 6, 93, 227, 54, 6, 145, 54, 29, -1, 228, 93, 229, 93, 230, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 93, 141, 197, 304, 145, 145, 29, 231, 93, 232, 159, 197, 230, 44, 213, 93, 230, 183, 213, 145, 29, 233, 159, 232, 44, 221, 29, 151, 227, 54, 29, 234, 93, 235, 159, 197, 50, 197, 209, 64, 231, 145, 71, 236, 197, 200, 145, 93, 50, 197, 209, 64, 231, 64, 303, 145, 71, 236, 197, 200, 145, 145, 29, 212, 159, 235, 4, 234, 29, 237, 159, 56, 197, 212, 93, 217, 145, 29, 238, 159, 56, 197, 212, 93, 218, 145, 29, 239, 159, 50, 197, 210, 64, 231, 145, 71, 236, 197, 200, 145, 29, 153, 29, 28, 54, 29, 234, 93, 235, 159, 197, 231, 198, 212, 93, 231, 198, 212, 64, 212, 145, 29, 237, 159, 56, 197, 212, 93, 217, 145, 29, 238, 159, 56, 197, 212, 93, 218, 145, 29, 239, 159, 231, 198, 238, 29, 49, 29, 240, 159, 146, 197, 186, 219, 93, 220, 25, 93, 78, 159, 124, 145, 29, 151, 226, 54, 29, 241, 159, 178, 197, 207, 64, 230, 198, 215, 198, 216, 93, 197, 215, 93, 216, 145, 93, 197, 216, 93, 303, 145, 93, 197, 228, 198, 219, 93, 229, 198, 220, 145, 93, 197, 219, 93, 220, 145, 93, 197, 303, 93, 302, 145, 145, 29, 240, 142, 50, 197, 241, 93, 242, 159, 197, 302, 93, 303, 145, 145, 71, 236, 197, 124, 145, 29, 153, 29, 115, 243, 132, 5, 197, 237, 4, 303, 93, 4, 303, 93, 4, 303, 145, 54, 29, 244, 159, 243, 44, 197, 218, 44, 217, 145, 29, 245, 159, 197, 197, 239, 64, 244, 145, 198, 214, 64, 233, 145, 71, 236, 197, 150, 145, 198, 215, 198, 216, 29, 246, 159, 178, 197, 206, 64, 245, 93, 197, 215, 93, 216, 145, 93, 197, 216, 93, 303, 145, 93, 197, 228, 198, 219, 93, 229, 198, 220, 145, 93, 197, 219, 93, 220, 145, 93, 197, 303, 93, 302, 145, 145, 29, 151, 243, 183, 197, 218, 44, 217, 145, 66, 302, 54, 29, 10, 197, 246, 93, 240, 71, 236, 197, 246, 71, 78, 71, 98, 145, 93, 242, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 247, 159, 36, 197, 243, 198, 217, 64, 217, 93, 212, 145, 4, 303, 29, 248, 159, 178, 197, 201, 64, 197, 234, 198, 213, 64, 232, 145, 198, 215, 93, 197, 215, 93, 212, 145, 93, 197, 303, 93, 213, 198, 215, 145, 93, 197, 228, 198, 219, 93, 243, 198, 217, 145, 93, 197, 219, 93, 217, 145, 93, 197, 302, 93, 303, 145, 145, 29, 249, 159, 178, 197, 205, 64, 197, 234, 198, 213, 64, 232, 145, 198, 216, 93, 197, 212, 93, 216, 145, 93, 197, 213, 198, 216, 93, 303, 145, 93, 197, 243, 198, 217, 93, 229, 198, 220, 145, 93, 197, 217, 93, 220, 145, 93, 197, 303, 93, 302, 145, 145, 29, 250, 159, 50, 197, 248, 93, 242, 159, 197, 302, 93, 303, 145, 145, 29, 250, 159, 197, 250, 198, 211, 145, 71, 236, 197, 250, 71, 78, 145, 29, 251, 159, 50, 197, 249, 93, 242, 159, 197, 302, 93, 303, 145, 145, 29, 151, 222, 54, 29, 252, 159, 202, 64, 197, 234, 64, 243, 198, 217, 64, 65, 197, 302, 93, 217, 145, 145, 198, 214, 64, 233, 29, 253, 159, 50, 197, 202, 64, 197, 234, 64, 247, 145, 198, 214, 64, 233, 145, 29, 254, 159, 50, 197, 252, 93, 255, 159, 243, 198, 217, 64, 65, 197, 302, 93, 217, 145, 1, 212, 93, 256, 159, 302, 145, 29, 250, 159, 197, 250, 198, 169, 197, 254, 145, 186, 164, 93, 54, 25, 145, 71, 236, 197, 250, 71, 78, 145, 29, 240, 22, 169, 197, 253, 145, 29, 153, 29, 151, 223, 54, 29, 257, 159, 178, 197, 203, 64, 197, 234, 198, 214, 64, 233, 145, 198, 215, 93, 197, 215, 93, 212, 145, 93, 197, 303, 93, 214, 198, 215, 145, 93, 197, 228, 198, 219, 93, 243, 198, 217, 145, 93, 197, 219, 93, 217, 145, 93, 197, 302, 93, 303, 145, 145, 29, 258, 159, 203, 64, 197, 234, 64, 247, 145, 198, 214, 198, 215, 64, 233, 198, 215, 64, 228, 198, 219, 64, 65, 197, 302, 93, 219, 145, 29, 259, 159, 50, 197, 257, 93, 242, 159, 197, 302, 93, 303, 145, 145, 29, 250, 159, 197, 250, 198, 169, 197, 259, 145, 145, 71, 236, 197, 250, 71, 78, 145, 29, 260, 159, 50, 197, 258, 93, 255, 159, 228, 198, 219, 64, 65, 197, 302, 93, 219, 145, 1, 215, 93, 256, 159, 302, 145, 29, 240, 22, 169, 197, 260, 145, 186, 54, 93, 164, 25, 29, 153, 29, 151, 224, 54, 29, 261, 159, 178, 197, 204, 64, 197, 234, 198, 214, 64, 233, 145, 198, 216, 93, 197, 212, 93, 216, 145, 93, 197, 214, 198, 216, 93, 303, 145, 93, 197, 243, 198, 217, 93, 229, 198, 220, 145, 93, 197, 217, 93, 220, 145, 93, 197, 303, 93, 302, 145, 145, 29, 262, 159, 204, 64, 197, 234, 64, 247, 145, 198, 214, 198, 216, 64, 233, 198, 216, 64, 229, 198, 220, 64, 65, 197, 302, 93, 220, 145, 29, 263, 159, 50, 197, 261, 93, 242, 159, 197, 302, 93, 303, 145, 145, 29, 251, 159, 197, 251, 198, 169, 197, 263, 145, 145, 71, 236, 197, 251, 71, 78, 145, 29, 264, 159, 50, 197, 262, 93, 255, 159, 229, 198, 220, 64, 65, 197, 302, 93, 220, 145, 1, 216, 93, 256, 159, 302, 145, 29, 240, 22, 169, 197, 264, 145, 186, 164, 93, 54, 25, 29, 153, 29, 240, 142, 15, 197, 250, 93, 251, 145, 29, 67, 29, 151, 225, 54, 29, 265, 159, 178, 197, 208, 64, 230, 198, 215, 198, 216, 93, 197, 215, 93, 216, 145, 93, 197, 216, 93, 303, 145, 93, 197, 228, 198, 219, 93, 229, 198, 220, 145, 93, 197, 219, 93, 220, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 265, 93, 240, 71, 236, 197, 265, 71, 78, 71, 98, 145, 93, 242, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 3, 29]}, {"code": "def chunk_fwd_kernel_h_parallel(\n    k,\n    v,\n    h,\n    g,\n    gk,\n    gv,\n    h0,\n    ht,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_GK: tl.constexpr,\n    USE_GV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    NV = tl.cdiv(V, BV)\n\n    i_k, i_v = i_kv // NV, i_kv % NV\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        NT = tl.cdiv(T, BT)\n        i_n, i_tg = i_b, i_b * NT + i_t\n    i_nh = i_n * H + i_h\n\n    p_k = tl.make_block_ptr(\n        k + (bos * H + i_h) * K,\n        (K, T),\n        (1, H * K),\n        (i_k * BK, i_t * BT),\n        (BK, BT),\n        (0, 1),\n    )\n    p_v = tl.make_block_ptr(\n        v + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    p_h = tl.make_block_ptr(\n        h + (i_tg * H + i_h) * K * V,\n        (K, V),\n        (V, 1),\n        (i_k * BK, i_v * BV),\n        (BK, BV),\n        (1, 0),\n    )\n\n    if i_t == 0:\n        if USE_INITIAL_STATE:\n            p_h0 = tl.make_block_ptr(\n                h0 + i_nh * K * V,\n                (K, V),\n                (V, 1),\n                (i_k * BK, i_v * BV),\n                (BK, BV),\n                (1, 0),\n            )\n            b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n        else:\n            b_h = tl.zeros([BK, BV], dtype=tl.float32)\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n\n    last_idx = min(i_t * BT + BT, T) - 1\n\n    if USE_G:\n        b_g_last = tl.load(g + bos * H + last_idx * H + i_h)\n        p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h\n        b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)\n        b_v = (b_v * exp(b_g_last - b_g)[:, None]).to(b_v.dtype)\n\n    if USE_GK:\n        p_gk = tl.make_block_ptr(\n            gk + (bos * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_gk_last = (\n            gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        )\n        b_gk_last = tl.load(\n            p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0\n        )\n\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_k = (b_k * exp(b_gk_last[:, None] - b_gk)).to(b_k.dtype)\n\n    if USE_GV:\n        p_gv = tl.make_block_ptr(\n            gv + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_gv_last = (\n            gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        )\n\n        b_gv_last = tl.load(\n            p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0\n        )\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_v = (b_v * exp(b_gv_last[None, :] - b_gv)).to(b_v.dtype)\n\n    b_h = tl.dot(b_k, b_v)\n    if i_t < NT - 1:\n        p_h = tl.make_block_ptr(\n            h + ((i_tg + 1) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n    elif STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(\n            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 94, 209, 94, 210, 94, 211, 94, 212, 55, 6, 94, 213, 55, 6, 94, 214, 55, 6, 94, 215, 55, 6, 94, 216, 55, 6, 94, 217, 55, 6, 94, 218, 55, 6, 94, 219, 55, 6, 94, 220, 55, 6, 94, 221, 55, 6, 94, 222, 55, 6, 94, 223, 55, 6, 145, 55, 29, -1, 224, 94, 225, 94, 226, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 94, 141, 197, 304, 145, 145, 29, 227, 159, 57, 197, 214, 94, 217, 145, 29, 228, 94, 229, 159, 197, 224, 44, 227, 94, 224, 183, 227, 145, 29, 230, 94, 231, 159, 197, 226, 44, 212, 94, 226, 183, 212, 145, 29, 151, 223, 55, 29, 232, 159, 225, 29, 233, 94, 225, 159, 197, 49, 197, 210, 65, 225, 198, 304, 145, 72, 234, 197, 54, 145, 94, 49, 197, 210, 65, 225, 198, 304, 65, 303, 145, 72, 234, 197, 54, 145, 145, 29, 235, 94, 236, 159, 197, 49, 197, 209, 65, 233, 145, 72, 234, 197, 54, 145, 94, 49, 197, 209, 65, 233, 65, 303, 145, 72, 234, 197, 54, 145, 145, 29, 211, 159, 236, 4, 235, 29, 237, 159, 57, 197, 211, 94, 215, 145, 29, 153, 29, 28, 55, 29, 235, 94, 236, 159, 197, 230, 198, 211, 94, 230, 198, 211, 65, 211, 145, 29, 237, 159, 57, 197, 211, 94, 215, 145, 29, 233, 94, 232, 159, 197, 230, 94, 230, 198, 237, 65, 225, 145, 29, 50, 29, 238, 159, 233, 198, 212, 65, 231, 29, 239, 159, 178, 197, 201, 65, 197, 235, 198, 212, 65, 231, 145, 198, 213, 94, 197, 213, 94, 211, 145, 94, 197, 303, 94, 212, 198, 213, 145, 94, 197, 228, 198, 216, 94, 225, 198, 215, 145, 94, 197, 216, 94, 215, 145, 94, 197, 302, 94, 303, 145, 145, 29, 240, 159, 178, 197, 202, 65, 197, 235, 198, 212, 65, 231, 145, 198, 214, 94, 197, 211, 94, 214, 145, 94, 197, 212, 198, 214, 94, 303, 145, 94, 197, 225, 198, 215, 94, 229, 198, 217, 145, 94, 197, 215, 94, 217, 145, 94, 197, 303, 94, 302, 145, 145, 29, 241, 159, 178, 197, 203, 65, 197, 232, 198, 212, 65, 231, 145, 198, 213, 198, 214, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 228, 198, 216, 94, 229, 198, 217, 145, 94, 197, 216, 94, 217, 145, 94, 197, 303, 94, 302, 145, 145, 29, 151, 225, 67, 302, 55, 29, 151, 221, 55, 29, 242, 159, 178, 197, 207, 65, 238, 198, 213, 198, 214, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 228, 198, 216, 94, 229, 198, 217, 145, 94, 197, 216, 94, 217, 145, 94, 197, 303, 94, 302, 145, 145, 29, 243, 159, 49, 197, 242, 94, 244, 159, 197, 302, 94, 303, 145, 145, 72, 234, 197, 125, 145, 29, 153, 29, 28, 55, 29, 243, 159, 146, 197, 186, 216, 94, 217, 25, 94, 79, 159, 125, 145, 29, 50, 29, 10, 197, 241, 94, 243, 72, 234, 197, 241, 72, 79, 72, 99, 145, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 245, 159, 49, 197, 239, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 246, 159, 49, 197, 240, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 247, 159, 36, 197, 225, 198, 215, 65, 215, 94, 211, 145, 4, 303, 29, 151, 218, 55, 29, 248, 159, 49, 197, 204, 65, 235, 198, 212, 65, 247, 198, 212, 65, 231, 145, 29, 249, 159, 204, 65, 235, 198, 212, 65, 197, 225, 198, 215, 65, 66, 197, 302, 94, 215, 145, 145, 198, 212, 65, 231, 29, 250, 159, 49, 197, 249, 94, 251, 159, 225, 198, 215, 65, 66, 197, 302, 94, 215, 145, 1, 211, 94, 252, 159, 302, 145, 29, 246, 159, 197, 246, 198, 169, 197, 248, 4, 250, 145, 186, 55, 94, 164, 25, 145, 72, 234, 197, 246, 72, 79, 145, 29, 153, 29, 151, 219, 55, 29, 253, 159, 178, 197, 205, 65, 197, 235, 198, 212, 65, 231, 145, 198, 213, 94, 197, 213, 94, 211, 145, 94, 197, 303, 94, 212, 198, 213, 145, 94, 197, 228, 198, 216, 94, 225, 198, 215, 145, 94, 197, 216, 94, 215, 145, 94, 197, 302, 94, 303, 145, 145, 29, 254, 159, 205, 65, 197, 235, 65, 247, 145, 198, 212, 198, 213, 65, 231, 198, 213, 65, 228, 198, 216, 65, 66, 197, 302, 94, 216, 145, 29, 255, 159, 49, 197, 254, 94, 251, 159, 228, 198, 216, 65, 66, 197, 302, 94, 216, 145, 1, 213, 94, 252, 159, 302, 145, 29, 256, 159, 49, 197, 253, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 245, 159, 197, 245, 198, 169, 197, 255, 186, 55, 94, 164, 25, 4, 256, 145, 145, 72, 234, 197, 245, 72, 79, 145, 29, 153, 29, 151, 220, 55, 29, 257, 159, 178, 197, 206, 65, 197, 235, 198, 212, 65, 231, 145, 198, 214, 94, 197, 211, 94, 214, 145, 94, 197, 212, 198, 214, 94, 303, 145, 94, 197, 225, 198, 215, 94, 229, 198, 217, 145, 94, 197, 215, 94, 217, 145, 94, 197, 303, 94, 302, 145, 145, 29, 258, 159, 206, 65, 197, 235, 65, 247, 145, 198, 212, 198, 214, 65, 231, 198, 214, 65, 229, 198, 217, 65, 66, 197, 302, 94, 217, 145, 29, 259, 159, 49, 197, 258, 94, 251, 159, 229, 198, 217, 65, 66, 197, 302, 94, 217, 145, 1, 214, 94, 252, 159, 302, 145, 29, 260, 159, 49, 197, 257, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 246, 159, 197, 246, 198, 169, 197, 259, 186, 164, 94, 55, 25, 4, 260, 145, 145, 72, 234, 197, 246, 72, 79, 145, 29, 153, 29, 243, 159, 15, 197, 245, 94, 246, 145, 29, 151, 225, 1, 237, 4, 303, 55, 29, 241, 159, 178, 197, 203, 65, 197, 197, 232, 65, 303, 145, 198, 212, 65, 231, 145, 198, 213, 198, 214, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 228, 198, 216, 94, 229, 198, 217, 145, 94, 197, 216, 94, 217, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 241, 94, 243, 72, 234, 197, 241, 72, 79, 72, 99, 145, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 58, 29, 34, 222, 55, 29, 261, 159, 178, 197, 208, 65, 238, 198, 213, 198, 214, 94, 197, 213, 94, 214, 145, 94, 197, 214, 94, 303, 145, 94, 197, 228, 198, 216, 94, 229, 198, 217, 145, 94, 197, 216, 94, 217, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 261, 94, 243, 72, 234, 197, 261, 72, 79, 72, 99, 145, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 3, 29]}, {"code": "def chunk_fwd_kernel_h_reduction(\n    h,\n    g,\n    gk,\n    gv,\n    kvt,\n    ht,\n    cu_seqlens,\n    chunk_offsets,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_GK: tl.constexpr,\n    USE_GV: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT):\n        p_h = tl.make_block_ptr(\n            h + ((boh + i_t) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n        if i_t > 0:\n            tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n\n        last_idx = min(i_t * BT + BT, T) - 1\n\n        if USE_G:\n            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)\n            b_h *= exp(b_g_last)\n\n        if USE_GK:\n            p_gk_last = (\n                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n            )\n\n            b_gk_last = tl.load(\n                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0\n            )\n            b_h *= exp(b_gk_last)[:, None]\n\n        if USE_GV:\n            p_gv_last = (\n                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n            )\n\n            b_gv_last = tl.load(\n                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0\n            )\n            b_h *= exp(b_gv_last)[None, :]\n\n    if STORE_FINAL_STATE:\n        p_kvt = tl.make_block_ptr(\n            kvt + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        p_ht = tl.make_block_ptr(\n            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_h += tl.load(p_kvt, boundary_check=(0, 1)).to(tl.float32)\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 54, 6, 93, 211, 54, 6, 93, 212, 54, 6, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 93, 218, 54, 6, 93, 219, 54, 6, 93, 220, 54, 6, 145, 54, 29, -1, 221, 93, 222, 93, 223, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 93, 141, 197, 304, 145, 145, 29, 224, 93, 225, 159, 197, 223, 44, 210, 93, 223, 183, 210, 145, 29, 151, 220, 54, 29, 226, 93, 227, 159, 197, 50, 197, 207, 64, 224, 145, 71, 228, 197, 200, 145, 93, 50, 197, 207, 64, 224, 64, 303, 145, 71, 228, 197, 200, 145, 145, 29, 209, 159, 227, 4, 226, 29, 229, 159, 56, 197, 209, 93, 213, 145, 29, 230, 159, 50, 197, 208, 64, 224, 145, 71, 228, 197, 200, 145, 29, 153, 29, 28, 54, 29, 226, 93, 227, 159, 197, 224, 198, 209, 93, 224, 198, 209, 64, 209, 145, 29, 229, 159, 56, 197, 209, 93, 213, 145, 29, 230, 159, 224, 198, 229, 29, 49, 29, 231, 159, 146, 197, 186, 214, 93, 215, 25, 93, 78, 159, 124, 145, 29, 115, 232, 132, 5, 197, 229, 145, 54, 29, 233, 159, 178, 197, 201, 64, 197, 197, 230, 64, 232, 145, 198, 210, 64, 225, 145, 198, 211, 198, 212, 93, 197, 211, 93, 212, 145, 93, 197, 212, 93, 303, 145, 93, 197, 221, 198, 214, 93, 222, 198, 215, 145, 93, 197, 214, 93, 215, 145, 93, 197, 303, 93, 302, 145, 145, 29, 231, 142, 50, 197, 233, 93, 234, 159, 197, 302, 93, 303, 145, 145, 71, 228, 197, 124, 145, 29, 151, 232, 106, 302, 54, 29, 10, 197, 233, 93, 231, 71, 228, 197, 233, 71, 78, 71, 98, 145, 93, 234, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 235, 159, 36, 197, 232, 198, 213, 64, 213, 93, 209, 145, 4, 303, 29, 151, 216, 54, 29, 236, 159, 50, 197, 202, 64, 226, 198, 210, 64, 235, 198, 210, 64, 225, 145, 29, 231, 22, 169, 197, 236, 145, 29, 153, 29, 151, 217, 54, 29, 237, 159, 203, 64, 197, 226, 64, 235, 145, 198, 210, 198, 211, 64, 225, 198, 211, 64, 221, 198, 214, 64, 65, 197, 302, 93, 214, 145, 29, 238, 159, 50, 197, 237, 93, 239, 159, 221, 198, 214, 64, 65, 197, 302, 93, 214, 145, 1, 211, 93, 240, 159, 302, 145, 29, 231, 22, 169, 197, 238, 145, 186, 54, 93, 164, 25, 29, 153, 29, 151, 218, 54, 29, 241, 159, 204, 64, 197, 226, 64, 235, 145, 198, 210, 198, 212, 64, 225, 198, 212, 64, 222, 198, 215, 64, 65, 197, 302, 93, 215, 145, 29, 242, 159, 50, 197, 241, 93, 239, 159, 222, 198, 215, 64, 65, 197, 302, 93, 215, 145, 1, 212, 93, 240, 159, 302, 145, 29, 231, 22, 169, 197, 242, 145, 186, 164, 93, 54, 25, 29, 153, 29, 67, 29, 151, 219, 54, 29, 243, 159, 178, 197, 205, 64, 223, 198, 211, 198, 212, 93, 197, 211, 93, 212, 145, 93, 197, 212, 93, 303, 145, 93, 197, 221, 198, 214, 93, 222, 198, 215, 145, 93, 197, 214, 93, 215, 145, 93, 197, 303, 93, 302, 145, 145, 29, 244, 159, 178, 197, 206, 64, 223, 198, 211, 198, 212, 93, 197, 211, 93, 212, 145, 93, 197, 212, 93, 303, 145, 93, 197, 221, 198, 214, 93, 222, 198, 215, 145, 93, 197, 214, 93, 215, 145, 93, 197, 303, 93, 302, 145, 145, 29, 231, 142, 50, 197, 243, 93, 234, 159, 197, 302, 93, 303, 145, 145, 71, 228, 197, 124, 145, 29, 10, 197, 244, 93, 231, 71, 228, 197, 244, 71, 78, 71, 98, 145, 93, 234, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 3, 29]}, {"code": "def chunk_bwd_kernel_dh_parallel(\n    q,\n    g,\n    gk,\n    gv,\n    do,\n    dh,\n    dht,\n    dh0,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NG: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_GK: tl.constexpr,\n    USE_GV: tl.constexpr,\n    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    NV = tl.cdiv(V, BV)\n    i_k, i_v = i_kv // NV, i_kv % NV\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // NG\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        NT = tl.cdiv(T, BT)\n        i_n, i_tg = i_b, i_b * NT + i_t\n    i_nh = i_n * HQ + i_hq\n\n    p_q = tl.make_block_ptr(\n        q + (bos * HQ + i_hq) * K,\n        (K, T),\n        (1, HQ * K),\n        (i_k * BK, i_t * BT),\n        (BK, BT),\n        (0, 1),\n    )\n    p_do = tl.make_block_ptr(\n        do + (bos * HQ + i_hq) * V,\n        (T, V),\n        (HQ * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    p_dh = tl.make_block_ptr(\n        dh + (i_tg * H + i_h) * K * V,\n        (K, V),\n        (V, 1),\n        (i_k * BK, i_v * BV),\n        (BK, BV),\n        (1, 0),\n    )\n\n    if i_t == NT - 1:\n        if USE_FINAL_STATE_GRADIENT:\n            p_dht = tl.make_block_ptr(\n                dht + i_nh * K * V,\n                (K, V),\n                (V, 1),\n                (i_k * BK, i_v * BV),\n                (BK, BV),\n                (1, 0),\n            )\n            b_dh = tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)\n        else:\n            b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n\n    if USE_G:\n        p_g = g + (bos + i_t * BT + tl.arange(0, BT)) * H + i_h\n        b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)\n        b_q = (b_q * exp(b_g)[None, :]).to(b_q.dtype)\n\n    if USE_GK:\n        p_gk = tl.make_block_ptr(\n            gk + (bos * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_q = (b_q * exp(b_gk)).to(b_q.dtype)\n\n    if USE_GV:\n        p_gv = tl.make_block_ptr(\n            gv + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_do = (b_do * exp(b_gv)).to(b_do.dtype)\n\n    b_dh = tl.dot(b_q, b_do)\n    if i_t > 0:\n        p_dh = tl.make_block_ptr(\n            dh + ((i_tg - 1) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n    elif STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = tl.make_block_ptr(\n            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 94, 209, 94, 210, 94, 211, 94, 212, 94, 213, 55, 6, 94, 214, 55, 6, 94, 215, 55, 6, 94, 216, 55, 6, 94, 217, 55, 6, 94, 218, 55, 6, 94, 219, 55, 6, 94, 220, 55, 6, 94, 221, 55, 6, 94, 222, 55, 6, 94, 223, 55, 6, 94, 224, 55, 6, 94, 225, 55, 6, 94, 226, 55, 6, 145, 55, 29, -1, 227, 94, 228, 94, 229, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 94, 141, 197, 304, 145, 145, 29, 230, 159, 57, 197, 216, 94, 219, 145, 29, 231, 94, 232, 159, 197, 227, 44, 230, 94, 227, 183, 230, 145, 29, 233, 94, 234, 159, 197, 229, 44, 213, 94, 229, 183, 213, 145, 29, 235, 159, 234, 44, 220, 29, 151, 226, 55, 29, 236, 159, 228, 29, 237, 94, 228, 159, 197, 49, 197, 210, 65, 228, 198, 304, 145, 72, 238, 197, 54, 145, 94, 49, 197, 210, 65, 228, 198, 304, 65, 303, 145, 72, 238, 197, 54, 145, 145, 29, 239, 94, 240, 159, 197, 49, 197, 209, 65, 237, 145, 72, 238, 197, 54, 145, 94, 49, 197, 209, 65, 237, 65, 303, 145, 72, 238, 197, 54, 145, 145, 29, 212, 159, 240, 4, 239, 29, 241, 159, 57, 197, 212, 94, 217, 145, 29, 153, 29, 28, 55, 29, 239, 94, 240, 159, 197, 233, 198, 212, 94, 233, 198, 212, 65, 212, 145, 29, 241, 159, 57, 197, 212, 94, 217, 145, 29, 237, 94, 236, 159, 197, 233, 94, 233, 198, 241, 65, 228, 145, 29, 50, 29, 242, 159, 237, 198, 213, 65, 234, 29, 243, 159, 178, 197, 201, 65, 197, 239, 198, 213, 65, 234, 145, 198, 215, 94, 197, 215, 94, 212, 145, 94, 197, 303, 94, 213, 198, 215, 145, 94, 197, 231, 198, 218, 94, 228, 198, 217, 145, 94, 197, 218, 94, 217, 145, 94, 197, 302, 94, 303, 145, 145, 29, 244, 159, 178, 197, 205, 65, 197, 239, 198, 213, 65, 234, 145, 198, 216, 94, 197, 212, 94, 216, 145, 94, 197, 213, 198, 216, 94, 303, 145, 94, 197, 228, 198, 217, 94, 232, 198, 219, 145, 94, 197, 217, 94, 219, 145, 94, 197, 303, 94, 302, 145, 145, 29, 245, 159, 178, 197, 206, 65, 197, 236, 198, 214, 65, 235, 145, 198, 215, 198, 216, 94, 197, 215, 94, 216, 145, 94, 197, 216, 94, 303, 145, 94, 197, 231, 198, 218, 94, 232, 198, 219, 145, 94, 197, 218, 94, 219, 145, 94, 197, 303, 94, 302, 145, 145, 29, 151, 228, 67, 241, 4, 303, 55, 29, 151, 225, 55, 29, 246, 159, 178, 197, 207, 65, 242, 198, 215, 198, 216, 94, 197, 215, 94, 216, 145, 94, 197, 216, 94, 303, 145, 94, 197, 231, 198, 218, 94, 232, 198, 219, 145, 94, 197, 218, 94, 219, 145, 94, 197, 303, 94, 302, 145, 145, 29, 247, 159, 49, 197, 246, 94, 248, 159, 197, 302, 94, 303, 145, 145, 72, 238, 197, 125, 145, 29, 153, 29, 28, 55, 29, 247, 159, 146, 197, 186, 218, 94, 219, 25, 94, 79, 159, 125, 145, 29, 50, 29, 10, 197, 245, 94, 247, 72, 238, 197, 245, 72, 79, 72, 99, 145, 94, 248, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 249, 159, 49, 197, 243, 94, 248, 159, 197, 302, 94, 303, 145, 145, 29, 249, 159, 197, 249, 198, 211, 145, 72, 238, 197, 249, 72, 79, 145, 29, 250, 159, 49, 197, 244, 94, 248, 159, 197, 302, 94, 303, 145, 145, 29, 151, 221, 55, 29, 251, 159, 202, 65, 197, 239, 65, 228, 198, 217, 65, 66, 197, 302, 94, 217, 145, 145, 198, 214, 65, 235, 29, 252, 159, 49, 197, 251, 94, 253, 159, 228, 198, 217, 65, 66, 197, 302, 94, 217, 145, 1, 212, 94, 254, 159, 302, 145, 29, 249, 159, 197, 249, 198, 169, 197, 252, 145, 186, 164, 94, 55, 25, 145, 72, 238, 197, 249, 72, 79, 145, 29, 153, 29, 151, 222, 55, 29, 255, 159, 178, 197, 203, 65, 197, 239, 198, 214, 65, 235, 145, 198, 215, 94, 197, 215, 94, 212, 145, 94, 197, 303, 94, 214, 198, 215, 145, 94, 197, 231, 198, 218, 94, 228, 198, 217, 145, 94, 197, 218, 94, 217, 145, 94, 197, 302, 94, 303, 145, 145, 29, 256, 159, 49, 197, 255, 94, 248, 159, 197, 302, 94, 303, 145, 145, 29, 249, 159, 197, 249, 198, 169, 197, 256, 145, 145, 72, 238, 197, 249, 72, 79, 145, 29, 153, 29, 151, 223, 55, 29, 257, 159, 178, 197, 204, 65, 197, 239, 198, 214, 65, 235, 145, 198, 216, 94, 197, 212, 94, 216, 145, 94, 197, 214, 198, 216, 94, 303, 145, 94, 197, 228, 198, 217, 94, 232, 198, 219, 145, 94, 197, 217, 94, 219, 145, 94, 197, 303, 94, 302, 145, 145, 29, 258, 159, 49, 197, 257, 94, 248, 159, 197, 302, 94, 303, 145, 145, 29, 250, 159, 197, 250, 198, 169, 197, 258, 145, 145, 72, 238, 197, 250, 72, 79, 145, 29, 153, 29, 247, 159, 15, 197, 249, 94, 250, 145, 29, 151, 228, 107, 302, 55, 29, 245, 159, 178, 197, 206, 65, 197, 197, 236, 4, 303, 145, 198, 214, 65, 235, 145, 198, 215, 198, 216, 94, 197, 215, 94, 216, 145, 94, 197, 216, 94, 303, 145, 94, 197, 231, 198, 218, 94, 232, 198, 219, 145, 94, 197, 218, 94, 219, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 245, 94, 247, 72, 238, 197, 245, 72, 79, 72, 99, 145, 94, 248, 159, 197, 302, 94, 303, 145, 145, 29, 58, 29, 34, 224, 55, 29, 259, 159, 178, 197, 208, 65, 242, 198, 215, 198, 216, 94, 197, 215, 94, 216, 145, 94, 197, 216, 94, 303, 145, 94, 197, 231, 198, 218, 94, 232, 198, 219, 145, 94, 197, 218, 94, 219, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 259, 94, 247, 72, 238, 197, 259, 72, 79, 72, 99, 145, 94, 248, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 3, 29]}, {"code": "def chunk_bwd_kernel_dh_reduction(\n    g,\n    gk,\n    gv,\n    dh,\n    doq0,\n    dh0,\n    cu_seqlens,\n    chunk_offsets,\n    T,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NG: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_GK: tl.constexpr,\n    USE_GV: tl.constexpr,\n    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_hq = i_nh // HQ, i_nh % HQ\n    i_h = i_hq // NG\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_dh = tl.make_block_ptr(\n            dh + ((boh + i_t) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        b_dh += tl.load(p_dh, boundary_check=(0, 1)).to(tl.float32)\n        if i_t < NT - 1:\n            tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n\n        last_idx = min(i_t * BT + BT, T) - 1\n        if USE_G:\n            b_g_last = tl.load(g + (bos + last_idx) * H + i_h)\n            b_dh *= exp(b_g_last)\n\n        if USE_GK:\n            p_gk_last = (\n                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n            )\n            b_gk_last = tl.load(\n                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0\n            )\n            b_dh *= exp(b_gk_last)[:, None]\n\n        if USE_GV:\n            p_gv_last = (\n                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n            )\n            b_gv_last = tl.load(\n                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0\n            )\n            b_dh *= exp(b_gv_last)[None, :]\n\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_doq0 = tl.make_block_ptr(\n            doq0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        p_dh0 = tl.make_block_ptr(\n            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_dh += tl.load(p_doq0, boundary_check=(0, 1)).to(tl.float32)\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 54, 6, 93, 211, 54, 6, 93, 212, 54, 6, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 93, 218, 54, 6, 93, 219, 54, 6, 93, 220, 54, 6, 93, 221, 54, 6, 93, 222, 54, 6, 145, 54, 29, -1, 223, 93, 224, 93, 225, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 93, 141, 197, 304, 145, 145, 29, 226, 93, 227, 159, 197, 225, 44, 210, 93, 225, 183, 210, 145, 29, 228, 159, 227, 44, 217, 29, 151, 222, 54, 29, 229, 93, 230, 159, 197, 50, 197, 207, 64, 226, 145, 71, 231, 197, 200, 145, 93, 50, 197, 207, 64, 226, 64, 303, 145, 71, 231, 197, 200, 145, 145, 29, 209, 159, 230, 4, 229, 29, 232, 159, 56, 197, 209, 93, 214, 145, 29, 233, 159, 50, 197, 208, 64, 226, 145, 71, 231, 197, 200, 145, 29, 153, 29, 28, 54, 29, 229, 93, 230, 159, 197, 226, 198, 209, 93, 226, 198, 209, 64, 209, 145, 29, 232, 159, 56, 197, 209, 93, 214, 145, 29, 233, 159, 226, 198, 232, 29, 49, 29, 234, 159, 146, 197, 186, 215, 93, 216, 25, 93, 78, 159, 124, 145, 29, 115, 235, 132, 5, 197, 232, 4, 303, 93, 4, 303, 93, 4, 303, 145, 54, 29, 236, 159, 178, 197, 204, 64, 197, 197, 233, 64, 235, 145, 198, 211, 64, 228, 145, 198, 212, 198, 213, 93, 197, 212, 93, 213, 145, 93, 197, 213, 93, 303, 145, 93, 197, 223, 198, 215, 93, 224, 198, 216, 145, 93, 197, 215, 93, 216, 145, 93, 197, 303, 93, 302, 145, 145, 29, 234, 142, 50, 197, 236, 93, 237, 159, 197, 302, 93, 303, 145, 145, 71, 231, 197, 124, 145, 29, 151, 235, 1, 232, 4, 303, 54, 29, 10, 197, 236, 93, 234, 71, 231, 197, 236, 71, 78, 71, 98, 145, 93, 237, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 238, 159, 36, 197, 235, 198, 214, 64, 214, 93, 209, 145, 4, 303, 29, 151, 218, 54, 29, 239, 159, 50, 197, 201, 64, 197, 229, 64, 238, 145, 198, 211, 64, 228, 145, 29, 234, 22, 169, 197, 239, 145, 29, 153, 29, 151, 219, 54, 29, 240, 159, 202, 64, 197, 229, 64, 238, 145, 198, 211, 198, 212, 64, 228, 198, 212, 64, 223, 198, 215, 64, 65, 197, 302, 93, 215, 145, 29, 241, 159, 50, 197, 240, 93, 242, 159, 223, 198, 215, 64, 65, 197, 302, 93, 215, 145, 1, 212, 93, 243, 159, 302, 145, 29, 234, 22, 169, 197, 241, 145, 186, 54, 93, 164, 25, 29, 153, 29, 151, 220, 54, 29, 244, 159, 203, 64, 197, 229, 64, 238, 145, 198, 211, 198, 213, 64, 228, 198, 213, 64, 224, 198, 216, 64, 65, 197, 302, 93, 216, 145, 29, 245, 159, 50, 197, 244, 93, 242, 159, 224, 198, 216, 64, 65, 197, 302, 93, 216, 145, 1, 213, 93, 243, 159, 302, 145, 29, 234, 22, 169, 197, 245, 145, 186, 164, 93, 54, 25, 29, 153, 29, 67, 29, 151, 221, 54, 29, 246, 159, 178, 197, 205, 64, 225, 198, 212, 198, 213, 93, 197, 212, 93, 213, 145, 93, 197, 213, 93, 303, 145, 93, 197, 223, 198, 215, 93, 224, 198, 216, 145, 93, 197, 215, 93, 216, 145, 93, 197, 303, 93, 302, 145, 145, 29, 247, 159, 178, 197, 206, 64, 225, 198, 212, 198, 213, 93, 197, 212, 93, 213, 145, 93, 197, 213, 93, 303, 145, 93, 197, 223, 198, 215, 93, 224, 198, 216, 145, 93, 197, 215, 93, 216, 145, 93, 197, 303, 93, 302, 145, 145, 29, 234, 142, 50, 197, 246, 93, 237, 159, 197, 302, 93, 303, 145, 145, 71, 231, 197, 124, 145, 29, 10, 197, 247, 93, 234, 71, 231, 197, 247, 71, 78, 71, 98, 145, 93, 237, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 3, 29]}, {"code": "def chunk_fwd_kernel_h_split(\n    k,\n    v,\n    g,\n    gk,\n    gv,\n    hs,\n    hr,\n    h0,\n    ht,\n    cu_seqlens,\n    split_indices,\n    T,\n    S: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_GK: tl.constexpr,\n    USE_GV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n\n    i_k, i_v, i_sh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_ss, i_h = i_sh // H, i_sh % H\n    if IS_VARLEN:\n        i_n, i_s = tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(\n            split_indices + i_ss * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NS = tl.cdiv(T, S)\n    else:\n        NS = tl.cdiv(T, S)\n        i_n, i_s = i_ss // NS, i_ss % NS\n        bos, eos = i_n * T, i_n * T + T\n    i_nh = i_n * H + i_h\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if i_s == 0:\n        if USE_INITIAL_STATE:\n            p_h0 = tl.make_block_ptr(\n                h0 + i_nh * K * V,\n                (K, V),\n                (V, 1),\n                (i_k * BK, i_v * BV),\n                (BK, BV),\n                (1, 0),\n            )\n            b_h += tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n        p_hr = tl.make_block_ptr(\n            hr + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_hr, b_h.to(p_hr.dtype.element_ty), boundary_check=(0, 1))\n    for i_t in range(tl.cdiv(i_s * S, BT), tl.cdiv(min(i_s * S + S, T), BT)):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        last_idx = min(i_t * BT + BT, T) - 1\n\n        if USE_G:\n            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)\n            p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h\n            b_h *= exp(b_g_last)\n            b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)\n            b_v = (b_v * exp(b_g_last - b_g)[:, None]).to(b_v.dtype)\n\n        if USE_GK:\n            p_gk = tl.make_block_ptr(\n                gk + (bos * H + i_h) * K,\n                (K, T),\n                (1, H * K),\n                (i_k * BK, i_t * BT),\n                (BK, BT),\n                (0, 1),\n            )\n            p_gk_last = (\n                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n            )\n\n            b_gk_last = tl.load(\n                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0\n            )\n            b_h *= exp(b_gk_last)[:, None]\n\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_k = (b_k * exp(b_gk_last[:, None] - b_gk)).to(b_k.dtype)\n\n        if USE_GV:\n            p_gv = tl.make_block_ptr(\n                gv + (bos * H + i_h) * V,\n                (T, V),\n                (H * V, 1),\n                (i_t * BT, i_v * BV),\n                (BT, BV),\n                (1, 0),\n            )\n            p_gv_last = (\n                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n            )\n\n            b_gv_last = tl.load(\n                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0\n            )\n            b_h *= exp(b_gv_last)[None, :]\n\n            b_gv = tl.load(p_gv, boundary_check=(0, 1))\n            b_v = (b_v * exp(b_gv_last[None, :] - b_gv)).to(b_v.dtype)\n\n        b_h += tl.dot(b_k, b_v)\n\n    if NS > 1:\n        p_hs = tl.make_block_ptr(\n            hs + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_hs, b_h.to(p_hs.dtype.element_ty), boundary_check=(0, 1))\n    elif STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(\n            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 94, 209, 94, 210, 94, 211, 94, 212, 94, 213, 55, 6, 94, 214, 55, 6, 94, 215, 55, 6, 94, 216, 55, 6, 94, 217, 55, 6, 94, 218, 55, 6, 94, 219, 55, 6, 94, 220, 55, 6, 94, 221, 55, 6, 94, 222, 55, 6, 94, 223, 55, 6, 94, 224, 55, 6, 94, 225, 55, 6, 145, 55, 29, -1, 226, 94, 227, 94, 228, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 94, 141, 197, 304, 145, 145, 29, 229, 94, 230, 159, 197, 228, 44, 214, 94, 228, 183, 214, 145, 29, 151, 225, 55, 29, 231, 94, 232, 159, 197, 49, 197, 211, 65, 229, 198, 304, 145, 72, 233, 197, 54, 145, 94, 49, 197, 211, 65, 229, 198, 304, 65, 303, 145, 72, 233, 197, 54, 145, 145, 29, 234, 94, 235, 159, 197, 49, 197, 210, 65, 231, 145, 72, 233, 197, 54, 145, 94, 49, 197, 210, 65, 231, 65, 303, 145, 72, 233, 197, 54, 145, 145, 29, 212, 159, 235, 4, 234, 29, 236, 159, 57, 197, 212, 94, 213, 145, 29, 153, 29, 28, 55, 29, 236, 159, 57, 197, 212, 94, 213, 145, 29, 231, 94, 232, 159, 197, 229, 44, 236, 94, 229, 183, 236, 145, 29, 234, 94, 235, 159, 197, 231, 198, 212, 94, 231, 198, 212, 65, 212, 145, 29, 50, 29, 237, 159, 231, 198, 214, 65, 230, 29, 238, 159, 146, 197, 186, 218, 94, 219, 25, 94, 79, 159, 125, 145, 29, 151, 232, 67, 302, 55, 29, 151, 223, 55, 29, 239, 159, 178, 197, 208, 65, 237, 198, 215, 198, 216, 94, 197, 215, 94, 216, 145, 94, 197, 216, 94, 303, 145, 94, 197, 226, 198, 218, 94, 227, 198, 219, 145, 94, 197, 218, 94, 219, 145, 94, 197, 303, 94, 302, 145, 145, 29, 238, 142, 49, 197, 239, 94, 240, 159, 197, 302, 94, 303, 145, 145, 72, 233, 197, 125, 145, 29, 153, 29, 241, 159, 178, 197, 207, 65, 228, 198, 215, 198, 216, 94, 197, 215, 94, 216, 145, 94, 197, 216, 94, 303, 145, 94, 197, 226, 198, 218, 94, 227, 198, 219, 145, 94, 197, 218, 94, 219, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 241, 94, 238, 72, 233, 197, 241, 72, 79, 72, 99, 145, 94, 240, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 116, 242, 132, 5, 197, 57, 197, 232, 198, 213, 94, 217, 145, 94, 57, 197, 36, 197, 232, 198, 213, 65, 213, 94, 212, 145, 94, 217, 145, 145, 55, 29, 243, 159, 178, 197, 201, 65, 197, 234, 198, 214, 65, 230, 145, 198, 215, 94, 197, 215, 94, 212, 145, 94, 197, 303, 94, 214, 198, 215, 145, 94, 197, 226, 198, 218, 94, 242, 198, 217, 145, 94, 197, 218, 94, 217, 145, 94, 197, 302, 94, 303, 145, 145, 29, 244, 159, 178, 197, 202, 65, 197, 234, 198, 214, 65, 230, 145, 198, 216, 94, 197, 212, 94, 216, 145, 94, 197, 214, 198, 216, 94, 303, 145, 94, 197, 242, 198, 217, 94, 227, 198, 219, 145, 94, 197, 217, 94, 219, 145, 94, 197, 303, 94, 302, 145, 145, 29, 245, 159, 49, 197, 243, 94, 240, 159, 197, 302, 94, 303, 145, 145, 29, 246, 159, 49, 197, 244, 94, 240, 159, 197, 302, 94, 303, 145, 145, 29, 247, 159, 36, 197, 242, 198, 217, 65, 217, 94, 212, 145, 4, 303, 29, 151, 220, 55, 29, 248, 159, 49, 197, 203, 65, 234, 198, 214, 65, 247, 198, 214, 65, 230, 145, 29, 249, 159, 203, 65, 234, 198, 214, 65, 197, 242, 198, 217, 65, 66, 197, 302, 94, 217, 145, 145, 198, 214, 65, 230, 29, 238, 22, 169, 197, 248, 145, 29, 250, 159, 49, 197, 249, 94, 251, 159, 242, 198, 217, 65, 66, 197, 302, 94, 217, 145, 1, 212, 94, 252, 159, 302, 145, 29, 246, 159, 197, 246, 198, 169, 197, 248, 4, 250, 145, 186, 55, 94, 164, 25, 145, 72, 233, 197, 246, 72, 79, 145, 29, 153, 29, 151, 221, 55, 29, 253, 159, 178, 197, 204, 65, 197, 234, 198, 214, 65, 230, 145, 198, 215, 94, 197, 215, 94, 212, 145, 94, 197, 303, 94, 214, 198, 215, 145, 94, 197, 226, 198, 218, 94, 242, 198, 217, 145, 94, 197, 218, 94, 217, 145, 94, 197, 302, 94, 303, 145, 145, 29, 254, 159, 204, 65, 197, 234, 65, 247, 145, 198, 214, 198, 215, 65, 230, 198, 215, 65, 226, 198, 218, 65, 66, 197, 302, 94, 218, 145, 29, 255, 159, 49, 197, 254, 94, 251, 159, 226, 198, 218, 65, 66, 197, 302, 94, 218, 145, 1, 215, 94, 252, 159, 302, 145, 29, 238, 22, 169, 197, 255, 145, 186, 55, 94, 164, 25, 29, 256, 159, 49, 197, 253, 94, 240, 159, 197, 302, 94, 303, 145, 145, 29, 245, 159, 197, 245, 198, 169, 197, 255, 186, 55, 94, 164, 25, 4, 256, 145, 145, 72, 233, 197, 245, 72, 79, 145, 29, 153, 29, 151, 222, 55, 29, 257, 159, 178, 197, 205, 65, 197, 234, 198, 214, 65, 230, 145, 198, 216, 94, 197, 212, 94, 216, 145, 94, 197, 214, 198, 216, 94, 303, 145, 94, 197, 242, 198, 217, 94, 227, 198, 219, 145, 94, 197, 217, 94, 219, 145, 94, 197, 303, 94, 302, 145, 145, 29, 258, 159, 205, 65, 197, 234, 65, 247, 145, 198, 214, 198, 216, 65, 230, 198, 216, 65, 227, 198, 219, 65, 66, 197, 302, 94, 219, 145, 29, 259, 159, 49, 197, 258, 94, 251, 159, 227, 198, 219, 65, 66, 197, 302, 94, 219, 145, 1, 216, 94, 252, 159, 302, 145, 29, 238, 22, 169, 197, 259, 145, 186, 164, 94, 55, 25, 29, 260, 159, 49, 197, 257, 94, 240, 159, 197, 302, 94, 303, 145, 145, 29, 246, 159, 197, 246, 198, 169, 197, 259, 186, 164, 94, 55, 25, 4, 260, 145, 145, 72, 233, 197, 246, 72, 79, 145, 29, 153, 29, 238, 142, 15, 197, 245, 94, 246, 145, 29, 68, 29, 151, 236, 107, 303, 55, 29, 261, 159, 178, 197, 206, 65, 228, 198, 215, 198, 216, 94, 197, 215, 94, 216, 145, 94, 197, 216, 94, 303, 145, 94, 197, 226, 198, 218, 94, 227, 198, 219, 145, 94, 197, 218, 94, 219, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 261, 94, 238, 72, 233, 197, 261, 72, 79, 72, 99, 145, 94, 240, 159, 197, 302, 94, 303, 145, 145, 29, 58, 29, 34, 224, 55, 29, 262, 159, 178, 197, 209, 65, 237, 198, 215, 198, 216, 94, 197, 215, 94, 216, 145, 94, 197, 216, 94, 303, 145, 94, 197, 226, 198, 218, 94, 227, 198, 219, 145, 94, 197, 218, 94, 219, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 262, 94, 238, 72, 233, 197, 262, 72, 79, 72, 99, 145, 94, 240, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 3, 29]}, {"code": "def chunk_fwd_kernel_h_reduction(\n    g,\n    gk,\n    gv,\n    hs,\n    hr,\n    ht,\n    cu_seqlens,\n    split_offsets,\n    T,\n    S: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_GK: tl.constexpr,\n    USE_GV: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NS = tl.cdiv(T, S)\n        boh = tl.load(split_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NS = tl.cdiv(T, S)\n        boh = i_n * NS\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    for i_s in range(1, NS):\n        p_hs = tl.make_block_ptr(\n            hs + ((boh + i_s - 1) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        p_hr = tl.make_block_ptr(\n            hr + ((boh + i_s) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        b_h += tl.load(p_hs, boundary_check=(0, 1)).to(tl.float32)\n        tl.store(p_hr, b_h.to(p_hr.dtype.element_ty), boundary_check=(0, 1))\n\n        for i_t in range(tl.cdiv(i_s * S, BT), tl.cdiv(min(i_s * S + S, T), BT)):\n            last_idx = min(i_t * BT + BT, T) - 1\n\n            if USE_G:\n                b_g_last = tl.load(g + bos * H + last_idx * H + i_h)\n                b_h *= exp(b_g_last)\n\n            if USE_GK:\n                p_gk_last = (\n                    gk\n                    + (bos + last_idx) * H * K\n                    + i_h * K\n                    + i_k * BK\n                    + tl.arange(0, BK)\n                )\n                b_gk_last = tl.load(\n                    p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0\n                )\n                b_h *= exp(b_gk_last)[:, None]\n\n            if USE_GV:\n                p_gv_last = (\n                    gv\n                    + (bos + last_idx) * H * V\n                    + i_h * V\n                    + i_v * BV\n                    + tl.arange(0, BV)\n                )\n                b_gv_last = tl.load(\n                    p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0\n                )\n                b_h *= exp(b_gv_last)[None, :]\n\n    if NS > 1:\n        if STORE_FINAL_STATE:\n            p_hs = tl.make_block_ptr(\n                hs + ((boh + NS - 1) * H + i_h) * K * V,\n                (K, V),\n                (V, 1),\n                (i_k * BK, i_v * BV),\n                (BK, BV),\n                (1, 0),\n            )\n            p_ht = tl.make_block_ptr(\n                ht + i_nh * K * V,\n                (K, V),\n                (V, 1),\n                (i_k * BK, i_v * BV),\n                (BK, BV),\n                (1, 0),\n            )\n            b_h += tl.load(p_hs, boundary_check=(0, 1)).to(tl.float32)\n            tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 54, 6, 93, 211, 54, 6, 93, 212, 54, 6, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 93, 218, 54, 6, 93, 219, 54, 6, 93, 220, 54, 6, 93, 221, 54, 6, 145, 54, 29, -1, 222, 93, 223, 93, 224, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 93, 141, 197, 304, 145, 145, 29, 225, 93, 226, 159, 197, 224, 44, 211, 93, 224, 183, 211, 145, 29, 151, 221, 54, 29, 227, 93, 228, 159, 197, 50, 197, 207, 64, 225, 145, 71, 229, 197, 200, 145, 93, 50, 197, 207, 64, 225, 64, 303, 145, 71, 229, 197, 200, 145, 145, 29, 209, 159, 228, 4, 227, 29, 230, 159, 56, 197, 209, 93, 210, 145, 29, 231, 159, 50, 197, 208, 64, 225, 145, 71, 229, 197, 200, 145, 29, 153, 29, 28, 54, 29, 227, 93, 228, 159, 197, 225, 198, 209, 93, 225, 198, 209, 64, 209, 145, 29, 230, 159, 56, 197, 209, 93, 210, 145, 29, 231, 159, 225, 198, 230, 29, 49, 29, 232, 159, 146, 197, 186, 215, 93, 216, 25, 93, 78, 159, 124, 145, 29, 115, 233, 132, 5, 197, 303, 93, 230, 145, 54, 29, 234, 159, 178, 197, 204, 64, 197, 197, 231, 64, 233, 4, 303, 145, 198, 211, 64, 226, 145, 198, 212, 198, 213, 93, 197, 212, 93, 213, 145, 93, 197, 213, 93, 303, 145, 93, 197, 222, 198, 215, 93, 223, 198, 216, 145, 93, 197, 215, 93, 216, 145, 93, 197, 303, 93, 302, 145, 145, 29, 235, 159, 178, 197, 205, 64, 197, 197, 231, 64, 233, 145, 198, 211, 64, 226, 145, 198, 212, 198, 213, 93, 197, 212, 93, 213, 145, 93, 197, 213, 93, 303, 145, 93, 197, 222, 198, 215, 93, 223, 198, 216, 145, 93, 197, 215, 93, 216, 145, 93, 197, 303, 93, 302, 145, 145, 29, 232, 142, 50, 197, 234, 93, 236, 159, 197, 302, 93, 303, 145, 145, 71, 229, 197, 124, 145, 29, 10, 197, 235, 93, 232, 71, 229, 197, 235, 71, 78, 71, 98, 145, 93, 236, 159, 197, 302, 93, 303, 145, 145, 29, 115, 237, 132, 5, 197, 56, 197, 233, 198, 210, 93, 214, 145, 93, 56, 197, 36, 197, 233, 198, 210, 64, 210, 93, 209, 145, 93, 214, 145, 145, 54, 29, 238, 159, 36, 197, 237, 198, 214, 64, 214, 93, 209, 145, 4, 303, 29, 151, 217, 54, 29, 239, 159, 50, 197, 201, 64, 227, 198, 211, 64, 238, 198, 211, 64, 226, 145, 29, 232, 22, 169, 197, 239, 145, 29, 153, 29, 151, 218, 54, 29, 240, 159, 202, 64, 197, 227, 64, 238, 145, 198, 211, 198, 212, 64, 226, 198, 212, 64, 222, 198, 215, 64, 65, 197, 302, 93, 215, 145, 29, 241, 159, 50, 197, 240, 93, 242, 159, 222, 198, 215, 64, 65, 197, 302, 93, 215, 145, 1, 212, 93, 243, 159, 302, 145, 29, 232, 22, 169, 197, 241, 145, 186, 54, 93, 164, 25, 29, 153, 29, 151, 219, 54, 29, 244, 159, 203, 64, 197, 227, 64, 238, 145, 198, 211, 198, 213, 64, 226, 198, 213, 64, 223, 198, 216, 64, 65, 197, 302, 93, 216, 145, 29, 245, 159, 50, 197, 244, 93, 242, 159, 223, 198, 216, 64, 65, 197, 302, 93, 216, 145, 1, 213, 93, 243, 159, 302, 145, 29, 232, 22, 169, 197, 245, 145, 186, 164, 93, 54, 25, 29, 153, 29, 67, 29, 67, 29, 151, 230, 106, 303, 54, 29, 151, 220, 54, 29, 234, 159, 178, 197, 204, 64, 197, 197, 231, 64, 230, 4, 303, 145, 198, 211, 64, 226, 145, 198, 212, 198, 213, 93, 197, 212, 93, 213, 145, 93, 197, 213, 93, 303, 145, 93, 197, 222, 198, 215, 93, 223, 198, 216, 145, 93, 197, 215, 93, 216, 145, 93, 197, 303, 93, 302, 145, 145, 29, 246, 159, 178, 197, 206, 64, 224, 198, 212, 198, 213, 93, 197, 212, 93, 213, 145, 93, 197, 213, 93, 303, 145, 93, 197, 222, 198, 215, 93, 223, 198, 216, 145, 93, 197, 215, 93, 216, 145, 93, 197, 303, 93, 302, 145, 145, 29, 232, 142, 50, 197, 234, 93, 236, 159, 197, 302, 93, 303, 145, 145, 71, 229, 197, 124, 145, 29, 10, 197, 246, 93, 232, 71, 229, 197, 246, 71, 78, 71, 98, 145, 93, 236, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 153, 29, 3, 29]}, {"code": "def chunk_bwd_kernel_dh_split(\n    q,\n    g,\n    gk,\n    gv,\n    do,\n    dht,\n    dhs,\n    dhr,\n    dh0,\n    cu_seqlens,\n    split_indices,\n    scale,\n    T,\n    S: tl.constexpr,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NG: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_GK: tl.constexpr,\n    USE_GV: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT: tl.constexpr,\n    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n\n    i_k, i_v, i_sh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_ss, i_hq = i_sh // HQ, i_sh % HQ\n    if IS_VARLEN:\n        i_n, i_s = tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(\n            split_indices + i_ss * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NS = tl.cdiv(T, S)\n    else:\n        NS = tl.cdiv(T, S)\n        i_n, i_s = i_ss // NS, i_ss % NS\n        bos, eos = i_n * T, i_n * T + T\n    i_nh = i_n * HQ + i_hq\n    i_h = i_hq // NG\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if i_s == NS - 1:\n        if USE_FINAL_STATE_GRADIENT:\n            p_dht = tl.make_block_ptr(\n                dht + i_nh * K * V,\n                (K, V),\n                (V, 1),\n                (i_k * BK, i_v * BV),\n                (BK, BV),\n                (1, 0),\n            )\n            b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)\n        p_dhr = tl.make_block_ptr(\n            dhr + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_dhr, b_dh.to(p_dhr.dtype.element_ty), boundary_check=(0, 1))\n\n    for i_t in range(\n        tl.cdiv(min(i_s * S + S, T), BT) - 1, tl.cdiv(i_s * S, BT) - 1, -1\n    ):\n        p_q = tl.make_block_ptr(\n            q + (bos * HQ + i_hq) * K,\n            (K, T),\n            (1, HQ * K),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_do = tl.make_block_ptr(\n            do + (bos * HQ + i_hq) * V,\n            (T, V),\n            (HQ * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        last_idx = min(i_t * BT + BT, T) - 1\n        if USE_G:\n            p_g = g + (bos + i_t * BT + tl.arange(0, BT)) * H + i_h\n            b_g_last = tl.load(g + (bos + last_idx) * H + i_h)\n            b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)\n            b_q = (b_q * exp(b_g)[None, :]).to(b_q.dtype)\n            b_dh *= exp(b_g_last)\n\n        if USE_GK:\n            p_gk = tl.make_block_ptr(\n                gk + (bos * H + i_h) * K,\n                (K, T),\n                (1, H * K),\n                (i_k * BK, i_t * BT),\n                (BK, BT),\n                (0, 1),\n            )\n            p_gk_last = (\n                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n            )\n\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_q = (b_q * exp(b_gk)).to(b_q.dtype)\n            b_gk_last = tl.load(\n                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0\n            )\n            b_dh *= exp(b_gk_last)[:, None]\n\n        if USE_GV:\n            p_gv = tl.make_block_ptr(\n                gv + (bos * H + i_h) * V,\n                (T, V),\n                (H * V, 1),\n                (i_t * BT, i_v * BV),\n                (BT, BV),\n                (1, 0),\n            )\n            p_gv_last = (\n                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n            )\n\n            b_gv = tl.load(p_gv, boundary_check=(0, 1))\n            b_do = (b_do * exp(b_gv)).to(b_do.dtype)\n\n            b_gv_last = tl.load(\n                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0\n            )\n            b_dh *= exp(b_gv_last)[None, :]\n\n        b_dh += tl.dot(b_q, b_do)\n\n    if NS > 1:\n        p_dhs = tl.make_block_ptr(\n            dhs + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_dhs, b_dh.to(p_dhs.dtype.element_ty), boundary_check=(0, 1))\n    elif STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = tl.make_block_ptr(\n            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 94, 209, 94, 210, 94, 211, 94, 212, 94, 213, 94, 214, 55, 6, 94, 215, 55, 6, 94, 216, 55, 6, 94, 217, 55, 6, 94, 218, 55, 6, 94, 219, 55, 6, 94, 220, 55, 6, 94, 221, 55, 6, 94, 222, 55, 6, 94, 223, 55, 6, 94, 224, 55, 6, 94, 225, 55, 6, 94, 226, 55, 6, 94, 227, 55, 6, 94, 228, 55, 6, 145, 55, 29, -1, 229, 94, 230, 94, 231, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 94, 141, 197, 304, 145, 145, 29, 232, 94, 233, 159, 197, 231, 44, 215, 94, 231, 183, 215, 145, 29, 151, 228, 55, 29, 234, 94, 235, 159, 197, 49, 197, 211, 65, 232, 198, 304, 145, 72, 236, 197, 54, 145, 94, 49, 197, 211, 65, 232, 198, 304, 65, 303, 145, 72, 236, 197, 54, 145, 145, 29, 237, 94, 238, 159, 197, 49, 197, 210, 65, 234, 145, 72, 236, 197, 54, 145, 94, 49, 197, 210, 65, 234, 65, 303, 145, 72, 236, 197, 54, 145, 145, 29, 213, 159, 238, 4, 237, 29, 239, 159, 57, 197, 213, 94, 214, 145, 29, 153, 29, 28, 55, 29, 239, 159, 57, 197, 213, 94, 214, 145, 29, 234, 94, 235, 159, 197, 232, 44, 239, 94, 232, 183, 239, 145, 29, 237, 94, 238, 159, 197, 234, 198, 213, 94, 234, 198, 213, 65, 213, 145, 29, 50, 29, 240, 159, 234, 198, 215, 65, 233, 29, 241, 159, 233, 44, 222, 29, 242, 159, 146, 197, 186, 220, 94, 221, 25, 94, 79, 159, 125, 145, 29, 151, 235, 67, 239, 4, 303, 55, 29, 151, 226, 55, 29, 243, 159, 178, 197, 206, 65, 240, 198, 217, 198, 218, 94, 197, 217, 94, 218, 145, 94, 197, 218, 94, 303, 145, 94, 197, 229, 198, 220, 94, 230, 198, 221, 145, 94, 197, 220, 94, 221, 145, 94, 197, 303, 94, 302, 145, 145, 29, 242, 142, 49, 197, 243, 94, 244, 159, 197, 302, 94, 303, 145, 145, 72, 236, 197, 125, 145, 29, 153, 29, 245, 159, 178, 197, 208, 65, 231, 198, 217, 198, 218, 94, 197, 217, 94, 218, 145, 94, 197, 218, 94, 303, 145, 94, 197, 229, 198, 220, 94, 230, 198, 221, 145, 94, 197, 220, 94, 221, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 245, 94, 242, 72, 236, 197, 245, 72, 79, 72, 99, 145, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 116, 246, 132, 5, 197, 57, 197, 36, 197, 235, 198, 214, 65, 214, 94, 213, 145, 94, 219, 145, 4, 303, 94, 57, 197, 235, 198, 214, 94, 219, 145, 4, 303, 94, 4, 303, 145, 55, 29, 247, 159, 178, 197, 201, 65, 197, 237, 198, 215, 65, 233, 145, 198, 217, 94, 197, 217, 94, 213, 145, 94, 197, 303, 94, 215, 198, 217, 145, 94, 197, 229, 198, 220, 94, 246, 198, 219, 145, 94, 197, 220, 94, 219, 145, 94, 197, 302, 94, 303, 145, 145, 29, 248, 159, 178, 197, 205, 65, 197, 237, 198, 215, 65, 233, 145, 198, 218, 94, 197, 213, 94, 218, 145, 94, 197, 215, 198, 218, 94, 303, 145, 94, 197, 246, 198, 219, 94, 230, 198, 221, 145, 94, 197, 219, 94, 221, 145, 94, 197, 303, 94, 302, 145, 145, 29, 249, 159, 49, 197, 247, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 249, 159, 197, 249, 198, 212, 145, 72, 236, 197, 249, 72, 79, 145, 29, 250, 159, 49, 197, 248, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 251, 159, 36, 197, 246, 198, 219, 65, 219, 94, 213, 145, 4, 303, 29, 151, 223, 55, 29, 252, 159, 202, 65, 197, 237, 65, 246, 198, 219, 65, 66, 197, 302, 94, 219, 145, 145, 198, 216, 65, 241, 29, 253, 159, 49, 197, 202, 65, 197, 237, 65, 251, 145, 198, 216, 65, 241, 145, 29, 254, 159, 49, 197, 252, 94, 255, 159, 246, 198, 219, 65, 66, 197, 302, 94, 219, 145, 1, 213, 94, 256, 159, 302, 145, 29, 249, 159, 197, 249, 198, 169, 197, 254, 145, 186, 164, 94, 55, 25, 145, 72, 236, 197, 249, 72, 79, 145, 29, 242, 22, 169, 197, 253, 145, 29, 153, 29, 151, 224, 55, 29, 257, 159, 178, 197, 203, 65, 197, 237, 198, 216, 65, 241, 145, 198, 217, 94, 197, 217, 94, 213, 145, 94, 197, 303, 94, 216, 198, 217, 145, 94, 197, 229, 198, 220, 94, 246, 198, 219, 145, 94, 197, 220, 94, 219, 145, 94, 197, 302, 94, 303, 145, 145, 29, 258, 159, 203, 65, 197, 237, 65, 251, 145, 198, 216, 198, 217, 65, 241, 198, 217, 65, 229, 198, 220, 65, 66, 197, 302, 94, 220, 145, 29, 259, 159, 49, 197, 257, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 249, 159, 197, 249, 198, 169, 197, 259, 145, 145, 72, 236, 197, 249, 72, 79, 145, 29, 260, 159, 49, 197, 258, 94, 255, 159, 229, 198, 220, 65, 66, 197, 302, 94, 220, 145, 1, 217, 94, 256, 159, 302, 145, 29, 242, 22, 169, 197, 260, 145, 186, 55, 94, 164, 25, 29, 153, 29, 151, 225, 55, 29, 261, 159, 178, 197, 204, 65, 197, 237, 198, 216, 65, 241, 145, 198, 218, 94, 197, 213, 94, 218, 145, 94, 197, 216, 198, 218, 94, 303, 145, 94, 197, 246, 198, 219, 94, 230, 198, 221, 145, 94, 197, 219, 94, 221, 145, 94, 197, 303, 94, 302, 145, 145, 29, 262, 159, 204, 65, 197, 237, 65, 251, 145, 198, 216, 198, 218, 65, 241, 198, 218, 65, 230, 198, 221, 65, 66, 197, 302, 94, 221, 145, 29, 263, 159, 49, 197, 261, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 250, 159, 197, 250, 198, 169, 197, 263, 145, 145, 72, 236, 197, 250, 72, 79, 145, 29, 264, 159, 49, 197, 262, 94, 255, 159, 230, 198, 221, 65, 66, 197, 302, 94, 221, 145, 1, 218, 94, 256, 159, 302, 145, 29, 242, 22, 169, 197, 264, 145, 186, 164, 94, 55, 25, 29, 153, 29, 242, 142, 15, 197, 249, 94, 250, 145, 29, 68, 29, 151, 239, 107, 303, 55, 29, 265, 159, 178, 197, 207, 65, 231, 198, 217, 198, 218, 94, 197, 217, 94, 218, 145, 94, 197, 218, 94, 303, 145, 94, 197, 229, 198, 220, 94, 230, 198, 221, 145, 94, 197, 220, 94, 221, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 265, 94, 242, 72, 236, 197, 265, 72, 79, 72, 99, 145, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 58, 29, 34, 227, 55, 29, 266, 159, 178, 197, 209, 65, 240, 198, 217, 198, 218, 94, 197, 217, 94, 218, 145, 94, 197, 218, 94, 303, 145, 94, 197, 229, 198, 220, 94, 230, 198, 221, 145, 94, 197, 220, 94, 221, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 266, 94, 242, 72, 236, 197, 266, 72, 79, 72, 99, 145, 94, 244, 159, 197, 302, 94, 303, 145, 145, 29, 153, 29, 3, 29]}, {"code": "def chunk_bwd_kernel_dh_reduction(\n    g,\n    gk,\n    gv,\n    dhs,\n    dhr,\n    dh0,\n    cu_seqlens,\n    split_offsets,\n    T,\n    S: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NG: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_GK: tl.constexpr,\n    USE_GV: tl.constexpr,\n    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_hq = i_nh // HQ, i_nh % HQ\n    i_h = i_hq // NG\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NS = tl.cdiv(T, S)\n        boh = tl.load(split_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NS = tl.cdiv(T, S)\n        boh = i_n * NS\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_s in range(NS - 2, -1, -1):\n        p_dhs = tl.make_block_ptr(\n            dhs + ((boh + i_s + 1) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        p_dhr = tl.make_block_ptr(\n            dhr + ((boh + i_s) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        b_dh += tl.load(p_dhs, boundary_check=(0, 1)).to(tl.float32)\n        tl.store(p_dhr, b_dh.to(p_dhr.dtype.element_ty), boundary_check=(0, 1))\n\n        for i_t in range(\n            tl.cdiv(min(i_s * S + S, T), BT) - 1, tl.cdiv(i_s * S, BT) - 1, -1\n        ):\n            last_idx = min(i_t * BT + BT, T) - 1\n\n            if USE_G:\n                b_g_last = tl.load(g + (bos + last_idx) * H + i_h)\n                b_dh *= exp(b_g_last)\n\n            if USE_GK:\n                p_gk_last = (\n                    gk\n                    + (bos + last_idx) * H * K\n                    + i_h * K\n                    + i_k * BK\n                    + tl.arange(0, BK)\n                )\n                b_gk_last = tl.load(\n                    p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0\n                )\n                b_dh *= exp(b_gk_last)[:, None]\n\n            if USE_GV:\n                p_gv_last = (\n                    gv\n                    + (bos + last_idx) * H * V\n                    + i_h * V\n                    + i_v * BV\n                    + tl.arange(0, BV)\n                )\n                b_gv_last = tl.load(\n                    p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0\n                )\n                b_dh *= exp(b_gv_last)[None, :]\n\n    if NS > 1:\n        if STORE_INITIAL_STATE_GRADIENT:\n            p_dhs = tl.make_block_ptr(\n                dhs + (boh * H + i_h) * K * V,\n                (K, V),\n                (V, 1),\n                (i_k * BK, i_v * BV),\n                (BK, BV),\n                (1, 0),\n            )\n            p_dh0 = tl.make_block_ptr(\n                dh0 + i_nh * K * V,\n                (K, V),\n                (V, 1),\n                (i_k * BK, i_v * BV),\n                (BK, BV),\n                (1, 0),\n            )\n            b_dh += tl.load(p_dhs, boundary_check=(0, 1)).to(tl.float32)\n            tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 54, 6, 93, 211, 54, 6, 93, 212, 54, 6, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 93, 218, 54, 6, 93, 219, 54, 6, 93, 220, 54, 6, 93, 221, 54, 6, 93, 222, 54, 6, 93, 223, 54, 6, 145, 54, 29, -1, 224, 93, 225, 93, 226, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 93, 141, 197, 304, 145, 145, 29, 227, 93, 228, 159, 197, 226, 44, 212, 93, 226, 183, 212, 145, 29, 229, 159, 228, 44, 218, 29, 151, 223, 54, 29, 230, 93, 231, 159, 197, 50, 197, 207, 64, 227, 145, 71, 232, 197, 200, 145, 93, 50, 197, 207, 64, 227, 64, 303, 145, 71, 232, 197, 200, 145, 145, 29, 209, 159, 231, 4, 230, 29, 233, 159, 56, 197, 209, 93, 210, 145, 29, 234, 159, 50, 197, 208, 64, 227, 145, 71, 232, 197, 200, 145, 29, 153, 29, 28, 54, 29, 230, 93, 231, 159, 197, 227, 198, 209, 93, 227, 198, 209, 64, 209, 145, 29, 233, 159, 56, 197, 209, 93, 210, 145, 29, 234, 159, 227, 198, 233, 29, 49, 29, 235, 159, 146, 197, 186, 216, 93, 217, 25, 93, 78, 159, 124, 145, 29, 115, 236, 132, 5, 197, 233, 4, 304, 93, 4, 303, 93, 4, 303, 145, 54, 29, 237, 159, 178, 197, 204, 64, 197, 197, 234, 64, 236, 64, 303, 145, 198, 211, 64, 229, 145, 198, 213, 198, 214, 93, 197, 213, 93, 214, 145, 93, 197, 214, 93, 303, 145, 93, 197, 224, 198, 216, 93, 225, 198, 217, 145, 93, 197, 216, 93, 217, 145, 93, 197, 303, 93, 302, 145, 145, 29, 238, 159, 178, 197, 205, 64, 197, 197, 234, 64, 236, 145, 198, 211, 64, 229, 145, 198, 213, 198, 214, 93, 197, 213, 93, 214, 145, 93, 197, 214, 93, 303, 145, 93, 197, 224, 198, 216, 93, 225, 198, 217, 145, 93, 197, 216, 93, 217, 145, 93, 197, 303, 93, 302, 145, 145, 29, 235, 142, 50, 197, 237, 93, 239, 159, 197, 302, 93, 303, 145, 145, 71, 232, 197, 124, 145, 29, 10, 197, 238, 93, 235, 71, 232, 197, 238, 71, 78, 71, 98, 145, 93, 239, 159, 197, 302, 93, 303, 145, 145, 29, 115, 240, 132, 5, 197, 56, 197, 36, 197, 236, 198, 210, 64, 210, 93, 209, 145, 93, 215, 145, 4, 303, 93, 56, 197, 236, 198, 210, 93, 215, 145, 4, 303, 93, 4, 303, 145, 54, 29, 241, 159, 36, 197, 240, 198, 215, 64, 215, 93, 209, 145, 4, 303, 29, 151, 219, 54, 29, 242, 159, 50, 197, 201, 64, 197, 230, 64, 241, 145, 198, 211, 64, 229, 145, 29, 235, 22, 169, 197, 242, 145, 29, 153, 29, 151, 220, 54, 29, 243, 159, 202, 64, 197, 230, 64, 241, 145, 198, 211, 198, 213, 64, 229, 198, 213, 64, 224, 198, 216, 64, 65, 197, 302, 93, 216, 145, 29, 244, 159, 50, 197, 243, 93, 245, 159, 224, 198, 216, 64, 65, 197, 302, 93, 216, 145, 1, 213, 93, 246, 159, 302, 145, 29, 235, 22, 169, 197, 244, 145, 186, 54, 93, 164, 25, 29, 153, 29, 151, 221, 54, 29, 247, 159, 203, 64, 197, 230, 64, 241, 145, 198, 211, 198, 214, 64, 229, 198, 214, 64, 225, 198, 217, 64, 65, 197, 302, 93, 217, 145, 29, 248, 159, 50, 197, 247, 93, 245, 159, 225, 198, 217, 64, 65, 197, 302, 93, 217, 145, 1, 214, 93, 246, 159, 302, 145, 29, 235, 22, 169, 197, 248, 145, 186, 164, 93, 54, 25, 29, 153, 29, 67, 29, 67, 29, 151, 233, 106, 303, 54, 29, 151, 222, 54, 29, 237, 159, 178, 197, 204, 64, 197, 234, 198, 211, 64, 229, 145, 198, 213, 198, 214, 93, 197, 213, 93, 214, 145, 93, 197, 214, 93, 303, 145, 93, 197, 224, 198, 216, 93, 225, 198, 217, 145, 93, 197, 216, 93, 217, 145, 93, 197, 303, 93, 302, 145, 145, 29, 249, 159, 178, 197, 206, 64, 226, 198, 213, 198, 214, 93, 197, 213, 93, 214, 145, 93, 197, 214, 93, 303, 145, 93, 197, 224, 198, 216, 93, 225, 198, 217, 145, 93, 197, 216, 93, 217, 145, 93, 197, 303, 93, 302, 145, 145, 29, 235, 142, 50, 197, 237, 93, 239, 159, 197, 302, 93, 303, 145, 145, 71, 232, 197, 124, 145, 29, 10, 197, 249, 93, 235, 71, 232, 197, 249, 71, 78, 71, 98, 145, 93, 239, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 153, 29, 3, 29]}, {"code": "def chunk_fwd_kernel_o(\n    q,\n    k,\n    v,\n    h,\n    g,\n    o,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    q += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V\n    o += (bos * H + i_h) * V\n    h += (i_tg * H + i_h).to(tl.int64) * K * V\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(\n            q, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_k = tl.make_block_ptr(\n            k, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)\n        )\n        p_h = tl.make_block_ptr(\n            h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n\n        b_o += tl.dot(b_q, b_h)\n\n        b_A += tl.dot(b_q, b_k)\n\n    if USE_G:\n        g += bos * H + i_h\n        p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))\n        b_g = tl.load(p_g, boundary_check=(0,))\n        b_o = b_o * exp(b_g)[:, None]\n        b_A = b_A * safe_exp(b_g[:, None] - b_g[None, :])\n\n    o_i = tl.arange(0, BT)\n    m_A = o_i[:, None] >= o_i[None, :]\n    b_A = tl.where(m_A, b_A, 0)\n\n    p_v = tl.make_block_ptr(\n        v, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_o = tl.make_block_ptr(\n        o, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n\n    b_o = b_o * scale + tl.dot(b_A.to(b_v.dtype), b_v) * scale\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 94, 209, 94, 210, 94, 211, 55, 6, 94, 212, 55, 6, 94, 213, 55, 6, 94, 214, 55, 6, 94, 215, 55, 6, 94, 216, 55, 6, 94, 217, 55, 6, 94, 218, 55, 6, 145, 55, 29, -1, 219, 94, 220, 94, 221, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 94, 141, 197, 304, 145, 145, 29, 222, 94, 223, 159, 197, 221, 44, 211, 94, 221, 183, 211, 145, 29, 151, 218, 55, 29, 224, 159, 220, 29, 225, 94, 220, 159, 197, 49, 197, 208, 65, 220, 198, 304, 145, 72, 226, 197, 54, 145, 94, 49, 197, 208, 65, 220, 198, 304, 65, 303, 145, 72, 226, 197, 54, 145, 145, 29, 227, 94, 228, 159, 197, 49, 197, 207, 65, 225, 145, 72, 226, 197, 54, 145, 94, 49, 197, 207, 65, 225, 65, 303, 145, 72, 226, 197, 54, 145, 145, 29, 210, 159, 228, 4, 227, 29, 229, 159, 57, 197, 210, 94, 214, 145, 29, 153, 29, 28, 55, 29, 229, 159, 57, 197, 210, 94, 214, 145, 29, 224, 159, 222, 198, 229, 65, 220, 29, 227, 94, 228, 159, 197, 222, 198, 210, 94, 222, 198, 210, 65, 210, 145, 29, 50, 29, 201, 142, 197, 227, 198, 211, 65, 223, 145, 198, 212, 29, 202, 142, 197, 227, 198, 211, 65, 223, 145, 198, 212, 29, 203, 142, 197, 227, 198, 211, 65, 223, 145, 198, 213, 29, 206, 142, 197, 227, 198, 211, 65, 223, 145, 198, 213, 29, 204, 142, 197, 224, 198, 211, 65, 223, 145, 72, 226, 197, 150, 145, 198, 212, 198, 213, 29, 230, 159, 146, 197, 186, 214, 94, 216, 25, 94, 79, 159, 125, 145, 29, 231, 159, 146, 197, 186, 214, 94, 214, 25, 94, 79, 159, 125, 145, 29, 116, 232, 132, 5, 197, 57, 197, 212, 94, 215, 145, 145, 55, 29, 233, 159, 178, 197, 201, 94, 197, 210, 94, 212, 145, 94, 197, 211, 198, 212, 94, 303, 145, 94, 197, 220, 198, 214, 94, 232, 198, 215, 145, 94, 197, 214, 94, 215, 145, 94, 197, 303, 94, 302, 145, 145, 29, 234, 159, 178, 197, 202, 94, 197, 212, 94, 210, 145, 94, 197, 303, 94, 211, 198, 212, 145, 94, 197, 232, 198, 215, 94, 220, 198, 214, 145, 94, 197, 215, 94, 214, 145, 94, 197, 302, 94, 303, 145, 145, 29, 235, 159, 178, 197, 204, 94, 197, 212, 94, 213, 145, 94, 197, 213, 94, 303, 145, 94, 197, 232, 198, 215, 94, 219, 198, 216, 145, 94, 197, 215, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 236, 159, 49, 197, 233, 94, 237, 159, 197, 302, 94, 303, 145, 145, 29, 238, 159, 49, 197, 234, 94, 237, 159, 197, 302, 94, 303, 145, 145, 29, 239, 159, 49, 197, 235, 94, 237, 159, 197, 302, 94, 303, 145, 145, 29, 230, 142, 15, 197, 236, 94, 239, 145, 29, 231, 142, 15, 197, 236, 94, 238, 145, 29, 68, 29, 151, 217, 55, 29, 205, 142, 227, 198, 211, 65, 223, 29, 240, 159, 178, 197, 205, 94, 197, 210, 94, 145, 94, 197, 211, 94, 145, 94, 197, 220, 198, 214, 94, 145, 94, 197, 214, 94, 145, 94, 197, 302, 94, 145, 145, 29, 241, 159, 49, 197, 240, 94, 237, 159, 197, 302, 94, 145, 145, 29, 230, 159, 230, 198, 169, 197, 241, 145, 186, 55, 94, 164, 25, 29, 231, 159, 231, 198, 242, 197, 241, 186, 55, 94, 164, 25, 4, 241, 186, 164, 94, 55, 25, 145, 29, 153, 29, 243, 159, 66, 197, 302, 94, 214, 145, 29, 244, 159, 243, 186, 55, 94, 164, 25, 124, 243, 186, 164, 94, 55, 25, 29, 231, 159, 167, 197, 244, 94, 231, 94, 302, 145, 29, 245, 159, 178, 197, 203, 94, 197, 210, 94, 213, 145, 94, 197, 211, 198, 213, 94, 303, 145, 94, 197, 220, 198, 214, 94, 219, 198, 216, 145, 94, 197, 214, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 246, 159, 178, 197, 206, 94, 197, 210, 94, 213, 145, 94, 197, 211, 198, 213, 94, 303, 145, 94, 197, 220, 198, 214, 94, 219, 198, 216, 145, 94, 197, 214, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 247, 159, 49, 197, 245, 94, 237, 159, 197, 302, 94, 303, 145, 145, 29, 230, 159, 230, 198, 209, 65, 15, 197, 231, 72, 226, 197, 247, 72, 79, 145, 94, 247, 145, 198, 209, 29, 10, 197, 246, 94, 230, 72, 226, 197, 246, 72, 79, 72, 99, 145, 94, 237, 159, 197, 302, 94, 303, 145, 145, 29, 3, 29]}, {"code": "def chunk_bwd_kernel_dqkwg(\n    q,\n    k,\n    v,\n    h,\n    g,\n    do,\n    dh,\n    dq,\n    dk,\n    dg,\n    w,\n    dv,\n    dw,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    B: tl.constexpr,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_DW: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_G:\n        dg += i_k * B * H * T\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    v += (bos * H + i_h) * V\n    do += (bos * H + i_h) * V\n    h += (i_tg * H + i_h).to(tl.int64) * K * V\n    dh += (i_tg * H + i_h).to(tl.int64) * K * V\n    q += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    dq += (bos * H + i_h) * K\n    dk += (bos * H + i_h) * K\n\n    if USE_DW:\n        dw += (bos * H + i_h) * K\n        dv += (bos * H + i_h) * V\n        w += (bos * H + i_h) * K\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    b_dg_last = (\n        tl.zeros(\n            [\n                1,\n            ],\n            dtype=tl.float32,\n        )\n        if USE_G\n        else None\n    )\n    b_dw = tl.zeros([BT, BK], dtype=tl.float32) if USE_DW else None\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_do = tl.make_block_ptr(\n            do, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_h = tl.make_block_ptr(\n            h, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)\n        )\n        p_dh = tl.make_block_ptr(\n            dh, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)\n        )\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        if USE_G:\n            b_dg_last += tl.sum(b_h * b_dh)\n\n        b_ds += tl.dot(b_do, tl.trans(b_v))\n\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n\n        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\n        if USE_DW:\n            p_dv = tl.make_block_ptr(\n                dv, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n            )\n            b_dv = tl.load(p_dv, boundary_check=(0, 1))\n            b_dw += tl.dot(b_dv.to(b_v.dtype), b_h.to(b_v.dtype))\n\n    if USE_DW:\n        p_dw = tl.make_block_ptr(\n            dw, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        tl.store(p_dw, -b_dw.to(p_dw.dtype.element_ty), boundary_check=(0, 1))\n\n    tl.debug_barrier()\n    o_i = tl.arange(0, BT)\n    p_q = tl.make_block_ptr(\n        q, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_k = tl.make_block_ptr(\n        k, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n\n    p_dq = tl.make_block_ptr(\n        dq, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_dk = tl.make_block_ptr(\n        dk, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n\n    if USE_G:\n        b_dg = tl.zeros(\n            [\n                BT,\n            ],\n            dtype=tl.float32,\n        )\n        g += bos * H + i_h\n        dg += bos * H + i_h\n        p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))\n        b_g = tl.load(p_g, boundary_check=(0,))\n        b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * H)\n        b_dg_last *= exp(b_g_last)\n\n        b_dq = b_dq * exp(b_g)[:, None] * scale\n        b_dg += tl.sum(b_dq * b_q, axis=1)\n\n        b_dk = b_dk * safe_exp(-b_g + b_g_last)[:, None]\n        b_dg -= tl.sum(b_k * b_dk, axis=1)\n        b_dg_last += tl.sum(b_dk * b_k)\n\n        b_ds = (\n            tl.where(\n                o_i[:, None] >= o_i[None, :],\n                b_ds * safe_exp(b_g[:, None] - b_g[None, :]),\n                0,\n            )\n            * scale\n        )\n        b_ds2 = b_ds * tl.dot(b_q, tl.trans(b_k))\n        b_dg += tl.sum(b_ds2, axis=1)\n        b_dg -= tl.sum(b_ds2, axis=0)\n\n        b_ds = b_ds.to(b_k.dtype)\n\n        b_dq += tl.dot(b_ds, b_k)\n        b_dk += tl.dot(tl.trans(b_ds), b_q)\n        p_dg = tl.make_block_ptr(dg, (T,), (H,), (i_t * BT,), (BT,), (0,))\n\n        b_dg = tl.where(o_i < min(BT, T - i_t * BT) - 1, b_dg, b_dg + b_dg_last)\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))\n    else:\n        b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds, 0)\n        b_ds = b_ds.to(b_k.dtype)\n        b_dq += tl.dot(b_ds, b_k)\n        b_dk += tl.dot(tl.trans(b_ds), b_q) * scale\n        b_dq *= scale\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 93, 211, 93, 212, 93, 213, 93, 214, 93, 215, 93, 216, 93, 217, 54, 6, 93, 218, 93, 219, 54, 6, 93, 220, 54, 6, 93, 221, 54, 6, 93, 222, 54, 6, 93, 223, 54, 6, 93, 224, 54, 6, 93, 225, 54, 6, 93, 226, 54, 6, 93, 227, 54, 6, 145, 54, 29, -1, 228, 93, 229, 93, 230, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 93, 141, 197, 304, 145, 145, 29, 231, 93, 232, 159, 197, 230, 44, 219, 93, 230, 183, 219, 145, 29, 151, 225, 54, 29, 210, 142, 228, 198, 217, 198, 219, 198, 218, 29, 153, 29, 151, 227, 54, 29, 233, 159, 229, 29, 234, 93, 229, 159, 197, 50, 197, 215, 64, 229, 198, 304, 145, 71, 235, 197, 200, 145, 93, 50, 197, 215, 64, 229, 198, 304, 64, 303, 145, 71, 235, 197, 200, 145, 145, 29, 236, 93, 237, 159, 197, 50, 197, 214, 64, 234, 145, 71, 235, 197, 200, 145, 93, 50, 197, 214, 64, 234, 64, 303, 145, 71, 235, 197, 200, 145, 145, 29, 218, 159, 237, 4, 236, 29, 238, 159, 56, 197, 218, 93, 222, 145, 29, 153, 29, 28, 54, 29, 238, 159, 56, 197, 218, 93, 222, 145, 29, 233, 159, 231, 198, 238, 64, 229, 29, 236, 93, 237, 159, 197, 231, 198, 218, 93, 231, 198, 218, 64, 218, 145, 29, 49, 29, 203, 142, 197, 236, 198, 219, 64, 232, 145, 198, 221, 29, 206, 142, 197, 236, 198, 219, 64, 232, 145, 198, 221, 29, 204, 142, 197, 233, 198, 219, 64, 232, 145, 71, 235, 197, 150, 145, 198, 220, 198, 221, 29, 207, 142, 197, 233, 198, 219, 64, 232, 145, 71, 235, 197, 150, 145, 198, 220, 198, 221, 29, 201, 142, 197, 236, 198, 219, 64, 232, 145, 198, 220, 29, 202, 142, 197, 236, 198, 219, 64, 232, 145, 198, 220, 29, 208, 142, 197, 236, 198, 219, 64, 232, 145, 198, 220, 29, 209, 142, 197, 236, 198, 219, 64, 232, 145, 198, 220, 29, 151, 226, 54, 29, 213, 142, 197, 236, 198, 219, 64, 232, 145, 198, 220, 29, 212, 142, 197, 236, 198, 219, 64, 232, 145, 198, 221, 29, 211, 142, 197, 236, 198, 219, 64, 232, 145, 198, 220, 29, 153, 29, 239, 159, 146, 197, 186, 222, 93, 223, 25, 93, 78, 159, 124, 145, 29, 240, 159, 146, 197, 186, 222, 93, 223, 25, 93, 78, 159, 124, 145, 29, 241, 159, 146, 197, 186, 222, 93, 222, 25, 93, 78, 159, 124, 145, 29, 242, 159, 146, 197, 186, 303, 25, 93, 78, 159, 124, 145, 151, 225, 28, 164, 29, 243, 159, 146, 197, 186, 222, 93, 223, 25, 93, 78, 159, 124, 145, 151, 226, 28, 164, 29, 115, 244, 132, 5, 197, 56, 197, 221, 93, 224, 145, 145, 54, 29, 245, 159, 178, 197, 203, 93, 197, 218, 93, 221, 145, 93, 197, 219, 198, 221, 93, 303, 145, 93, 197, 229, 198, 222, 93, 244, 198, 224, 145, 93, 197, 222, 93, 224, 145, 93, 197, 303, 93, 302, 145, 145, 29, 246, 159, 178, 197, 206, 93, 197, 218, 93, 221, 145, 93, 197, 219, 198, 221, 93, 303, 145, 93, 197, 229, 198, 222, 93, 244, 198, 224, 145, 93, 197, 222, 93, 224, 145, 93, 197, 303, 93, 302, 145, 145, 29, 247, 159, 178, 197, 204, 93, 197, 221, 93, 220, 145, 93, 197, 303, 93, 221, 145, 93, 197, 244, 198, 224, 93, 228, 198, 223, 145, 93, 197, 224, 93, 223, 145, 93, 197, 302, 93, 303, 145, 145, 29, 248, 159, 178, 197, 207, 93, 197, 221, 93, 220, 145, 93, 197, 303, 93, 221, 145, 93, 197, 244, 198, 224, 93, 228, 198, 223, 145, 93, 197, 224, 93, 223, 145, 93, 197, 302, 93, 303, 145, 145, 29, 249, 159, 50, 197, 245, 93, 250, 159, 197, 302, 93, 303, 145, 145, 29, 251, 159, 50, 197, 246, 93, 250, 159, 197, 302, 93, 303, 145, 145, 29, 252, 159, 50, 197, 247, 93, 250, 159, 197, 302, 93, 303, 145, 145, 29, 253, 159, 50, 197, 248, 93, 250, 159, 197, 302, 93, 303, 145, 145, 29, 151, 225, 54, 29, 242, 142, 180, 197, 252, 198, 253, 145, 29, 153, 29, 241, 142, 15, 197, 251, 93, 62, 197, 249, 145, 145, 29, 239, 142, 15, 197, 251, 93, 252, 71, 235, 197, 251, 71, 78, 145, 145, 29, 240, 142, 15, 197, 249, 93, 253, 71, 235, 197, 249, 71, 78, 145, 145, 29, 151, 226, 54, 29, 254, 159, 178, 197, 212, 93, 197, 218, 93, 221, 145, 93, 197, 219, 198, 221, 93, 303, 145, 93, 197, 229, 198, 222, 93, 244, 198, 224, 145, 93, 197, 222, 93, 224, 145, 93, 197, 303, 93, 302, 145, 145, 29, 255, 159, 50, 197, 254, 93, 250, 159, 197, 302, 93, 303, 145, 145, 29, 243, 142, 15, 197, 255, 71, 235, 197, 249, 71, 78, 145, 93, 252, 71, 235, 197, 249, 71, 78, 145, 145, 29, 153, 29, 67, 29, 151, 226, 54, 29, 256, 159, 178, 197, 213, 93, 197, 218, 93, 220, 145, 93, 197, 219, 198, 220, 93, 303, 145, 93, 197, 229, 198, 222, 93, 228, 198, 223, 145, 93, 197, 222, 93, 223, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 256, 93, 4, 243, 71, 235, 197, 256, 71, 78, 71, 98, 145, 93, 250, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 46, 197, 145, 29, 257, 159, 65, 197, 302, 93, 222, 145, 29, 258, 159, 178, 197, 201, 93, 197, 218, 93, 220, 145, 93, 197, 219, 198, 220, 93, 303, 145, 93, 197, 229, 198, 222, 93, 228, 198, 223, 145, 93, 197, 222, 93, 223, 145, 93, 197, 303, 93, 302, 145, 145, 29, 259, 159, 178, 197, 202, 93, 197, 218, 93, 220, 145, 93, 197, 219, 198, 220, 93, 303, 145, 93, 197, 229, 198, 222, 93, 228, 198, 223, 145, 93, 197, 222, 93, 223, 145, 93, 197, 303, 93, 302, 145, 145, 29, 260, 159, 50, 197, 258, 93, 250, 159, 197, 302, 93, 303, 145, 145, 29, 261, 159, 50, 197, 259, 93, 250, 159, 197, 302, 93, 303, 145, 145, 29, 262, 159, 178, 197, 208, 93, 197, 218, 93, 220, 145, 93, 197, 219, 198, 220, 93, 303, 145, 93, 197, 229, 198, 222, 93, 228, 198, 223, 145, 93, 197, 222, 93, 223, 145, 93, 197, 303, 93, 302, 145, 145, 29, 263, 159, 178, 197, 209, 93, 197, 218, 93, 220, 145, 93, 197, 219, 198, 220, 93, 303, 145, 93, 197, 229, 198, 222, 93, 228, 198, 223, 145, 93, 197, 222, 93, 223, 145, 93, 197, 303, 93, 302, 145, 145, 29, 151, 225, 54, 29, 264, 159, 146, 197, 186, 222, 25, 93, 78, 159, 124, 145, 29, 205, 142, 236, 198, 219, 64, 232, 29, 210, 142, 236, 198, 219, 64, 232, 29, 265, 159, 178, 197, 205, 93, 197, 218, 93, 145, 93, 197, 219, 93, 145, 93, 197, 229, 198, 222, 93, 145, 93, 197, 222, 93, 145, 93, 197, 302, 93, 145, 145, 29, 266, 159, 50, 197, 265, 93, 250, 159, 197, 302, 93, 145, 145, 29, 267, 159, 50, 197, 205, 64, 197, 36, 197, 229, 198, 222, 64, 222, 93, 218, 145, 4, 303, 145, 198, 219, 145, 29, 242, 22, 169, 197, 267, 145, 29, 239, 159, 239, 198, 169, 197, 266, 145, 186, 54, 93, 164, 25, 198, 216, 29, 264, 142, 180, 197, 239, 198, 260, 93, 268, 159, 303, 145, 29, 240, 159, 240, 198, 269, 197, 4, 266, 64, 267, 145, 186, 54, 93, 164, 25, 29, 264, 2, 180, 197, 261, 198, 240, 93, 268, 159, 303, 145, 29, 242, 142, 180, 197, 240, 198, 261, 145, 29, 241, 159, 167, 197, 257, 186, 54, 93, 164, 25, 123, 257, 186, 164, 93, 54, 25, 93, 241, 198, 269, 197, 266, 186, 54, 93, 164, 25, 4, 266, 186, 164, 93, 54, 25, 145, 93, 302, 145, 198, 216, 29, 270, 159, 241, 198, 15, 197, 260, 93, 62, 197, 261, 145, 145, 29, 264, 142, 180, 197, 270, 93, 268, 159, 303, 145, 29, 264, 2, 180, 197, 270, 93, 268, 159, 302, 145, 29, 241, 159, 241, 71, 235, 197, 261, 71, 78, 145, 29, 239, 142, 15, 197, 241, 93, 261, 145, 29, 240, 142, 15, 197, 62, 197, 241, 145, 93, 260, 145, 29, 271, 159, 178, 197, 210, 93, 197, 218, 93, 145, 93, 197, 219, 93, 145, 93, 197, 229, 198, 222, 93, 145, 93, 197, 222, 93, 145, 93, 197, 302, 93, 145, 145, 29, 264, 159, 167, 197, 257, 1, 36, 197, 222, 93, 218, 4, 229, 198, 222, 145, 4, 303, 93, 264, 93, 264, 64, 242, 145, 29, 10, 197, 262, 93, 239, 71, 235, 197, 262, 71, 78, 71, 98, 145, 93, 250, 159, 197, 302, 93, 303, 145, 145, 29, 10, 197, 263, 93, 240, 71, 235, 197, 263, 71, 78, 71, 98, 145, 93, 250, 159, 197, 302, 93, 303, 145, 145, 29, 10, 197, 271, 93, 264, 71, 235, 197, 271, 71, 78, 71, 98, 145, 93, 250, 159, 197, 302, 93, 145, 145, 29, 153, 29, 28, 54, 29, 241, 159, 167, 197, 257, 186, 54, 93, 164, 25, 123, 257, 186, 164, 93, 54, 25, 93, 241, 93, 302, 145, 29, 241, 159, 241, 71, 235, 197, 261, 71, 78, 145, 29, 239, 142, 15, 197, 241, 93, 261, 145, 29, 240, 142, 15, 197, 62, 197, 241, 145, 93, 260, 145, 198, 216, 29, 239, 22, 216, 29, 10, 197, 262, 93, 239, 71, 235, 197, 262, 71, 78, 71, 98, 145, 93, 250, 159, 197, 302, 93, 303, 145, 145, 29, 10, 197, 263, 93, 240, 71, 235, 197, 263, 71, 78, 71, 98, 145, 93, 250, 159, 197, 302, 93, 303, 145, 145, 29, 49, 29, 3, 29]}, {"code": "def chunk_bwd_kernel_dv(\n    q,\n    k,\n    g,\n    do,\n    dv,\n    dh,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n\n    q += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    do += (bos * H + i_h) * V\n    dv += (bos * H + i_h) * V\n    dh += (i_tg * H + i_h).to(tl.int64) * K * V\n\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(\n            k, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_q = tl.make_block_ptr(\n            q, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)\n        )\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_A += tl.dot(b_k, b_q)\n        p_dh = tl.make_block_ptr(\n            dh, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dv += tl.dot(b_k, b_dh.to(b_k.dtype))\n\n    if USE_G:\n        g += bos * H + i_h\n        p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))\n        b_g = tl.load(p_g, boundary_check=(0,))\n        b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * H)\n        b_dv *= safe_exp(-b_g + b_g_last)[:, None]\n\n    mask = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]\n    if USE_G:\n        b_A = tl.where(mask, b_A * safe_exp(b_g[None, :] - b_g[:, None]) * scale, 0).to(\n            do.dtype.element_ty\n        )\n    else:\n        b_A = tl.where(mask, b_A * scale, 0).to(do.dtype.element_ty)\n    p_do = tl.make_block_ptr(\n        do, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_dv = tl.make_block_ptr(\n        dv, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv += tl.dot(b_A.to(b_do.dtype), b_do)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 94, 209, 94, 210, 94, 211, 55, 6, 94, 212, 55, 6, 94, 213, 55, 6, 94, 214, 55, 6, 94, 215, 55, 6, 94, 216, 55, 6, 94, 217, 55, 6, 94, 218, 55, 6, 145, 55, 29, -1, 219, 94, 220, 94, 221, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 94, 141, 197, 304, 145, 145, 29, 222, 94, 223, 159, 197, 221, 44, 211, 94, 221, 183, 211, 145, 29, 151, 218, 55, 29, 224, 159, 220, 29, 225, 94, 220, 159, 197, 49, 197, 208, 65, 220, 198, 304, 145, 72, 226, 197, 54, 145, 94, 49, 197, 208, 65, 220, 198, 304, 65, 303, 145, 72, 226, 197, 54, 145, 145, 29, 227, 94, 228, 159, 197, 49, 197, 207, 65, 225, 145, 72, 226, 197, 54, 145, 94, 49, 197, 207, 65, 225, 65, 303, 145, 72, 226, 197, 54, 145, 145, 29, 210, 159, 228, 4, 227, 29, 229, 159, 57, 197, 210, 94, 214, 145, 29, 153, 29, 28, 55, 29, 229, 159, 57, 197, 210, 94, 214, 145, 29, 224, 159, 222, 198, 229, 65, 220, 29, 227, 94, 228, 159, 197, 222, 198, 210, 94, 222, 198, 210, 65, 210, 145, 29, 50, 29, 230, 159, 146, 197, 186, 214, 94, 216, 25, 94, 79, 159, 125, 145, 29, 201, 142, 197, 227, 198, 211, 65, 223, 145, 198, 212, 29, 202, 142, 197, 227, 198, 211, 65, 223, 145, 198, 212, 29, 204, 142, 197, 227, 198, 211, 65, 223, 145, 198, 213, 29, 205, 142, 197, 227, 198, 211, 65, 223, 145, 198, 213, 29, 206, 142, 197, 224, 198, 211, 65, 223, 145, 72, 226, 197, 150, 145, 198, 212, 198, 213, 29, 231, 159, 146, 197, 186, 214, 94, 214, 25, 94, 79, 159, 125, 145, 29, 116, 232, 132, 5, 197, 57, 197, 212, 94, 215, 145, 145, 55, 29, 233, 159, 178, 197, 202, 94, 197, 210, 94, 212, 145, 94, 197, 211, 198, 212, 94, 303, 145, 94, 197, 220, 198, 214, 94, 232, 198, 215, 145, 94, 197, 214, 94, 215, 145, 94, 197, 303, 94, 302, 145, 145, 29, 234, 159, 178, 197, 201, 94, 197, 212, 94, 210, 145, 94, 197, 303, 94, 211, 198, 212, 145, 94, 197, 232, 198, 215, 94, 220, 198, 214, 145, 94, 197, 215, 94, 214, 145, 94, 197, 302, 94, 303, 145, 145, 29, 235, 159, 49, 197, 234, 94, 236, 159, 197, 302, 94, 303, 145, 145, 29, 237, 159, 49, 197, 233, 94, 236, 159, 197, 302, 94, 303, 145, 145, 29, 231, 142, 15, 197, 237, 94, 235, 145, 29, 238, 159, 178, 197, 206, 94, 197, 212, 94, 213, 145, 94, 197, 213, 94, 303, 145, 94, 197, 232, 198, 215, 94, 219, 198, 216, 145, 94, 197, 215, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 239, 159, 49, 197, 238, 94, 236, 159, 197, 302, 94, 303, 145, 145, 29, 230, 142, 15, 197, 237, 94, 239, 72, 226, 197, 237, 72, 79, 145, 145, 29, 68, 29, 151, 217, 55, 29, 203, 142, 227, 198, 211, 65, 223, 29, 240, 159, 178, 197, 203, 94, 197, 210, 94, 145, 94, 197, 211, 94, 145, 94, 197, 220, 198, 214, 94, 145, 94, 197, 214, 94, 145, 94, 197, 302, 94, 145, 145, 29, 241, 159, 49, 197, 240, 94, 236, 159, 197, 302, 94, 145, 145, 29, 242, 159, 49, 197, 203, 65, 197, 36, 197, 220, 198, 214, 65, 214, 94, 210, 145, 4, 303, 145, 198, 211, 145, 29, 230, 22, 243, 197, 4, 241, 65, 242, 145, 186, 55, 94, 164, 25, 29, 153, 29, 244, 159, 66, 197, 302, 94, 214, 145, 186, 55, 94, 164, 25, 177, 66, 197, 302, 94, 214, 145, 186, 164, 94, 55, 25, 29, 151, 217, 55, 29, 231, 159, 167, 197, 244, 94, 231, 198, 243, 197, 241, 186, 164, 94, 55, 25, 4, 241, 186, 55, 94, 164, 25, 145, 198, 209, 94, 302, 145, 72, 226, 197, 204, 72, 79, 72, 99, 145, 29, 153, 29, 28, 55, 29, 231, 159, 167, 197, 244, 94, 231, 198, 209, 94, 302, 145, 72, 226, 197, 204, 72, 79, 72, 99, 145, 29, 50, 29, 245, 159, 178, 197, 204, 94, 197, 210, 94, 213, 145, 94, 197, 211, 198, 213, 94, 303, 145, 94, 197, 220, 198, 214, 94, 219, 198, 216, 145, 94, 197, 214, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 246, 159, 178, 197, 205, 94, 197, 210, 94, 213, 145, 94, 197, 211, 198, 213, 94, 303, 145, 94, 197, 220, 198, 214, 94, 219, 198, 216, 145, 94, 197, 214, 94, 216, 145, 94, 197, 303, 94, 302, 145, 145, 29, 247, 159, 49, 197, 245, 94, 236, 159, 197, 302, 94, 303, 145, 145, 29, 230, 142, 15, 197, 231, 72, 226, 197, 247, 72, 79, 145, 94, 247, 145, 29, 10, 197, 246, 94, 230, 72, 226, 197, 246, 72, 79, 72, 99, 145, 94, 236, 159, 197, 302, 94, 303, 145, 145, 29, 3, 29]}, {"code": "def chunk_bwd_kernel_dv_local(\n    q,\n    k,\n    g,\n    do,\n    dv,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    q += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    do += (bos * H + i_h) * V\n    dv += (bos * H + i_h) * V\n\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(\n            k, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_q = tl.make_block_ptr(\n            q, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)\n        )\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_A += tl.dot(b_k, b_q)\n\n    if USE_G:\n        g += bos * H + i_h\n        p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))\n        b_g = tl.load(p_g, boundary_check=(0,))\n\n    mask = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]\n    if USE_G:\n        b_A = tl.where(mask, b_A * safe_exp(b_g[None, :] - b_g[:, None]) * scale, 0).to(\n            do.dtype.element_ty\n        )\n    else:\n        b_A = tl.where(mask, b_A * scale, 0).to(do.dtype.element_ty)\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_do = tl.make_block_ptr(\n            do, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_dv = tl.make_block_ptr(\n            dv, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dv = tl.dot(b_A.to(b_do.dtype), b_do)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 54, 6, 93, 211, 54, 6, 93, 212, 54, 6, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 145, 54, 29, -1, 218, 93, 219, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 145, 29, 220, 93, 221, 159, 197, 219, 44, 210, 93, 219, 183, 210, 145, 29, 151, 217, 54, 29, 222, 93, 218, 159, 197, 50, 197, 207, 64, 218, 198, 304, 145, 71, 223, 197, 200, 145, 93, 50, 197, 207, 64, 218, 198, 304, 64, 303, 145, 71, 223, 197, 200, 145, 145, 29, 224, 93, 225, 159, 197, 50, 197, 206, 64, 222, 145, 71, 223, 197, 200, 145, 93, 50, 197, 206, 64, 222, 64, 303, 145, 71, 223, 197, 200, 145, 145, 29, 209, 159, 225, 4, 224, 29, 153, 29, 28, 54, 29, 224, 93, 225, 159, 197, 220, 198, 209, 93, 220, 198, 209, 64, 209, 145, 29, 49, 29, 201, 142, 197, 224, 198, 210, 64, 221, 145, 198, 211, 29, 202, 142, 197, 224, 198, 210, 64, 221, 145, 198, 211, 29, 204, 142, 197, 224, 198, 210, 64, 221, 145, 198, 212, 29, 205, 142, 197, 224, 198, 210, 64, 221, 145, 198, 212, 29, 226, 159, 146, 197, 186, 213, 93, 213, 25, 93, 78, 159, 124, 145, 29, 115, 227, 132, 5, 197, 56, 197, 211, 93, 214, 145, 145, 54, 29, 228, 159, 178, 197, 202, 93, 197, 209, 93, 211, 145, 93, 197, 210, 198, 211, 93, 303, 145, 93, 197, 218, 198, 213, 93, 227, 198, 214, 145, 93, 197, 213, 93, 214, 145, 93, 197, 303, 93, 302, 145, 145, 29, 229, 159, 178, 197, 201, 93, 197, 211, 93, 209, 145, 93, 197, 303, 93, 210, 198, 211, 145, 93, 197, 227, 198, 214, 93, 218, 198, 213, 145, 93, 197, 214, 93, 213, 145, 93, 197, 302, 93, 303, 145, 145, 29, 230, 159, 50, 197, 229, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 232, 159, 50, 197, 228, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 226, 142, 15, 197, 232, 93, 230, 145, 29, 67, 29, 151, 216, 54, 29, 203, 142, 224, 198, 210, 64, 221, 29, 233, 159, 178, 197, 203, 93, 197, 209, 93, 145, 93, 197, 210, 93, 145, 93, 197, 218, 198, 213, 93, 145, 93, 197, 213, 93, 145, 93, 197, 302, 93, 145, 145, 29, 234, 159, 50, 197, 233, 93, 231, 159, 197, 302, 93, 145, 145, 29, 153, 29, 235, 159, 65, 197, 302, 93, 213, 145, 186, 54, 93, 164, 25, 177, 65, 197, 302, 93, 213, 145, 186, 164, 93, 54, 25, 29, 151, 216, 54, 29, 226, 159, 167, 197, 235, 93, 226, 198, 236, 197, 234, 186, 164, 93, 54, 25, 4, 234, 186, 54, 93, 164, 25, 145, 198, 208, 93, 302, 145, 71, 223, 197, 204, 71, 78, 71, 98, 145, 29, 153, 29, 28, 54, 29, 226, 159, 167, 197, 235, 93, 226, 198, 208, 93, 302, 145, 71, 223, 197, 204, 71, 78, 71, 98, 145, 29, 49, 29, 115, 237, 132, 5, 197, 56, 197, 212, 93, 215, 145, 145, 54, 29, 238, 159, 178, 197, 204, 93, 197, 209, 93, 212, 145, 93, 197, 210, 198, 212, 93, 303, 145, 93, 197, 218, 198, 213, 93, 237, 198, 215, 145, 93, 197, 213, 93, 215, 145, 93, 197, 303, 93, 302, 145, 145, 29, 239, 159, 178, 197, 205, 93, 197, 209, 93, 212, 145, 93, 197, 210, 198, 212, 93, 303, 145, 93, 197, 218, 198, 213, 93, 237, 198, 215, 145, 93, 197, 213, 93, 215, 145, 93, 197, 303, 93, 302, 145, 145, 29, 240, 159, 50, 197, 238, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 241, 159, 15, 197, 226, 71, 223, 197, 240, 71, 78, 145, 93, 240, 145, 29, 10, 197, 239, 93, 241, 71, 223, 197, 239, 71, 78, 71, 98, 145, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 67, 29, 3, 29]}, {"code": "def chunk_scaled_dot_kkt_fwd_kernel(\n    k,\n    beta,\n    g_cumsum,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    USE_G: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n    o_t = tl.arange(0, BT)\n\n    p_beta = tl.make_block_ptr(\n        beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)\n    )\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = b_k * b_beta[:, None]\n        b_A += tl.dot(b_kb.to(b_k.dtype), tl.trans(b_k))\n\n    if USE_G:\n        p_g = tl.make_block_ptr(\n            g_cumsum + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)\n        )\n        b_g = tl.load(p_g, boundary_check=(0,))\n        b_g_diff = b_g[:, None] - b_g[None, :]\n        b_A = b_A * safe_exp(b_g_diff)\n\n    b_A = tl.where(o_t[:, None] > o_t[None, :], b_A, 0)\n    p_A = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 55, 6, 94, 209, 55, 6, 94, 210, 55, 6, 94, 211, 55, 6, 94, 212, 55, 6, 94, 213, 55, 6, 145, 55, 29, -1, 214, 94, 215, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 145, 29, 216, 94, 217, 159, 197, 215, 44, 208, 94, 215, 183, 208, 145, 29, 151, 212, 55, 29, 218, 94, 214, 159, 197, 49, 197, 206, 65, 214, 198, 304, 145, 72, 219, 197, 54, 145, 94, 49, 197, 206, 65, 214, 198, 304, 65, 303, 145, 72, 219, 197, 54, 145, 145, 29, 220, 94, 221, 159, 197, 49, 197, 205, 65, 218, 145, 72, 219, 197, 54, 145, 94, 49, 197, 205, 65, 218, 65, 303, 145, 72, 219, 197, 54, 145, 145, 29, 207, 159, 221, 4, 220, 29, 153, 29, 28, 55, 29, 220, 94, 221, 159, 197, 216, 198, 207, 94, 216, 198, 207, 65, 207, 145, 29, 50, 29, 222, 159, 66, 197, 302, 94, 210, 145, 29, 223, 159, 178, 197, 202, 65, 220, 198, 208, 65, 217, 94, 197, 207, 94, 145, 94, 197, 208, 94, 145, 94, 197, 214, 198, 210, 94, 145, 94, 197, 210, 94, 145, 94, 197, 302, 94, 145, 145, 29, 224, 159, 49, 197, 223, 94, 225, 159, 197, 302, 94, 145, 145, 29, 226, 159, 146, 197, 186, 210, 94, 210, 25, 94, 79, 159, 125, 145, 29, 116, 227, 132, 5, 197, 57, 197, 209, 94, 211, 145, 145, 55, 29, 228, 159, 178, 197, 201, 65, 197, 220, 198, 208, 65, 217, 145, 198, 209, 94, 197, 207, 94, 209, 145, 94, 197, 208, 198, 209, 94, 303, 145, 94, 197, 214, 198, 210, 94, 227, 198, 211, 145, 94, 197, 210, 94, 211, 145, 94, 197, 303, 94, 302, 145, 145, 29, 229, 159, 49, 197, 228, 94, 225, 159, 197, 302, 94, 303, 145, 145, 29, 230, 159, 229, 198, 224, 186, 55, 94, 164, 25, 29, 226, 142, 15, 197, 230, 72, 219, 197, 229, 72, 79, 145, 94, 63, 197, 229, 145, 145, 29, 68, 29, 151, 213, 55, 29, 231, 159, 178, 197, 203, 65, 220, 198, 208, 65, 217, 94, 197, 207, 94, 145, 94, 197, 208, 94, 145, 94, 197, 214, 198, 210, 94, 145, 94, 197, 210, 94, 145, 94, 197, 302, 94, 145, 145, 29, 232, 159, 49, 197, 231, 94, 225, 159, 197, 302, 94, 145, 145, 29, 233, 159, 232, 186, 55, 94, 164, 25, 4, 232, 186, 164, 94, 55, 25, 29, 226, 159, 226, 198, 234, 197, 233, 145, 29, 153, 29, 226, 159, 167, 197, 222, 186, 55, 94, 164, 25, 107, 222, 186, 164, 94, 55, 25, 94, 226, 94, 302, 145, 29, 235, 159, 178, 197, 204, 65, 197, 220, 198, 208, 65, 217, 145, 198, 210, 94, 197, 207, 94, 210, 145, 94, 197, 210, 198, 208, 94, 303, 145, 94, 197, 214, 198, 210, 94, 302, 145, 94, 197, 210, 94, 210, 145, 94, 197, 303, 94, 302, 145, 145, 29, 10, 197, 235, 94, 226, 72, 219, 197, 235, 72, 79, 72, 99, 145, 94, 225, 159, 197, 302, 94, 303, 145, 145, 29, 3, 29]}, {"code": "def fused_recurrent_fwd_kernel(\n    q,\n    k,\n    v,\n    g,\n    gk,\n    gv,\n    o,\n    h0,\n    ht,\n    cu_seqlens,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    REVERSE: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_GK: tl.constexpr,\n    USE_GV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_k, i_nh = (\n        tl.program_id(0).to(tl.int64),\n        tl.program_id(1).to(tl.int64),\n        tl.program_id(2).to(tl.int64),\n    )\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        all = T\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        all = B * T\n\n    p_q = (\n        q\n        + (bos + ((T - 1) if REVERSE else 0)) * H * K\n        + i_h * K\n        + i_k * BK\n        + tl.arange(0, BK)\n    )\n    p_k = (\n        k\n        + (bos + ((T - 1) if REVERSE else 0)) * H * K\n        + i_h * K\n        + i_k * BK\n        + tl.arange(0, BK)\n    )\n    p_v = (\n        v\n        + (bos + ((T - 1) if REVERSE else 0)) * H * V\n        + i_h * V\n        + i_v * BV\n        + tl.arange(0, BV)\n    )\n    p_o = (\n        o\n        + ((i_k * all + bos) + ((T - 1) if REVERSE else 0)) * H * V\n        + i_h * V\n        + i_v * BV\n        + tl.arange(0, BV)\n    )\n    if USE_G:\n        p_g = g + (bos + ((T - 1) if REVERSE else 0)) * H + i_h\n    if USE_GK:\n        p_gk = (\n            gk\n            + (bos + ((T - 1) if REVERSE else 0)) * H * K\n            + i_h * K\n            + i_k * BK\n            + tl.arange(0, BK)\n        )\n    if USE_GV:\n        p_gv = (\n            gv\n            + (bos + ((T - 1) if REVERSE else 0)) * H * V\n            + i_h * V\n            + i_v * BV\n            + tl.arange(0, BV)\n        )\n\n    mask_k = (i_k * BK + tl.arange(0, BK)) < K\n    mask_v = (i_v * BV + tl.arange(0, BV)) < V\n    mask_h = mask_k[None, :] & mask_v[:, None]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = (\n            h0\n            + i_nh * K * V\n            + (i_k * BK + tl.arange(0, BK)[None, :]) * V\n            + (i_v * BV + tl.arange(0, BV)[:, None])\n        )\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        if USE_GK:\n            b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)\n            b_h = b_h * exp(b_gk[None, :])\n        if USE_GV:\n            b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)\n            b_h = b_h * exp(b_gv[:, None])\n        if USE_G:\n            b_g = tl.load(p_g).to(tl.float32)\n            b_h = b_h * exp(b_g)\n        b_h += b_k[None, :] * b_v[:, None]\n        b_o = b_h * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n        p_q += (-1 if REVERSE else 1) * H * K\n        p_k += (-1 if REVERSE else 1) * H * K\n        p_v += (-1 if REVERSE else 1) * H * V\n        p_o += (-1 if REVERSE else 1) * H * V\n        if USE_GK:\n            p_gk += (-1 if REVERSE else 1) * H * K\n        if USE_GV:\n            p_gv += (-1 if REVERSE else 1) * H * V\n        if USE_G:\n            p_g += (-1 if REVERSE else 1) * H\n\n    if STORE_FINAL_STATE:\n        p_ht = (\n            ht\n            + i_nh * K * V\n            + (i_k * BK + tl.arange(0, BK)[None, :]) * V\n            + (i_v * BV + tl.arange(0, BV)[:, None])\n        )\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 93, 211, 93, 212, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 93, 218, 54, 6, 93, 219, 54, 6, 93, 220, 54, 6, 93, 221, 54, 6, 93, 222, 54, 6, 93, 223, 54, 6, 93, 224, 54, 6, 93, 225, 54, 6, 145, 54, 29, -1, 226, 93, 227, 93, 228, 159, 197, 141, 197, 302, 145, 71, 229, 197, 150, 145, 93, 141, 197, 303, 145, 71, 229, 197, 150, 145, 93, 141, 197, 304, 145, 71, 229, 197, 150, 145, 145, 29, 230, 93, 231, 159, 197, 228, 44, 214, 93, 228, 183, 214, 145, 29, 151, 225, 54, 29, 232, 93, 233, 159, 197, 50, 197, 210, 64, 230, 145, 71, 229, 197, 150, 145, 93, 50, 197, 210, 64, 230, 64, 303, 145, 71, 229, 197, 150, 145, 145, 29, 137, 159, 212, 29, 212, 159, 233, 4, 232, 29, 153, 29, 28, 54, 29, 232, 93, 233, 159, 197, 230, 198, 212, 93, 230, 198, 212, 64, 212, 145, 29, 137, 159, 213, 198, 212, 29, 49, 29, 234, 159, 201, 64, 197, 232, 64, 197, 212, 4, 303, 151, 219, 28, 302, 145, 145, 198, 214, 198, 215, 64, 231, 198, 215, 64, 227, 198, 217, 64, 65, 197, 302, 93, 217, 145, 29, 235, 159, 202, 64, 197, 232, 64, 197, 212, 4, 303, 151, 219, 28, 302, 145, 145, 198, 214, 198, 215, 64, 231, 198, 215, 64, 227, 198, 217, 64, 65, 197, 302, 93, 217, 145, 29, 236, 159, 203, 64, 197, 232, 64, 197, 212, 4, 303, 151, 219, 28, 302, 145, 145, 198, 214, 198, 216, 64, 231, 198, 216, 64, 226, 198, 218, 64, 65, 197, 302, 93, 218, 145, 29, 237, 159, 207, 64, 197, 227, 198, 137, 64, 232, 64, 197, 212, 4, 303, 151, 219, 28, 302, 145, 145, 198, 214, 198, 216, 64, 231, 198, 216, 64, 226, 198, 218, 64, 65, 197, 302, 93, 218, 145, 29, 151, 220, 54, 29, 238, 159, 204, 64, 197, 232, 64, 197, 212, 4, 303, 151, 219, 28, 302, 145, 145, 198, 214, 64, 231, 29, 153, 29, 151, 221, 54, 29, 239, 159, 205, 64, 197, 232, 64, 197, 212, 4, 303, 151, 219, 28, 302, 145, 145, 198, 214, 198, 215, 64, 231, 198, 215, 64, 227, 198, 217, 64, 65, 197, 302, 93, 217, 145, 29, 153, 29, 151, 222, 54, 29, 240, 159, 206, 64, 197, 232, 64, 197, 212, 4, 303, 151, 219, 28, 302, 145, 145, 198, 214, 198, 216, 64, 231, 198, 216, 64, 226, 198, 218, 64, 65, 197, 302, 93, 218, 145, 29, 153, 29, 241, 159, 227, 198, 217, 64, 65, 197, 302, 93, 217, 145, 1, 215, 29, 242, 159, 226, 198, 218, 64, 65, 197, 302, 93, 218, 145, 1, 216, 29, 243, 159, 241, 186, 164, 93, 54, 25, 143, 242, 186, 54, 93, 164, 25, 29, 244, 159, 146, 197, 186, 218, 93, 217, 25, 93, 78, 159, 124, 145, 29, 151, 223, 54, 29, 245, 159, 208, 64, 228, 198, 215, 198, 216, 64, 197, 227, 198, 217, 64, 65, 197, 302, 93, 217, 145, 186, 164, 93, 54, 25, 145, 198, 216, 64, 197, 226, 198, 218, 64, 65, 197, 302, 93, 218, 145, 186, 54, 93, 164, 25, 145, 29, 244, 142, 50, 197, 245, 93, 246, 159, 243, 93, 247, 159, 302, 145, 71, 229, 197, 124, 145, 29, 153, 29, 115, 248, 132, 5, 197, 302, 93, 212, 145, 54, 29, 249, 159, 50, 197, 234, 93, 246, 159, 241, 93, 247, 159, 302, 145, 71, 229, 197, 124, 145, 198, 211, 29, 250, 159, 50, 197, 235, 93, 246, 159, 241, 93, 247, 159, 302, 145, 71, 229, 197, 124, 145, 29, 251, 159, 50, 197, 236, 93, 246, 159, 242, 93, 247, 159, 302, 145, 71, 229, 197, 124, 145, 29, 151, 221, 54, 29, 252, 159, 50, 197, 239, 93, 246, 159, 241, 93, 247, 159, 302, 145, 71, 229, 197, 124, 145, 29, 244, 159, 244, 198, 169, 197, 252, 186, 164, 93, 54, 25, 145, 29, 153, 29, 151, 222, 54, 29, 253, 159, 50, 197, 240, 93, 246, 159, 242, 93, 247, 159, 302, 145, 71, 229, 197, 124, 145, 29, 244, 159, 244, 198, 169, 197, 253, 186, 54, 93, 164, 25, 145, 29, 153, 29, 151, 220, 54, 29, 254, 159, 50, 197, 238, 145, 71, 229, 197, 124, 145, 29, 244, 159, 244, 198, 169, 197, 254, 145, 29, 153, 29, 244, 142, 250, 186, 164, 93, 54, 25, 198, 251, 186, 54, 93, 164, 25, 29, 255, 159, 244, 198, 249, 186, 164, 93, 54, 25, 29, 255, 159, 180, 197, 255, 93, 256, 159, 303, 145, 29, 10, 197, 237, 93, 255, 71, 229, 197, 237, 71, 78, 71, 98, 145, 93, 246, 159, 242, 145, 29, 234, 142, 197, 4, 303, 151, 219, 28, 303, 145, 198, 214, 198, 215, 29, 235, 142, 197, 4, 303, 151, 219, 28, 303, 145, 198, 214, 198, 215, 29, 236, 142, 197, 4, 303, 151, 219, 28, 303, 145, 198, 214, 198, 216, 29, 237, 142, 197, 4, 303, 151, 219, 28, 303, 145, 198, 214, 198, 216, 29, 151, 221, 54, 29, 239, 142, 197, 4, 303, 151, 219, 28, 303, 145, 198, 214, 198, 215, 29, 153, 29, 151, 222, 54, 29, 240, 142, 197, 4, 303, 151, 219, 28, 303, 145, 198, 214, 198, 216, 29, 153, 29, 151, 220, 54, 29, 238, 142, 197, 4, 303, 151, 219, 28, 303, 145, 198, 214, 29, 153, 29, 67, 29, 151, 224, 54, 29, 257, 159, 209, 64, 228, 198, 215, 198, 216, 64, 197, 227, 198, 217, 64, 65, 197, 302, 93, 217, 145, 186, 164, 93, 54, 25, 145, 198, 216, 64, 197, 226, 198, 218, 64, 65, 197, 302, 93, 218, 145, 186, 54, 93, 164, 25, 145, 29, 10, 197, 257, 93, 244, 71, 229, 197, 257, 71, 78, 71, 98, 145, 93, 246, 159, 243, 145, 29, 153, 29, 3, 29]}, {"code": "def fused_recurrent_bwd_kernel(\n    q,\n    k,\n    v,\n    g,\n    gk,\n    gv,\n    h0,\n    do,\n    dq,\n    dk,\n    dv,\n    dht,\n    dh0,\n    cu_seqlens,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    REVERSE: tl.constexpr,\n    USE_G: tl.constexpr,\n    USE_GK: tl.constexpr,\n    USE_GV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_k, i_nh = (\n        tl.program_id(0).to(tl.int64),\n        tl.program_id(1).to(tl.int64),\n        tl.program_id(2).to(tl.int64),\n    )\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        all = T\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        all = B * T\n\n    p_k = (\n        k\n        + (bos + ((T - 1) if REVERSE else 0)) * H * K\n        + i_h * K\n        + i_k * BK\n        + tl.arange(0, BK)\n    )\n    p_v = (\n        v\n        + (bos + ((T - 1) if REVERSE else 0)) * H * V\n        + i_h * V\n        + i_v * BV\n        + tl.arange(0, BV)\n    )\n    p_do = (\n        do\n        + (bos + ((T - 1) if REVERSE else 0)) * H * V\n        + i_h * V\n        + i_v * BV\n        + tl.arange(0, BV)\n    )\n    p_dq = (\n        dq\n        + ((i_v * all + bos) + ((T - 1) if REVERSE else 0)) * H * K\n        + i_h * K\n        + i_k * BK\n        + tl.arange(0, BK)\n    )\n    if USE_G:\n        p_g = g + (bos + ((T - 1) if REVERSE else 0)) * H + i_h\n    if USE_GK:\n        p_gk = (\n            gk\n            + (bos + ((T - 1) if REVERSE else 0)) * H * K\n            + i_h * K\n            + i_k * BK\n            + tl.arange(0, BK)\n        )\n    if USE_GV:\n        p_gv = (\n            gv\n            + (bos + ((T - 1) if REVERSE else 0)) * H * V\n            + i_h * V\n            + i_v * BV\n            + tl.arange(0, BV)\n        )\n\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n    mask_h = mask_k[:, None] & mask_v[None, :]\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = (\n            h0\n            + i_nh * K * V\n            + (i_k * BK + tl.arange(0, BK)[:, None]) * V\n            + (i_v * BV + tl.arange(0, BV)[None, :])\n        )\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        if USE_G:\n            b_g = tl.load(p_g).to(tl.float32)\n            b_h = b_h * exp(b_g)\n        if USE_GK:\n            b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)\n            b_h = b_h * exp(b_gk[:, None])\n        if USE_GV:\n            b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)\n            b_h = b_h * exp(b_gv[None, :])\n        b_h += b_k[:, None] * b_v[None, :]\n        b_dq = b_h * b_do[None, :]\n        b_dq = tl.sum(b_dq, axis=1) * scale\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), mask=mask_k)\n\n        p_k += (-1 if REVERSE else 1) * H * K\n        p_v += (-1 if REVERSE else 1) * H * V\n        p_do += (-1 if REVERSE else 1) * H * V\n        p_dq += (-1 if REVERSE else 1) * H * K\n        if USE_G:\n            p_g += (-1 if REVERSE else 1) * H\n        if USE_GK:\n            p_gk += (-1 if REVERSE else 1) * H * K\n        if USE_GV:\n            p_gv += (-1 if REVERSE else 1) * H * V\n\n    tl.debug_barrier()\n\n    p_q = (\n        q\n        + (bos + ((T - 1) if not REVERSE else 0)) * H * K\n        + i_h * K\n        + i_k * BK\n        + tl.arange(0, BK)\n    )\n    p_k = (\n        k\n        + (bos + ((T - 1) if not REVERSE else 0)) * H * K\n        + i_h * K\n        + i_k * BK\n        + tl.arange(0, BK)\n    )\n    p_v = (\n        v\n        + (bos + ((T - 1) if not REVERSE else 0)) * H * V\n        + i_h * V\n        + i_v * BV\n        + tl.arange(0, BV)\n    )\n    p_do = (\n        do\n        + (bos + ((T - 1) if not REVERSE else 0)) * H * V\n        + i_h * V\n        + i_v * BV\n        + tl.arange(0, BV)\n    )\n    p_dk = (\n        dk\n        + ((i_v * all + bos) + ((T - 1) if not REVERSE else 0)) * H * K\n        + i_h * K\n        + i_k * BK\n        + tl.arange(0, BK)\n    )\n    p_dv = (\n        dv\n        + ((i_k * all + bos) + ((T - 1) if not REVERSE else 0)) * H * V\n        + i_h * V\n        + i_v * BV\n        + tl.arange(0, BV)\n    )\n    if USE_G:\n        p_g = g + (bos + ((T - 1) if not REVERSE else 0)) * H + i_h\n    if USE_GK:\n        p_gk = (\n            gk\n            + (bos + ((T - 1) if not REVERSE else 0)) * H * K\n            + i_h * K\n            + i_k * BK\n            + tl.arange(0, BK)\n        )\n    if USE_GV:\n        p_gv = (\n            gv\n            + (bos + ((T - 1) if not REVERSE else 0)) * H * V\n            + i_h * V\n            + i_v * BV\n            + tl.arange(0, BV)\n        )\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = (\n            dht\n            + i_nh * K * V\n            + (i_k * BK + tl.arange(0, BK)[:, None]) * V\n            + (i_v * BV + tl.arange(0, BV)[None, :])\n        )\n        b_dh += tl.load(p_dht, mask=mask_h, other=0).to(tl.float32)\n\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        b_dh += b_q[:, None] * b_do[None, :]\n        b_dk = tl.sum(b_dh * b_v[None, :], axis=1)\n        b_dv = tl.sum(b_dh * b_k[:, None], axis=0)\n        if USE_G:\n            b_g = tl.load(p_g).to(tl.float32)\n            b_dh *= exp(b_g)\n        if USE_GK:\n            b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)\n            b_dh *= exp(b_gk)[:, None]\n        if USE_GV:\n            b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)\n            b_dh *= exp(b_gv)[None, :]\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)\n\n        p_q += (1 if REVERSE else -1) * H * K\n        p_k += (1 if REVERSE else -1) * H * K\n        p_v += (1 if REVERSE else -1) * H * V\n        p_do += (1 if REVERSE else -1) * H * V\n        p_dk += (1 if REVERSE else -1) * H * K\n        p_dv += (1 if REVERSE else -1) * H * V\n        if USE_G:\n            p_g += (1 if REVERSE else -1) * H\n        if USE_GK:\n            p_gk += (1 if REVERSE else -1) * H * K\n        if USE_GV:\n            p_gv += (1 if REVERSE else -1) * H * V\n\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = (\n            dh0\n            + i_nh * K * V\n            + (i_k * BK + tl.arange(0, BK)[:, None]) * V\n            + (i_v * BV + tl.arange(0, BV)[None, :])\n        )\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_h)", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 94, 209, 94, 210, 94, 211, 94, 212, 94, 213, 94, 214, 94, 215, 94, 216, 94, 217, 55, 6, 94, 218, 55, 6, 94, 219, 55, 6, 94, 220, 55, 6, 94, 221, 55, 6, 94, 222, 55, 6, 94, 223, 55, 6, 94, 224, 55, 6, 94, 225, 55, 6, 94, 226, 55, 6, 94, 227, 55, 6, 94, 228, 55, 6, 94, 229, 55, 6, 94, 230, 55, 6, 145, 55, 29, -1, 231, 94, 232, 94, 233, 159, 197, 141, 197, 302, 145, 72, 234, 197, 150, 145, 94, 141, 197, 303, 145, 72, 234, 197, 150, 145, 94, 141, 197, 304, 145, 72, 234, 197, 150, 145, 145, 29, 235, 94, 236, 159, 197, 233, 44, 218, 94, 233, 183, 218, 145, 29, 151, 230, 55, 29, 237, 94, 238, 159, 197, 49, 197, 214, 65, 235, 145, 72, 234, 197, 150, 145, 94, 49, 197, 214, 65, 235, 65, 303, 145, 72, 234, 197, 150, 145, 145, 29, 137, 159, 216, 29, 216, 159, 238, 4, 237, 29, 153, 29, 28, 55, 29, 237, 94, 238, 159, 197, 235, 198, 216, 94, 235, 198, 216, 65, 216, 145, 29, 137, 159, 217, 198, 216, 29, 50, 29, 239, 159, 202, 65, 197, 237, 65, 197, 216, 4, 303, 151, 223, 28, 302, 145, 145, 198, 218, 198, 219, 65, 236, 198, 219, 65, 232, 198, 221, 65, 66, 197, 302, 94, 221, 145, 29, 240, 159, 203, 65, 197, 237, 65, 197, 216, 4, 303, 151, 223, 28, 302, 145, 145, 198, 218, 198, 220, 65, 236, 198, 220, 65, 231, 198, 222, 65, 66, 197, 302, 94, 222, 145, 29, 241, 159, 208, 65, 197, 237, 65, 197, 216, 4, 303, 151, 223, 28, 302, 145, 145, 198, 218, 198, 220, 65, 236, 198, 220, 65, 231, 198, 222, 65, 66, 197, 302, 94, 222, 145, 29, 242, 159, 209, 65, 197, 231, 198, 137, 65, 237, 65, 197, 216, 4, 303, 151, 223, 28, 302, 145, 145, 198, 218, 198, 219, 65, 236, 198, 219, 65, 232, 198, 221, 65, 66, 197, 302, 94, 221, 145, 29, 151, 224, 55, 29, 243, 159, 204, 65, 197, 237, 65, 197, 216, 4, 303, 151, 223, 28, 302, 145, 145, 198, 218, 65, 236, 29, 153, 29, 151, 225, 55, 29, 244, 159, 205, 65, 197, 237, 65, 197, 216, 4, 303, 151, 223, 28, 302, 145, 145, 198, 218, 198, 219, 65, 236, 198, 219, 65, 232, 198, 221, 65, 66, 197, 302, 94, 221, 145, 29, 153, 29, 151, 226, 55, 29, 245, 159, 206, 65, 197, 237, 65, 197, 216, 4, 303, 151, 223, 28, 302, 145, 145, 198, 218, 198, 220, 65, 236, 198, 220, 65, 231, 198, 222, 65, 66, 197, 302, 94, 222, 145, 29, 153, 29, 246, 159, 232, 198, 221, 65, 66, 197, 302, 94, 221, 145, 1, 219, 29, 247, 159, 231, 198, 222, 65, 66, 197, 302, 94, 222, 145, 1, 220, 29, 248, 159, 246, 186, 55, 94, 164, 25, 143, 247, 186, 164, 94, 55, 25, 29, 249, 159, 146, 197, 186, 221, 94, 222, 25, 94, 79, 159, 125, 145, 29, 151, 227, 55, 29, 250, 159, 207, 65, 233, 198, 219, 198, 220, 65, 197, 232, 198, 221, 65, 66, 197, 302, 94, 221, 145, 186, 55, 94, 164, 25, 145, 198, 220, 65, 197, 231, 198, 222, 65, 66, 197, 302, 94, 222, 145, 186, 164, 94, 55, 25, 145, 29, 249, 142, 49, 197, 250, 94, 251, 159, 248, 94, 252, 159, 302, 145, 72, 234, 197, 125, 145, 29, 153, 29, 116, 253, 132, 5, 197, 302, 94, 216, 145, 55, 29, 254, 159, 49, 197, 239, 94, 251, 159, 246, 94, 252, 159, 302, 145, 72, 234, 197, 125, 145, 29, 255, 159, 49, 197, 240, 94, 251, 159, 247, 94, 252, 159, 302, 145, 72, 234, 197, 125, 145, 29, 256, 159, 49, 197, 241, 94, 251, 159, 247, 94, 252, 159, 302, 145, 72, 234, 197, 125, 145, 29, 151, 224, 55, 29, 257, 159, 49, 197, 243, 145, 72, 234, 197, 125, 145, 29, 249, 159, 249, 198, 169, 197, 257, 145, 29, 153, 29, 151, 225, 55, 29, 258, 159, 49, 197, 244, 94, 251, 159, 246, 94, 252, 159, 302, 145, 72, 234, 197, 125, 145, 29, 249, 159, 249, 198, 169, 197, 258, 186, 55, 94, 164, 25, 145, 29, 153, 29, 151, 226, 55, 29, 259, 159, 49, 197, 245, 94, 251, 159, 247, 94, 252, 159, 302, 145, 72, 234, 197, 125, 145, 29, 249, 159, 249, 198, 169, 197, 259, 186, 164, 94, 55, 25, 145, 29, 153, 29, 249, 142, 254, 186, 55, 94, 164, 25, 198, 255, 186, 164, 94, 55, 25, 29, 260, 159, 249, 198, 256, 186, 164, 94, 55, 25, 29, 260, 159, 180, 197, 260, 94, 261, 159, 303, 145, 198, 215, 29, 10, 197, 242, 94, 260, 72, 234, 197, 242, 72, 79, 72, 99, 145, 94, 251, 159, 246, 145, 29, 239, 142, 197, 4, 303, 151, 223, 28, 303, 145, 198, 218, 198, 219, 29, 240, 142, 197, 4, 303, 151, 223, 28, 303, 145, 198, 218, 198, 220, 29, 241, 142, 197, 4, 303, 151, 223, 28, 303, 145, 198, 218, 198, 220, 29, 242, 142, 197, 4, 303, 151, 223, 28, 303, 145, 198, 218, 198, 219, 29, 151, 224, 55, 29, 243, 142, 197, 4, 303, 151, 223, 28, 303, 145, 198, 218, 29, 153, 29, 151, 225, 55, 29, 244, 142, 197, 4, 303, 151, 223, 28, 303, 145, 198, 218, 198, 219, 29, 153, 29, 151, 226, 55, 29, 245, 142, 197, 4, 303, 151, 223, 28, 303, 145, 198, 218, 198, 220, 29, 153, 29, 68, 29, 46, 197, 145, 29, 262, 159, 201, 65, 197, 237, 65, 197, 216, 4, 303, 151, 56, 223, 28, 302, 145, 145, 198, 218, 198, 219, 65, 236, 198, 219, 65, 232, 198, 221, 65, 66, 197, 302, 94, 221, 145, 29, 239, 159, 202, 65, 197, 237, 65, 197, 216, 4, 303, 151, 56, 223, 28, 302, 145, 145, 198, 218, 198, 219, 65, 236, 198, 219, 65, 232, 198, 221, 65, 66, 197, 302, 94, 221, 145, 29, 240, 159, 203, 65, 197, 237, 65, 197, 216, 4, 303, 151, 56, 223, 28, 302, 145, 145, 198, 218, 198, 220, 65, 236, 198, 220, 65, 231, 198, 222, 65, 66, 197, 302, 94, 222, 145, 29, 241, 159, 208, 65, 197, 237, 65, 197, 216, 4, 303, 151, 56, 223, 28, 302, 145, 145, 198, 218, 198, 220, 65, 236, 198, 220, 65, 231, 198, 222, 65, 66, 197, 302, 94, 222, 145, 29, 263, 159, 210, 65, 197, 231, 198, 137, 65, 237, 65, 197, 216, 4, 303, 151, 56, 223, 28, 302, 145, 145, 198, 218, 198, 219, 65, 236, 198, 219, 65, 232, 198, 221, 65, 66, 197, 302, 94, 221, 145, 29, 264, 159, 211, 65, 197, 232, 198, 137, 65, 237, 65, 197, 216, 4, 303, 151, 56, 223, 28, 302, 145, 145, 198, 218, 198, 220, 65, 236, 198, 220, 65, 231, 198, 222, 65, 66, 197, 302, 94, 222, 145, 29, 151, 224, 55, 29, 243, 159, 204, 65, 197, 237, 65, 197, 216, 4, 303, 151, 56, 223, 28, 302, 145, 145, 198, 218, 65, 236, 29, 153, 29, 151, 225, 55, 29, 244, 159, 205, 65, 197, 237, 65, 197, 216, 4, 303, 151, 56, 223, 28, 302, 145, 145, 198, 218, 198, 219, 65, 236, 198, 219, 65, 232, 198, 221, 65, 66, 197, 302, 94, 221, 145, 29, 153, 29, 151, 226, 55, 29, 245, 159, 206, 65, 197, 237, 65, 197, 216, 4, 303, 151, 56, 223, 28, 302, 145, 145, 198, 218, 198, 220, 65, 236, 198, 220, 65, 231, 198, 222, 65, 66, 197, 302, 94, 222, 145, 29, 153, 29, 265, 159, 146, 197, 186, 221, 94, 222, 25, 94, 79, 159, 125, 145, 29, 151, 229, 55, 29, 266, 159, 212, 65, 233, 198, 219, 198, 220, 65, 197, 232, 198, 221, 65, 66, 197, 302, 94, 221, 145, 186, 55, 94, 164, 25, 145, 198, 220, 65, 197, 231, 198, 222, 65, 66, 197, 302, 94, 222, 145, 186, 164, 94, 55, 25, 145, 29, 265, 142, 49, 197, 266, 94, 251, 159, 248, 94, 252, 159, 302, 145, 72, 234, 197, 125, 145, 29, 153, 29, 116, 253, 132, 5, 197, 216, 145, 55, 29, 267, 159, 49, 197, 262, 94, 251, 159, 246, 94, 252, 159, 302, 145, 72, 234, 197, 125, 145, 198, 215, 29, 254, 159, 49, 197, 239, 94, 251, 159, 246, 94, 252, 159, 302, 145, 72, 234, 197, 125, 145, 29, 255, 159, 49, 197, 240, 94, 251, 159, 247, 94, 252, 159, 302, 145, 72, 234, 197, 125, 145, 29, 256, 159, 49, 197, 241, 94, 251, 159, 247, 94, 252, 159, 302, 145, 72, 234, 197, 125, 145, 29, 265, 142, 267, 186, 55, 94, 164, 25, 198, 256, 186, 164, 94, 55, 25, 29, 268, 159, 180, 197, 265, 198, 255, 186, 164, 94, 55, 25, 94, 261, 159, 303, 145, 29, 269, 159, 180, 197, 265, 198, 254, 186, 55, 94, 164, 25, 94, 261, 159, 302, 145, 29, 151, 224, 55, 29, 257, 159, 49, 197, 243, 145, 72, 234, 197, 125, 145, 29, 265, 22, 169, 197, 257, 145, 29, 153, 29, 151, 225, 55, 29, 258, 159, 49, 197, 244, 94, 251, 159, 246, 94, 252, 159, 302, 145, 72, 234, 197, 125, 145, 29, 265, 22, 169, 197, 258, 145, 186, 55, 94, 164, 25, 29, 153, 29, 151, 226, 55, 29, 259, 159, 49, 197, 245, 94, 251, 159, 247, 94, 252, 159, 302, 145, 72, 234, 197, 125, 145, 29, 265, 22, 169, 197, 259, 145, 186, 164, 94, 55, 25, 29, 153, 29, 10, 197, 263, 94, 268, 72, 234, 197, 263, 72, 79, 72, 99, 145, 94, 251, 159, 246, 145, 29, 10, 197, 264, 94, 269, 72, 234, 197, 264, 72, 79, 72, 99, 145, 94, 251, 159, 247, 145, 29, 262, 142, 197, 303, 151, 223, 28, 4, 303, 145, 198, 218, 198, 219, 29, 239, 142, 197, 303, 151, 223, 28, 4, 303, 145, 198, 218, 198, 219, 29, 240, 142, 197, 303, 151, 223, 28, 4, 303, 145, 198, 218, 198, 220, 29, 241, 142, 197, 303, 151, 223, 28, 4, 303, 145, 198, 218, 198, 220, 29, 263, 142, 197, 303, 151, 223, 28, 4, 303, 145, 198, 218, 198, 219, 29, 264, 142, 197, 303, 151, 223, 28, 4, 303, 145, 198, 218, 198, 220, 29, 151, 224, 55, 29, 243, 142, 197, 303, 151, 223, 28, 4, 303, 145, 198, 218, 29, 153, 29, 151, 225, 55, 29, 244, 142, 197, 303, 151, 223, 28, 4, 303, 145, 198, 218, 198, 219, 29, 153, 29, 151, 226, 55, 29, 245, 142, 197, 303, 151, 223, 28, 4, 303, 145, 198, 218, 198, 220, 29, 153, 29, 68, 29, 151, 228, 55, 29, 270, 159, 213, 65, 233, 198, 219, 198, 220, 65, 197, 232, 198, 221, 65, 66, 197, 302, 94, 221, 145, 186, 55, 94, 164, 25, 145, 198, 220, 65, 197, 231, 198, 222, 65, 66, 197, 302, 94, 222, 145, 186, 164, 94, 55, 25, 145, 29, 10, 197, 270, 94, 265, 72, 234, 197, 270, 72, 79, 72, 99, 145, 94, 251, 159, 248, 145, 29, 153, 29, 3, 29]}, {"code": "def fused_recurrent_delta_rule_fwd_kernel(\n    q,\n    k,\n    v,\n    u,\n    beta,\n    o,\n    h0,\n    ht,\n    cu_seqlens,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_BETA_HEADWISE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_k, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        all = T\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        all = B * T\n\n    p_q = q + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)\n    p_k = k + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)\n    p_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\n    p_u = u + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\n    if IS_BETA_HEADWISE:\n        p_beta = beta + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\n    else:\n        p_beta = beta + bos * H + i_h\n    p_o = o + ((i_k * all + bos) * H + i_h) * V + i_v * BV + tl.arange(0, BV)\n\n    mask_k = (i_k * BK + tl.arange(0, BK)) < K\n    mask_v = (i_v * BV + tl.arange(0, BV)) < V\n    mask_h = mask_k[None, :] & mask_v[:, None]\n\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = (\n            h0\n            + i_nh * K * V\n            + (i_k * BK + tl.arange(0, BK)[None, :]) * V\n            + (i_v * BV + tl.arange(0, BV)[:, None])\n        )\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_v_minus = tl.sum(b_h * b_k[None, :], axis=1)\n        b_v -= b_v_minus\n        if IS_BETA_HEADWISE:\n            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)\n        else:\n            b_beta = tl.load(p_beta).to(tl.float32)\n        tl.store(p_u, b_v.to(p_v.dtype.element_ty), mask=mask_v)\n        b_v *= b_beta\n        b_h += b_k[None, :] * b_v[:, None]\n        b_o = b_h * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n\n        p_q += H * K\n        p_k += H * K\n        p_o += H * V\n        p_v += H * V\n        p_u += H * V\n        p_beta += H * (V if IS_BETA_HEADWISE else 1)\n\n    if STORE_FINAL_STATE:\n        p_ht = (\n            ht\n            + i_nh * K * V\n            + (i_k * BK + tl.arange(0, BK)[None, :]) * V\n            + (i_v * BV + tl.arange(0, BV)[:, None])\n        )\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 93, 211, 93, 212, 54, 6, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 93, 218, 54, 6, 93, 219, 54, 6, 93, 220, 54, 6, 93, 221, 54, 6, 145, 54, 29, -1, 222, 93, 223, 93, 224, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 93, 141, 197, 304, 145, 145, 29, 225, 93, 226, 159, 197, 224, 44, 213, 93, 224, 183, 213, 145, 29, 151, 221, 54, 29, 227, 93, 228, 159, 197, 50, 197, 209, 64, 225, 145, 71, 229, 197, 150, 145, 93, 50, 197, 209, 64, 225, 64, 303, 145, 71, 229, 197, 150, 145, 145, 29, 137, 159, 211, 29, 211, 159, 228, 4, 227, 29, 153, 29, 28, 54, 29, 227, 93, 228, 159, 197, 225, 198, 211, 93, 225, 198, 211, 64, 211, 145, 29, 137, 159, 212, 198, 211, 29, 49, 29, 230, 159, 201, 64, 197, 227, 198, 213, 64, 226, 145, 198, 214, 64, 223, 198, 216, 64, 65, 197, 302, 93, 216, 145, 29, 231, 159, 202, 64, 197, 227, 198, 213, 64, 226, 145, 198, 214, 64, 223, 198, 216, 64, 65, 197, 302, 93, 216, 145, 29, 232, 159, 203, 64, 197, 227, 198, 213, 64, 226, 145, 198, 215, 64, 222, 198, 217, 64, 65, 197, 302, 93, 217, 145, 29, 233, 159, 204, 64, 197, 227, 198, 213, 64, 226, 145, 198, 215, 64, 222, 198, 217, 64, 65, 197, 302, 93, 217, 145, 29, 151, 220, 54, 29, 234, 159, 205, 64, 197, 227, 198, 213, 64, 226, 145, 198, 215, 64, 222, 198, 217, 64, 65, 197, 302, 93, 217, 145, 29, 153, 29, 28, 54, 29, 234, 159, 205, 64, 227, 198, 213, 64, 226, 29, 49, 29, 235, 159, 206, 64, 197, 197, 223, 198, 137, 64, 227, 145, 198, 213, 64, 226, 145, 198, 215, 64, 222, 198, 217, 64, 65, 197, 302, 93, 217, 145, 29, 236, 159, 223, 198, 216, 64, 65, 197, 302, 93, 216, 145, 1, 214, 29, 237, 159, 222, 198, 217, 64, 65, 197, 302, 93, 217, 145, 1, 215, 29, 238, 159, 236, 186, 164, 93, 54, 25, 143, 237, 186, 54, 93, 164, 25, 29, 239, 159, 146, 197, 186, 217, 93, 216, 25, 93, 78, 159, 124, 145, 29, 151, 218, 54, 29, 240, 159, 207, 64, 224, 198, 214, 198, 215, 64, 197, 223, 198, 216, 64, 65, 197, 302, 93, 216, 145, 186, 164, 93, 54, 25, 145, 198, 215, 64, 197, 222, 198, 217, 64, 65, 197, 302, 93, 217, 145, 186, 54, 93, 164, 25, 145, 29, 239, 142, 50, 197, 240, 93, 241, 159, 238, 93, 242, 159, 302, 145, 71, 229, 197, 124, 145, 29, 153, 29, 115, 243, 132, 5, 197, 302, 93, 211, 145, 54, 29, 244, 159, 50, 197, 231, 93, 241, 159, 236, 93, 242, 159, 302, 145, 71, 229, 197, 124, 145, 29, 245, 159, 50, 197, 232, 93, 241, 159, 237, 93, 242, 159, 302, 145, 71, 229, 197, 124, 145, 29, 246, 159, 50, 197, 230, 93, 241, 159, 236, 93, 242, 159, 302, 145, 71, 229, 197, 124, 145, 198, 210, 29, 247, 159, 180, 197, 239, 198, 244, 186, 164, 93, 54, 25, 93, 248, 159, 303, 145, 29, 245, 2, 247, 29, 151, 220, 54, 29, 249, 159, 50, 197, 234, 93, 241, 159, 237, 93, 242, 159, 302, 145, 71, 229, 197, 124, 145, 29, 153, 29, 28, 54, 29, 249, 159, 50, 197, 234, 145, 71, 229, 197, 124, 145, 29, 49, 29, 10, 197, 233, 93, 245, 71, 229, 197, 232, 71, 78, 71, 98, 145, 93, 241, 159, 237, 145, 29, 245, 22, 249, 29, 239, 142, 244, 186, 164, 93, 54, 25, 198, 245, 186, 54, 93, 164, 25, 29, 250, 159, 239, 198, 246, 186, 164, 93, 54, 25, 29, 250, 159, 180, 197, 250, 93, 248, 159, 303, 145, 29, 10, 197, 235, 93, 250, 71, 229, 197, 235, 71, 78, 71, 98, 145, 93, 241, 159, 237, 145, 29, 230, 142, 213, 198, 214, 29, 231, 142, 213, 198, 214, 29, 235, 142, 213, 198, 215, 29, 232, 142, 213, 198, 215, 29, 233, 142, 213, 198, 215, 29, 234, 142, 213, 198, 197, 215, 151, 220, 28, 303, 145, 29, 67, 29, 151, 219, 54, 29, 251, 159, 208, 64, 224, 198, 214, 198, 215, 64, 197, 223, 198, 216, 64, 65, 197, 302, 93, 216, 145, 186, 164, 93, 54, 25, 145, 198, 215, 64, 197, 222, 198, 217, 64, 65, 197, 302, 93, 217, 145, 186, 54, 93, 164, 25, 145, 29, 10, 197, 251, 93, 239, 71, 229, 197, 251, 71, 78, 71, 98, 145, 93, 241, 159, 238, 145, 29, 153, 29, 3, 29]}, {"code": "def fused_recurrent_delta_rule_bwd_kernel(\n    q,\n    k,\n    v,\n    beta,\n    h0,\n    dh0,\n    dht,\n    do,\n    dq,\n    dk,\n    dv,\n    db,\n    cu_seqlens,\n    scale,\n    B: tl.constexpr,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NK: tl.constexpr,\n    IS_BETA_HEADWISE: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_k, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        all = T\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        all = B * T\n\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n\n    p_q = q + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK) + (T - 1) * H * K\n    p_k = k + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK) + (T - 1) * H * K\n    p_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV) + (T - 1) * H * V\n    p_do = do + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV) + (T - 1) * H * V\n    p_dk = (\n        dk\n        + ((i_v * all + bos) * H + i_h) * K\n        + i_k * BK\n        + tl.arange(0, BK)\n        + (T - 1) * H * K\n    )\n    p_dv = (\n        dv\n        + ((i_k * all + bos) * H + i_h) * V\n        + i_v * BV\n        + tl.arange(0, BV)\n        + (T - 1) * H * V\n    )\n    if IS_BETA_HEADWISE:\n        p_beta = beta + (bos + T - 1) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_dbeta = (\n            db\n            + ((i_v * NK + i_k) * all + bos + T - 1) * H * V\n            + i_h * V\n            + tl.arange(0, BV)\n        )\n    else:\n        p_beta = beta + (bos + T - 1) * H + i_h\n        p_dbeta = db + (i_v * all + bos + T - 1) * H + i_h\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_ht = (\n            dht\n            + i_nh * K * V\n            + (i_k * BK + tl.arange(0, BK)[:, None]) * V\n            + (i_v * BV + tl.arange(0, BV)[None, :])\n        )\n        b_dh += tl.load(p_ht, mask=mask_k[:, None] & mask_v[None, :], other=0).to(\n            tl.float32\n        )\n\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        if IS_BETA_HEADWISE:\n            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)\n        else:\n            b_beta = tl.load(p_beta).to(tl.float32)\n        b_dh += b_q[:, None] * b_do[None, :]\n        b_dk = tl.sum(b_dh * (b_v * b_beta)[None, :], axis=1)\n        b_dv = tl.sum(b_dh * b_k[:, None], axis=0)\n\n        b_db = b_dv * b_v if IS_BETA_HEADWISE else tl.sum(b_dv * b_v)\n        b_dv = b_dv * b_beta\n\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)\n        if IS_BETA_HEADWISE:\n            tl.store(p_dbeta, b_db.to(p_dbeta.dtype.element_ty), mask=mask_v)\n        else:\n            tl.store(p_dbeta, b_db.to(p_dbeta.dtype.element_ty))\n\n        b_dh -= b_k[:, None] * b_dv[None, :]\n\n        p_q -= H * K\n        p_k -= H * K\n        p_v -= H * V\n        p_do -= H * V\n        p_dk -= H * K\n        p_dv -= H * V\n        p_dbeta -= H * (V if IS_BETA_HEADWISE else 1)\n        p_beta -= H * (V if IS_BETA_HEADWISE else 1)\n\n    if USE_INITIAL_STATE:\n        p_dh0 = (\n            dh0\n            + i_nh * K * V\n            + (i_k * BK + tl.arange(0, BK)[:, None]) * V\n            + (i_v * BV + tl.arange(0, BV)[None, :])\n        )\n        tl.store(\n            p_dh0,\n            b_dh.to(p_dh0.dtype.element_ty),\n            mask=mask_k[:, None] & mask_v[None, :],\n        )\n\n    tl.debug_barrier()\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    p_q = q + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)\n    p_k = k + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)\n    p_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\n    if IS_BETA_HEADWISE:\n        p_beta = beta + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\n    else:\n        p_beta = beta + bos * H + i_h\n    p_do = do + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + ((i_v * all + bos) * H + i_h) * K + i_k * BK + tl.arange(0, BK)\n    p_dk = dk + ((i_v * all + bos) * H + i_h) * K + i_k * BK + tl.arange(0, BK)\n    p_dv = dv + ((i_k * all + bos) * H + i_h) * V + i_v * BV + tl.arange(0, BV)\n\n    if USE_INITIAL_STATE:\n        mask_h = mask_k[:, None] & mask_v[None, :]\n        p_h0 = (\n            h0\n            + i_nh * K * V\n            + (i_k * BK + tl.arange(0, BK)[:, None]) * V\n            + (i_v * BV + tl.arange(0, BV)[None, :])\n        )\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        b_dk = tl.load(p_dk, mask=mask_k, other=0).to(tl.float32)\n        b_dv = tl.load(p_dv, mask=mask_v, other=0).to(tl.float32)\n        b_dk -= tl.sum(b_dv[None, :] * b_h, axis=1)\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)\n\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        if IS_BETA_HEADWISE:\n            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)\n        else:\n            b_beta = tl.load(p_beta).to(tl.float32)\n        b_v *= b_beta\n\n        b_h += b_k[:, None] * b_v[None, :]\n        b_dq = b_h * b_do[None, :]\n        d_q = tl.sum(b_dq, axis=1) * scale\n        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_k)\n\n        p_k += H * K\n        p_v += H * V\n        p_do += H * V\n        p_dq += H * K\n        p_dk += H * K\n        p_dv += H * V\n        p_beta += H * (V if IS_BETA_HEADWISE else 1)", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 94, 209, 94, 210, 94, 211, 94, 212, 94, 213, 94, 214, 94, 215, 55, 6, 94, 216, 94, 217, 55, 6, 94, 218, 55, 6, 94, 219, 55, 6, 94, 220, 55, 6, 94, 221, 55, 6, 94, 222, 55, 6, 94, 223, 55, 6, 94, 224, 55, 6, 94, 225, 55, 6, 94, 226, 55, 6, 145, 55, 29, -1, 227, 94, 228, 94, 229, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 94, 141, 197, 304, 145, 145, 29, 230, 94, 231, 159, 197, 229, 44, 217, 94, 229, 183, 217, 145, 29, 151, 226, 55, 29, 232, 94, 233, 159, 197, 49, 197, 213, 65, 230, 145, 72, 234, 197, 150, 145, 94, 49, 197, 213, 65, 230, 65, 303, 145, 72, 234, 197, 150, 145, 145, 29, 137, 159, 216, 29, 216, 159, 233, 4, 232, 29, 153, 29, 28, 55, 29, 232, 94, 233, 159, 197, 230, 198, 216, 94, 230, 198, 216, 65, 216, 145, 29, 137, 159, 215, 198, 216, 29, 50, 29, 235, 159, 228, 198, 220, 65, 66, 197, 302, 94, 220, 145, 1, 218, 29, 236, 159, 227, 198, 221, 65, 66, 197, 302, 94, 221, 145, 1, 219, 29, 237, 159, 201, 65, 197, 232, 198, 217, 65, 231, 145, 198, 218, 65, 228, 198, 220, 65, 66, 197, 302, 94, 220, 145, 65, 197, 216, 4, 303, 145, 198, 217, 198, 218, 29, 238, 159, 202, 65, 197, 232, 198, 217, 65, 231, 145, 198, 218, 65, 228, 198, 220, 65, 66, 197, 302, 94, 220, 145, 65, 197, 216, 4, 303, 145, 198, 217, 198, 218, 29, 239, 159, 203, 65, 197, 232, 198, 217, 65, 231, 145, 198, 219, 65, 227, 198, 221, 65, 66, 197, 302, 94, 221, 145, 65, 197, 216, 4, 303, 145, 198, 217, 198, 219, 29, 240, 159, 208, 65, 197, 232, 198, 217, 65, 231, 145, 198, 219, 65, 227, 198, 221, 65, 66, 197, 302, 94, 221, 145, 65, 197, 216, 4, 303, 145, 198, 217, 198, 219, 29, 241, 159, 210, 65, 197, 197, 227, 198, 137, 65, 232, 145, 198, 217, 65, 231, 145, 198, 218, 65, 228, 198, 220, 65, 66, 197, 302, 94, 220, 145, 65, 197, 216, 4, 303, 145, 198, 217, 198, 218, 29, 242, 159, 211, 65, 197, 197, 228, 198, 137, 65, 232, 145, 198, 217, 65, 231, 145, 198, 219, 65, 227, 198, 221, 65, 66, 197, 302, 94, 221, 145, 65, 197, 216, 4, 303, 145, 198, 217, 198, 219, 29, 151, 223, 55, 29, 243, 159, 204, 65, 197, 232, 65, 216, 4, 303, 145, 198, 217, 198, 219, 65, 231, 198, 219, 65, 227, 198, 221, 65, 66, 197, 302, 94, 221, 145, 29, 244, 159, 212, 65, 197, 197, 227, 198, 222, 65, 228, 145, 198, 137, 65, 232, 65, 216, 4, 303, 145, 198, 217, 198, 219, 65, 231, 198, 219, 65, 66, 197, 302, 94, 221, 145, 29, 153, 29, 28, 55, 29, 243, 159, 204, 65, 197, 232, 65, 216, 4, 303, 145, 198, 217, 65, 231, 29, 244, 159, 212, 65, 197, 227, 198, 137, 65, 232, 65, 216, 4, 303, 145, 198, 217, 65, 231, 29, 50, 29, 245, 159, 146, 197, 186, 220, 94, 221, 25, 94, 79, 159, 125, 145, 29, 151, 225, 55, 29, 246, 159, 207, 65, 229, 198, 218, 198, 219, 65, 197, 228, 198, 220, 65, 66, 197, 302, 94, 220, 145, 186, 55, 94, 164, 25, 145, 198, 219, 65, 197, 227, 198, 221, 65, 66, 197, 302, 94, 221, 145, 186, 164, 94, 55, 25, 145, 29, 245, 142, 49, 197, 246, 94, 247, 159, 235, 186, 55, 94, 164, 25, 143, 236, 186, 164, 94, 55, 25, 94, 248, 159, 302, 145, 72, 234, 197, 125, 145, 29, 153, 29, 116, 249, 132, 5, 197, 216, 145, 55, 29, 250, 159, 49, 197, 237, 94, 247, 159, 235, 94, 248, 159, 302, 145, 72, 234, 197, 125, 145, 198, 214, 29, 251, 159, 49, 197, 238, 94, 247, 159, 235, 94, 248, 159, 302, 145, 72, 234, 197, 125, 145, 29, 252, 159, 49, 197, 239, 94, 247, 159, 236, 94, 248, 159, 302, 145, 72, 234, 197, 125, 145, 29, 253, 159, 49, 197, 240, 94, 247, 159, 236, 94, 248, 159, 302, 145, 72, 234, 197, 125, 145, 29, 151, 223, 55, 29, 254, 159, 49, 197, 243, 94, 247, 159, 236, 94, 248, 159, 302, 145, 72, 234, 197, 125, 145, 29, 153, 29, 28, 55, 29, 254, 159, 49, 197, 243, 145, 72, 234, 197, 125, 145, 29, 50, 29, 245, 142, 250, 186, 55, 94, 164, 25, 198, 253, 186, 164, 94, 55, 25, 29, 255, 159, 180, 197, 245, 198, 197, 252, 198, 254, 145, 186, 164, 94, 55, 25, 94, 256, 159, 303, 145, 29, 257, 159, 180, 197, 245, 198, 251, 186, 55, 94, 164, 25, 94, 256, 159, 302, 145, 29, 258, 159, 257, 198, 252, 151, 223, 28, 180, 197, 257, 198, 252, 145, 29, 257, 159, 257, 198, 254, 29, 10, 197, 241, 94, 255, 72, 234, 197, 241, 72, 79, 72, 99, 145, 94, 247, 159, 235, 145, 29, 10, 197, 242, 94, 257, 72, 234, 197, 242, 72, 79, 72, 99, 145, 94, 247, 159, 236, 145, 29, 151, 223, 55, 29, 10, 197, 244, 94, 258, 72, 234, 197, 244, 72, 79, 72, 99, 145, 94, 247, 159, 236, 145, 29, 153, 29, 28, 55, 29, 10, 197, 244, 94, 258, 72, 234, 197, 244, 72, 79, 72, 99, 145, 145, 29, 50, 29, 245, 2, 251, 186, 55, 94, 164, 25, 198, 257, 186, 164, 94, 55, 25, 29, 237, 2, 217, 198, 218, 29, 238, 2, 217, 198, 218, 29, 239, 2, 217, 198, 219, 29, 240, 2, 217, 198, 219, 29, 241, 2, 217, 198, 218, 29, 242, 2, 217, 198, 219, 29, 244, 2, 217, 198, 197, 219, 151, 223, 28, 303, 145, 29, 243, 2, 217, 198, 197, 219, 151, 223, 28, 303, 145, 29, 68, 29, 151, 224, 55, 29, 259, 159, 206, 65, 229, 198, 218, 198, 219, 65, 197, 228, 198, 220, 65, 66, 197, 302, 94, 220, 145, 186, 55, 94, 164, 25, 145, 198, 219, 65, 197, 227, 198, 221, 65, 66, 197, 302, 94, 221, 145, 186, 164, 94, 55, 25, 145, 29, 10, 197, 259, 94, 245, 72, 234, 197, 259, 72, 79, 72, 99, 145, 94, 247, 159, 235, 186, 55, 94, 164, 25, 143, 236, 186, 164, 94, 55, 25, 145, 29, 153, 29, 46, 197, 145, 29, 260, 159, 146, 197, 186, 220, 94, 221, 25, 94, 79, 159, 125, 145, 29, 237, 159, 201, 65, 197, 232, 198, 217, 65, 231, 145, 198, 218, 65, 228, 198, 220, 65, 66, 197, 302, 94, 220, 145, 29, 238, 159, 202, 65, 197, 232, 198, 217, 65, 231, 145, 198, 218, 65, 228, 198, 220, 65, 66, 197, 302, 94, 220, 145, 29, 239, 159, 203, 65, 197, 232, 198, 217, 65, 231, 145, 198, 219, 65, 227, 198, 221, 65, 66, 197, 302, 94, 221, 145, 29, 151, 223, 55, 29, 243, 159, 204, 65, 197, 232, 198, 217, 65, 231, 145, 198, 219, 65, 227, 198, 221, 65, 66, 197, 302, 94, 221, 145, 29, 153, 29, 28, 55, 29, 243, 159, 204, 65, 232, 198, 217, 65, 231, 29, 50, 29, 240, 159, 208, 65, 197, 232, 198, 217, 65, 231, 145, 198, 219, 65, 227, 198, 221, 65, 66, 197, 302, 94, 221, 145, 29, 261, 159, 209, 65, 197, 197, 227, 198, 137, 65, 232, 145, 198, 217, 65, 231, 145, 198, 218, 65, 228, 198, 220, 65, 66, 197, 302, 94, 220, 145, 29, 241, 159, 210, 65, 197, 197, 227, 198, 137, 65, 232, 145, 198, 217, 65, 231, 145, 198, 218, 65, 228, 198, 220, 65, 66, 197, 302, 94, 220, 145, 29, 242, 159, 211, 65, 197, 197, 228, 198, 137, 65, 232, 145, 198, 217, 65, 231, 145, 198, 219, 65, 227, 198, 221, 65, 66, 197, 302, 94, 221, 145, 29, 151, 224, 55, 29, 262, 159, 235, 186, 55, 94, 164, 25, 143, 236, 186, 164, 94, 55, 25, 29, 263, 159, 205, 65, 229, 198, 218, 198, 219, 65, 197, 228, 198, 220, 65, 66, 197, 302, 94, 220, 145, 186, 55, 94, 164, 25, 145, 198, 219, 65, 197, 227, 198, 221, 65, 66, 197, 302, 94, 221, 145, 186, 164, 94, 55, 25, 145, 29, 260, 142, 49, 197, 263, 94, 247, 159, 262, 94, 248, 159, 302, 145, 72, 234, 197, 125, 145, 29, 153, 29, 116, 249, 132, 5, 197, 302, 94, 216, 145, 55, 29, 255, 159, 49, 197, 241, 94, 247, 159, 235, 94, 248, 159, 302, 145, 72, 234, 197, 125, 145, 29, 257, 159, 49, 197, 242, 94, 247, 159, 236, 94, 248, 159, 302, 145, 72, 234, 197, 125, 145, 29, 255, 2, 180, 197, 257, 186, 164, 94, 55, 25, 198, 260, 94, 256, 159, 303, 145, 29, 10, 197, 241, 94, 255, 72, 234, 197, 241, 72, 79, 72, 99, 145, 94, 247, 159, 235, 145, 29, 251, 159, 49, 197, 238, 94, 247, 159, 235, 94, 248, 159, 302, 145, 72, 234, 197, 125, 145, 29, 252, 159, 49, 197, 239, 94, 247, 159, 236, 94, 248, 159, 302, 145, 72, 234, 197, 125, 145, 29, 253, 159, 49, 197, 240, 94, 247, 159, 236, 94, 248, 159, 302, 145, 72, 234, 197, 125, 145, 29, 151, 223, 55, 29, 254, 159, 49, 197, 243, 94, 247, 159, 236, 94, 248, 159, 302, 145, 72, 234, 197, 125, 145, 29, 153, 29, 28, 55, 29, 254, 159, 49, 197, 243, 145, 72, 234, 197, 125, 145, 29, 50, 29, 252, 22, 254, 29, 260, 142, 251, 186, 55, 94, 164, 25, 198, 252, 186, 164, 94, 55, 25, 29, 264, 159, 260, 198, 253, 186, 164, 94, 55, 25, 29, 265, 159, 180, 197, 264, 94, 256, 159, 303, 145, 198, 214, 29, 10, 197, 261, 94, 265, 72, 234, 197, 261, 72, 79, 72, 99, 145, 94, 247, 159, 235, 145, 29, 238, 142, 217, 198, 218, 29, 239, 142, 217, 198, 219, 29, 240, 142, 217, 198, 219, 29, 261, 142, 217, 198, 218, 29, 241, 142, 217, 198, 218, 29, 242, 142, 217, 198, 219, 29, 243, 142, 217, 198, 197, 219, 151, 223, 28, 303, 145, 29, 68, 29, 3, 29]}, {"code": "def chunk_transform_qk_fwd_kernel(\n    q,\n    k,\n    v,\n    beta,\n    o,\n    A,\n    q_new,\n    k_new,\n    A_local,\n    scale,\n    T,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    BT: tl.constexpr,\n    OUTPUT_ATTENTIONS: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n\n    p_q = tl.make_block_ptr(\n        q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    p_v = tl.make_block_ptr(\n        v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n    )\n    b_q = (tl.load(p_q, boundary_check=(0, 1)) * scale).to(p_q.dtype.element_ty)\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n\n    p_T = tl.make_block_ptr(\n        A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n    b_T = tl.load(p_T, boundary_check=(0, 1))\n\n    o_i = tl.arange(0, BT)\n    m_t = o_i[:, None] >= o_i[None, :]\n    b_qk = tl.where(m_t, tl.dot(b_q, tl.trans(b_k), allow_tf32=False), 0).to(b_q.dtype)\n    m_t = o_i[:, None] > o_i[None, :]\n    b_kk = tl.where(m_t, tl.dot(b_k, tl.trans(b_k), allow_tf32=False), 0).to(b_k.dtype)\n\n    p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)\n\n    b_qkT = tl.dot(b_qk, b_T, allow_tf32=False).to(b_k.dtype)\n\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(\n            A_local + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n        )\n        tl.store(p_a, b_qkT.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n\n    b_kkT = tl.dot(b_kk, b_T, allow_tf32=False).to(b_k.dtype)\n    p_o = tl.make_block_ptr(\n        o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n    )\n    tl.store(p_o, tl.dot(b_qkT, b_v).to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n    p_q_new = tl.make_block_ptr(\n        q_new + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    tl.store(\n        p_q_new,\n        (b_q - tl.dot(b_qkT, b_k_beta, allow_tf32=False)).to(p_q_new.dtype.element_ty),\n        boundary_check=(0, 1),\n    )\n\n    p_k_new = tl.make_block_ptr(\n        k_new + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    b_k_new = b_k - tl.dot(tl.trans(b_kkT), b_k_beta, allow_tf32=False)\n    tl.store(p_k_new, b_k_new.to(p_k_new.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 93, 211, 93, 212, 54, 6, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 145, 54, 29, -1, 218, 93, 219, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 145, 29, 220, 159, 178, 197, 201, 64, 219, 198, 211, 198, 212, 93, 197, 211, 93, 212, 145, 93, 197, 212, 93, 303, 145, 93, 197, 218, 198, 216, 93, 302, 145, 93, 197, 216, 93, 214, 145, 93, 197, 303, 93, 302, 145, 145, 29, 221, 159, 178, 197, 202, 64, 219, 198, 211, 198, 212, 93, 197, 211, 93, 212, 145, 93, 197, 212, 93, 303, 145, 93, 197, 218, 198, 216, 93, 302, 145, 93, 197, 216, 93, 214, 145, 93, 197, 303, 93, 302, 145, 145, 29, 222, 159, 178, 197, 203, 64, 219, 198, 211, 198, 213, 93, 197, 211, 93, 213, 145, 93, 197, 213, 93, 303, 145, 93, 197, 218, 198, 216, 93, 302, 145, 93, 197, 216, 93, 215, 145, 93, 197, 303, 93, 302, 145, 145, 29, 223, 159, 197, 50, 197, 220, 93, 224, 159, 197, 302, 93, 303, 145, 145, 198, 210, 145, 71, 225, 197, 220, 71, 78, 71, 98, 145, 29, 226, 159, 50, 197, 221, 93, 224, 159, 197, 302, 93, 303, 145, 145, 29, 227, 159, 50, 197, 222, 93, 224, 159, 197, 302, 93, 303, 145, 145, 29, 228, 159, 178, 197, 206, 64, 219, 198, 211, 198, 216, 93, 197, 211, 93, 216, 145, 93, 197, 216, 93, 303, 145, 93, 197, 218, 198, 216, 93, 302, 145, 93, 197, 216, 93, 216, 145, 93, 197, 303, 93, 302, 145, 145, 29, 229, 159, 50, 197, 228, 93, 224, 159, 197, 302, 93, 303, 145, 145, 29, 230, 159, 65, 197, 302, 93, 216, 145, 29, 231, 159, 230, 186, 54, 93, 164, 25, 123, 230, 186, 164, 93, 54, 25, 29, 232, 159, 167, 197, 231, 93, 15, 197, 223, 93, 62, 197, 226, 145, 93, 233, 159, 52, 145, 93, 302, 145, 71, 225, 197, 223, 71, 78, 145, 29, 231, 159, 230, 186, 54, 93, 164, 25, 106, 230, 186, 164, 93, 54, 25, 29, 234, 159, 167, 197, 231, 93, 15, 197, 226, 93, 62, 197, 226, 145, 93, 233, 159, 52, 145, 93, 302, 145, 71, 225, 197, 226, 71, 78, 145, 29, 235, 159, 178, 197, 204, 64, 219, 198, 211, 93, 197, 211, 93, 145, 93, 197, 303, 93, 145, 93, 197, 218, 198, 216, 93, 145, 93, 197, 216, 93, 145, 93, 197, 302, 93, 145, 145, 29, 236, 159, 50, 197, 235, 93, 224, 159, 197, 302, 93, 145, 145, 29, 237, 159, 197, 226, 198, 236, 186, 54, 93, 164, 25, 145, 71, 225, 197, 226, 71, 78, 145, 29, 238, 159, 15, 197, 232, 93, 229, 93, 233, 159, 52, 145, 71, 225, 197, 226, 71, 78, 145, 29, 151, 217, 54, 29, 239, 159, 178, 197, 209, 64, 219, 198, 211, 198, 216, 93, 197, 211, 93, 216, 145, 93, 197, 216, 93, 303, 145, 93, 197, 218, 198, 216, 93, 302, 145, 93, 197, 216, 93, 216, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 239, 93, 238, 71, 225, 197, 239, 71, 78, 71, 98, 145, 93, 224, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 240, 159, 15, 197, 234, 93, 229, 93, 233, 159, 52, 145, 71, 225, 197, 226, 71, 78, 145, 29, 241, 159, 178, 197, 205, 64, 219, 198, 211, 198, 213, 93, 197, 211, 93, 213, 145, 93, 197, 213, 93, 303, 145, 93, 197, 218, 198, 216, 93, 302, 145, 93, 197, 216, 93, 215, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 241, 93, 15, 197, 238, 93, 227, 145, 71, 225, 197, 241, 71, 78, 71, 98, 145, 93, 224, 159, 197, 302, 93, 303, 145, 145, 29, 242, 159, 178, 197, 207, 64, 219, 198, 211, 198, 212, 93, 197, 211, 93, 212, 145, 93, 197, 212, 93, 303, 145, 93, 197, 218, 198, 216, 93, 302, 145, 93, 197, 216, 93, 214, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 242, 93, 197, 223, 4, 15, 197, 238, 93, 237, 93, 233, 159, 52, 145, 145, 71, 225, 197, 242, 71, 78, 71, 98, 145, 93, 224, 159, 197, 302, 93, 303, 145, 145, 29, 243, 159, 178, 197, 208, 64, 219, 198, 211, 198, 212, 93, 197, 211, 93, 212, 145, 93, 197, 212, 93, 303, 145, 93, 197, 218, 198, 216, 93, 302, 145, 93, 197, 216, 93, 214, 145, 93, 197, 303, 93, 302, 145, 145, 29, 244, 159, 226, 4, 15, 197, 62, 197, 240, 145, 93, 237, 93, 233, 159, 52, 145, 29, 10, 197, 243, 93, 244, 71, 225, 197, 243, 71, 78, 71, 98, 145, 93, 224, 159, 197, 302, 93, 303, 145, 145, 29, 3, 29]}, {"code": "def save_intra_chunk_attn(\n    A,\n    A_local,\n    T,\n    BT: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_A = tl.make_block_ptr(\n        A + i_bh * T * T, (T, T), (T, 1), (i_t * BT, i_t * BT), (BT, BT), (1, 0)\n    )\n    p_A_local = tl.make_block_ptr(\n        A_local + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n    b_A_local = tl.load(p_A_local, boundary_check=(0, 1))\n    tl.store(p_A, b_A_local.to(p_A.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 55, 6, 145, 55, 29, -1, 205, 94, 206, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 145, 29, 207, 159, 178, 197, 201, 65, 206, 198, 203, 198, 203, 94, 197, 203, 94, 203, 145, 94, 197, 203, 94, 303, 145, 94, 197, 205, 198, 204, 94, 205, 198, 204, 145, 94, 197, 204, 94, 204, 145, 94, 197, 303, 94, 302, 145, 145, 29, 208, 159, 178, 197, 202, 65, 206, 198, 203, 198, 204, 94, 197, 203, 94, 204, 145, 94, 197, 204, 94, 303, 145, 94, 197, 205, 198, 204, 94, 302, 145, 94, 197, 204, 94, 204, 145, 94, 197, 303, 94, 302, 145, 145, 29, 209, 159, 49, 197, 208, 94, 210, 159, 197, 302, 94, 303, 145, 145, 29, 10, 197, 207, 94, 209, 72, 211, 197, 207, 72, 79, 72, 99, 145, 94, 210, 159, 197, 302, 94, 303, 145, 145, 29, 3, 29]}, {"code": "def parallel_delta_rule_fwd_kernel(\n    q,\n    k,\n    k2,\n    v,\n    beta,\n    o,\n    o_new,\n    attn,\n    T,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    OUTPUT_ATTENTIONS: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_q = tl.make_block_ptr(\n        q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n\n    b_q = tl.zeros([BT, BK], dtype=tl.float32)\n    b_q += tl.load(p_q, boundary_check=(0, 1))\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    p_o = tl.make_block_ptr(\n        o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n    )\n    b_o += tl.load(p_o, boundary_check=(0, 1))\n\n    for offset in range((i_t + 1) * BT - 2 * BS, i_t * BT - BS, -BS):\n        p_k = tl.make_block_ptr(\n            k + i_bh * T * K, (K, T), (1, K), (0, offset), (BK, BS), (0, 1)\n        )\n        p_k2 = tl.make_block_ptr(\n            k2 + i_bh * T * K, (T, K), (K, 1), (offset, 0), (BS, BK), (1, 0)\n        )\n        p_v = tl.make_block_ptr(\n            v + i_bh * T * V, (T, V), (V, 1), (offset, 0), (BS, BV), (1, 0)\n        )\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (offset,), (BS,), (0,))\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_beta = tl.load(p_beta, boundary_check=(0,))\n\n        m_s = tl.arange(0, BT) >= (offset - i_t * BT + BS)\n        b_s = tl.dot(b_q.to(b_k.dtype), b_k, allow_tf32=False)\n        b_s = tl.where(m_s[:, None], b_s, 0)\n\n        b_o += tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        b_k2 = (tl.load(p_k2, boundary_check=(0, 1)) * b_beta[:, None]).to(b_v.dtype)\n        b_q -= tl.dot(b_s.to(b_v.dtype), b_k2, allow_tf32=False)\n\n        if OUTPUT_ATTENTIONS:\n            p_a = tl.make_block_ptr(\n                attn + i_bh * T * T,\n                (T, T),\n                (T, 1),\n                (i_t * BT, offset),\n                (BT, BS),\n                (1, 0),\n            )\n            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n\n    for offset in range(i_t * BT - BS, -BS, -BS):\n        p_k = tl.make_block_ptr(\n            k + i_bh * T * K, (K, T), (1, K), (0, offset), (BK, BS), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + i_bh * T * V, (T, V), (V, 1), (offset, 0), (BS, BV), (1, 0)\n        )\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (offset,), (BS,), (0,))\n        p_k2 = tl.make_block_ptr(\n            k2 + i_bh * T * K, (T, K), (K, 1), (offset, 0), (BS, BK), (1, 0)\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_beta = tl.load(p_beta, boundary_check=(0,))\n\n        b_s = tl.dot(b_q.to(b_k.dtype), b_k, allow_tf32=False)\n\n        b_o += tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        b_k2 = (tl.load(p_k2, boundary_check=(0, 1)) * b_beta[:, None]).to(b_v.dtype)\n        b_q -= tl.dot(b_s.to(b_v.dtype), b_k2, allow_tf32=False).to(b_q.dtype)\n\n        if OUTPUT_ATTENTIONS:\n            p_a = tl.make_block_ptr(\n                attn + i_bh * T * T,\n                (T, T),\n                (T, 1),\n                (i_t * BT, offset),\n                (BT, BS),\n                (1, 0),\n            )\n            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n\n    p_o_new = tl.make_block_ptr(\n        o_new + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n    )\n    tl.store(p_o_new, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 54, 6, 93, 211, 54, 6, 93, 212, 54, 6, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 145, 54, 29, -1, 217, 93, 218, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 145, 29, 219, 159, 178, 197, 201, 64, 218, 198, 209, 198, 210, 93, 197, 209, 93, 210, 145, 93, 197, 210, 93, 303, 145, 93, 197, 217, 198, 212, 93, 302, 145, 93, 197, 212, 93, 214, 145, 93, 197, 303, 93, 302, 145, 145, 29, 220, 159, 146, 197, 186, 212, 93, 214, 25, 93, 78, 159, 124, 145, 29, 220, 142, 50, 197, 219, 93, 221, 159, 197, 302, 93, 303, 145, 145, 29, 222, 159, 146, 197, 186, 212, 93, 215, 25, 93, 78, 159, 124, 145, 29, 223, 159, 178, 197, 206, 64, 218, 198, 209, 198, 211, 93, 197, 209, 93, 211, 145, 93, 197, 211, 93, 303, 145, 93, 197, 217, 198, 212, 93, 302, 145, 93, 197, 212, 93, 215, 145, 93, 197, 303, 93, 302, 145, 145, 29, 222, 142, 50, 197, 223, 93, 221, 159, 197, 302, 93, 303, 145, 145, 29, 115, 224, 132, 5, 197, 197, 217, 64, 303, 145, 198, 212, 4, 304, 198, 213, 93, 217, 198, 212, 4, 213, 93, 4, 213, 145, 54, 29, 225, 159, 178, 197, 202, 64, 218, 198, 209, 198, 210, 93, 197, 210, 93, 209, 145, 93, 197, 303, 93, 210, 145, 93, 197, 302, 93, 224, 145, 93, 197, 214, 93, 213, 145, 93, 197, 302, 93, 303, 145, 145, 29, 226, 159, 178, 197, 203, 64, 218, 198, 209, 198, 210, 93, 197, 209, 93, 210, 145, 93, 197, 210, 93, 303, 145, 93, 197, 224, 93, 302, 145, 93, 197, 213, 93, 214, 145, 93, 197, 303, 93, 302, 145, 145, 29, 227, 159, 178, 197, 204, 64, 218, 198, 209, 198, 211, 93, 197, 209, 93, 211, 145, 93, 197, 211, 93, 303, 145, 93, 197, 224, 93, 302, 145, 93, 197, 213, 93, 215, 145, 93, 197, 303, 93, 302, 145, 145, 29, 228, 159, 178, 197, 205, 64, 218, 198, 209, 93, 197, 209, 93, 145, 93, 197, 303, 93, 145, 93, 197, 224, 93, 145, 93, 197, 213, 93, 145, 93, 197, 302, 93, 145, 145, 29, 229, 159, 50, 197, 225, 93, 221, 159, 197, 302, 93, 303, 145, 145, 29, 230, 159, 50, 197, 227, 93, 221, 159, 197, 302, 93, 303, 145, 145, 29, 231, 159, 50, 197, 228, 93, 221, 159, 197, 302, 93, 145, 145, 29, 232, 159, 65, 197, 302, 93, 212, 145, 123, 224, 4, 217, 198, 212, 64, 213, 29, 233, 159, 15, 197, 220, 71, 234, 197, 229, 71, 78, 145, 93, 229, 93, 235, 159, 52, 145, 29, 233, 159, 167, 197, 232, 186, 54, 93, 164, 25, 93, 233, 93, 302, 145, 29, 222, 142, 15, 197, 233, 71, 234, 197, 230, 71, 78, 145, 93, 230, 93, 235, 159, 52, 145, 29, 236, 159, 197, 50, 197, 226, 93, 221, 159, 197, 302, 93, 303, 145, 145, 198, 231, 186, 54, 93, 164, 25, 145, 71, 234, 197, 230, 71, 78, 145, 29, 220, 2, 15, 197, 233, 71, 234, 197, 230, 71, 78, 145, 93, 236, 93, 235, 159, 52, 145, 29, 151, 216, 54, 29, 237, 159, 178, 197, 208, 64, 218, 198, 209, 198, 209, 93, 197, 209, 93, 209, 145, 93, 197, 209, 93, 303, 145, 93, 197, 217, 198, 212, 93, 224, 145, 93, 197, 212, 93, 213, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 237, 93, 233, 71, 234, 197, 237, 71, 78, 71, 98, 145, 93, 221, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 67, 29, 115, 224, 132, 5, 197, 217, 198, 212, 4, 213, 93, 4, 213, 93, 4, 213, 145, 54, 29, 225, 159, 178, 197, 202, 64, 218, 198, 209, 198, 210, 93, 197, 210, 93, 209, 145, 93, 197, 303, 93, 210, 145, 93, 197, 302, 93, 224, 145, 93, 197, 214, 93, 213, 145, 93, 197, 302, 93, 303, 145, 145, 29, 227, 159, 178, 197, 204, 64, 218, 198, 209, 198, 211, 93, 197, 209, 93, 211, 145, 93, 197, 211, 93, 303, 145, 93, 197, 224, 93, 302, 145, 93, 197, 213, 93, 215, 145, 93, 197, 303, 93, 302, 145, 145, 29, 228, 159, 178, 197, 205, 64, 218, 198, 209, 93, 197, 209, 93, 145, 93, 197, 303, 93, 145, 93, 197, 224, 93, 145, 93, 197, 213, 93, 145, 93, 197, 302, 93, 145, 145, 29, 226, 159, 178, 197, 203, 64, 218, 198, 209, 198, 210, 93, 197, 209, 93, 210, 145, 93, 197, 210, 93, 303, 145, 93, 197, 224, 93, 302, 145, 93, 197, 213, 93, 214, 145, 93, 197, 303, 93, 302, 145, 145, 29, 229, 159, 50, 197, 225, 93, 221, 159, 197, 302, 93, 303, 145, 145, 29, 230, 159, 50, 197, 227, 93, 221, 159, 197, 302, 93, 303, 145, 145, 29, 231, 159, 50, 197, 228, 93, 221, 159, 197, 302, 93, 145, 145, 29, 233, 159, 15, 197, 220, 71, 234, 197, 229, 71, 78, 145, 93, 229, 93, 235, 159, 52, 145, 29, 222, 142, 15, 197, 233, 71, 234, 197, 230, 71, 78, 145, 93, 230, 93, 235, 159, 52, 145, 29, 236, 159, 197, 50, 197, 226, 93, 221, 159, 197, 302, 93, 303, 145, 145, 198, 231, 186, 54, 93, 164, 25, 145, 71, 234, 197, 230, 71, 78, 145, 29, 220, 2, 15, 197, 233, 71, 234, 197, 230, 71, 78, 145, 93, 236, 93, 235, 159, 52, 145, 71, 234, 197, 220, 71, 78, 145, 29, 151, 216, 54, 29, 237, 159, 178, 197, 208, 64, 218, 198, 209, 198, 209, 93, 197, 209, 93, 209, 145, 93, 197, 209, 93, 303, 145, 93, 197, 217, 198, 212, 93, 224, 145, 93, 197, 212, 93, 213, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 237, 93, 233, 71, 234, 197, 237, 71, 78, 71, 98, 145, 93, 221, 159, 197, 302, 93, 303, 145, 145, 29, 153, 29, 67, 29, 238, 159, 178, 197, 207, 64, 218, 198, 209, 198, 211, 93, 197, 209, 93, 211, 145, 93, 197, 211, 93, 303, 145, 93, 197, 217, 198, 212, 93, 302, 145, 93, 197, 212, 93, 215, 145, 93, 197, 303, 93, 302, 145, 145, 29, 10, 197, 238, 93, 222, 71, 234, 197, 223, 71, 78, 71, 98, 145, 93, 221, 159, 197, 302, 93, 303, 145, 145, 29, 3, 29]}, {"code": "def recompute_w_u_fwd_kernel(\n    k,\n    v,\n    beta,\n    w,\n    u,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    p_beta = tl.make_block_ptr(\n        beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)\n    )\n    p_A = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_u = tl.make_block_ptr(\n            u + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_vb = (b_v * b_beta[:, None]).to(b_v.dtype)\n        b_u = tl.dot(b_A.to(b_vb.dtype), b_vb, allow_tf32=False)\n        tl.store(p_u, (b_u).to(p_u.dtype.element_ty), boundary_check=(0, 1))\n\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_w = tl.make_block_ptr(\n            w + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = (b_k * b_beta[:, None]).to(b_k.dtype)\n        b_w = tl.dot(b_A.to(b_kb.dtype), b_kb, allow_tf32=False)\n        tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 94, 209, 94, 210, 55, 6, 94, 211, 55, 6, 94, 212, 55, 6, 94, 213, 55, 6, 94, 214, 55, 6, 94, 215, 55, 6, 94, 216, 55, 6, 145, 55, 29, -1, 217, 94, 218, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 145, 29, 219, 94, 220, 159, 197, 218, 44, 210, 94, 218, 183, 210, 145, 29, 151, 216, 55, 29, 221, 94, 217, 159, 197, 49, 197, 208, 65, 217, 198, 304, 145, 72, 222, 197, 54, 145, 94, 49, 197, 208, 65, 217, 198, 304, 65, 303, 145, 72, 222, 197, 54, 145, 145, 29, 223, 94, 224, 159, 197, 49, 197, 207, 65, 221, 145, 72, 222, 197, 54, 145, 94, 49, 197, 207, 65, 221, 65, 303, 145, 72, 222, 197, 54, 145, 145, 29, 209, 159, 224, 4, 223, 29, 153, 29, 28, 55, 29, 223, 94, 224, 159, 197, 219, 198, 209, 94, 219, 198, 209, 65, 209, 145, 29, 50, 29, 225, 159, 178, 197, 203, 65, 223, 198, 210, 65, 220, 94, 197, 209, 94, 145, 94, 197, 210, 94, 145, 94, 197, 217, 198, 213, 94, 145, 94, 197, 213, 94, 145, 94, 197, 302, 94, 145, 145, 29, 226, 159, 178, 197, 206, 65, 197, 223, 198, 210, 65, 220, 145, 198, 213, 94, 197, 209, 94, 213, 145, 94, 197, 210, 198, 213, 94, 303, 145, 94, 197, 217, 198, 213, 94, 302, 145, 94, 197, 213, 94, 213, 145, 94, 197, 303, 94, 302, 145, 145, 29, 227, 159, 49, 197, 225, 94, 228, 159, 197, 302, 94, 145, 145, 29, 229, 159, 49, 197, 226, 94, 228, 159, 197, 302, 94, 303, 145, 145, 29, 116, 230, 132, 5, 197, 57, 197, 212, 94, 215, 145, 145, 55, 29, 231, 159, 178, 197, 202, 65, 197, 223, 198, 210, 65, 220, 145, 198, 212, 94, 197, 209, 94, 212, 145, 94, 197, 210, 198, 212, 94, 303, 145, 94, 197, 217, 198, 213, 94, 230, 198, 215, 145, 94, 197, 213, 94, 215, 145, 94, 197, 303, 94, 302, 145, 145, 29, 232, 159, 178, 197, 205, 65, 197, 223, 198, 210, 65, 220, 145, 198, 212, 94, 197, 209, 94, 212, 145, 94, 197, 210, 198, 212, 94, 303, 145, 94, 197, 217, 198, 213, 94, 230, 198, 215, 145, 94, 197, 213, 94, 215, 145, 94, 197, 303, 94, 302, 145, 145, 29, 233, 159, 49, 197, 231, 94, 228, 159, 197, 302, 94, 303, 145, 145, 29, 234, 159, 197, 233, 198, 227, 186, 55, 94, 164, 25, 145, 72, 222, 197, 233, 72, 79, 145, 29, 235, 159, 15, 197, 229, 72, 222, 197, 234, 72, 79, 145, 94, 234, 94, 236, 159, 52, 145, 29, 10, 197, 232, 94, 235, 72, 222, 197, 232, 72, 79, 72, 99, 145, 94, 228, 159, 197, 302, 94, 303, 145, 145, 29, 68, 29, 116, 237, 132, 5, 197, 57, 197, 211, 94, 214, 145, 145, 55, 29, 238, 159, 178, 197, 201, 65, 197, 223, 198, 210, 65, 220, 145, 198, 211, 94, 197, 209, 94, 211, 145, 94, 197, 210, 198, 211, 94, 303, 145, 94, 197, 217, 198, 213, 94, 237, 198, 214, 145, 94, 197, 213, 94, 214, 145, 94, 197, 303, 94, 302, 145, 145, 29, 239, 159, 178, 197, 204, 65, 197, 223, 198, 210, 65, 220, 145, 198, 211, 94, 197, 209, 94, 211, 145, 94, 197, 210, 198, 211, 94, 303, 145, 94, 197, 217, 198, 213, 94, 237, 198, 214, 145, 94, 197, 213, 94, 214, 145, 94, 197, 303, 94, 302, 145, 145, 29, 240, 159, 49, 197, 238, 94, 228, 159, 197, 302, 94, 303, 145, 145, 29, 241, 159, 197, 240, 198, 227, 186, 55, 94, 164, 25, 145, 72, 222, 197, 240, 72, 79, 145, 29, 242, 159, 15, 197, 229, 72, 222, 197, 241, 72, 79, 145, 94, 241, 94, 236, 159, 52, 145, 29, 10, 197, 239, 94, 242, 72, 222, 197, 239, 72, 79, 72, 99, 145, 94, 228, 159, 197, 302, 94, 303, 145, 145, 29, 68, 29, 3, 29]}, {"code": "def prepare_wy_repr_bwd_kernel(\n    k,\n    v,\n    beta,\n    A,\n    dw,\n    du,\n    dk,\n    dv,\n    dbeta,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    p_beta = tl.make_block_ptr(\n        beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)\n    )\n    p_A = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1)\n    )\n\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n\n    b_dbeta = tl.zeros([BT], dtype=tl.float32)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dv = tl.make_block_ptr(\n            dv + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_du = tl.make_block_ptr(\n            du + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v_beta = (b_v * b_beta[:, None]).to(b_v.dtype)\n        b_du = tl.load(p_du, boundary_check=(0, 1))\n        b_dA += tl.dot(b_du, tl.trans(b_v_beta), allow_tf32=False)\n        b_dv_beta = tl.dot(b_A, b_du, allow_tf32=False)\n        b_dv = b_dv_beta * b_beta[:, None]\n        b_dbeta += tl.sum(b_dv_beta * b_v, 1)\n\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dk = tl.make_block_ptr(\n            dk + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dw = tl.make_block_ptr(\n            dw + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)\n        b_dw = tl.load(p_dw, boundary_check=(0, 1))\n        b_dA += tl.dot(b_dw, tl.trans(b_k_beta), allow_tf32=False)\n        b_dk_beta = tl.dot(b_A, b_dw, allow_tf32=False)\n        b_dk = b_dk_beta * b_beta[:, None]\n        b_dbeta += tl.sum(b_dk_beta * b_k, 1)\n\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n\n    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dA, 0)\n    b_dA = tl.dot(b_dA.to(b_A.dtype), b_A)\n    b_dA = tl.dot(b_A, b_dA.to(b_A.dtype))\n    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dA, 0).to(\n        k.dtype.element_ty\n    )\n\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dk = tl.make_block_ptr(\n            dk + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_dk = tl.load(p_dk, boundary_check=(0, 1))\n        b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)\n\n        b_dk_beta = tl.dot(b_dA, b_k, allow_tf32=False)\n        b_dbeta += tl.sum(b_dk_beta * b_k, 1)\n        b_dk += tl.dot(tl.trans(b_dA), b_k_beta, allow_tf32=False)\n        b_dk += b_dk_beta * b_beta[:, None]\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n\n    p_dbeta = tl.make_block_ptr(\n        dbeta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)\n    )\n    tl.store(p_dbeta, b_dbeta.to(p_dbeta.dtype.element_ty), boundary_check=(0,))", "encoded": [27, 301, 197, 201, 93, 202, 93, 203, 93, 204, 93, 205, 93, 206, 93, 207, 93, 208, 93, 209, 93, 210, 93, 211, 93, 212, 93, 213, 54, 6, 93, 214, 54, 6, 93, 215, 54, 6, 93, 216, 54, 6, 93, 217, 54, 6, 93, 218, 54, 6, 93, 219, 54, 6, 145, 54, 29, -1, 220, 93, 221, 159, 197, 141, 197, 302, 145, 93, 141, 197, 303, 145, 93, 141, 197, 304, 145, 145, 29, 222, 93, 223, 159, 197, 221, 44, 213, 93, 221, 183, 213, 145, 29, 151, 219, 54, 29, 224, 93, 220, 159, 197, 50, 197, 211, 64, 220, 198, 304, 145, 71, 225, 197, 200, 145, 93, 50, 197, 211, 64, 220, 198, 304, 64, 303, 145, 71, 225, 197, 200, 145, 145, 29, 226, 93, 227, 159, 197, 50, 197, 210, 64, 224, 145, 71, 225, 197, 200, 145, 93, 50, 197, 210, 64, 224, 64, 303, 145, 71, 225, 197, 200, 145, 145, 29, 212, 159, 227, 4, 226, 29, 153, 29, 28, 54, 29, 226, 93, 227, 159, 197, 222, 198, 212, 93, 222, 198, 212, 64, 212, 145, 29, 49, 29, 228, 159, 178, 197, 203, 64, 226, 198, 213, 64, 223, 93, 197, 212, 93, 145, 93, 197, 213, 93, 145, 93, 197, 220, 198, 216, 93, 145, 93, 197, 216, 93, 145, 93, 197, 302, 93, 145, 145, 29, 229, 159, 178, 197, 204, 64, 197, 226, 198, 213, 64, 223, 145, 198, 216, 93, 197, 216, 93, 212, 145, 93, 197, 303, 93, 213, 198, 216, 145, 93, 197, 302, 93, 220, 198, 216, 145, 93, 197, 216, 93, 216, 145, 93, 197, 302, 93, 303, 145, 145, 29, 230, 159, 50, 197, 228, 93, 231, 159, 197, 302, 93, 145, 145, 29, 232, 159, 50, 197, 229, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 233, 159, 146, 197, 186, 216, 25, 93, 78, 159, 124, 145, 29, 234, 159, 146, 197, 186, 216, 93, 216, 25, 93, 78, 159, 124, 145, 29, 115, 235, 132, 5, 197, 56, 197, 215, 93, 218, 145, 145, 54, 29, 236, 159, 178, 197, 202, 64, 197, 226, 198, 213, 64, 223, 145, 198, 215, 93, 197, 212, 93, 215, 145, 93, 197, 213, 198, 215, 93, 303, 145, 93, 197, 220, 198, 216, 93, 235, 198, 218, 145, 93, 197, 216, 93, 218, 145, 93, 197, 303, 93, 302, 145, 145, 29, 237, 159, 178, 197, 208, 64, 197, 226, 198, 213, 64, 223, 145, 198, 215, 93, 197, 212, 93, 215, 145, 93, 197, 213, 198, 215, 93, 303, 145, 93, 197, 220, 198, 216, 93, 235, 198, 218, 145, 93, 197, 216, 93, 218, 145, 93, 197, 303, 93, 302, 145, 145, 29, 238, 159, 178, 197, 206, 64, 197, 226, 198, 213, 64, 223, 145, 198, 215, 93, 197, 212, 93, 215, 145, 93, 197, 213, 198, 215, 93, 303, 145, 93, 197, 220, 198, 216, 93, 235, 198, 218, 145, 93, 197, 216, 93, 218, 145, 93, 197, 303, 93, 302, 145, 145, 29, 239, 159, 50, 197, 236, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 240, 159, 197, 239, 198, 230, 186, 54, 93, 164, 25, 145, 71, 225, 197, 239, 71, 78, 145, 29, 241, 159, 50, 197, 238, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 234, 142, 15, 197, 241, 93, 62, 197, 240, 145, 93, 242, 159, 52, 145, 29, 243, 159, 15, 197, 232, 93, 241, 93, 242, 159, 52, 145, 29, 244, 159, 243, 198, 230, 186, 54, 93, 164, 25, 29, 233, 142, 180, 197, 243, 198, 239, 93, 303, 145, 29, 10, 197, 237, 93, 244, 71, 225, 197, 237, 71, 78, 71, 98, 145, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 67, 29, 115, 245, 132, 5, 197, 56, 197, 214, 93, 217, 145, 145, 54, 29, 246, 159, 178, 197, 201, 64, 197, 226, 198, 213, 64, 223, 145, 198, 214, 93, 197, 212, 93, 214, 145, 93, 197, 213, 198, 214, 93, 303, 145, 93, 197, 220, 198, 216, 93, 245, 198, 217, 145, 93, 197, 216, 93, 217, 145, 93, 197, 303, 93, 302, 145, 145, 29, 247, 159, 178, 197, 207, 64, 197, 226, 198, 213, 64, 223, 145, 198, 214, 93, 197, 212, 93, 214, 145, 93, 197, 213, 198, 214, 93, 303, 145, 93, 197, 220, 198, 216, 93, 245, 198, 217, 145, 93, 197, 216, 93, 217, 145, 93, 197, 303, 93, 302, 145, 145, 29, 248, 159, 178, 197, 205, 64, 197, 226, 198, 213, 64, 223, 145, 198, 214, 93, 197, 212, 93, 214, 145, 93, 197, 213, 198, 214, 93, 303, 145, 93, 197, 220, 198, 216, 93, 245, 198, 217, 145, 93, 197, 216, 93, 217, 145, 93, 197, 303, 93, 302, 145, 145, 29, 249, 159, 50, 197, 246, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 250, 159, 197, 249, 198, 230, 186, 54, 93, 164, 25, 145, 71, 225, 197, 249, 71, 78, 145, 29, 251, 159, 50, 197, 248, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 234, 142, 15, 197, 251, 93, 62, 197, 250, 145, 93, 242, 159, 52, 145, 29, 252, 159, 15, 197, 232, 93, 251, 93, 242, 159, 52, 145, 29, 253, 159, 252, 198, 230, 186, 54, 93, 164, 25, 29, 233, 142, 180, 197, 252, 198, 249, 93, 303, 145, 29, 10, 197, 247, 93, 253, 71, 225, 197, 247, 71, 78, 71, 98, 145, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 67, 29, 234, 159, 167, 197, 65, 197, 302, 93, 216, 145, 186, 54, 93, 164, 25, 106, 65, 197, 302, 93, 216, 145, 186, 164, 93, 54, 25, 93, 234, 93, 302, 145, 29, 234, 159, 15, 197, 234, 71, 225, 197, 232, 71, 78, 145, 93, 232, 145, 29, 234, 159, 15, 197, 232, 93, 234, 71, 225, 197, 232, 71, 78, 145, 145, 29, 234, 159, 167, 197, 65, 197, 302, 93, 216, 145, 186, 54, 93, 164, 25, 106, 65, 197, 302, 93, 216, 145, 186, 164, 93, 54, 25, 93, 4, 234, 93, 302, 145, 71, 225, 197, 201, 71, 78, 71, 98, 145, 29, 115, 245, 132, 5, 197, 56, 197, 214, 93, 217, 145, 145, 54, 29, 246, 159, 178, 197, 201, 64, 197, 226, 198, 213, 64, 223, 145, 198, 214, 93, 197, 212, 93, 214, 145, 93, 197, 213, 198, 214, 93, 303, 145, 93, 197, 220, 198, 216, 93, 245, 198, 217, 145, 93, 197, 216, 93, 217, 145, 93, 197, 303, 93, 302, 145, 145, 29, 247, 159, 178, 197, 207, 64, 197, 226, 198, 213, 64, 223, 145, 198, 214, 93, 197, 212, 93, 214, 145, 93, 197, 213, 198, 214, 93, 303, 145, 93, 197, 220, 198, 216, 93, 245, 198, 217, 145, 93, 197, 216, 93, 217, 145, 93, 197, 303, 93, 302, 145, 145, 29, 249, 159, 50, 197, 246, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 253, 159, 50, 197, 247, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 250, 159, 197, 249, 198, 230, 186, 54, 93, 164, 25, 145, 71, 225, 197, 249, 71, 78, 145, 29, 252, 159, 15, 197, 234, 93, 249, 93, 242, 159, 52, 145, 29, 233, 142, 180, 197, 252, 198, 249, 93, 303, 145, 29, 253, 142, 15, 197, 62, 197, 234, 145, 93, 250, 93, 242, 159, 52, 145, 29, 253, 142, 252, 198, 230, 186, 54, 93, 164, 25, 29, 10, 197, 247, 93, 253, 71, 225, 197, 247, 71, 78, 71, 98, 145, 93, 231, 159, 197, 302, 93, 303, 145, 145, 29, 67, 29, 254, 159, 178, 197, 209, 64, 226, 198, 213, 64, 223, 93, 197, 212, 93, 145, 93, 197, 213, 93, 145, 93, 197, 220, 198, 216, 93, 145, 93, 197, 216, 93, 145, 93, 197, 302, 93, 145, 145, 29, 10, 197, 254, 93, 233, 71, 225, 197, 254, 71, 78, 71, 98, 145, 93, 231, 159, 197, 302, 93, 145, 145, 29, 3, 29]}, {"code": "def fused_recurrent_gated_delta_rule_fwd_kernel(\n    q,\n    k,\n    v,\n    g,\n    beta,\n    o,\n    h0,\n    ht,\n    cu_seqlens,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    HV: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_BETA_HEADWISE: tl.constexpr,\n    USE_QK_L2NORM_IN_KERNEL: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_hv = i_nh // HV, i_nh % HV\n    i_h = i_hv // (HV // H)\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        all = T\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        all = B * T\n    o_k = i_k * BK + tl.arange(0, BK)\n    o_v = i_v * BV + tl.arange(0, BV)\n\n    p_q = q + (bos * H + i_h) * K + o_k\n    p_k = k + (bos * H + i_h) * K + o_k\n    p_v = v + (bos * HV + i_hv) * V + o_v\n    if IS_BETA_HEADWISE:\n        p_beta = beta + (bos * HV + i_hv) * V + o_v\n    else:\n        p_beta = beta + bos * HV + i_hv\n    p_g = g + bos * HV + i_hv\n    p_o = o + ((i_k * all + bos) * HV + i_hv) * V + o_v\n\n    mask_k = o_k < K\n    mask_v = o_v < V\n    mask_h = mask_k[:, None] & mask_v[None, :]\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32)\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_g = tl.load(p_g).to(tl.float32)\n\n        if USE_QK_L2NORM_IN_KERNEL:\n            b_q = b_q / (tl.sqrt(tl.sum(b_q * b_q)) + 1e-6)\n            b_k = b_k / (tl.sqrt(tl.sum(b_k * b_k)) + 1e-6)\n        b_q = b_q * scale\n\n        b_h *= exp(b_g)\n\n        b_v -= tl.sum(b_h * b_k[:, None], 0)\n        if IS_BETA_HEADWISE:\n            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)\n        else:\n            b_beta = tl.load(p_beta).to(tl.float32)\n        b_v *= b_beta\n\n        b_h += b_k[:, None] * b_v[None, :]\n\n        b_o = tl.sum(b_h * b_q[:, None], 0)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n\n        p_q += H * K\n        p_k += H * K\n        p_o += HV * V\n        p_v += HV * V\n        p_g += HV\n        p_beta += HV * (V if IS_BETA_HEADWISE else 1)\n\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_nh * K * V + o_k[:, None] * V + o_v[None, :]\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)", "encoded": [27, 301, 197, 201, 94, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 94, 209, 94, 210, 94, 211, 94, 212, 55, 6, 94, 213, 55, 6, 94, 214, 55, 6, 94, 215, 55, 6, 94, 216, 55, 6, 94, 217, 55, 6, 94, 218, 55, 6, 94, 219, 55, 6, 94, 220, 55, 6, 94, 221, 55, 6, 94, 222, 55, 6, 94, 223, 55, 6, 145, 55, 29, -1, 224, 94, 225, 94, 226, 159, 197, 141, 197, 302, 145, 94, 141, 197, 303, 145, 94, 141, 197, 304, 145, 145, 29, 227, 94, 228, 159, 197, 226, 44, 214, 94, 226, 183, 214, 145, 29, 229, 159, 228, 44, 197, 214, 44, 213, 145, 29, 151, 223, 55, 29, 230, 94, 231, 159, 197, 49, 197, 209, 65, 227, 145, 72, 232, 197, 150, 145, 94, 49, 197, 209, 65, 227, 65, 303, 145, 72, 232, 197, 150, 145, 145, 29, 137, 159, 211, 29, 211, 159, 231, 4, 230, 29, 153, 29, 28, 55, 29, 230, 94, 231, 159, 197, 227, 198, 211, 94, 227, 198, 211, 65, 211, 145, 29, 137, 159, 212, 198, 211, 29, 50, 29, 233, 159, 224, 198, 217, 65, 66, 197, 302, 94, 217, 145, 29, 234, 159, 225, 198, 218, 65, 66, 197, 302, 94, 218, 145, 29, 235, 159, 201, 65, 197, 230, 198, 213, 65, 229, 145, 198, 215, 65, 233, 29, 236, 159, 202, 65, 197, 230, 198, 213, 65, 229, 145, 198, 215, 65, 233, 29, 237, 159, 203, 65, 197, 230, 198, 214, 65, 228, 145, 198, 216, 65, 234, 29, 151, 221, 55, 29, 238, 159, 205, 65, 197, 230, 198, 214, 65, 228, 145, 198, 216, 65, 234, 29, 153, 29, 28, 55, 29, 238, 159, 205, 65, 230, 198, 214, 65, 228, 29, 50, 29, 239, 159, 204, 65, 230, 198, 214, 65, 228, 29, 240, 159, 206, 65, 197, 197, 224, 198, 137, 65, 230, 145, 198, 214, 65, 228, 145, 198, 216, 65, 234, 29, 241, 159, 233, 1, 215, 29, 242, 159, 234, 1, 216, 29, 243, 159, 241, 186, 55, 94, 164, 25, 143, 242, 186, 164, 94, 55, 25, 29, 244, 159, 146, 197, 186, 217, 94, 218, 25, 94, 79, 159, 125, 145, 29, 151, 219, 55, 29, 245, 159, 207, 65, 226, 198, 215, 198, 216, 65, 233, 186, 55, 94, 164, 25, 198, 216, 65, 234, 186, 164, 94, 55, 25, 29, 244, 142, 49, 197, 245, 94, 246, 159, 243, 94, 247, 159, 302, 145, 72, 232, 197, 125, 145, 29, 153, 29, 116, 248, 132, 5, 197, 302, 94, 211, 145, 55, 29, 249, 159, 49, 197, 235, 94, 246, 159, 241, 94, 247, 159, 302, 145, 72, 232, 197, 125, 145, 29, 250, 159, 49, 197, 236, 94, 246, 159, 241, 94, 247, 159, 302, 145, 72, 232, 197, 125, 145, 29, 251, 159, 49, 197, 237, 94, 246, 159, 242, 94, 247, 159, 302, 145, 72, 232, 197, 125, 145, 29, 252, 159, 49, 197, 239, 145, 72, 232, 197, 125, 145, 29, 151, 222, 55, 29, 249, 159, 249, 39, 197, 111, 197, 180, 197, 249, 198, 249, 145, 145, 65, 305, 145, 29, 250, 159, 250, 39, 197, 111, 197, 180, 197, 250, 198, 250, 145, 145, 65, 305, 145, 29, 153, 29, 249, 159, 249, 198, 210, 29, 244, 22, 169, 197, 252, 145, 29, 251, 2, 180, 197, 244, 198, 250, 186, 55, 94, 164, 25, 94, 302, 145, 29, 151, 221, 55, 29, 253, 159, 49, 197, 238, 94, 246, 159, 242, 94, 247, 159, 302, 145, 72, 232, 197, 125, 145, 29, 153, 29, 28, 55, 29, 253, 159, 49, 197, 238, 145, 72, 232, 197, 125, 145, 29, 50, 29, 251, 22, 253, 29, 244, 142, 250, 186, 55, 94, 164, 25, 198, 251, 186, 164, 94, 55, 25, 29, 254, 159, 180, 197, 244, 198, 249, 186, 55, 94, 164, 25, 94, 302, 145, 29, 10, 197, 240, 94, 254, 72, 232, 197, 240, 72, 79, 72, 99, 145, 94, 246, 159, 242, 145, 29, 235, 142, 213, 198, 215, 29, 236, 142, 213, 198, 215, 29, 240, 142, 214, 198, 216, 29, 237, 142, 214, 198, 216, 29, 239, 142, 214, 29, 238, 142, 214, 198, 197, 216, 151, 221, 28, 303, 145, 29, 68, 29, 151, 220, 55, 29, 255, 159, 208, 65, 226, 198, 215, 198, 216, 65, 233, 186, 55, 94, 164, 25, 198, 216, 65, 234, 186, 164, 94, 55, 25, 29, 10, 197, 255, 94, 244, 72, 232, 197, 255, 72, 79, 72, 99, 145, 94, 246, 159, 243, 145, 29, 153, 29, 3, 29]}, {"code": "def prepare_wy_repr_bwd_kernel(\n    k,\n    v,\n    beta,\n    g,\n    A,\n    dw,\n    du,\n    dk,\n    dv,\n    dbeta,\n    dg,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    p_beta = tl.make_block_ptr(\n        beta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)\n    )\n    p_g = tl.make_block_ptr(g + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))\n    p_A = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1)\n    )\n\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_g_exp = tl.exp(b_g)\n\n    b_dbeta = tl.zeros([BT], dtype=tl.float32)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    b_dg = tl.zeros([BT], dtype=tl.float32)\n\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dk = tl.make_block_ptr(\n            dk + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dw = tl.make_block_ptr(\n            dw + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k_beta_g = (b_k * b_beta[:, None] * b_g_exp[:, None]).to(b_k.dtype)\n        b_dw = tl.load(p_dw, boundary_check=(0, 1))\n        b_dA += tl.dot(b_dw, tl.trans(b_k_beta_g))\n        b_dk_beta_g = tl.dot(b_A, b_dw)\n        b_dk = b_dk_beta_g * b_beta[:, None] * b_g_exp[:, None]\n        b_dbeta += tl.sum(b_dk_beta_g * b_k * b_g_exp[:, None], 1)\n        b_dg += tl.sum(b_dk_beta_g * b_k * b_g_exp[:, None] * b_beta[:, None], 1)\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dv = tl.make_block_ptr(\n            dv + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_du = tl.make_block_ptr(\n            du + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v_beta = (b_v * b_beta[:, None]).to(b_v.dtype)\n        b_du = tl.load(p_du, boundary_check=(0, 1))\n        b_dA += tl.dot(b_du, tl.trans(b_v_beta))\n        b_dv_beta = tl.dot(b_A, b_du)\n        b_dv = b_dv_beta * b_beta[:, None]\n        b_dbeta += tl.sum(b_dv_beta * b_v, 1)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n\n    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dA, 0)\n    b_dA = tl.dot(b_dA.to(b_A.dtype), b_A)\n    b_dA = tl.dot(b_A, b_dA.to(b_A.dtype))\n    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dA, 0).to(\n        k.dtype.element_ty\n    )\n\n    b_dA *= safe_exp(b_g[:, None] - b_g[None, :])\n    b_dA = b_dA.to(k.dtype.element_ty)\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dk = tl.make_block_ptr(\n            dk + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_dk = tl.load(p_dk, boundary_check=(0, 1))\n        b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)\n        b_A += tl.dot(b_k_beta, tl.trans(b_k))\n        b_dk_beta = tl.dot(b_dA, b_k)\n        b_dbeta += tl.sum(b_dk_beta * b_k, 1)\n        b_dk += tl.dot(tl.trans(b_dA), b_k_beta)\n        b_dk += b_dk_beta * b_beta[:, None]\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n\n    b_dA_A = b_dA * b_A\n    b_dg += tl.sum(b_dA_A, axis=1) - tl.sum(b_dA_A, axis=0)\n    p_dg = tl.make_block_ptr(dg + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))\n    p_dbeta = tl.make_block_ptr(\n        dbeta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)\n    )\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))\n    tl.store(p_dbeta, b_dbeta.to(p_dbeta.dtype.element_ty), boundary_check=(0,))", "encoded": [27, 302, 198, 202, 94, 203, 94, 204, 94, 205, 94, 206, 94, 207, 94, 208, 94, 209, 94, 210, 94, 211, 94, 212, 94, 213, 94, 214, 94, 215, 94, 216, 54, 6, 94, 217, 54, 6, 94, 218, 54, 6, 94, 219, 54, 6, 94, 220, 54, 6, 94, 221, 54, 6, 94, 222, 54, 6, 146, 54, 29, -1, 223, 94, 224, 160, 198, 142, 198, 303, 146, 94, 142, 198, 304, 146, 146, 29, 225, 94, 226, 160, 198, 224, 44, 216, 94, 224, 184, 216, 146, 29, 152, 222, 54, 29, 227, 94, 223, 160, 198, 50, 198, 214, 64, 223, 199, 305, 146, 71, 228, 198, 201, 146, 94, 50, 198, 214, 64, 223, 199, 305, 64, 304, 146, 71, 228, 198, 201, 146, 146, 29, 229, 94, 230, 160, 198, 50, 198, 213, 64, 227, 146, 71, 228, 198, 201, 146, 94, 50, 198, 213, 64, 227, 64, 304, 146, 71, 228, 198, 201, 146, 146, 29, 215, 160, 230, 4, 229, 29, 154, 29, 28, 54, 29, 229, 94, 230, 160, 198, 225, 199, 215, 94, 225, 199, 215, 64, 215, 146, 29, 49, 29, 231, 160, 179, 198, 204, 64, 198, 229, 199, 216, 64, 226, 146, 94, 198, 215, 94, 146, 94, 198, 216, 94, 146, 94, 198, 223, 199, 219, 94, 146, 94, 198, 219, 94, 146, 94, 198, 303, 94, 146, 146, 29, 232, 160, 179, 198, 205, 64, 198, 229, 199, 216, 64, 226, 146, 94, 198, 215, 94, 146, 94, 198, 216, 94, 146, 94, 198, 223, 199, 219, 94, 146, 94, 198, 219, 94, 146, 94, 198, 303, 94, 146, 146, 29, 233, 160, 179, 198, 206, 64, 198, 229, 199, 216, 64, 226, 146, 199, 219, 94, 198, 219, 94, 215, 146, 94, 198, 304, 94, 216, 199, 219, 146, 94, 198, 303, 94, 223, 199, 219, 146, 94, 198, 219, 94, 219, 146, 94, 198, 303, 94, 304, 146, 146, 29, 234, 160, 50, 198, 233, 94, 235, 160, 198, 303, 94, 304, 146, 146, 29, 236, 160, 50, 198, 231, 94, 235, 160, 198, 303, 94, 146, 146, 29, 237, 160, 50, 198, 232, 94, 235, 160, 198, 303, 94, 146, 146, 29, 238, 160, 93, 198, 237, 146, 29, 239, 160, 147, 198, 187, 219, 25, 94, 78, 160, 125, 146, 29, 240, 160, 147, 198, 187, 219, 94, 219, 25, 94, 78, 160, 125, 146, 29, 241, 160, 147, 198, 187, 219, 25, 94, 78, 160, 125, 146, 29, 116, 242, 133, 5, 198, 56, 198, 217, 94, 220, 146, 146, 54, 29, 243, 160, 179, 198, 202, 64, 198, 229, 199, 216, 64, 226, 146, 199, 217, 94, 198, 215, 94, 217, 146, 94, 198, 216, 199, 217, 94, 304, 146, 94, 198, 223, 199, 219, 94, 242, 199, 220, 146, 94, 198, 219, 94, 220, 146, 94, 198, 304, 94, 303, 146, 146, 29, 244, 160, 179, 198, 209, 64, 198, 229, 199, 216, 64, 226, 146, 199, 217, 94, 198, 215, 94, 217, 146, 94, 198, 216, 199, 217, 94, 304, 146, 94, 198, 223, 199, 219, 94, 242, 199, 220, 146, 94, 198, 219, 94, 220, 146, 94, 198, 304, 94, 303, 146, 146, 29, 245, 160, 179, 198, 207, 64, 198, 229, 199, 216, 64, 226, 146, 199, 217, 94, 198, 215, 94, 217, 146, 94, 198, 216, 199, 217, 94, 304, 146, 94, 198, 223, 199, 219, 94, 242, 199, 220, 146, 94, 198, 219, 94, 220, 146, 94, 198, 304, 94, 303, 146, 146, 29, 246, 160, 50, 198, 243, 94, 235, 160, 198, 303, 94, 304, 146, 146, 29, 247, 160, 198, 246, 199, 236, 187, 54, 94, 165, 25, 199, 238, 187, 54, 94, 165, 25, 146, 71, 228, 198, 246, 71, 78, 146, 29, 248, 160, 50, 198, 245, 94, 235, 160, 198, 303, 94, 304, 146, 146, 29, 240, 143, 15, 198, 248, 94, 62, 198, 247, 146, 146, 29, 249, 160, 15, 198, 234, 94, 248, 146, 29, 250, 160, 249, 199, 236, 187, 54, 94, 165, 25, 199, 238, 187, 54, 94, 165, 25, 29, 239, 143, 181, 198, 249, 199, 246, 199, 238, 187, 54, 94, 165, 25, 94, 304, 146, 29, 241, 143, 181, 198, 249, 199, 246, 199, 238, 187, 54, 94, 165, 25, 199, 236, 187, 54, 94, 165, 25, 94, 304, 146, 29, 10, 198, 244, 94, 250, 71, 228, 198, 244, 71, 78, 71, 99, 146, 94, 235, 160, 198, 303, 94, 304, 146, 146, 29, 67, 29, 116, 251, 133, 5, 198, 56, 198, 218, 94, 221, 146, 146, 54, 29, 252, 160, 179, 198, 203, 64, 198, 229, 199, 216, 64, 226, 146, 199, 218, 94, 198, 215, 94, 218, 146, 94, 198, 216, 199, 218, 94, 304, 146, 94, 198, 223, 199, 219, 94, 251, 199, 221, 146, 94, 198, 219, 94, 221, 146, 94, 198, 304, 94, 303, 146, 146, 29, 253, 160, 179, 198, 210, 64, 198, 229, 199, 216, 64, 226, 146, 199, 218, 94, 198, 215, 94, 218, 146, 94, 198, 216, 199, 218, 94, 304, 146, 94, 198, 223, 199, 219, 94, 251, 199, 221, 146, 94, 198, 219, 94, 221, 146, 94, 198, 304, 94, 303, 146, 146, 29, 254, 160, 179, 198, 208, 64, 198, 229, 199, 216, 64, 226, 146, 199, 218, 94, 198, 215, 94, 218, 146, 94, 198, 216, 199, 218, 94, 304, 146, 94, 198, 223, 199, 219, 94, 251, 199, 221, 146, 94, 198, 219, 94, 221, 146, 94, 198, 304, 94, 303, 146, 146, 29, 255, 160, 50, 198, 252, 94, 235, 160, 198, 303, 94, 304, 146, 146, 29, 256, 160, 198, 255, 199, 236, 187, 54, 94, 165, 25, 146, 71, 228, 198, 255, 71, 78, 146, 29, 257, 160, 50, 198, 254, 94, 235, 160, 198, 303, 94, 304, 146, 146, 29, 240, 143, 15, 198, 257, 94, 62, 198, 256, 146, 146, 29, 258, 160, 15, 198, 234, 94, 257, 146, 29, 259, 160, 258, 199, 236, 187, 54, 94, 165, 25, 29, 239, 143, 181, 198, 258, 199, 255, 94, 304, 146, 29, 10, 198, 253, 94, 259, 71, 228, 198, 253, 71, 78, 71, 99, 146, 94, 235, 160, 198, 303, 94, 304, 146, 146, 29, 67, 29, 240, 160, 168, 198, 65, 198, 303, 94, 219, 146, 187, 54, 94, 165, 25, 107, 65, 198, 303, 94, 219, 146, 187, 165, 94, 54, 25, 94, 240, 94, 303, 146, 29, 240, 160, 15, 198, 240, 71, 228, 198, 234, 71, 78, 146, 94, 234, 146, 29, 240, 160, 15, 198, 234, 94, 240, 71, 228, 198, 234, 71, 78, 146, 146, 29, 240, 160, 168, 198, 65, 198, 303, 94, 219, 146, 187, 54, 94, 165, 25, 107, 65, 198, 303, 94, 219, 146, 187, 165, 94, 54, 25, 94, 4, 240, 94, 303, 146, 71, 228, 198, 202, 71, 78, 71, 99, 146, 29, 240, 22, 260, 198, 237, 187, 54, 94, 165, 25, 4, 237, 187, 165, 94, 54, 25, 146, 29, 240, 160, 240, 71, 228, 198, 202, 71, 78, 71, 99, 146, 29, 234, 160, 147, 198, 187, 219, 94, 219, 25, 94, 78, 160, 125, 146, 29, 116, 242, 133, 5, 198, 56, 198, 217, 94, 220, 146, 146, 54, 29, 243, 160, 179, 198, 202, 64, 198, 229, 199, 216, 64, 226, 146, 199, 217, 94, 198, 215, 94, 217, 146, 94, 198, 216, 199, 217, 94, 304, 146, 94, 198, 223, 199, 219, 94, 242, 199, 220, 146, 94, 198, 219, 94, 220, 146, 94, 198, 304, 94, 303, 146, 146, 29, 244, 160, 179, 198, 209, 64, 198, 229, 199, 216, 64, 226, 146, 199, 217, 94, 198, 215, 94, 217, 146, 94, 198, 216, 199, 217, 94, 304, 146, 94, 198, 223, 199, 219, 94, 242, 199, 220, 146, 94, 198, 219, 94, 220, 146, 94, 198, 304, 94, 303, 146, 146, 29, 246, 160, 50, 198, 243, 94, 235, 160, 198, 303, 94, 304, 146, 146, 29, 250, 160, 50, 198, 244, 94, 235, 160, 198, 303, 94, 304, 146, 146, 29, 261, 160, 198, 246, 199, 236, 187, 54, 94, 165, 25, 146, 71, 228, 198, 246, 71, 78, 146, 29, 234, 143, 15, 198, 261, 94, 62, 198, 246, 146, 146, 29, 262, 160, 15, 198, 240, 94, 246, 146, 29, 239, 143, 181, 198, 262, 199, 246, 94, 304, 146, 29, 250, 143, 15, 198, 62, 198, 240, 146, 94, 261, 146, 29, 250, 143, 262, 199, 236, 187, 54, 94, 165, 25, 29, 10, 198, 244, 94, 250, 71, 228, 198, 244, 71, 78, 71, 99, 146, 94, 235, 160, 198, 303, 94, 304, 146, 146, 29, 67, 29, 263, 160, 240, 199, 234, 29, 241, 143, 181, 198, 263, 94, 264, 160, 304, 146, 4, 181, 198, 263, 94, 264, 160, 303, 146, 29, 265, 160, 179, 198, 212, 64, 198, 229, 199, 216, 64, 226, 146, 94, 198, 215, 94, 146, 94, 198, 216, 94, 146, 94, 198, 223, 199, 219, 94, 146, 94, 198, 219, 94, 146, 94, 198, 303, 94, 146, 146, 29, 266, 160, 179, 198, 211, 64, 198, 229, 199, 216, 64, 226, 146, 94, 198, 215, 94, 146, 94, 198, 216, 94, 146, 94, 198, 223, 199, 219, 94, 146, 94, 198, 219, 94, 146, 94, 198, 303, 94, 146, 146, 29, 10, 198, 265, 94, 241, 71, 228, 198, 265, 71, 78, 71, 99, 146, 94, 235, 160, 198, 303, 94, 146, 146, 29, 10, 198, 266, 94, 239, 71, 228, 198, 266, 71, 78, 71, 99, 146, 94, 235, 160, 198, 303, 94, 146, 146, 29, 3, 29]}, {"code": "def recompute_w_u_fwd_kernel(\n    k,\n    v,\n    beta,\n    w,\n    u,\n    A,\n    g,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n    p_beta = tl.make_block_ptr(\n        beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)\n    )\n    p_g = tl.make_block_ptr(g + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))\n    p_A = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_g = tl.exp(tl.load(p_g, boundary_check=(0,)))\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_u = tl.make_block_ptr(\n            u + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_vb = (b_v * b_beta[:, None]).to(b_v.dtype)\n        b_u = tl.dot(b_A, b_vb, allow_tf32=False)\n        tl.store(p_u, b_u.to(p_u.dtype.element_ty), boundary_check=(0, 1))\n\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_w = tl.make_block_ptr(\n            w + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = (b_k * b_beta[:, None] * b_g[:, None]).to(b_k.dtype)\n        b_w = tl.dot(b_A, b_kb)\n        tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))", "encoded": [27, 302, 198, 202, 95, 203, 95, 204, 95, 205, 95, 206, 95, 207, 95, 208, 95, 209, 95, 210, 95, 211, 95, 212, 55, 6, 95, 213, 55, 6, 95, 214, 55, 6, 95, 215, 55, 6, 95, 216, 55, 6, 95, 217, 55, 6, 95, 218, 55, 6, 146, 55, 29, -1, 219, 95, 220, 160, 198, 142, 198, 303, 146, 95, 142, 198, 304, 146, 146, 29, 221, 95, 222, 160, 198, 220, 44, 212, 95, 220, 184, 212, 146, 29, 152, 218, 55, 29, 223, 95, 219, 160, 198, 49, 198, 210, 65, 219, 199, 305, 146, 72, 224, 198, 54, 146, 95, 49, 198, 210, 65, 219, 199, 305, 65, 304, 146, 72, 224, 198, 54, 146, 146, 29, 225, 95, 226, 160, 198, 49, 198, 209, 65, 223, 146, 72, 224, 198, 54, 146, 95, 49, 198, 209, 65, 223, 65, 304, 146, 72, 224, 198, 54, 146, 146, 29, 211, 160, 226, 4, 225, 29, 154, 29, 28, 55, 29, 225, 95, 226, 160, 198, 221, 199, 211, 95, 221, 199, 211, 65, 211, 146, 29, 50, 29, 227, 160, 179, 198, 204, 65, 225, 199, 212, 65, 222, 95, 198, 211, 95, 146, 95, 198, 212, 95, 146, 95, 198, 219, 199, 215, 95, 146, 95, 198, 215, 95, 146, 95, 198, 303, 95, 146, 146, 29, 228, 160, 179, 198, 208, 65, 198, 225, 199, 212, 65, 222, 146, 95, 198, 211, 95, 146, 95, 198, 212, 95, 146, 95, 198, 219, 199, 215, 95, 146, 95, 198, 215, 95, 146, 95, 198, 303, 95, 146, 146, 29, 229, 160, 179, 198, 207, 65, 198, 225, 199, 212, 65, 222, 146, 199, 215, 95, 198, 211, 95, 215, 146, 95, 198, 212, 199, 215, 95, 304, 146, 95, 198, 219, 199, 215, 95, 303, 146, 95, 198, 215, 95, 215, 146, 95, 198, 304, 95, 303, 146, 146, 29, 230, 160, 49, 198, 227, 95, 231, 160, 198, 303, 95, 146, 146, 29, 232, 160, 49, 198, 229, 95, 231, 160, 198, 303, 95, 304, 146, 146, 29, 233, 160, 94, 198, 49, 198, 228, 95, 231, 160, 198, 303, 95, 146, 146, 146, 29, 117, 234, 133, 5, 198, 57, 198, 214, 95, 217, 146, 146, 55, 29, 235, 160, 179, 198, 203, 65, 198, 225, 199, 212, 65, 222, 146, 199, 214, 95, 198, 211, 95, 214, 146, 95, 198, 212, 199, 214, 95, 304, 146, 95, 198, 219, 199, 215, 95, 234, 199, 217, 146, 95, 198, 215, 95, 217, 146, 95, 198, 304, 95, 303, 146, 146, 29, 236, 160, 179, 198, 206, 65, 198, 225, 199, 212, 65, 222, 146, 199, 214, 95, 198, 211, 95, 214, 146, 95, 198, 212, 199, 214, 95, 304, 146, 95, 198, 219, 199, 215, 95, 234, 199, 217, 146, 95, 198, 215, 95, 217, 146, 95, 198, 304, 95, 303, 146, 146, 29, 237, 160, 49, 198, 235, 95, 231, 160, 198, 303, 95, 304, 146, 146, 29, 238, 160, 198, 237, 199, 230, 187, 55, 95, 165, 25, 146, 72, 224, 198, 237, 72, 79, 146, 29, 239, 160, 15, 198, 232, 95, 238, 95, 240, 160, 52, 146, 29, 10, 198, 236, 95, 239, 72, 224, 198, 236, 72, 79, 72, 100, 146, 95, 231, 160, 198, 303, 95, 304, 146, 146, 29, 68, 29, 117, 241, 133, 5, 198, 57, 198, 213, 95, 216, 146, 146, 55, 29, 242, 160, 179, 198, 202, 65, 198, 225, 199, 212, 65, 222, 146, 199, 213, 95, 198, 211, 95, 213, 146, 95, 198, 212, 199, 213, 95, 304, 146, 95, 198, 219, 199, 215, 95, 241, 199, 216, 146, 95, 198, 215, 95, 216, 146, 95, 198, 304, 95, 303, 146, 146, 29, 243, 160, 179, 198, 205, 65, 198, 225, 199, 212, 65, 222, 146, 199, 213, 95, 198, 211, 95, 213, 146, 95, 198, 212, 199, 213, 95, 304, 146, 95, 198, 219, 199, 215, 95, 241, 199, 216, 146, 95, 198, 215, 95, 216, 146, 95, 198, 304, 95, 303, 146, 146, 29, 244, 160, 49, 198, 242, 95, 231, 160, 198, 303, 95, 304, 146, 146, 29, 245, 160, 198, 244, 199, 230, 187, 55, 95, 165, 25, 199, 233, 187, 55, 95, 165, 25, 146, 72, 224, 198, 244, 72, 79, 146, 29, 246, 160, 15, 198, 232, 95, 245, 146, 29, 10, 198, 243, 95, 246, 72, 224, 198, 243, 72, 79, 72, 100, 146, 95, 231, 160, 198, 303, 95, 304, 146, 146, 29, 68, 29, 3, 29]}, {"code": "def chunk_dplr_bwd_kernel_intra(\n    q,\n    k,\n    a,\n    b,\n    gi,\n    ge,\n    dAqk,\n    dAqb,\n    dAak,\n    dAab,\n    dq,\n    dk,\n    da,\n    db,\n    dqg,\n    dkg,\n    dag,\n    dbg,\n    dgk,\n    dgk_offset,\n    cu_seqlens,\n    chunk_indices,\n    scale: tl.constexpr,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    GATHER_SUPPORTED: tl.constexpr,\n):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = (i_b * T).to(tl.int32), (i_b * T + T).to(tl.int32)\n\n    if i_t * BT >= T:\n        return\n\n    ge += (bos * H + i_h) * K\n    gi += (bos * H + i_h) * K\n    q += (bos * H + i_h) * K\n    a += (bos * H + i_h) * K\n    b += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    dq += (bos * H + i_h) * K\n    dk += (bos * H + i_h) * K\n    da += (bos * H + i_h) * K\n    db += (bos * H + i_h) * K\n    dqg += (bos * H + i_h) * K\n    dag += (bos * H + i_h) * K\n    dkg += (bos * H + i_h) * K\n    dbg += (bos * H + i_h) * K\n    dgk += (bos * H + i_h) * K\n    dgk_offset += (bos * H + i_h) * K\n    dAqk += (bos * H + i_h) * BT\n    dAqb += (bos * H + i_h) * BT\n    dAak += (bos * H + i_h) * BT\n    dAab += (bos * H + i_h) * BT\n\n    stride_qk = H * K\n    stride_A = H * BT\n\n    p_ge = tl.make_block_ptr(\n        ge, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_gi = tl.make_block_ptr(\n        gi, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n\n    b_ge = tl.load(p_ge, boundary_check=(0, 1))\n    b_gi = tl.load(p_gi, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    b_da = tl.zeros([BC, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    b_db = tl.zeros([BC, BK], dtype=tl.float32)\n\n    p_dAqk = tl.make_block_ptr(\n        dAqk, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0)\n    )\n    p_dAab = tl.make_block_ptr(\n        dAab, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0)\n    )\n    p_dAqb = tl.make_block_ptr(\n        dAqb, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0)\n    )\n    p_dAak = tl.make_block_ptr(\n        dAak, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0)\n    )\n    o_i = tl.arange(0, BC)\n    p_k = tl.make_block_ptr(\n        k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_b = tl.make_block_ptr(\n        b, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_a = tl.make_block_ptr(\n        a, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_q = tl.make_block_ptr(\n        q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_b = tl.load(p_b, boundary_check=(0, 1))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_a = tl.load(p_a, boundary_check=(0, 1))\n    b_dAqk = tl.load(p_dAqk, boundary_check=(0, 1))\n    b_dAab = tl.load(p_dAab, boundary_check=(0, 1))\n    b_dAqb = tl.load(p_dAqb, boundary_check=(0, 1))\n    b_dAak = tl.load(p_dAak, boundary_check=(0, 1))\n\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n\n    for j in range(0, min(BC, T - i_t * BT)):\n\n        if GATHER_SUPPORTED:\n            row_idx = tl.full([1, BK], j, dtype=tl.int16)\n            col_idx = tl.full([BC, 1], j, dtype=tl.int16)\n            row_idx_bc = tl.full([1, BC], j, dtype=tl.int16)\n\n            b_kj = gather(b_k, row_idx, axis=0)\n            b_bj = gather(b_b, row_idx, axis=0)\n            b_gij = gather(b_gi, row_idx, axis=0)\n            b_gej = gather(b_ge, row_idx, axis=0)\n            b_qj = gather(b_q, row_idx, axis=0)\n            b_aj = gather(b_a, row_idx, axis=0)\n\n            b_dAqk_j = gather(b_dAqk, col_idx, axis=1)\n            b_dAab_j = gather(b_dAab, col_idx, axis=1)\n            b_dAqb_j = gather(b_dAqb, col_idx, axis=1)\n            b_dAak_j = gather(b_dAak, col_idx, axis=1)\n\n            b_dA_qk_j = tl.sum(gather(b_dAqk, row_idx_bc, axis=0), 0)[:, None]\n            b_dA_qk_j = tl.sum(gather(b_dAqk, row_idx_bc, axis=0), 0)[:, None]\n            b_dA_ab_j = tl.sum(gather(b_dAab, row_idx_bc, axis=0), 0)[:, None]\n            b_dA_qb_j = tl.sum(gather(b_dAqb, row_idx_bc, axis=0), 0)[:, None]\n            b_dA_ak_j = tl.sum(gather(b_dAak, row_idx_bc, axis=0), 0)[:, None]\n        else:\n            mask_idx = tl.arange(0, BC) == j\n            b_kj = tl.sum(tl.where(mask_idx[:, None], b_k, 0), 0)[None, :]\n            b_bj = tl.sum(tl.where(mask_idx[:, None], b_b, 0), 0)[None, :]\n            b_gij = tl.sum(tl.where(mask_idx[:, None], b_gi, 0), 0)[None, :]\n            b_gej = tl.sum(tl.where(mask_idx[:, None], b_ge, 0), 0)[None, :]\n            b_dAqk_j = tl.sum(tl.where(mask_idx[None, :], b_dAqk, 0), 1)[:, None]\n            b_dAab_j = tl.sum(tl.where(mask_idx[None, :], b_dAab, 0), 1)[:, None]\n            b_dAqb_j = tl.sum(tl.where(mask_idx[None, :], b_dAqb, 0), 1)[:, None]\n            b_dAak_j = tl.sum(tl.where(mask_idx[None, :], b_dAak, 0), 1)[:, None]\n            b_dA_qk_j = tl.sum(tl.where(mask_idx[:, None], b_dAqk, 0), 0)[:, None]\n            b_dA_ab_j = tl.sum(tl.where(mask_idx[:, None], b_dAab, 0), 0)[:, None]\n            b_dA_qb_j = tl.sum(tl.where(mask_idx[:, None], b_dAqb, 0), 0)[:, None]\n            b_dA_ak_j = tl.sum(tl.where(mask_idx[:, None], b_dAak, 0), 0)[:, None]\n\n            b_qj = tl.sum(tl.where(mask_idx[:, None], b_q, 0), 0)[None, :]\n            b_aj = tl.sum(tl.where(mask_idx[:, None], b_a, 0), 0)[None, :]\n\n        m_e = o_i[:, None] > j\n        m_i = o_i[:, None] >= j\n        tmp1 = exp(b_gi - b_gij)\n        tmp2 = exp(b_ge - b_gij)\n        b_dq += tl.where(m_i, b_dAqk_j * b_kj * tmp1, 0.0)\n        b_dq += tl.where(m_i, b_dAqb_j * b_bj * tmp1, 0.0)\n        b_da += tl.where(m_e, b_dAab_j * b_bj * tmp2, 0.0)\n        b_da += tl.where(m_e, b_dAak_j * b_kj * tmp2, 0.0)\n\n        m_i = o_i[:, None] <= j\n        m_e = o_i[:, None] < j\n        tmp1 = exp(b_gij - b_gi)\n        tmp2 = exp(b_gej - b_gi)\n        b_dk += tl.where(m_i, b_dA_qk_j * b_qj * tmp1, 0.0)\n        b_dk += tl.where(m_e, b_dA_ak_j * b_aj * tmp2, 0.0)\n        b_db += tl.where(m_i, b_dA_qb_j * b_qj * tmp1, 0.0)\n        b_db += tl.where(m_e, b_dA_ab_j * b_aj * tmp2, 0.0)\n\n    p_dq = tl.make_block_ptr(\n        dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_dk = tl.make_block_ptr(\n        dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_da = tl.make_block_ptr(\n        da, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_db = tl.make_block_ptr(\n        db, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_dgk = tl.make_block_ptr(\n        dgk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_dgk_offset = tl.make_block_ptr(\n        dgk_offset, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_dqg = tl.make_block_ptr(\n        dqg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_dkg = tl.make_block_ptr(\n        dkg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_dag = tl.make_block_ptr(\n        dag, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_dbg = tl.make_block_ptr(\n        dbg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)\n    )\n    p_gn = gi + (min(i_t * BT + BT, T) - 1) * stride_qk + o_k\n    p_gn = tl.max_contiguous(tl.multiple_of(p_gn, BK), BK)\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    b_da += tl.load(p_dag, boundary_check=(0, 1)) * exp(b_ge)\n    b_dq += tl.load(p_dqg, boundary_check=(0, 1)) * exp(b_gi) * scale\n    tmp = exp(b_gn[None, :] - b_gi)\n    b_dk += tl.load(p_dkg, boundary_check=(0, 1)).to(tl.float32) * tmp\n    b_db += tl.load(p_dbg, boundary_check=(0, 1)).to(tl.float32) * tmp\n    tl.store(p_dq, (b_dq).to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_da, b_da.to(p_da.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0, 1))\n    b_dgk = (b_dq * b_q + b_da * b_a - b_dk * b_k - b_db * b_b).to(tl.float32)\n    b_dgk_offset = b_da * b_a\n    tl.store(p_dgk, b_dgk.to(p_dgk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(\n        p_dgk_offset,\n        b_dgk_offset.to(p_dgk_offset.dtype.element_ty),\n        boundary_check=(0, 1),\n    )", "encoded": [28, 305, 201, 205, 96, 206, 96, 207, 96, 208, 96, 209, 96, 210, 96, 211, 96, 212, 96, 213, 96, 214, 96, 215, 96, 216, 96, 217, 96, 218, 96, 219, 96, 220, 96, 221, 96, 222, 96, 223, 96, 224, 96, 225, 96, 226, 96, 227, 56, 6, 96, 228, 96, 229, 56, 6, 96, 230, 56, 6, 96, 231, 56, 6, 96, 232, 56, 6, 96, 233, 56, 6, 96, 234, 56, 6, 96, 235, 56, 6, 148, 56, 30, -1, 236, 96, 237, 96, 238, 162, 201, 144, 201, 306, 148, 96, 144, 201, 307, 148, 96, 144, 201, 308, 148, 148, 30, 239, 96, 240, 162, 201, 238, 45, 229, 96, 238, 187, 229, 148, 30, 154, 234, 56, 30, 241, 96, 237, 162, 201, 51, 201, 226, 66, 237, 202, 308, 148, 73, 242, 201, 204, 148, 96, 51, 201, 226, 66, 237, 202, 308, 66, 307, 148, 73, 242, 201, 204, 148, 148, 30, 243, 96, 244, 162, 201, 51, 201, 225, 66, 241, 148, 73, 242, 201, 204, 148, 96, 51, 201, 225, 66, 241, 66, 307, 148, 73, 242, 201, 204, 148, 148, 30, 228, 162, 244, 4, 243, 30, 156, 30, 29, 56, 30, 243, 96, 244, 162, 201, 201, 239, 202, 228, 148, 73, 242, 201, 204, 148, 96, 201, 239, 202, 228, 66, 228, 148, 73, 242, 201, 204, 148, 148, 30, 50, 30, 154, 237, 202, 231, 126, 228, 56, 30, 191, 30, 156, 30, 210, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 209, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 205, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 207, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 208, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 206, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 215, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 216, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 217, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 218, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 219, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 221, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 220, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 222, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 223, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 224, 145, 201, 243, 202, 229, 66, 240, 148, 202, 230, 30, 211, 145, 201, 243, 202, 229, 66, 240, 148, 202, 231, 30, 212, 145, 201, 243, 202, 229, 66, 240, 148, 202, 231, 30, 213, 145, 201, 243, 202, 229, 66, 240, 148, 202, 231, 30, 214, 145, 201, 243, 202, 229, 66, 240, 148, 202, 231, 30, 245, 162, 229, 202, 230, 30, 246, 162, 229, 202, 231, 30, 247, 162, 182, 201, 210, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 248, 162, 182, 201, 209, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 249, 162, 51, 201, 247, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 251, 162, 51, 201, 248, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 252, 162, 149, 201, 190, 232, 96, 233, 26, 96, 80, 162, 127, 148, 30, 253, 162, 149, 201, 190, 232, 96, 233, 26, 96, 80, 162, 127, 148, 30, 254, 162, 149, 201, 190, 232, 96, 233, 26, 96, 80, 162, 127, 148, 30, 255, 162, 149, 201, 190, 232, 96, 233, 26, 96, 80, 162, 127, 148, 30, 256, 162, 182, 201, 211, 96, 201, 228, 96, 231, 148, 96, 201, 246, 96, 307, 148, 96, 201, 237, 202, 231, 96, 306, 148, 96, 201, 232, 96, 232, 148, 96, 201, 307, 96, 306, 148, 148, 30, 257, 162, 182, 201, 214, 96, 201, 228, 96, 231, 148, 96, 201, 246, 96, 307, 148, 96, 201, 237, 202, 231, 96, 306, 148, 96, 201, 232, 96, 232, 148, 96, 201, 307, 96, 306, 148, 148, 30, 258, 162, 182, 201, 212, 96, 201, 228, 96, 231, 148, 96, 201, 246, 96, 307, 148, 96, 201, 237, 202, 231, 96, 306, 148, 96, 201, 232, 96, 232, 148, 96, 201, 307, 96, 306, 148, 148, 30, 259, 162, 182, 201, 213, 96, 201, 228, 96, 231, 148, 96, 201, 246, 96, 307, 148, 96, 201, 237, 202, 231, 96, 306, 148, 96, 201, 232, 96, 232, 148, 96, 201, 307, 96, 306, 148, 148, 30, 260, 162, 67, 201, 306, 96, 232, 148, 30, 261, 162, 182, 201, 206, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 262, 162, 182, 201, 208, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 263, 162, 182, 201, 207, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 264, 162, 182, 201, 205, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 265, 162, 51, 201, 261, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 266, 162, 51, 201, 262, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 267, 162, 51, 201, 264, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 268, 162, 51, 201, 263, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 269, 162, 51, 201, 256, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 270, 162, 51, 201, 257, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 271, 162, 51, 201, 258, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 272, 162, 51, 201, 259, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 273, 162, 236, 202, 233, 66, 67, 201, 306, 96, 233, 148, 30, 274, 162, 273, 1, 230, 30, 118, 275, 135, 5, 201, 306, 96, 37, 201, 232, 96, 228, 4, 237, 202, 231, 148, 148, 56, 30, 154, 235, 56, 30, 276, 162, 196, 201, 190, 307, 96, 233, 26, 96, 275, 96, 80, 162, 17, 148, 30, 277, 162, 196, 201, 190, 232, 96, 307, 26, 96, 275, 96, 80, 162, 17, 148, 30, 278, 162, 196, 201, 190, 307, 96, 232, 26, 96, 275, 96, 80, 162, 17, 148, 30, 279, 162, 280, 201, 265, 96, 276, 96, 281, 162, 306, 148, 30, 282, 162, 280, 201, 266, 96, 276, 96, 281, 162, 306, 148, 30, 283, 162, 280, 201, 251, 96, 276, 96, 281, 162, 306, 148, 30, 284, 162, 280, 201, 249, 96, 276, 96, 281, 162, 306, 148, 30, 285, 162, 280, 201, 267, 96, 276, 96, 281, 162, 306, 148, 30, 286, 162, 280, 201, 268, 96, 276, 96, 281, 162, 306, 148, 30, 287, 162, 280, 201, 269, 96, 277, 96, 281, 162, 307, 148, 30, 288, 162, 280, 201, 270, 96, 277, 96, 281, 162, 307, 148, 30, 289, 162, 280, 201, 271, 96, 277, 96, 281, 162, 307, 148, 30, 290, 162, 280, 201, 272, 96, 277, 96, 281, 162, 307, 148, 30, 291, 162, 184, 201, 280, 201, 269, 96, 278, 96, 281, 162, 306, 148, 96, 306, 148, 190, 56, 96, 167, 26, 30, 291, 162, 184, 201, 280, 201, 269, 96, 278, 96, 281, 162, 306, 148, 96, 306, 148, 190, 56, 96, 167, 26, 30, 292, 162, 184, 201, 280, 201, 270, 96, 278, 96, 281, 162, 306, 148, 96, 306, 148, 190, 56, 96, 167, 26, 30, 293, 162, 184, 201, 280, 201, 271, 96, 278, 96, 281, 162, 306, 148, 96, 306, 148, 190, 56, 96, 167, 26, 30, 294, 162, 184, 201, 280, 201, 272, 96, 278, 96, 281, 162, 306, 148, 96, 306, 148, 190, 56, 96, 167, 26, 30, 156, 30, 29, 56, 30, 295, 162, 67, 201, 306, 96, 232, 148, 68, 275, 30, 279, 162, 184, 201, 170, 201, 295, 190, 56, 96, 167, 26, 96, 265, 96, 306, 148, 96, 306, 148, 190, 167, 96, 56, 26, 30, 282, 162, 184, 201, 170, 201, 295, 190, 56, 96, 167, 26, 96, 266, 96, 306, 148, 96, 306, 148, 190, 167, 96, 56, 26, 30, 283, 162, 184, 201, 170, 201, 295, 190, 56, 96, 167, 26, 96, 251, 96, 306, 148, 96, 306, 148, 190, 167, 96, 56, 26, 30, 284, 162, 184, 201, 170, 201, 295, 190, 56, 96, 167, 26, 96, 249, 96, 306, 148, 96, 306, 148, 190, 167, 96, 56, 26, 30, 287, 162, 184, 201, 170, 201, 295, 190, 167, 96, 56, 26, 96, 269, 96, 306, 148, 96, 307, 148, 190, 56, 96, 167, 26, 30, 288, 162, 184, 201, 170, 201, 295, 190, 167, 96, 56, 26, 96, 270, 96, 306, 148, 96, 307, 148, 190, 56, 96, 167, 26, 30, 289, 162, 184, 201, 170, 201, 295, 190, 167, 96, 56, 26, 96, 271, 96, 306, 148, 96, 307, 148, 190, 56, 96, 167, 26, 30, 290, 162, 184, 201, 170, 201, 295, 190, 167, 96, 56, 26, 96, 272, 96, 306, 148, 96, 307, 148, 190, 56, 96, 167, 26, 30, 291, 162, 184, 201, 170, 201, 295, 190, 56, 96, 167, 26, 96, 269, 96, 306, 148, 96, 306, 148, 190, 56, 96, 167, 26, 30, 292, 162, 184, 201, 170, 201, 295, 190, 56, 96, 167, 26, 96, 270, 96, 306, 148, 96, 306, 148, 190, 56, 96, 167, 26, 30, 293, 162, 184, 201, 170, 201, 295, 190, 56, 96, 167, 26, 96, 271, 96, 306, 148, 96, 306, 148, 190, 56, 96, 167, 26, 30, 294, 162, 184, 201, 170, 201, 295, 190, 56, 96, 167, 26, 96, 272, 96, 306, 148, 96, 306, 148, 190, 56, 96, 167, 26, 30, 285, 162, 184, 201, 170, 201, 295, 190, 56, 96, 167, 26, 96, 267, 96, 306, 148, 96, 306, 148, 190, 167, 96, 56, 26, 30, 286, 162, 184, 201, 170, 201, 295, 190, 56, 96, 167, 26, 96, 268, 96, 306, 148, 96, 306, 148, 190, 167, 96, 56, 26, 30, 50, 30, 296, 162, 260, 190, 56, 96, 167, 26, 109, 275, 30, 297, 162, 260, 190, 56, 96, 167, 26, 126, 275, 30, 298, 162, 172, 201, 251, 4, 283, 148, 30, 299, 162, 172, 201, 249, 4, 283, 148, 30, 252, 145, 170, 201, 297, 96, 287, 202, 279, 202, 298, 96, 306, 148, 30, 252, 145, 170, 201, 297, 96, 289, 202, 282, 202, 298, 96, 306, 148, 30, 253, 145, 170, 201, 296, 96, 288, 202, 282, 202, 299, 96, 306, 148, 30, 253, 145, 170, 201, 296, 96, 290, 202, 279, 202, 299, 96, 306, 148, 30, 297, 162, 260, 190, 56, 96, 167, 26, 181, 275, 30, 296, 162, 260, 190, 56, 96, 167, 26, 1, 275, 30, 298, 162, 172, 201, 283, 4, 251, 148, 30, 299, 162, 172, 201, 284, 4, 251, 148, 30, 254, 145, 170, 201, 297, 96, 291, 202, 285, 202, 298, 96, 306, 148, 30, 254, 145, 170, 201, 296, 96, 294, 202, 286, 202, 299, 96, 306, 148, 30, 255, 145, 170, 201, 297, 96, 293, 202, 285, 202, 298, 96, 306, 148, 30, 255, 145, 170, 201, 296, 96, 292, 202, 286, 202, 299, 96, 306, 148, 30, 69, 30, 300, 162, 182, 201, 215, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 301, 162, 182, 201, 216, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 302, 162, 182, 201, 217, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 303, 162, 182, 201, 218, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 304, 162, 182, 201, 223, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 309, 162, 182, 201, 224, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 310, 162, 182, 201, 219, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 311, 162, 182, 201, 220, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 312, 162, 182, 201, 221, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 313, 162, 182, 201, 222, 96, 201, 228, 96, 230, 148, 96, 201, 245, 96, 307, 148, 96, 201, 237, 202, 231, 96, 236, 202, 233, 148, 96, 201, 232, 96, 233, 148, 96, 201, 307, 96, 306, 148, 148, 30, 314, 162, 209, 66, 201, 37, 201, 237, 202, 231, 66, 231, 96, 228, 148, 4, 307, 148, 202, 245, 66, 273, 30, 314, 162, 175, 201, 53, 201, 314, 96, 233, 148, 96, 233, 148, 30, 315, 162, 51, 201, 314, 96, 316, 162, 274, 96, 317, 162, 306, 148, 30, 253, 145, 51, 201, 312, 96, 250, 162, 201, 306, 96, 307, 148, 148, 202, 172, 201, 249, 148, 30, 252, 145, 51, 201, 310, 96, 250, 162, 201, 306, 96, 307, 148, 148, 202, 172, 201, 251, 148, 202, 227, 30, 318, 162, 172, 201, 315, 190, 167, 96, 56, 26, 4, 251, 148, 30, 254, 145, 51, 201, 311, 96, 250, 162, 201, 306, 96, 307, 148, 148, 73, 242, 201, 127, 148, 202, 318, 30, 255, 145, 51, 201, 313, 96, 250, 162, 201, 306, 96, 307, 148, 148, 73, 242, 201, 127, 148, 202, 318, 30, 10, 201, 300, 96, 252, 73, 242, 201, 300, 73, 80, 73, 101, 148, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 10, 201, 301, 96, 254, 73, 242, 201, 301, 73, 80, 73, 101, 148, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 10, 201, 302, 96, 253, 73, 242, 201, 302, 73, 80, 73, 101, 148, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 10, 201, 303, 96, 255, 73, 242, 201, 303, 73, 80, 73, 101, 148, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 319, 162, 201, 252, 202, 267, 66, 253, 202, 268, 4, 254, 202, 265, 4, 255, 202, 266, 148, 73, 242, 201, 127, 148, 30, 320, 162, 253, 202, 268, 30, 10, 201, 304, 96, 319, 73, 242, 201, 304, 73, 80, 73, 101, 148, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 10, 201, 309, 96, 320, 73, 242, 201, 309, 73, 80, 73, 101, 148, 96, 250, 162, 201, 306, 96, 307, 148, 148, 30, 3, 30]}, {"code": "def chunk_dplr_bwd_dgk_kernel(\n    dgk,\n    dgk_offset,\n    dgk_last,\n    dgk_output,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = (i_b * NT + i_t).to(tl.int32)\n        bos, eos = (i_b * T).to(tl.int32), (i_b * T + T).to(tl.int32)\n\n    stride_qk = H * K\n    dgk += (bos * H + i_h) * K\n    dgk_offset += (bos * H + i_h) * K\n    dgk_last += (i_tg * H + i_h) * K\n    dgk_output += (bos * H + i_h) * K\n    p_dgk_last = dgk_last + tl.arange(0, BK) + i_k * BK\n    m_k = tl.arange(0, BK) + i_k * BK < K\n    b_dgk_last = tl.load(p_dgk_last, mask=m_k, other=0)\n    p_dgk_offset = tl.make_block_ptr(\n        dgk_offset, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_dgk = tl.make_block_ptr(\n        dgk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    b_dgk = tl.load(p_dgk, boundary_check=(0, 1))\n    b_dgk_offset = tl.load(p_dgk_offset, boundary_check=(0, 1))\n\n    b_dgk_cumsum = tl.cumsum(b_dgk, 0, reverse=True)\n    b_dgk_cumsum += b_dgk_last[None, :]\n    b_dgk_cumsum -= b_dgk_offset\n    p_dgk_output = tl.make_block_ptr(\n        dgk_output, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    tl.store(\n        p_dgk_output,\n        b_dgk_cumsum.to(p_dgk_output.dtype.element_ty),\n        boundary_check=(0, 1),\n    )", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 57, 6, 98, 214, 57, 6, 98, 215, 57, 6, 98, 216, 57, 6, 98, 217, 57, 6, 149, 57, 30, -1, 218, 98, 219, 98, 220, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 221, 98, 222, 163, 202, 220, 45, 213, 98, 220, 188, 213, 149, 30, 155, 217, 57, 30, 223, 163, 218, 30, 224, 98, 218, 163, 202, 51, 202, 211, 67, 218, 203, 309, 149, 74, 225, 202, 56, 149, 98, 51, 202, 211, 67, 218, 203, 309, 67, 308, 149, 74, 225, 202, 56, 149, 149, 30, 226, 98, 227, 163, 202, 51, 202, 210, 67, 224, 149, 74, 225, 202, 56, 149, 98, 51, 202, 210, 67, 224, 67, 308, 149, 74, 225, 202, 56, 149, 149, 30, 212, 163, 227, 4, 226, 30, 228, 163, 59, 202, 212, 98, 215, 149, 30, 157, 30, 29, 57, 30, 228, 163, 59, 202, 212, 98, 215, 149, 30, 223, 163, 202, 221, 203, 228, 67, 218, 149, 74, 225, 202, 56, 149, 30, 226, 98, 227, 163, 202, 202, 221, 203, 212, 149, 74, 225, 202, 56, 149, 98, 202, 221, 203, 212, 67, 212, 149, 74, 225, 202, 56, 149, 149, 30, 52, 30, 229, 163, 213, 203, 214, 30, 206, 146, 202, 226, 203, 213, 67, 222, 149, 203, 214, 30, 207, 146, 202, 226, 203, 213, 67, 222, 149, 203, 214, 30, 208, 146, 202, 223, 203, 213, 67, 222, 149, 203, 214, 30, 209, 146, 202, 226, 203, 213, 67, 222, 149, 203, 214, 30, 230, 163, 208, 67, 68, 202, 307, 98, 216, 149, 67, 219, 203, 216, 30, 231, 163, 68, 202, 307, 98, 216, 149, 67, 219, 203, 216, 1, 214, 30, 232, 163, 51, 202, 230, 98, 233, 163, 231, 98, 234, 163, 307, 149, 30, 235, 163, 183, 202, 207, 98, 202, 212, 98, 214, 149, 98, 202, 229, 98, 308, 149, 98, 202, 218, 203, 215, 98, 219, 203, 216, 149, 98, 202, 215, 98, 216, 149, 98, 202, 308, 98, 307, 149, 149, 30, 236, 163, 183, 202, 206, 98, 202, 212, 98, 214, 149, 98, 202, 229, 98, 308, 149, 98, 202, 218, 203, 215, 98, 219, 203, 216, 149, 98, 202, 215, 98, 216, 149, 98, 202, 308, 98, 307, 149, 149, 30, 237, 163, 51, 202, 236, 98, 238, 163, 202, 307, 98, 308, 149, 149, 30, 239, 163, 51, 202, 235, 98, 238, 163, 202, 307, 98, 308, 149, 149, 30, 240, 163, 76, 202, 237, 98, 307, 98, 241, 163, 139, 149, 30, 240, 146, 232, 191, 168, 98, 57, 26, 30, 240, 2, 239, 30, 242, 163, 183, 202, 209, 98, 202, 212, 98, 214, 149, 98, 202, 229, 98, 308, 149, 98, 202, 218, 203, 215, 98, 219, 203, 216, 149, 98, 202, 215, 98, 216, 149, 98, 202, 308, 98, 307, 149, 149, 30, 10, 202, 242, 98, 240, 74, 225, 202, 242, 74, 82, 74, 103, 149, 98, 238, 163, 202, 307, 98, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_dplr_fwd_A_kernel_intra_sub_intra(\n    q,\n    k,\n    a,\n    b,\n    gi,\n    ge,\n    qg,\n    kg,\n    ag,\n    bg,\n    Aqk,\n    Aqb,\n    Aab,\n    Aak,\n    cu_seqlens,\n    chunk_indices,\n    scale: tl.constexpr,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    GATHER_SUPPORTED: tl.constexpr,\n):\n    i_t, i_b, i_h = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    if i_t * BT >= T:\n        return\n\n    o_i = tl.arange(0, BC)\n    o_k = tl.arange(0, BK)\n    m_k = o_k < K\n    m_A = (i_t * BT + tl.arange(0, BC)) < T\n    last_idx = min((i_t + 1) * BT, T) - 1\n    o_A = (bos + i_t * BT + tl.arange(0, BC)) * H * BT + i_h * BT\n    p_q = tl.make_block_ptr(\n        q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)\n    )\n    p_k = tl.make_block_ptr(\n        k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)\n    )\n    p_a = tl.make_block_ptr(\n        a + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)\n    )\n    p_b = tl.make_block_ptr(\n        b + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)\n    )\n    p_gi = tl.make_block_ptr(\n        gi + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)\n    )\n    p_ge = tl.make_block_ptr(\n        ge + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)\n    )\n    p_g_last = gi + (bos * H + i_h) * K + last_idx * H * K + tl.arange(0, BK)\n    b_g_last = tl.load(p_g_last, mask=m_k, other=0)\n    p_qg = tl.make_block_ptr(\n        qg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)\n    )\n    p_kg = tl.make_block_ptr(\n        kg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)\n    )\n    p_ag = tl.make_block_ptr(\n        ag + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)\n    )\n    p_bg = tl.make_block_ptr(\n        bg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_a = tl.load(p_a, boundary_check=(0, 1))\n    b_b = tl.load(p_b, boundary_check=(0, 1))\n    b_gi = tl.load(p_gi, boundary_check=(0, 1)).to(tl.float32)\n    b_ge = tl.load(p_ge, boundary_check=(0, 1)).to(tl.float32)\n\n    g_exp = exp(b_gi)\n    g_exp_inv = exp(-b_gi + b_g_last[None, :])\n    b_qg = b_q * g_exp\n    b_kg = b_k * g_exp_inv\n    b_bg = b_b * g_exp_inv\n    b_ag = b_a * exp(b_ge)\n    tl.store(\n        p_qg,\n        b_qg.to(p_qg.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_bg,\n        b_bg.to(p_bg.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_ag,\n        b_ag.to(p_ag.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_kg,\n        b_kg.to(p_kg.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n\n    b_q = b_q.to(b_k.dtype)\n\n    for j in range(0, min(BC, T - i_t * BT)):\n\n        if GATHER_SUPPORTED:\n            row_idx = tl.full([1, BK], j, dtype=tl.int16)\n\n            b_k_j = gather(b_k, row_idx, axis=0)\n            b_gk_j = gather(b_gi, row_idx, axis=0)\n            b_b_j = gather(b_b, row_idx, axis=0)\n        else:\n            mask = tl.arange(0, BC) == j\n            b_k_j = tl.sum(tl.where(mask[:, None], b_k, 0), 0)[None, :]\n            b_gk_j = tl.sum(tl.where(mask[:, None], b_gi, 0), 0)[None, :]\n            b_b_j = tl.sum(tl.where(mask[:, None], b_b, 0), 0)[None, :]\n        tmp = exp(b_gi - b_gk_j)\n        b_A_qk = tl.sum(b_q * b_k_j * tmp, 1)\n        m_i = (o_i >= j).to(tl.float32)\n        b_A_qk = b_A_qk * m_i\n        b_A_qb = tl.sum(b_q * b_b_j * tmp, 1)\n        b_A_qb = b_A_qb * m_i\n        tmp2 = exp(b_ge - b_gk_j)\n        b_A_ak = tl.sum(b_a * b_k_j * tmp2, 1)\n        m_i2 = (o_i > j).to(tl.float32)\n        b_A_ak = b_A_ak * m_i2\n        b_A_ab = tl.sum(b_a * b_b_j * tmp2, 1)\n        b_A_ab = b_A_ab * m_i2\n\n        tl.store(\n            Aqk + o_A + j,\n            b_A_qk.to(dtype=Aqk.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n            mask=m_A,\n        )\n        tl.store(\n            Aqb + o_A + j,\n            b_A_qb.to(dtype=Aqb.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n            mask=m_A,\n        )\n        tl.store(\n            Aab + o_A + j,\n            b_A_ab.to(dtype=Aqb.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n            mask=m_A,\n        )\n        tl.store(\n            Aak + o_A + j,\n            b_A_ak.to(dtype=Aqk.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n            mask=m_A,\n        )", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 97, 217, 97, 218, 97, 219, 97, 220, 97, 221, 97, 222, 56, 6, 97, 223, 97, 224, 56, 6, 97, 225, 56, 6, 97, 226, 56, 6, 97, 227, 56, 6, 97, 228, 56, 6, 97, 229, 56, 6, 97, 230, 56, 6, 149, 56, 30, -1, 231, 97, 232, 97, 233, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 155, 229, 56, 30, 234, 97, 231, 163, 202, 50, 202, 221, 66, 231, 203, 309, 149, 73, 235, 202, 205, 149, 97, 50, 202, 221, 66, 231, 203, 309, 66, 308, 149, 73, 235, 202, 205, 149, 149, 30, 236, 97, 237, 163, 202, 50, 202, 220, 66, 234, 149, 73, 235, 202, 205, 149, 97, 50, 202, 220, 66, 234, 66, 308, 149, 73, 235, 202, 205, 149, 149, 30, 223, 163, 237, 4, 236, 30, 157, 30, 29, 56, 30, 236, 97, 237, 163, 202, 232, 203, 223, 97, 232, 203, 223, 66, 223, 149, 30, 51, 30, 155, 231, 203, 226, 127, 223, 56, 30, 192, 30, 157, 30, 238, 163, 67, 202, 307, 97, 227, 149, 30, 239, 163, 67, 202, 307, 97, 228, 149, 30, 240, 163, 239, 1, 225, 30, 241, 163, 231, 203, 226, 66, 67, 202, 307, 97, 227, 149, 1, 223, 30, 242, 163, 37, 202, 202, 231, 66, 308, 149, 203, 226, 97, 223, 149, 4, 308, 30, 243, 163, 202, 236, 66, 231, 203, 226, 66, 67, 202, 307, 97, 227, 149, 149, 203, 224, 203, 226, 66, 233, 203, 226, 30, 244, 163, 183, 202, 206, 66, 202, 236, 203, 224, 66, 233, 149, 203, 225, 97, 202, 223, 97, 225, 149, 97, 202, 224, 203, 225, 97, 308, 149, 97, 202, 231, 203, 226, 97, 307, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 245, 163, 183, 202, 207, 66, 202, 236, 203, 224, 66, 233, 149, 203, 225, 97, 202, 223, 97, 225, 149, 97, 202, 224, 203, 225, 97, 308, 149, 97, 202, 231, 203, 226, 97, 307, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 246, 163, 183, 202, 208, 66, 202, 236, 203, 224, 66, 233, 149, 203, 225, 97, 202, 223, 97, 225, 149, 97, 202, 224, 203, 225, 97, 308, 149, 97, 202, 231, 203, 226, 97, 307, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 247, 163, 183, 202, 209, 66, 202, 236, 203, 224, 66, 233, 149, 203, 225, 97, 202, 223, 97, 225, 149, 97, 202, 224, 203, 225, 97, 308, 149, 97, 202, 231, 203, 226, 97, 307, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 248, 163, 183, 202, 210, 66, 202, 236, 203, 224, 66, 233, 149, 203, 225, 97, 202, 223, 97, 225, 149, 97, 202, 224, 203, 225, 97, 308, 149, 97, 202, 231, 203, 226, 97, 307, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 249, 163, 183, 202, 211, 66, 202, 236, 203, 224, 66, 233, 149, 203, 225, 97, 202, 223, 97, 225, 149, 97, 202, 224, 203, 225, 97, 308, 149, 97, 202, 231, 203, 226, 97, 307, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 250, 163, 210, 66, 202, 236, 203, 224, 66, 233, 149, 203, 225, 66, 242, 203, 224, 203, 225, 66, 67, 202, 307, 97, 228, 149, 30, 251, 163, 50, 202, 250, 97, 252, 163, 240, 97, 253, 163, 307, 149, 30, 254, 163, 183, 202, 212, 66, 202, 236, 203, 224, 66, 233, 149, 203, 225, 97, 202, 223, 97, 225, 149, 97, 202, 224, 203, 225, 97, 308, 149, 97, 202, 231, 203, 226, 97, 307, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 255, 163, 183, 202, 213, 66, 202, 236, 203, 224, 66, 233, 149, 203, 225, 97, 202, 223, 97, 225, 149, 97, 202, 224, 203, 225, 97, 308, 149, 97, 202, 231, 203, 226, 97, 307, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 256, 163, 183, 202, 214, 66, 202, 236, 203, 224, 66, 233, 149, 203, 225, 97, 202, 223, 97, 225, 149, 97, 202, 224, 203, 225, 97, 308, 149, 97, 202, 231, 203, 226, 97, 307, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 257, 163, 183, 202, 215, 66, 202, 236, 203, 224, 66, 233, 149, 203, 225, 97, 202, 223, 97, 225, 149, 97, 202, 224, 203, 225, 97, 308, 149, 97, 202, 231, 203, 226, 97, 307, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 258, 163, 50, 202, 244, 97, 259, 163, 202, 307, 97, 308, 149, 149, 30, 258, 163, 258, 203, 222, 30, 260, 163, 50, 202, 245, 97, 259, 163, 202, 307, 97, 308, 149, 149, 30, 261, 163, 50, 202, 246, 97, 259, 163, 202, 307, 97, 308, 149, 149, 30, 262, 163, 50, 202, 247, 97, 259, 163, 202, 307, 97, 308, 149, 149, 30, 263, 163, 50, 202, 248, 97, 259, 163, 202, 307, 97, 308, 149, 149, 73, 235, 202, 128, 149, 30, 264, 163, 50, 202, 249, 97, 259, 163, 202, 307, 97, 308, 149, 149, 73, 235, 202, 128, 149, 30, 265, 163, 173, 202, 263, 149, 30, 266, 163, 173, 202, 4, 263, 66, 251, 191, 168, 97, 56, 26, 149, 30, 267, 163, 258, 203, 265, 30, 268, 163, 260, 203, 266, 30, 269, 163, 262, 203, 266, 30, 270, 163, 261, 203, 173, 202, 264, 149, 30, 10, 202, 254, 97, 267, 73, 235, 202, 254, 73, 81, 73, 102, 97, 142, 163, 310, 149, 97, 259, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 257, 97, 269, 73, 235, 202, 257, 73, 81, 73, 102, 97, 142, 163, 310, 149, 97, 259, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 256, 97, 270, 73, 235, 202, 256, 73, 81, 73, 102, 97, 142, 163, 310, 149, 97, 259, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 255, 97, 268, 73, 235, 202, 255, 73, 81, 73, 102, 97, 142, 163, 310, 149, 97, 259, 163, 202, 307, 97, 308, 149, 149, 30, 258, 163, 258, 73, 235, 202, 260, 73, 81, 149, 30, 119, 271, 136, 5, 202, 307, 97, 37, 202, 227, 97, 223, 4, 231, 203, 226, 149, 149, 56, 30, 155, 230, 56, 30, 272, 163, 197, 202, 191, 308, 97, 228, 26, 97, 271, 97, 81, 163, 17, 149, 30, 273, 163, 274, 202, 260, 97, 272, 97, 275, 163, 307, 149, 30, 276, 163, 274, 202, 263, 97, 272, 97, 275, 163, 307, 149, 30, 277, 163, 274, 202, 262, 97, 272, 97, 275, 163, 307, 149, 30, 157, 30, 29, 56, 30, 252, 163, 67, 202, 307, 97, 227, 149, 68, 271, 30, 273, 163, 185, 202, 171, 202, 252, 191, 56, 97, 168, 26, 97, 260, 97, 307, 149, 97, 307, 149, 191, 168, 97, 56, 26, 30, 276, 163, 185, 202, 171, 202, 252, 191, 56, 97, 168, 26, 97, 263, 97, 307, 149, 97, 307, 149, 191, 168, 97, 56, 26, 30, 277, 163, 185, 202, 171, 202, 252, 191, 56, 97, 168, 26, 97, 262, 97, 307, 149, 97, 307, 149, 191, 168, 97, 56, 26, 30, 51, 30, 278, 163, 173, 202, 263, 4, 276, 149, 30, 279, 163, 185, 202, 258, 203, 273, 203, 278, 97, 308, 149, 30, 280, 163, 202, 238, 127, 271, 149, 73, 235, 202, 128, 149, 30, 279, 163, 279, 203, 280, 30, 281, 163, 185, 202, 258, 203, 277, 203, 278, 97, 308, 149, 30, 281, 163, 281, 203, 280, 30, 282, 163, 173, 202, 264, 4, 276, 149, 30, 283, 163, 185, 202, 261, 203, 273, 203, 282, 97, 308, 149, 30, 284, 163, 202, 238, 110, 271, 149, 73, 235, 202, 128, 149, 30, 283, 163, 283, 203, 284, 30, 285, 163, 185, 202, 261, 203, 277, 203, 282, 97, 308, 149, 30, 285, 163, 285, 203, 284, 30, 10, 202, 216, 66, 243, 66, 271, 97, 279, 73, 235, 202, 81, 163, 216, 73, 81, 73, 102, 97, 142, 163, 310, 149, 97, 252, 163, 241, 149, 30, 10, 202, 217, 66, 243, 66, 271, 97, 281, 73, 235, 202, 81, 163, 217, 73, 81, 73, 102, 97, 142, 163, 310, 149, 97, 252, 163, 241, 149, 30, 10, 202, 218, 66, 243, 66, 271, 97, 285, 73, 235, 202, 81, 163, 217, 73, 81, 73, 102, 97, 142, 163, 310, 149, 97, 252, 163, 241, 149, 30, 10, 202, 219, 66, 243, 66, 271, 97, 283, 73, 235, 202, 81, 163, 216, 73, 81, 73, 102, 97, 142, 163, 310, 149, 97, 252, 163, 241, 149, 30, 69, 30, 3, 30]}, {"code": "def chunk_dplr_bwd_kernel_dhu(\n    qg,\n    bg,\n    w,\n    gk,\n    dht,\n    dh0,\n    do,\n    dh,\n    dv,\n    dv2,\n    cu_seqlens,\n    chunk_offsets,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(\n            dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_dh += tl.load(p_dht, boundary_check=(0, 1))\n\n    mask_k = tl.arange(0, BK) < K\n    for i_t in range(NT - 1, -1, -1):\n        p_dh = tl.make_block_ptr(\n            dh + ((boh + i_t) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        b_dh_tmp = tl.zeros([BK, BV], dtype=tl.float32)\n        for i_c in range(tl.cdiv(BT, BC) - 1, -1, -1):\n            p_qg = tl.make_block_ptr(\n                qg + (bos * H + i_h) * K,\n                (K, T),\n                (1, H * K),\n                (i_k * BK, i_t * BT + i_c * BC),\n                (BK, BC),\n                (0, 1),\n            )\n            p_bg = tl.make_block_ptr(\n                bg + (bos * H + i_h) * K,\n                (T, K),\n                (H * K, 1),\n                (i_t * BT + i_c * BC, i_k * BK),\n                (BC, BK),\n                (1, 0),\n            )\n            p_w = tl.make_block_ptr(\n                w + (bos * H + i_h) * K,\n                (K, T),\n                (1, H * K),\n                (i_k * BK, i_t * BT + i_c * BC),\n                (BK, BC),\n                (0, 1),\n            )\n            p_dv = tl.make_block_ptr(\n                dv + (bos * H + i_h) * V,\n                (T, V),\n                (H * V, 1),\n                (i_t * BT + i_c * BC, i_v * BV),\n                (BC, BV),\n                (1, 0),\n            )\n            p_do = tl.make_block_ptr(\n                do + (bos * H + i_h) * V,\n                (T, V),\n                (H * V, 1),\n                (i_t * BT + i_c * BC, i_v * BV),\n                (BC, BV),\n                (1, 0),\n            )\n            p_dv2 = tl.make_block_ptr(\n                dv2 + (bos * H + i_h) * V,\n                (T, V),\n                (H * V, 1),\n                (i_t * BT + i_c * BC, i_v * BV),\n                (BC, BV),\n                (1, 0),\n            )\n\n            b_qg = tl.load(p_qg, boundary_check=(0, 1))\n\n            b_bg = tl.load(p_bg, boundary_check=(0, 1))\n            b_w = tl.load(p_w, boundary_check=(0, 1))\n\n            b_do = tl.load(p_do, boundary_check=(0, 1))\n            b_dv = tl.load(p_dv, boundary_check=(0, 1))\n            b_dv2 = b_dv + tl.dot(b_bg, b_dh.to(b_bg.dtype))\n            tl.store(p_dv2, b_dv2.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n\n            b_dh_tmp += tl.dot(b_qg, b_do.to(b_qg.dtype))\n            b_dh_tmp += tl.dot(b_w, b_dv2.to(b_qg.dtype))\n        last_idx = min((i_t + 1) * BT, T) - 1\n        bg_last = tl.load(\n            gk + ((bos + last_idx) * H + i_h) * K + tl.arange(0, BK), mask=mask_k\n        )\n        b_dh *= exp(bg_last)[:, None]\n        b_dh += b_dh_tmp\n\n    if USE_INITIAL_STATE:\n        p_dh0 = tl.make_block_ptr(\n            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 149, 57, 30, -1, 229, 98, 230, 98, 231, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 232, 98, 233, 163, 202, 231, 45, 219, 98, 231, 188, 219, 149, 30, 155, 228, 57, 30, 234, 98, 235, 163, 202, 52, 202, 216, 67, 232, 149, 74, 236, 202, 56, 149, 98, 52, 202, 216, 67, 232, 67, 308, 149, 74, 236, 202, 56, 149, 149, 30, 218, 163, 235, 4, 234, 30, 237, 163, 59, 202, 218, 98, 222, 149, 30, 238, 163, 52, 202, 217, 67, 232, 149, 74, 236, 202, 56, 149, 30, 157, 30, 29, 57, 30, 234, 98, 235, 163, 202, 232, 203, 218, 98, 232, 203, 218, 67, 218, 149, 30, 237, 163, 59, 202, 218, 98, 222, 149, 30, 238, 163, 232, 203, 237, 30, 51, 30, 239, 163, 150, 202, 191, 224, 98, 225, 26, 98, 82, 163, 129, 149, 30, 155, 226, 57, 30, 240, 163, 183, 202, 210, 67, 231, 203, 220, 203, 221, 98, 202, 220, 98, 221, 149, 98, 202, 221, 98, 308, 149, 98, 202, 229, 203, 224, 98, 230, 203, 225, 149, 98, 202, 224, 98, 225, 149, 98, 202, 308, 98, 307, 149, 149, 30, 239, 146, 52, 202, 240, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 157, 30, 242, 163, 68, 202, 307, 98, 224, 149, 1, 220, 30, 120, 243, 136, 5, 202, 237, 4, 308, 98, 4, 308, 98, 4, 308, 149, 57, 30, 244, 163, 183, 202, 213, 67, 202, 202, 238, 67, 243, 149, 203, 219, 67, 233, 149, 203, 220, 203, 221, 98, 202, 220, 98, 221, 149, 98, 202, 221, 98, 308, 149, 98, 202, 229, 203, 224, 98, 230, 203, 225, 149, 98, 202, 224, 98, 225, 149, 98, 202, 308, 98, 307, 149, 149, 30, 10, 202, 244, 98, 239, 74, 236, 202, 244, 74, 82, 74, 103, 149, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 245, 163, 150, 202, 191, 224, 98, 225, 26, 98, 82, 163, 129, 149, 30, 120, 246, 136, 5, 202, 59, 202, 222, 98, 223, 149, 4, 308, 98, 4, 308, 98, 4, 308, 149, 57, 30, 247, 163, 183, 202, 206, 67, 202, 234, 203, 219, 67, 233, 149, 203, 220, 98, 202, 220, 98, 218, 149, 98, 202, 308, 98, 219, 203, 220, 149, 98, 202, 229, 203, 224, 98, 243, 203, 222, 67, 246, 203, 223, 149, 98, 202, 224, 98, 223, 149, 98, 202, 307, 98, 308, 149, 149, 30, 248, 163, 183, 202, 207, 67, 202, 234, 203, 219, 67, 233, 149, 203, 220, 98, 202, 218, 98, 220, 149, 98, 202, 219, 203, 220, 98, 308, 149, 98, 202, 243, 203, 222, 67, 246, 203, 223, 98, 229, 203, 224, 149, 98, 202, 223, 98, 224, 149, 98, 202, 308, 98, 307, 149, 149, 30, 249, 163, 183, 202, 208, 67, 202, 234, 203, 219, 67, 233, 149, 203, 220, 98, 202, 220, 98, 218, 149, 98, 202, 308, 98, 219, 203, 220, 149, 98, 202, 229, 203, 224, 98, 243, 203, 222, 67, 246, 203, 223, 149, 98, 202, 224, 98, 223, 149, 98, 202, 307, 98, 308, 149, 149, 30, 250, 163, 183, 202, 214, 67, 202, 234, 203, 219, 67, 233, 149, 203, 221, 98, 202, 218, 98, 221, 149, 98, 202, 219, 203, 221, 98, 308, 149, 98, 202, 243, 203, 222, 67, 246, 203, 223, 98, 230, 203, 225, 149, 98, 202, 223, 98, 225, 149, 98, 202, 308, 98, 307, 149, 149, 30, 251, 163, 183, 202, 212, 67, 202, 234, 203, 219, 67, 233, 149, 203, 221, 98, 202, 218, 98, 221, 149, 98, 202, 219, 203, 221, 98, 308, 149, 98, 202, 243, 203, 222, 67, 246, 203, 223, 98, 230, 203, 225, 149, 98, 202, 223, 98, 225, 149, 98, 202, 308, 98, 307, 149, 149, 30, 252, 163, 183, 202, 215, 67, 202, 234, 203, 219, 67, 233, 149, 203, 221, 98, 202, 218, 98, 221, 149, 98, 202, 219, 203, 221, 98, 308, 149, 98, 202, 243, 203, 222, 67, 246, 203, 223, 98, 230, 203, 225, 149, 98, 202, 223, 98, 225, 149, 98, 202, 308, 98, 307, 149, 149, 30, 253, 163, 52, 202, 247, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 254, 163, 52, 202, 248, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 255, 163, 52, 202, 249, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 256, 163, 52, 202, 251, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 257, 163, 52, 202, 250, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 258, 163, 257, 67, 15, 202, 254, 98, 239, 74, 236, 202, 254, 74, 82, 149, 149, 30, 10, 202, 252, 98, 258, 74, 236, 202, 250, 74, 82, 74, 103, 149, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 245, 146, 15, 202, 253, 98, 256, 74, 236, 202, 253, 74, 82, 149, 149, 30, 245, 146, 15, 202, 255, 98, 258, 74, 236, 202, 253, 74, 82, 149, 149, 30, 70, 30, 259, 163, 37, 202, 202, 243, 67, 308, 149, 203, 222, 98, 218, 149, 4, 308, 30, 260, 163, 52, 202, 209, 67, 202, 202, 234, 67, 259, 149, 203, 219, 67, 233, 149, 203, 220, 67, 68, 202, 307, 98, 224, 149, 98, 261, 163, 242, 149, 30, 239, 23, 173, 202, 260, 149, 191, 57, 98, 168, 26, 30, 239, 146, 245, 30, 70, 30, 155, 227, 57, 30, 262, 163, 183, 202, 211, 67, 231, 203, 220, 203, 221, 98, 202, 220, 98, 221, 149, 98, 202, 221, 98, 308, 149, 98, 202, 229, 203, 224, 98, 230, 203, 225, 149, 98, 202, 224, 98, 225, 149, 98, 202, 308, 98, 307, 149, 149, 30, 10, 202, 262, 98, 239, 74, 236, 202, 262, 74, 82, 74, 103, 149, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 157, 30, 3, 30]}, {"code": "def chunk_dplr_fwd_kernel_h(\n    kg,\n    v,\n    w,\n    bg,\n    u,\n    v_new,\n    gk,\n    h,\n    h0,\n    ht,\n    cu_seqlens,\n    chunk_offsets,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n    o_k = i_k * BK + tl.arange(0, BK)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(\n            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT):\n        p_h = tl.make_block_ptr(\n            h + ((boh + i_t) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n\n        b_hc = tl.zeros([BK, BV], dtype=tl.float32)\n\n        for i_c in range(tl.cdiv(min(BT, T - i_t * BT), BC)):\n            p_kg = tl.make_block_ptr(\n                kg + (bos * H + i_h) * K,\n                (K, T),\n                (1, H * K),\n                (i_k * BK, i_t * BT + i_c * BC),\n                (BK, BC),\n                (0, 1),\n            )\n            p_bg = tl.make_block_ptr(\n                bg + (bos * H + i_h) * K,\n                (K, T),\n                (1, H * K),\n                (i_k * BK, i_t * BT + i_c * BC),\n                (BK, BC),\n                (0, 1),\n            )\n            p_w = tl.make_block_ptr(\n                w + (bos * H + i_h) * K,\n                (T, K),\n                (H * K, 1),\n                (i_t * BT + i_c * BC, i_k * BK),\n                (BC, BK),\n                (1, 0),\n            )\n            p_v = tl.make_block_ptr(\n                v + (bos * H + i_h) * V,\n                (T, V),\n                (H * V, 1),\n                (i_t * BT + i_c * BC, i_v * BV),\n                (BC, BV),\n                (1, 0),\n            )\n            p_u = tl.make_block_ptr(\n                u + (bos * H + i_h) * V,\n                (T, V),\n                (H * V, 1),\n                (i_t * BT + i_c * BC, i_v * BV),\n                (BC, BV),\n                (1, 0),\n            )\n            p_v_new = tl.make_block_ptr(\n                v_new + (bos * H + i_h) * V,\n                (T, V),\n                (H * V, 1),\n                (i_t * BT + i_c * BC, i_v * BV),\n                (BC, BV),\n                (1, 0),\n            )\n\n            b_kg = tl.load(p_kg, boundary_check=(0, 1))\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n            b_w = tl.load(p_w, boundary_check=(0, 1))\n            b_bg = tl.load(p_bg, boundary_check=(0, 1))\n            b_v2 = tl.dot(b_w, b_h.to(b_w.dtype)) + tl.load(p_u, boundary_check=(0, 1))\n            b_hc += tl.dot(b_kg, b_v)\n            b_hc += tl.dot(b_bg.to(b_hc.dtype), b_v2)\n            tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))\n\n        last_idx = min((i_t + 1) * BT, T) - 1\n        b_g_last = tl.load(\n            gk + (bos + last_idx) * H * K + i_h * K + o_k, mask=o_k < K\n        ).to(tl.float32)\n        b_h *= exp(b_g_last[:, None])\n        b_h += b_hc\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(\n            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(\n            p_ht,\n            b_h.to(p_ht.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n            boundary_check=(0, 1),\n        )", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 97, 217, 97, 218, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 97, 224, 56, 6, 97, 225, 56, 6, 97, 226, 56, 6, 97, 227, 56, 6, 97, 228, 56, 6, 149, 56, 30, -1, 229, 97, 230, 97, 231, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 232, 97, 233, 163, 202, 231, 45, 219, 97, 231, 188, 219, 149, 30, 155, 228, 56, 30, 234, 97, 235, 163, 202, 50, 202, 216, 66, 232, 149, 73, 236, 202, 205, 149, 97, 50, 202, 216, 66, 232, 66, 308, 149, 73, 236, 202, 205, 149, 149, 30, 218, 163, 235, 4, 234, 30, 237, 163, 58, 202, 218, 97, 222, 149, 30, 238, 163, 50, 202, 217, 66, 232, 149, 73, 236, 202, 205, 149, 30, 157, 30, 29, 56, 30, 234, 97, 235, 163, 202, 232, 203, 218, 97, 232, 203, 218, 66, 218, 149, 30, 237, 163, 58, 202, 218, 97, 222, 149, 30, 238, 163, 232, 203, 237, 30, 51, 30, 239, 163, 229, 203, 224, 66, 67, 202, 307, 97, 224, 149, 30, 240, 163, 150, 202, 191, 224, 97, 225, 26, 97, 81, 163, 128, 149, 30, 155, 226, 56, 30, 241, 163, 183, 202, 214, 66, 231, 203, 220, 203, 221, 97, 202, 220, 97, 221, 149, 97, 202, 221, 97, 308, 149, 97, 202, 229, 203, 224, 97, 230, 203, 225, 149, 97, 202, 224, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 240, 163, 50, 202, 241, 97, 242, 163, 202, 307, 97, 308, 149, 149, 73, 236, 202, 128, 149, 30, 157, 30, 119, 243, 136, 5, 202, 237, 149, 56, 30, 244, 163, 183, 202, 213, 66, 202, 202, 238, 66, 243, 149, 203, 219, 66, 233, 149, 203, 220, 203, 221, 97, 202, 220, 97, 221, 149, 97, 202, 221, 97, 308, 149, 97, 202, 229, 203, 224, 97, 230, 203, 225, 149, 97, 202, 224, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 10, 202, 244, 97, 240, 73, 236, 202, 244, 73, 81, 73, 102, 149, 97, 242, 163, 202, 307, 97, 308, 149, 149, 30, 245, 163, 150, 202, 191, 224, 97, 225, 26, 97, 81, 163, 128, 149, 30, 119, 246, 136, 5, 202, 58, 202, 37, 202, 222, 97, 218, 4, 243, 203, 222, 149, 97, 223, 149, 149, 56, 30, 247, 163, 183, 202, 206, 66, 202, 234, 203, 219, 66, 233, 149, 203, 220, 97, 202, 220, 97, 218, 149, 97, 202, 308, 97, 219, 203, 220, 149, 97, 202, 229, 203, 224, 97, 243, 203, 222, 66, 246, 203, 223, 149, 97, 202, 224, 97, 223, 149, 97, 202, 307, 97, 308, 149, 149, 30, 248, 163, 183, 202, 209, 66, 202, 234, 203, 219, 66, 233, 149, 203, 220, 97, 202, 220, 97, 218, 149, 97, 202, 308, 97, 219, 203, 220, 149, 97, 202, 229, 203, 224, 97, 243, 203, 222, 66, 246, 203, 223, 149, 97, 202, 224, 97, 223, 149, 97, 202, 307, 97, 308, 149, 149, 30, 249, 163, 183, 202, 208, 66, 202, 234, 203, 219, 66, 233, 149, 203, 220, 97, 202, 218, 97, 220, 149, 97, 202, 219, 203, 220, 97, 308, 149, 97, 202, 243, 203, 222, 66, 246, 203, 223, 97, 229, 203, 224, 149, 97, 202, 223, 97, 224, 149, 97, 202, 308, 97, 307, 149, 149, 30, 250, 163, 183, 202, 207, 66, 202, 234, 203, 219, 66, 233, 149, 203, 221, 97, 202, 218, 97, 221, 149, 97, 202, 219, 203, 221, 97, 308, 149, 97, 202, 243, 203, 222, 66, 246, 203, 223, 97, 230, 203, 225, 149, 97, 202, 223, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 251, 163, 183, 202, 210, 66, 202, 234, 203, 219, 66, 233, 149, 203, 221, 97, 202, 218, 97, 221, 149, 97, 202, 219, 203, 221, 97, 308, 149, 97, 202, 243, 203, 222, 66, 246, 203, 223, 97, 230, 203, 225, 149, 97, 202, 223, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 252, 163, 183, 202, 211, 66, 202, 234, 203, 219, 66, 233, 149, 203, 221, 97, 202, 218, 97, 221, 149, 97, 202, 219, 203, 221, 97, 308, 149, 97, 202, 243, 203, 222, 66, 246, 203, 223, 97, 230, 203, 225, 149, 97, 202, 223, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 253, 163, 50, 202, 247, 97, 242, 163, 202, 307, 97, 308, 149, 149, 30, 254, 163, 50, 202, 250, 97, 242, 163, 202, 307, 97, 308, 149, 149, 30, 255, 163, 50, 202, 249, 97, 242, 163, 202, 307, 97, 308, 149, 149, 30, 256, 163, 50, 202, 248, 97, 242, 163, 202, 307, 97, 308, 149, 149, 30, 257, 163, 15, 202, 255, 97, 240, 73, 236, 202, 255, 73, 81, 149, 149, 66, 50, 202, 251, 97, 242, 163, 202, 307, 97, 308, 149, 149, 30, 245, 146, 15, 202, 253, 97, 254, 149, 30, 245, 146, 15, 202, 256, 73, 236, 202, 245, 73, 81, 149, 97, 257, 149, 30, 10, 202, 252, 97, 257, 73, 236, 202, 252, 73, 81, 73, 102, 149, 97, 242, 163, 202, 307, 97, 308, 149, 149, 30, 69, 30, 258, 163, 37, 202, 202, 243, 66, 308, 149, 203, 222, 97, 218, 149, 4, 308, 30, 259, 163, 50, 202, 212, 66, 202, 234, 66, 258, 149, 203, 219, 203, 220, 66, 233, 203, 220, 66, 239, 97, 260, 163, 239, 1, 220, 149, 73, 236, 202, 128, 149, 30, 240, 23, 173, 202, 259, 191, 56, 97, 168, 26, 149, 30, 240, 146, 245, 30, 69, 30, 155, 227, 56, 30, 261, 163, 183, 202, 215, 66, 231, 203, 220, 203, 221, 97, 202, 220, 97, 221, 149, 97, 202, 221, 97, 308, 149, 97, 202, 229, 203, 224, 97, 230, 203, 225, 149, 97, 202, 224, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 10, 202, 261, 97, 240, 73, 236, 202, 261, 73, 81, 73, 102, 97, 142, 163, 310, 149, 97, 242, 163, 202, 307, 97, 308, 149, 149, 30, 157, 30, 3, 30]}, {"code": "def chunk_dplr_bwd_kernel_dAu(\n    v,\n    do,\n    v_new,\n    A_qb,\n    dA_qk,\n    dA_qb,\n    dv_new,\n    cu_seqlens,\n    chunk_indices,\n    scale: tl.constexpr,\n    T,\n    H: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n    T = eos - bos\n\n    b_dA_qk = tl.zeros([BT, BT], dtype=tl.float32)\n    b_dA_qb = tl.zeros([BT, BT], dtype=tl.float32)\n\n    p_A_qb = tl.make_block_ptr(\n        A_qb + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n\n    b_A_qb = tl.load(p_A_qb, boundary_check=(0, 1))\n\n    b_A_qb = tl.where(\n        tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_A_qb, 0.0\n    ).to(b_A_qb.dtype)\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_do = tl.make_block_ptr(\n            do + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (V, T),\n            (1, H * V),\n            (i_v * BV, i_t * BT),\n            (BV, BT),\n            (0, 1),\n        )\n        p_v_new = tl.make_block_ptr(\n            v_new + (bos * H + i_h) * V,\n            (V, T),\n            (1, H * V),\n            (i_v * BV, i_t * BT),\n            (BV, BT),\n            (0, 1),\n        )\n        p_dv_new = tl.make_block_ptr(\n            dv_new + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_v_new = tl.load(p_v_new, boundary_check=(0, 1))\n        b_dA_qk += tl.dot(b_do, b_v)\n        b_dA_qb += tl.dot(b_do, b_v_new)\n        b_dv_new = tl.dot(tl.trans(b_A_qb), b_do)\n\n        tl.store(\n            p_dv_new, b_dv_new.to(p_dv_new.dtype.element_ty), boundary_check=(0, 1)\n        )\n\n    p_dA_qk = tl.make_block_ptr(\n        dA_qk + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n    p_dA_qb = tl.make_block_ptr(\n        dA_qb + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n    b_dA_qk = tl.where(m_s, b_dA_qk * scale, 0.0)\n    tl.store(p_dA_qk, b_dA_qk.to(p_dA_qk.dtype.element_ty), boundary_check=(0, 1))\n    b_dA_qb = tl.where(m_s, b_dA_qb * scale, 0.0)\n    tl.store(p_dA_qb, b_dA_qb.to(p_dA_qb.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 57, 6, 98, 216, 98, 217, 57, 6, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 149, 57, 30, -1, 222, 98, 223, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 149, 30, 224, 98, 225, 163, 202, 223, 45, 217, 98, 223, 188, 217, 149, 30, 155, 221, 57, 30, 226, 98, 222, 163, 202, 52, 202, 214, 67, 222, 203, 309, 149, 74, 227, 202, 56, 149, 98, 52, 202, 214, 67, 222, 203, 309, 67, 308, 149, 74, 227, 202, 56, 149, 149, 30, 228, 98, 229, 163, 202, 52, 202, 213, 67, 226, 149, 74, 227, 202, 56, 149, 98, 52, 202, 213, 67, 226, 67, 308, 149, 74, 227, 202, 56, 149, 149, 30, 157, 30, 29, 57, 30, 228, 98, 229, 163, 202, 224, 203, 216, 98, 224, 203, 216, 67, 216, 149, 30, 51, 30, 216, 163, 229, 4, 228, 30, 230, 163, 150, 202, 191, 219, 98, 219, 26, 98, 82, 163, 129, 149, 30, 231, 163, 150, 202, 191, 219, 98, 219, 26, 98, 82, 163, 129, 149, 30, 232, 163, 183, 202, 209, 67, 202, 228, 203, 217, 67, 225, 149, 203, 219, 98, 202, 216, 98, 219, 149, 98, 202, 217, 203, 219, 98, 308, 149, 98, 202, 222, 203, 219, 98, 307, 149, 98, 202, 219, 98, 219, 149, 98, 202, 308, 98, 307, 149, 149, 30, 233, 163, 52, 202, 232, 98, 234, 163, 202, 307, 98, 308, 149, 149, 30, 233, 163, 171, 202, 68, 202, 307, 98, 219, 149, 191, 57, 98, 168, 26, 128, 68, 202, 307, 98, 219, 149, 191, 168, 98, 57, 26, 98, 233, 98, 307, 149, 74, 227, 202, 233, 74, 82, 149, 30, 120, 235, 136, 5, 202, 59, 202, 218, 98, 220, 149, 149, 57, 30, 236, 163, 183, 202, 207, 67, 202, 228, 203, 217, 67, 225, 149, 203, 218, 98, 202, 216, 98, 218, 149, 98, 202, 217, 203, 218, 98, 308, 149, 98, 202, 222, 203, 219, 98, 235, 203, 220, 149, 98, 202, 219, 98, 220, 149, 98, 202, 308, 98, 307, 149, 149, 30, 237, 163, 183, 202, 206, 67, 202, 228, 203, 217, 67, 225, 149, 203, 218, 98, 202, 218, 98, 216, 149, 98, 202, 308, 98, 217, 203, 218, 149, 98, 202, 235, 203, 220, 98, 222, 203, 219, 149, 98, 202, 220, 98, 219, 149, 98, 202, 307, 98, 308, 149, 149, 30, 238, 163, 183, 202, 208, 67, 202, 228, 203, 217, 67, 225, 149, 203, 218, 98, 202, 218, 98, 216, 149, 98, 202, 308, 98, 217, 203, 218, 149, 98, 202, 235, 203, 220, 98, 222, 203, 219, 149, 98, 202, 220, 98, 219, 149, 98, 202, 307, 98, 308, 149, 149, 30, 239, 163, 183, 202, 212, 67, 202, 228, 203, 217, 67, 225, 149, 203, 218, 98, 202, 216, 98, 218, 149, 98, 202, 217, 203, 218, 98, 308, 149, 98, 202, 222, 203, 219, 98, 235, 203, 220, 149, 98, 202, 219, 98, 220, 149, 98, 202, 308, 98, 307, 149, 149, 30, 240, 163, 52, 202, 237, 98, 234, 163, 202, 307, 98, 308, 149, 149, 30, 241, 163, 52, 202, 236, 98, 234, 163, 202, 307, 98, 308, 149, 149, 30, 242, 163, 52, 202, 238, 98, 234, 163, 202, 307, 98, 308, 149, 149, 30, 230, 146, 15, 202, 241, 98, 240, 149, 30, 231, 146, 15, 202, 241, 98, 242, 149, 30, 243, 163, 15, 202, 65, 202, 233, 149, 98, 241, 149, 30, 10, 202, 239, 98, 243, 74, 227, 202, 239, 74, 82, 74, 103, 149, 98, 234, 163, 202, 307, 98, 308, 149, 149, 30, 70, 30, 244, 163, 183, 202, 210, 67, 202, 228, 203, 217, 67, 225, 149, 203, 219, 98, 202, 216, 98, 219, 149, 98, 202, 217, 203, 219, 98, 308, 149, 98, 202, 222, 203, 219, 98, 307, 149, 98, 202, 219, 98, 219, 149, 98, 202, 308, 98, 307, 149, 149, 30, 245, 163, 183, 202, 211, 67, 202, 228, 203, 217, 67, 225, 149, 203, 219, 98, 202, 216, 98, 219, 149, 98, 202, 217, 203, 219, 98, 308, 149, 98, 202, 222, 203, 219, 98, 307, 149, 98, 202, 219, 98, 219, 149, 98, 202, 308, 98, 307, 149, 149, 30, 246, 163, 68, 202, 307, 98, 219, 149, 191, 57, 98, 168, 26, 128, 68, 202, 307, 98, 219, 149, 191, 168, 98, 57, 26, 30, 230, 163, 171, 202, 246, 98, 230, 203, 215, 98, 307, 149, 30, 10, 202, 244, 98, 230, 74, 227, 202, 244, 74, 82, 74, 103, 149, 98, 234, 163, 202, 307, 98, 308, 149, 149, 30, 231, 163, 171, 202, 246, 98, 231, 203, 215, 98, 307, 149, 30, 10, 202, 245, 98, 231, 74, 227, 202, 245, 74, 82, 74, 103, 149, 98, 234, 163, 202, 307, 98, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_dplr_bwd_o_kernel(\n    v,\n    v_new,\n    h,\n    do,\n    dh,\n    dk,\n    db,\n    w,\n    dq,\n    dv,\n    dw,\n    gk,\n    dgk_last,\n    k,\n    b,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    v += (bos * H + i_h) * V\n    v_new += (bos * H + i_h) * V\n    do += (bos * H + i_h) * V\n    h += (i_tg * H + i_h) * K * V\n    dh += (i_tg * H + i_h) * K * V\n    dk += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    db += (bos * H + i_h) * K\n    b += (bos * H + i_h) * K\n    dw += (bos * H + i_h) * K\n    dv += (bos * H + i_h) * V\n    dq += (bos * H + i_h) * K\n    w += (bos * H + i_h) * K\n\n    dgk_last += (i_tg * H + i_h) * K\n    gk += (bos * H + i_h) * K\n\n    stride_qk = H * K\n    stride_vo = H * V\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dw = tl.zeros([BT, BK], dtype=tl.float32)\n    b_db = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dgk_last = tl.zeros([BK], dtype=tl.float32)\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_v_new = tl.make_block_ptr(\n            v_new, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_do = tl.make_block_ptr(\n            do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_h = tl.make_block_ptr(\n            h, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)\n        )\n        p_dh = tl.make_block_ptr(\n            dh, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)\n        )\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v_new = tl.load(p_v_new, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dgk_last += tl.sum((b_h * b_dh).to(tl.float32), axis=0)\n\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n\n        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\n        b_db += tl.dot(b_v_new, b_dh.to(b_v_new.dtype))\n        p_dv = tl.make_block_ptr(\n            dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        b_dv = tl.load(p_dv, boundary_check=(0, 1))\n        b_dw += tl.dot(b_dv.to(b_v.dtype), b_h.to(b_v.dtype))\n\n    m_k = (i_k * BK + tl.arange(0, BK)) < K\n    last_idx = min(i_t * BT + BT, T) - 1\n    b_gk_last = tl.load(\n        gk + last_idx * stride_qk + i_k * BK + tl.arange(0, BK),\n        mask=m_k,\n        other=float(\"-inf\"),\n    )\n    b_dgk_last *= exp(b_gk_last)\n    p_k = tl.make_block_ptr(\n        k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_b = tl.make_block_ptr(\n        b, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_b = tl.load(p_b, boundary_check=(0, 1))\n    b_dgk_last += tl.sum(b_k * b_dk, axis=0)\n    b_dgk_last += tl.sum(b_b * b_db, axis=0)\n    tl.store(dgk_last + tl.arange(0, BK) + i_k * BK, b_dgk_last, mask=m_k)\n\n    p_dw = tl.make_block_ptr(\n        dw, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_dk = tl.make_block_ptr(\n        dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_db = tl.make_block_ptr(\n        db, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_dq = tl.make_block_ptr(\n        dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    tl.store(p_dw, b_dw.to(p_dw.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 97, 217, 97, 218, 97, 219, 97, 220, 97, 221, 97, 222, 97, 223, 97, 224, 56, 6, 97, 225, 56, 6, 97, 226, 56, 6, 97, 227, 56, 6, 97, 228, 56, 6, 97, 229, 56, 6, 97, 230, 56, 6, 149, 56, 30, -1, 231, 97, 232, 97, 233, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 234, 97, 235, 163, 202, 233, 45, 224, 97, 233, 188, 224, 149, 30, 155, 230, 56, 30, 236, 163, 232, 30, 237, 97, 232, 163, 202, 50, 202, 222, 66, 232, 203, 309, 149, 73, 238, 202, 205, 149, 97, 50, 202, 222, 66, 232, 203, 309, 66, 308, 149, 73, 238, 202, 205, 149, 149, 30, 239, 97, 240, 163, 202, 50, 202, 221, 66, 237, 149, 73, 238, 202, 205, 149, 97, 50, 202, 221, 66, 237, 66, 308, 149, 73, 238, 202, 205, 149, 149, 30, 223, 163, 240, 4, 239, 30, 241, 163, 58, 202, 223, 97, 227, 149, 30, 157, 30, 29, 56, 30, 241, 163, 58, 202, 223, 97, 227, 149, 30, 236, 163, 234, 203, 241, 66, 232, 30, 239, 97, 240, 163, 202, 234, 203, 223, 97, 234, 203, 223, 66, 223, 149, 30, 51, 30, 206, 146, 202, 239, 203, 224, 66, 235, 149, 203, 226, 30, 207, 146, 202, 239, 203, 224, 66, 235, 149, 203, 226, 30, 209, 146, 202, 239, 203, 224, 66, 235, 149, 203, 226, 30, 208, 146, 202, 236, 203, 224, 66, 235, 149, 203, 225, 203, 226, 30, 210, 146, 202, 236, 203, 224, 66, 235, 149, 203, 225, 203, 226, 30, 211, 146, 202, 239, 203, 224, 66, 235, 149, 203, 225, 30, 219, 146, 202, 239, 203, 224, 66, 235, 149, 203, 225, 30, 212, 146, 202, 239, 203, 224, 66, 235, 149, 203, 225, 30, 220, 146, 202, 239, 203, 224, 66, 235, 149, 203, 225, 30, 216, 146, 202, 239, 203, 224, 66, 235, 149, 203, 225, 30, 215, 146, 202, 239, 203, 224, 66, 235, 149, 203, 226, 30, 214, 146, 202, 239, 203, 224, 66, 235, 149, 203, 225, 30, 213, 146, 202, 239, 203, 224, 66, 235, 149, 203, 225, 30, 218, 146, 202, 236, 203, 224, 66, 235, 149, 203, 225, 30, 217, 146, 202, 239, 203, 224, 66, 235, 149, 203, 225, 30, 242, 163, 224, 203, 225, 30, 243, 163, 224, 203, 226, 30, 244, 163, 150, 202, 191, 227, 97, 228, 26, 97, 81, 163, 128, 149, 30, 245, 163, 150, 202, 191, 227, 97, 228, 26, 97, 81, 163, 128, 149, 30, 246, 163, 150, 202, 191, 227, 97, 228, 26, 97, 81, 163, 128, 149, 30, 247, 163, 150, 202, 191, 227, 97, 228, 26, 97, 81, 163, 128, 149, 30, 248, 163, 150, 202, 191, 228, 26, 97, 81, 163, 128, 149, 30, 119, 249, 136, 5, 202, 58, 202, 226, 97, 229, 149, 149, 56, 30, 250, 163, 183, 202, 206, 97, 202, 223, 97, 226, 149, 97, 202, 243, 97, 308, 149, 97, 202, 232, 203, 227, 97, 249, 203, 229, 149, 97, 202, 227, 97, 229, 149, 97, 202, 308, 97, 307, 149, 149, 30, 251, 163, 183, 202, 207, 97, 202, 223, 97, 226, 149, 97, 202, 243, 97, 308, 149, 97, 202, 232, 203, 227, 97, 249, 203, 229, 149, 97, 202, 227, 97, 229, 149, 97, 202, 308, 97, 307, 149, 149, 30, 252, 163, 183, 202, 209, 97, 202, 223, 97, 226, 149, 97, 202, 243, 97, 308, 149, 97, 202, 232, 203, 227, 97, 249, 203, 229, 149, 97, 202, 227, 97, 229, 149, 97, 202, 308, 97, 307, 149, 149, 30, 253, 163, 183, 202, 208, 97, 202, 226, 97, 225, 149, 97, 202, 308, 97, 226, 149, 97, 202, 249, 203, 229, 97, 231, 203, 228, 149, 97, 202, 229, 97, 228, 149, 97, 202, 307, 97, 308, 149, 149, 30, 254, 163, 183, 202, 210, 97, 202, 226, 97, 225, 149, 97, 202, 308, 97, 226, 149, 97, 202, 249, 203, 229, 97, 231, 203, 228, 149, 97, 202, 229, 97, 228, 149, 97, 202, 307, 97, 308, 149, 149, 30, 255, 163, 50, 202, 250, 97, 256, 163, 202, 307, 97, 308, 149, 149, 30, 257, 163, 50, 202, 251, 97, 256, 163, 202, 307, 97, 308, 149, 149, 30, 258, 163, 50, 202, 252, 97, 256, 163, 202, 307, 97, 308, 149, 149, 30, 259, 163, 50, 202, 253, 97, 256, 163, 202, 307, 97, 308, 149, 149, 30, 260, 163, 50, 202, 254, 97, 256, 163, 202, 307, 97, 308, 149, 149, 30, 248, 146, 185, 202, 202, 259, 203, 260, 149, 73, 238, 202, 128, 149, 97, 261, 163, 307, 149, 30, 244, 146, 15, 202, 258, 97, 259, 73, 238, 202, 258, 73, 81, 149, 149, 30, 245, 146, 15, 202, 255, 97, 260, 73, 238, 202, 255, 73, 81, 149, 149, 30, 247, 146, 15, 202, 257, 97, 260, 73, 238, 202, 257, 73, 81, 149, 149, 30, 262, 163, 183, 202, 215, 97, 202, 223, 97, 226, 149, 97, 202, 243, 97, 308, 149, 97, 202, 232, 203, 227, 97, 249, 203, 229, 149, 97, 202, 227, 97, 229, 149, 97, 202, 308, 97, 307, 149, 149, 30, 263, 163, 50, 202, 262, 97, 256, 163, 202, 307, 97, 308, 149, 149, 30, 246, 146, 15, 202, 263, 73, 238, 202, 255, 73, 81, 149, 97, 259, 73, 238, 202, 255, 73, 81, 149, 149, 30, 69, 30, 264, 163, 231, 203, 228, 66, 67, 202, 307, 97, 228, 149, 1, 225, 30, 265, 163, 37, 202, 232, 203, 227, 66, 227, 97, 223, 149, 4, 308, 30, 266, 163, 50, 202, 217, 66, 265, 203, 242, 66, 231, 203, 228, 66, 67, 202, 307, 97, 228, 149, 97, 267, 163, 264, 97, 268, 163, 269, 202, 310, 149, 149, 30, 248, 23, 173, 202, 266, 149, 30, 270, 163, 183, 202, 219, 97, 202, 223, 97, 225, 149, 97, 202, 242, 97, 308, 149, 97, 202, 232, 203, 227, 97, 231, 203, 228, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 271, 163, 183, 202, 220, 97, 202, 223, 97, 225, 149, 97, 202, 242, 97, 308, 149, 97, 202, 232, 203, 227, 97, 231, 203, 228, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 272, 163, 50, 202, 270, 97, 256, 163, 202, 307, 97, 308, 149, 149, 30, 273, 163, 50, 202, 271, 97, 256, 163, 202, 307, 97, 308, 149, 149, 30, 248, 146, 185, 202, 272, 203, 245, 97, 261, 163, 307, 149, 30, 248, 146, 185, 202, 273, 203, 247, 97, 261, 163, 307, 149, 30, 10, 202, 218, 66, 67, 202, 307, 97, 228, 149, 66, 231, 203, 228, 97, 248, 97, 267, 163, 264, 149, 30, 274, 163, 183, 202, 216, 97, 202, 223, 97, 225, 149, 97, 202, 242, 97, 308, 149, 97, 202, 232, 203, 227, 97, 231, 203, 228, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 275, 163, 183, 202, 211, 97, 202, 223, 97, 225, 149, 97, 202, 242, 97, 308, 149, 97, 202, 232, 203, 227, 97, 231, 203, 228, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 276, 163, 183, 202, 212, 97, 202, 223, 97, 225, 149, 97, 202, 242, 97, 308, 149, 97, 202, 232, 203, 227, 97, 231, 203, 228, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 277, 163, 183, 202, 214, 97, 202, 223, 97, 225, 149, 97, 202, 242, 97, 308, 149, 97, 202, 232, 203, 227, 97, 231, 203, 228, 149, 97, 202, 227, 97, 228, 149, 97, 202, 308, 97, 307, 149, 149, 30, 10, 202, 274, 97, 246, 73, 238, 202, 274, 73, 81, 73, 102, 149, 97, 256, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 275, 97, 245, 73, 238, 202, 275, 73, 81, 73, 102, 149, 97, 256, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 276, 97, 247, 73, 238, 202, 276, 73, 81, 73, 102, 149, 97, 256, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 277, 97, 244, 73, 238, 202, 277, 73, 81, 73, 102, 149, 97, 256, 163, 202, 307, 97, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_dplr_bwd_kernel_dv(\n    A_qk,\n    kg,\n    do,\n    dv,\n    dh,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n\n    A_qk += (bos * H + i_h) * BT\n    do += (bos * H + i_h) * V\n    dv += (bos * H + i_h) * V\n    kg += (bos * H + i_h) * K\n    dh += (i_tg * H + i_h) * K * V\n\n    stride_qk = H * K\n    stride_vo = H * V\n    stride_A = H * BT\n\n    for i_k in range(tl.cdiv(K, BK)):\n        p_dh = tl.make_block_ptr(\n            dh, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        p_kg = tl.make_block_ptr(\n            kg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_kg = tl.load(p_kg, boundary_check=(0, 1))\n        b_dv += tl.dot(b_kg, b_dh.to(b_kg.dtype))\n\n    p_Aqk = tl.make_block_ptr(\n        A_qk, (BT, T), (1, stride_A), (0, i_t * BT), (BT, BT), (0, 1)\n    )\n    b_A = tl.where(\n        tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :],\n        tl.load(p_Aqk, boundary_check=(0, 1)),\n        0,\n    )\n    p_do = tl.make_block_ptr(\n        do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_dv = tl.make_block_ptr(\n        dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv += tl.dot(b_A.to(b_do.dtype), b_do)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 57, 6, 98, 215, 57, 6, 98, 216, 57, 6, 98, 217, 57, 6, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 149, 57, 30, -1, 221, 98, 222, 98, 223, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 224, 98, 225, 163, 202, 223, 45, 214, 98, 223, 188, 214, 149, 30, 155, 220, 57, 30, 226, 163, 222, 30, 227, 98, 222, 163, 202, 52, 202, 212, 67, 222, 203, 309, 149, 74, 228, 202, 56, 149, 98, 52, 202, 212, 67, 222, 203, 309, 67, 308, 149, 74, 228, 202, 56, 149, 149, 30, 229, 98, 230, 163, 202, 52, 202, 211, 67, 227, 149, 74, 228, 202, 56, 149, 98, 52, 202, 211, 67, 227, 67, 308, 149, 74, 228, 202, 56, 149, 149, 30, 213, 163, 230, 4, 229, 30, 231, 163, 59, 202, 213, 98, 217, 149, 30, 157, 30, 29, 57, 30, 231, 163, 59, 202, 213, 98, 217, 149, 30, 226, 163, 224, 203, 231, 67, 222, 30, 229, 98, 230, 163, 202, 224, 203, 213, 98, 224, 203, 213, 67, 213, 149, 30, 51, 30, 232, 163, 150, 202, 191, 217, 98, 219, 26, 98, 82, 163, 129, 149, 30, 206, 146, 202, 229, 203, 214, 67, 225, 149, 203, 217, 30, 208, 146, 202, 229, 203, 214, 67, 225, 149, 203, 216, 30, 209, 146, 202, 229, 203, 214, 67, 225, 149, 203, 216, 30, 207, 146, 202, 229, 203, 214, 67, 225, 149, 203, 215, 30, 210, 146, 202, 226, 203, 214, 67, 225, 149, 203, 215, 203, 216, 30, 233, 163, 214, 203, 215, 30, 234, 163, 214, 203, 216, 30, 235, 163, 214, 203, 217, 30, 120, 236, 136, 5, 202, 59, 202, 215, 98, 218, 149, 149, 57, 30, 237, 163, 183, 202, 210, 98, 202, 215, 98, 216, 149, 98, 202, 216, 98, 308, 149, 98, 202, 236, 203, 218, 98, 221, 203, 219, 149, 98, 202, 218, 98, 219, 149, 98, 202, 308, 98, 307, 149, 149, 30, 238, 163, 183, 202, 207, 98, 202, 213, 98, 215, 149, 98, 202, 233, 98, 308, 149, 98, 202, 222, 203, 217, 98, 236, 203, 218, 149, 98, 202, 217, 98, 218, 149, 98, 202, 308, 98, 307, 149, 149, 30, 239, 163, 52, 202, 237, 98, 240, 163, 202, 307, 98, 308, 149, 149, 30, 241, 163, 52, 202, 238, 98, 240, 163, 202, 307, 98, 308, 149, 149, 30, 232, 146, 15, 202, 241, 98, 239, 74, 228, 202, 241, 74, 82, 149, 149, 30, 70, 30, 242, 163, 183, 202, 206, 98, 202, 217, 98, 213, 149, 98, 202, 308, 98, 235, 149, 98, 202, 307, 98, 222, 203, 217, 149, 98, 202, 217, 98, 217, 149, 98, 202, 307, 98, 308, 149, 149, 30, 243, 163, 171, 202, 68, 202, 307, 98, 217, 149, 191, 57, 98, 168, 26, 182, 68, 202, 307, 98, 217, 149, 191, 168, 98, 57, 26, 98, 52, 202, 242, 98, 240, 163, 202, 307, 98, 308, 149, 149, 98, 307, 149, 30, 244, 163, 183, 202, 208, 98, 202, 213, 98, 216, 149, 98, 202, 234, 98, 308, 149, 98, 202, 222, 203, 217, 98, 221, 203, 219, 149, 98, 202, 217, 98, 219, 149, 98, 202, 308, 98, 307, 149, 149, 30, 245, 163, 183, 202, 209, 98, 202, 213, 98, 216, 149, 98, 202, 234, 98, 308, 149, 98, 202, 222, 203, 217, 98, 221, 203, 219, 149, 98, 202, 217, 98, 219, 149, 98, 202, 308, 98, 307, 149, 149, 30, 246, 163, 52, 202, 244, 98, 240, 163, 202, 307, 98, 308, 149, 149, 30, 232, 146, 15, 202, 243, 74, 228, 202, 246, 74, 82, 149, 98, 246, 149, 30, 10, 202, 245, 98, 232, 74, 228, 202, 245, 74, 82, 74, 103, 149, 98, 240, 163, 202, 307, 98, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_dplr_fwd_kernel_o(\n    qg,\n    v,\n    v_new,\n    A_qk,\n    A_qb,\n    h,\n    o,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_qg = tl.make_block_ptr(\n            qg + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_h = tl.make_block_ptr(\n            h + (i_tg * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        b_qg = tl.load(p_qg, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_qg, b_h)\n\n    p_Aqk = tl.make_block_ptr(\n        A_qk + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n    p_Aqb = tl.make_block_ptr(\n        A_qb + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n    p_v = tl.make_block_ptr(\n        v + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    p_v_new = tl.make_block_ptr(\n        v_new + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    p_o = tl.make_block_ptr(\n        o + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n    b_Aqk = tl.load(p_Aqk, boundary_check=(0, 1))\n    b_Aqb = tl.load(p_Aqb, boundary_check=(0, 1))\n    b_Aqk = tl.where(m_s, b_Aqk, 0)\n    b_Aqb = tl.where(m_s, b_Aqb, 0)\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_v_new = tl.load(p_v_new, boundary_check=(0, 1))\n    b_o = (\n        b_o\n        + tl.dot(b_Aqk.to(b_v.dtype), b_v)\n        + tl.dot(b_Aqb.to(b_v_new.dtype), b_v_new)\n    )\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 56, 6, 97, 217, 56, 6, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 149, 56, 30, -1, 223, 97, 224, 97, 225, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 226, 97, 227, 163, 202, 225, 45, 216, 97, 225, 188, 216, 149, 30, 155, 222, 56, 30, 228, 163, 224, 30, 229, 97, 224, 163, 202, 50, 202, 214, 66, 224, 203, 309, 149, 73, 230, 202, 205, 149, 97, 50, 202, 214, 66, 224, 203, 309, 66, 308, 149, 73, 230, 202, 205, 149, 149, 30, 231, 97, 232, 163, 202, 50, 202, 213, 66, 229, 149, 73, 230, 202, 205, 149, 97, 50, 202, 213, 66, 229, 66, 308, 149, 73, 230, 202, 205, 149, 149, 30, 215, 163, 232, 4, 231, 30, 233, 163, 58, 202, 215, 97, 219, 149, 30, 157, 30, 29, 56, 30, 233, 163, 58, 202, 215, 97, 219, 149, 30, 228, 163, 226, 203, 233, 66, 224, 30, 231, 97, 232, 163, 202, 226, 203, 215, 97, 226, 203, 215, 66, 215, 149, 30, 51, 30, 234, 163, 150, 202, 191, 219, 97, 221, 26, 97, 81, 163, 128, 149, 30, 119, 235, 136, 5, 202, 58, 202, 217, 97, 220, 149, 149, 56, 30, 236, 163, 183, 202, 206, 66, 202, 231, 203, 216, 66, 227, 149, 203, 217, 97, 202, 215, 97, 217, 149, 97, 202, 216, 203, 217, 97, 308, 149, 97, 202, 224, 203, 219, 97, 235, 203, 220, 149, 97, 202, 219, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 237, 163, 183, 202, 211, 66, 202, 228, 203, 216, 66, 227, 149, 203, 217, 203, 218, 97, 202, 217, 97, 218, 149, 97, 202, 218, 97, 308, 149, 97, 202, 235, 203, 220, 97, 223, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 238, 163, 50, 202, 236, 97, 239, 163, 202, 307, 97, 308, 149, 149, 30, 240, 163, 50, 202, 237, 97, 239, 163, 202, 307, 97, 308, 149, 149, 30, 234, 146, 15, 202, 238, 97, 240, 149, 30, 69, 30, 241, 163, 183, 202, 209, 66, 202, 231, 203, 216, 66, 227, 149, 203, 219, 97, 202, 215, 97, 219, 149, 97, 202, 216, 203, 219, 97, 308, 149, 97, 202, 224, 203, 219, 97, 307, 149, 97, 202, 219, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 242, 163, 183, 202, 210, 66, 202, 231, 203, 216, 66, 227, 149, 203, 219, 97, 202, 215, 97, 219, 149, 97, 202, 216, 203, 219, 97, 308, 149, 97, 202, 224, 203, 219, 97, 307, 149, 97, 202, 219, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 243, 163, 183, 202, 207, 66, 202, 231, 203, 216, 66, 227, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 216, 203, 218, 97, 308, 149, 97, 202, 224, 203, 219, 97, 223, 203, 221, 149, 97, 202, 219, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 244, 163, 183, 202, 208, 66, 202, 231, 203, 216, 66, 227, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 216, 203, 218, 97, 308, 149, 97, 202, 224, 203, 219, 97, 223, 203, 221, 149, 97, 202, 219, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 245, 163, 183, 202, 212, 66, 202, 231, 203, 216, 66, 227, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 216, 203, 218, 97, 308, 149, 97, 202, 224, 203, 219, 97, 223, 203, 221, 149, 97, 202, 219, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 246, 163, 67, 202, 307, 97, 219, 149, 191, 56, 97, 168, 26, 127, 67, 202, 307, 97, 219, 149, 191, 168, 97, 56, 26, 30, 247, 163, 50, 202, 241, 97, 239, 163, 202, 307, 97, 308, 149, 149, 30, 248, 163, 50, 202, 242, 97, 239, 163, 202, 307, 97, 308, 149, 149, 30, 247, 163, 171, 202, 246, 97, 247, 97, 307, 149, 30, 248, 163, 171, 202, 246, 97, 248, 97, 307, 149, 30, 249, 163, 50, 202, 243, 97, 239, 163, 202, 307, 97, 308, 149, 149, 30, 250, 163, 50, 202, 244, 97, 239, 163, 202, 307, 97, 308, 149, 149, 30, 234, 163, 234, 66, 15, 202, 247, 73, 230, 202, 249, 73, 81, 149, 97, 249, 149, 66, 15, 202, 248, 73, 230, 202, 250, 73, 81, 149, 97, 250, 149, 30, 10, 202, 245, 97, 234, 73, 230, 202, 245, 73, 81, 73, 102, 149, 97, 239, 163, 202, 307, 97, 308, 149, 149, 30, 3, 30]}, {"code": "def fused_recurrent_dplr_delta_rule_fwd_kernel(\n    q,\n    k,\n    v,\n    a,\n    b,\n    gk,\n    o,\n    h0,\n    ht,\n    cu_seqlens,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    REVERSE: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_nh = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64)\n    i_n, i_h = i_nh // H, i_nh % H\n\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n\n    o_k = tl.arange(0, BK)\n    o_v = i_v * BV + tl.arange(0, BV)\n    p_q = q + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_k = k + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_a = a + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_b = b + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_gk = gk + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_v = v + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v\n    p_o = o + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v\n\n    mask_k = o_k < K\n    mask_v = o_v < V\n    mask_h = mask_k[None, :] & mask_v[:, None]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_nh * K * V + o_k[None, :] * V + o_v[:, None]\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)\n        b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)\n        b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n\n        tmp = tl.sum(b_h * b_a[None, :], axis=1)\n        b_h = exp(b_gk)[None, :] * b_h + (\n            tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None]\n        )\n        b_o = tl.sum(b_h * b_q[None, :], axis=1)\n\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n        p_q += (-1 if REVERSE else 1) * H * K\n        p_k += (-1 if REVERSE else 1) * H * K\n        p_a += (-1 if REVERSE else 1) * H * K\n        p_b += (-1 if REVERSE else 1) * H * K\n        p_gk += (-1 if REVERSE else 1) * H * K\n        p_v += (-1 if REVERSE else 1) * H * V\n        p_o += (-1 if REVERSE else 1) * H * V\n\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_nh * K * V + o_k[None, :] * V + o_v[:, None]\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 98, 227, 57, 6, 149, 57, 30, -1, 228, 98, 229, 163, 202, 145, 202, 307, 149, 74, 230, 202, 154, 149, 98, 145, 202, 308, 149, 74, 230, 202, 154, 149, 149, 30, 231, 98, 232, 163, 202, 229, 45, 219, 98, 229, 188, 219, 149, 30, 155, 227, 57, 30, 233, 98, 234, 163, 202, 52, 202, 215, 67, 231, 149, 74, 230, 202, 154, 149, 98, 52, 202, 215, 67, 231, 67, 308, 149, 74, 230, 202, 154, 149, 149, 30, 217, 163, 234, 4, 233, 30, 157, 30, 29, 57, 30, 233, 98, 234, 163, 202, 231, 203, 217, 98, 231, 203, 217, 67, 217, 149, 30, 51, 30, 235, 163, 68, 202, 307, 98, 222, 149, 30, 236, 163, 228, 203, 223, 67, 68, 202, 307, 98, 223, 149, 30, 237, 163, 206, 67, 202, 233, 67, 202, 217, 4, 308, 155, 224, 29, 307, 149, 149, 203, 219, 203, 220, 67, 232, 203, 220, 67, 235, 30, 238, 163, 207, 67, 202, 233, 67, 202, 217, 4, 308, 155, 224, 29, 307, 149, 149, 203, 219, 203, 220, 67, 232, 203, 220, 67, 235, 30, 239, 163, 209, 67, 202, 233, 67, 202, 217, 4, 308, 155, 224, 29, 307, 149, 149, 203, 219, 203, 220, 67, 232, 203, 220, 67, 235, 30, 240, 163, 210, 67, 202, 233, 67, 202, 217, 4, 308, 155, 224, 29, 307, 149, 149, 203, 219, 203, 220, 67, 232, 203, 220, 67, 235, 30, 241, 163, 211, 67, 202, 233, 67, 202, 217, 4, 308, 155, 224, 29, 307, 149, 149, 203, 219, 203, 220, 67, 232, 203, 220, 67, 235, 30, 242, 163, 208, 67, 202, 233, 67, 202, 217, 4, 308, 155, 224, 29, 307, 149, 149, 203, 219, 203, 221, 67, 232, 203, 221, 67, 236, 30, 243, 163, 212, 67, 202, 233, 67, 202, 217, 4, 308, 155, 224, 29, 307, 149, 149, 203, 219, 203, 221, 67, 232, 203, 221, 67, 236, 30, 244, 163, 235, 1, 220, 30, 245, 163, 236, 1, 221, 30, 246, 163, 244, 191, 168, 98, 57, 26, 147, 245, 191, 57, 98, 168, 26, 30, 247, 163, 150, 202, 191, 223, 98, 222, 26, 98, 82, 163, 129, 149, 30, 155, 225, 57, 30, 248, 163, 213, 67, 229, 203, 220, 203, 221, 67, 235, 191, 168, 98, 57, 26, 203, 221, 67, 236, 191, 57, 98, 168, 26, 30, 247, 146, 52, 202, 248, 98, 249, 163, 246, 98, 250, 163, 307, 149, 74, 230, 202, 129, 149, 30, 157, 30, 120, 251, 136, 5, 202, 307, 98, 217, 149, 57, 30, 252, 163, 52, 202, 237, 98, 249, 163, 244, 98, 250, 163, 307, 149, 74, 230, 202, 129, 149, 203, 216, 30, 253, 163, 52, 202, 238, 98, 249, 163, 244, 98, 250, 163, 307, 149, 74, 230, 202, 129, 149, 30, 254, 163, 52, 202, 239, 98, 249, 163, 244, 98, 250, 163, 307, 149, 74, 230, 202, 129, 149, 30, 255, 163, 52, 202, 240, 98, 249, 163, 244, 98, 250, 163, 307, 149, 74, 230, 202, 129, 149, 30, 256, 163, 52, 202, 241, 98, 249, 163, 244, 98, 250, 163, 307, 149, 74, 230, 202, 129, 149, 30, 257, 163, 52, 202, 242, 98, 249, 163, 245, 98, 250, 163, 307, 149, 74, 230, 202, 129, 149, 30, 258, 163, 185, 202, 247, 203, 254, 191, 168, 98, 57, 26, 98, 259, 163, 308, 149, 30, 247, 163, 173, 202, 256, 149, 191, 168, 98, 57, 26, 203, 247, 67, 202, 258, 191, 57, 98, 168, 26, 203, 255, 191, 168, 98, 57, 26, 67, 253, 191, 168, 98, 57, 26, 203, 257, 191, 57, 98, 168, 26, 149, 30, 260, 163, 185, 202, 247, 203, 252, 191, 168, 98, 57, 26, 98, 259, 163, 308, 149, 30, 10, 202, 243, 98, 260, 74, 230, 202, 243, 74, 82, 74, 103, 149, 98, 249, 163, 245, 149, 30, 237, 146, 202, 4, 308, 155, 224, 29, 308, 149, 203, 219, 203, 220, 30, 238, 146, 202, 4, 308, 155, 224, 29, 308, 149, 203, 219, 203, 220, 30, 239, 146, 202, 4, 308, 155, 224, 29, 308, 149, 203, 219, 203, 220, 30, 240, 146, 202, 4, 308, 155, 224, 29, 308, 149, 203, 219, 203, 220, 30, 241, 146, 202, 4, 308, 155, 224, 29, 308, 149, 203, 219, 203, 220, 30, 242, 146, 202, 4, 308, 155, 224, 29, 308, 149, 203, 219, 203, 221, 30, 243, 146, 202, 4, 308, 155, 224, 29, 308, 149, 203, 219, 203, 221, 30, 70, 30, 155, 226, 57, 30, 261, 163, 214, 67, 229, 203, 220, 203, 221, 67, 235, 191, 168, 98, 57, 26, 203, 221, 67, 236, 191, 57, 98, 168, 26, 30, 10, 202, 261, 98, 247, 74, 230, 202, 261, 74, 82, 74, 103, 149, 98, 249, 163, 246, 149, 30, 157, 30, 3, 30]}, {"code": "def prepare_wy_repr_bwd_kernel(\n    A_ab_inv,\n    A_ak,\n    ag,\n    v,\n    dw,\n    du,\n    dv,\n    dv0,\n    dag,\n    dAak,\n    dAab,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    p_Aak_t = tl.make_block_ptr(\n        A_ak + (bos * H + i_h) * BT,\n        (BT, T),\n        (1, H * BT),\n        (0, i_t * BT),\n        (BT, BT),\n        (0, 1),\n    )\n    p_Aab_inv_t = tl.make_block_ptr(\n        A_ab_inv + (bos * H + i_h) * BT,\n        (BT, T),\n        (1, H * BT),\n        (0, i_t * BT),\n        (BT, BT),\n        (0, 1),\n    )\n    p_dAak = tl.make_block_ptr(\n        dAak + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n    p_dAab = tl.make_block_ptr(\n        dAab + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n\n    b_A_ab_inv_t = tl.load(p_Aab_inv_t, boundary_check=(0, 1))\n    b_A_ak_t = tl.load(p_Aak_t, boundary_check=(0, 1))\n    b_A_ak_t = tl.where(\n        tl.arange(0, BT)[:, None] < tl.arange(0, BT)[None, :], b_A_ak_t, 0\n    )\n    b_A_ab_inv_t = tl.where(\n        tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :], b_A_ab_inv_t, 0\n    )\n    b_A_tmp_t = tl.dot(b_A_ak_t, b_A_ab_inv_t).to(v.dtype.element_ty)\n    b_dA_tmp = tl.zeros([BT, BT], dtype=tl.float32)\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dv = tl.make_block_ptr(\n            dv + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dv0 = tl.make_block_ptr(\n            dv0 + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_du = tl.make_block_ptr(\n            du + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_du = tl.load(p_du, boundary_check=(0, 1))\n        b_dA_tmp += tl.dot(b_du.to(b_v.dtype), tl.trans(b_v))\n        b_dv0 = tl.load(p_dv0, boundary_check=(0, 1))\n        b_dv = b_dv0 + tl.dot(b_A_tmp_t, b_du)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n\n    m_i = tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :]\n    b_dA_tmp = tl.where(m_i, b_dA_tmp, 0)\n    b_dA_ak = tl.dot(b_A_ab_inv_t, b_dA_tmp)\n    b_dA_ak = tl.where(m_i, b_dA_ak, 0)\n    tl.store(p_dAak, b_dA_ak, boundary_check=(0, 1))\n    b_dA_ab_inv = tl.dot(b_dA_tmp, b_A_ak_t)\n\n    for i_k in range(tl.cdiv(K, BK)):\n        p_ag = tl.make_block_ptr(\n            ag + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dag = tl.make_block_ptr(\n            dag + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dw = tl.make_block_ptr(\n            dw + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        b_ag = tl.load(p_ag, boundary_check=(0, 1))\n        b_dw = tl.load(p_dw, boundary_check=(0, 1))\n        b_dA_ab_inv += tl.dot(b_dw, tl.trans(b_ag))\n        b_dag = tl.dot(b_A_ab_inv_t.to(b_dw.dtype), b_dw)\n        tl.store(p_dag, b_dag.to(p_dag.dtype.element_ty), boundary_check=(0, 1))\n\n    b_dA_ab_inv = tl.where(\n        tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_dA_ab_inv, 0\n    )\n    b_dA_ab_inv = tl.dot(b_A_ab_inv_t, b_dA_ab_inv)\n    b_dA_ab_inv = tl.dot(b_dA_ab_inv, b_A_ab_inv_t)\n    b_dA_ab_inv = tl.where(m_i, b_dA_ab_inv, 0)\n    tl.store(p_dAab, b_dA_ab_inv, boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 97, 217, 97, 218, 97, 219, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 97, 224, 56, 6, 97, 225, 56, 6, 97, 226, 56, 6, 149, 56, 30, -1, 227, 97, 228, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 149, 30, 229, 97, 230, 163, 202, 228, 45, 220, 97, 228, 188, 220, 149, 30, 155, 226, 56, 30, 231, 97, 227, 163, 202, 50, 202, 218, 66, 227, 203, 309, 149, 73, 232, 202, 205, 149, 97, 50, 202, 218, 66, 227, 203, 309, 66, 308, 149, 73, 232, 202, 205, 149, 149, 30, 233, 97, 234, 163, 202, 50, 202, 217, 66, 231, 149, 73, 232, 202, 205, 149, 97, 50, 202, 217, 66, 231, 66, 308, 149, 73, 232, 202, 205, 149, 149, 30, 219, 163, 234, 4, 233, 30, 157, 30, 29, 56, 30, 233, 97, 234, 163, 202, 229, 203, 219, 97, 229, 203, 219, 66, 219, 149, 30, 51, 30, 235, 163, 183, 202, 207, 66, 202, 233, 203, 220, 66, 230, 149, 203, 223, 97, 202, 223, 97, 219, 149, 97, 202, 308, 97, 220, 203, 223, 149, 97, 202, 307, 97, 227, 203, 223, 149, 97, 202, 223, 97, 223, 149, 97, 202, 307, 97, 308, 149, 149, 30, 236, 163, 183, 202, 206, 66, 202, 233, 203, 220, 66, 230, 149, 203, 223, 97, 202, 223, 97, 219, 149, 97, 202, 308, 97, 220, 203, 223, 149, 97, 202, 307, 97, 227, 203, 223, 149, 97, 202, 223, 97, 223, 149, 97, 202, 307, 97, 308, 149, 149, 30, 237, 163, 183, 202, 215, 66, 202, 233, 203, 220, 66, 230, 149, 203, 223, 97, 202, 219, 97, 223, 149, 97, 202, 220, 203, 223, 97, 308, 149, 97, 202, 227, 203, 223, 97, 307, 149, 97, 202, 223, 97, 223, 149, 97, 202, 308, 97, 307, 149, 149, 30, 238, 163, 183, 202, 216, 66, 202, 233, 203, 220, 66, 230, 149, 203, 223, 97, 202, 219, 97, 223, 149, 97, 202, 220, 203, 223, 97, 308, 149, 97, 202, 227, 203, 223, 97, 307, 149, 97, 202, 223, 97, 223, 149, 97, 202, 308, 97, 307, 149, 149, 30, 239, 163, 50, 202, 236, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 241, 163, 50, 202, 235, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 241, 163, 171, 202, 67, 202, 307, 97, 223, 149, 191, 56, 97, 168, 26, 1, 67, 202, 307, 97, 223, 149, 191, 168, 97, 56, 26, 97, 241, 97, 307, 149, 30, 239, 163, 171, 202, 67, 202, 307, 97, 223, 149, 191, 56, 97, 168, 26, 182, 67, 202, 307, 97, 223, 149, 191, 168, 97, 56, 26, 97, 239, 97, 307, 149, 30, 242, 163, 15, 202, 241, 97, 239, 149, 73, 232, 202, 209, 73, 81, 73, 102, 149, 30, 243, 163, 150, 202, 191, 223, 97, 223, 26, 97, 81, 163, 128, 149, 30, 119, 244, 136, 5, 202, 58, 202, 222, 97, 225, 149, 149, 56, 30, 245, 163, 183, 202, 209, 66, 202, 233, 203, 220, 66, 230, 149, 203, 222, 97, 202, 219, 97, 222, 149, 97, 202, 220, 203, 222, 97, 308, 149, 97, 202, 227, 203, 223, 97, 244, 203, 225, 149, 97, 202, 223, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 246, 163, 183, 202, 212, 66, 202, 233, 203, 220, 66, 230, 149, 203, 222, 97, 202, 219, 97, 222, 149, 97, 202, 220, 203, 222, 97, 308, 149, 97, 202, 227, 203, 223, 97, 244, 203, 225, 149, 97, 202, 223, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 247, 163, 183, 202, 213, 66, 202, 233, 203, 220, 66, 230, 149, 203, 222, 97, 202, 219, 97, 222, 149, 97, 202, 220, 203, 222, 97, 308, 149, 97, 202, 227, 203, 223, 97, 244, 203, 225, 149, 97, 202, 223, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 248, 163, 183, 202, 211, 66, 202, 233, 203, 220, 66, 230, 149, 203, 222, 97, 202, 219, 97, 222, 149, 97, 202, 220, 203, 222, 97, 308, 149, 97, 202, 227, 203, 223, 97, 244, 203, 225, 149, 97, 202, 223, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 249, 163, 50, 202, 245, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 250, 163, 50, 202, 248, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 243, 146, 15, 202, 250, 73, 232, 202, 249, 73, 81, 149, 97, 64, 202, 249, 149, 149, 30, 251, 163, 50, 202, 247, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 252, 163, 251, 66, 15, 202, 242, 97, 250, 149, 30, 10, 202, 246, 97, 252, 73, 232, 202, 246, 73, 81, 73, 102, 149, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 69, 30, 253, 163, 67, 202, 307, 97, 223, 149, 191, 56, 97, 168, 26, 110, 67, 202, 307, 97, 223, 149, 191, 168, 97, 56, 26, 30, 243, 163, 171, 202, 253, 97, 243, 97, 307, 149, 30, 254, 163, 15, 202, 239, 97, 243, 149, 30, 254, 163, 171, 202, 253, 97, 254, 97, 307, 149, 30, 10, 202, 237, 97, 254, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 255, 163, 15, 202, 243, 97, 241, 149, 30, 119, 256, 136, 5, 202, 58, 202, 221, 97, 224, 149, 149, 56, 30, 257, 163, 183, 202, 208, 66, 202, 233, 203, 220, 66, 230, 149, 203, 221, 97, 202, 219, 97, 221, 149, 97, 202, 220, 203, 221, 97, 308, 149, 97, 202, 227, 203, 223, 97, 256, 203, 224, 149, 97, 202, 223, 97, 224, 149, 97, 202, 308, 97, 307, 149, 149, 30, 258, 163, 183, 202, 214, 66, 202, 233, 203, 220, 66, 230, 149, 203, 221, 97, 202, 219, 97, 221, 149, 97, 202, 220, 203, 221, 97, 308, 149, 97, 202, 227, 203, 223, 97, 256, 203, 224, 149, 97, 202, 223, 97, 224, 149, 97, 202, 308, 97, 307, 149, 149, 30, 259, 163, 183, 202, 210, 66, 202, 233, 203, 220, 66, 230, 149, 203, 221, 97, 202, 219, 97, 221, 149, 97, 202, 220, 203, 221, 97, 308, 149, 97, 202, 227, 203, 223, 97, 256, 203, 224, 149, 97, 202, 223, 97, 224, 149, 97, 202, 308, 97, 307, 149, 149, 30, 260, 163, 50, 202, 257, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 261, 163, 50, 202, 259, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 255, 146, 15, 202, 261, 97, 64, 202, 260, 149, 149, 30, 262, 163, 15, 202, 239, 73, 232, 202, 261, 73, 81, 149, 97, 261, 149, 30, 10, 202, 258, 97, 262, 73, 232, 202, 258, 73, 81, 73, 102, 149, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 69, 30, 255, 163, 171, 202, 67, 202, 307, 97, 223, 149, 191, 56, 97, 168, 26, 127, 67, 202, 307, 97, 223, 149, 191, 168, 97, 56, 26, 97, 255, 97, 307, 149, 30, 255, 163, 15, 202, 239, 97, 255, 149, 30, 255, 163, 15, 202, 255, 97, 239, 149, 30, 255, 163, 171, 202, 253, 97, 255, 97, 307, 149, 30, 10, 202, 238, 97, 255, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 3, 30]}, {"code": "def prepare_wy_repr_fwd_kernel_chunk32(\n    A_ab,\n    A_ab_inv,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n    p_Aab = tl.make_block_ptr(\n        A_ab + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n    p_Aab_inv = tl.make_block_ptr(\n        A_ab_inv + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n    b_A_ab = tl.load(p_Aab, boundary_check=(0, 1))\n    b_A_ab = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_A_ab, 0)\n    for i in range(1, BT):\n        mask = tl.arange(0, BT) == i\n        b_a = tl.sum(tl.where(mask[:, None], b_A_ab, 0), 0)\n        b_a = b_a + tl.sum(b_a[:, None] * b_A_ab, 0) * (tl.arange(0, BT) < i)\n        b_A_ab = tl.where(mask[:, None], b_a, b_A_ab)\n    b_A_ab += tl.arange(0, BT)[:, None] == tl.arange(0, BT)[None, :]\n    tl.store(p_Aab_inv, b_A_ab.to(p_Aab_inv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 57, 6, 98, 212, 57, 6, 98, 213, 57, 6, 98, 214, 57, 6, 149, 57, 30, -1, 215, 98, 216, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 149, 30, 217, 98, 218, 163, 202, 216, 45, 211, 98, 216, 188, 211, 149, 30, 155, 214, 57, 30, 219, 98, 215, 163, 202, 52, 202, 209, 67, 215, 203, 309, 149, 74, 220, 202, 56, 149, 98, 52, 202, 209, 67, 215, 203, 309, 67, 308, 149, 74, 220, 202, 56, 149, 149, 30, 221, 98, 222, 163, 202, 52, 202, 208, 67, 219, 149, 74, 220, 202, 56, 149, 98, 52, 202, 208, 67, 219, 67, 308, 149, 74, 220, 202, 56, 149, 149, 30, 210, 163, 222, 4, 221, 30, 157, 30, 29, 57, 30, 221, 98, 222, 163, 202, 217, 203, 210, 98, 217, 203, 210, 67, 210, 149, 30, 51, 30, 223, 163, 183, 202, 206, 67, 202, 221, 203, 211, 67, 218, 149, 203, 212, 98, 202, 210, 98, 212, 149, 98, 202, 211, 203, 212, 98, 308, 149, 98, 202, 215, 203, 212, 98, 307, 149, 98, 202, 212, 98, 212, 149, 98, 202, 308, 98, 307, 149, 149, 30, 224, 163, 183, 202, 207, 67, 202, 221, 203, 211, 67, 218, 149, 203, 212, 98, 202, 210, 98, 212, 149, 98, 202, 211, 203, 212, 98, 308, 149, 98, 202, 215, 203, 212, 98, 307, 149, 98, 202, 212, 98, 212, 149, 98, 202, 308, 98, 307, 149, 149, 30, 225, 163, 52, 202, 223, 98, 226, 163, 202, 307, 98, 308, 149, 149, 30, 225, 163, 171, 202, 68, 202, 307, 98, 212, 149, 191, 57, 98, 168, 26, 111, 68, 202, 307, 98, 212, 149, 191, 168, 98, 57, 26, 98, 225, 98, 307, 149, 30, 120, 227, 136, 5, 202, 308, 98, 212, 149, 57, 30, 228, 163, 68, 202, 307, 98, 212, 149, 69, 227, 30, 229, 163, 185, 202, 171, 202, 228, 191, 57, 98, 168, 26, 98, 225, 98, 307, 149, 98, 307, 149, 30, 229, 163, 229, 67, 185, 202, 229, 191, 57, 98, 168, 26, 203, 225, 98, 307, 149, 203, 202, 68, 202, 307, 98, 212, 149, 1, 227, 149, 30, 225, 163, 171, 202, 228, 191, 57, 98, 168, 26, 98, 229, 98, 225, 149, 30, 70, 30, 225, 146, 68, 202, 307, 98, 212, 149, 191, 57, 98, 168, 26, 69, 68, 202, 307, 98, 212, 149, 191, 168, 98, 57, 26, 30, 10, 202, 224, 98, 225, 74, 220, 202, 224, 74, 82, 74, 103, 149, 98, 226, 163, 202, 307, 98, 308, 149, 149, 30, 3, 30]}, {"code": "def prepare_wy_repr_fwd_kernel_chunk64(\n    A_ab,\n    A_ab_inv,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    GATHER_SUPPORTED: tl.constexpr\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    p_A1 = tl.make_block_ptr(\n        A_ab + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, 0),\n        (BC, BC),\n        (1, 0),\n    )\n    p_A2 = tl.make_block_ptr(\n        A_ab + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT + BC, BC),\n        (BC, BC),\n        (1, 0),\n    )\n    p_A3 = tl.make_block_ptr(\n        A_ab + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT + BC, 0),\n        (BC, BC),\n        (1, 0),\n    )\n    p_A_inv1 = tl.make_block_ptr(\n        A_ab_inv + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, 0),\n        (BC, BC),\n        (1, 0),\n    )\n    p_A_inv2 = tl.make_block_ptr(\n        A_ab_inv + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT + BC, BC),\n        (BC, BC),\n        (1, 0),\n    )\n    p_A_inv3 = tl.make_block_ptr(\n        A_ab_inv + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT + BC, 0),\n        (BC, BC),\n        (1, 0),\n    )\n    p_A_inv4 = tl.make_block_ptr(\n        A_ab_inv + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, BC),\n        (BC, BC),\n        (1, 0),\n    )\n\n    b_A = tl.load(p_A1, boundary_check=(0, 1))\n    b_A2 = tl.load(p_A2, boundary_check=(0, 1))\n    b_A3 = tl.load(p_A3, boundary_check=(0, 1))\n    b_A = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A, 0)\n    b_A2 = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A2, 0)\n\n    for i in range(1, BC):\n        if GATHER_SUPPORTED:\n            row_idx = tl.full([1, BC], i, dtype=tl.int16)\n\n            b_a = tl.sum(gather(b_A, row_idx, axis=0), 0)\n            b_a2 = tl.sum(gather(b_A2, row_idx, axis=0), 0)\n        else:\n            mask = tl.arange(0, BC) == i\n            b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)\n            b_a2 = tl.sum(tl.where(mask[:, None], b_A2, 0), 0)\n        mask = tl.arange(0, BC) == i\n\n        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BC) < i)\n        b_a2 = b_a2 + tl.sum(b_a2[:, None] * b_A2, 0) * (tl.arange(0, BC) < i)\n        b_A = tl.where(mask[:, None], b_a, b_A)\n        b_A2 = tl.where(mask[:, None], b_a2, b_A2)\n\n    b_A += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\n    b_A2 += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\n    b_A3 = tl.dot(tl.dot(b_A2, b_A3), b_A)\n\n    tl.store(\n        p_A_inv1,\n        b_A.to(p_A_inv1.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_A_inv2,\n        b_A2.to(p_A_inv2.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_A_inv3,\n        b_A3.to(p_A_inv3.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n\n    tl.store(\n        p_A_inv4,\n        tl.zeros([BC, BC], dtype=tl.float32).to(p_A_inv4.dtype.element_ty),\n        boundary_check=(0, 1),\n    )", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 56, 6, 97, 212, 56, 6, 97, 213, 56, 6, 97, 214, 56, 6, 97, 215, 56, 6, 149, 56, 30, -1, 216, 97, 217, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 149, 30, 218, 97, 219, 163, 202, 217, 45, 211, 97, 217, 188, 211, 149, 30, 155, 214, 56, 30, 220, 97, 216, 163, 202, 50, 202, 209, 66, 216, 203, 309, 149, 73, 221, 202, 205, 149, 97, 50, 202, 209, 66, 216, 203, 309, 66, 308, 149, 73, 221, 202, 205, 149, 149, 30, 222, 97, 223, 163, 202, 50, 202, 208, 66, 220, 149, 73, 221, 202, 205, 149, 97, 50, 202, 208, 66, 220, 66, 308, 149, 73, 221, 202, 205, 149, 149, 30, 210, 163, 223, 4, 222, 30, 157, 30, 29, 56, 30, 222, 97, 223, 163, 202, 218, 203, 210, 97, 218, 203, 210, 66, 210, 149, 30, 51, 30, 224, 163, 183, 202, 206, 66, 202, 222, 203, 211, 66, 219, 149, 203, 212, 97, 202, 210, 97, 212, 149, 97, 202, 211, 203, 212, 97, 308, 149, 97, 202, 216, 203, 212, 97, 307, 149, 97, 202, 213, 97, 213, 149, 97, 202, 308, 97, 307, 149, 149, 30, 225, 163, 183, 202, 206, 66, 202, 222, 203, 211, 66, 219, 149, 203, 212, 97, 202, 210, 97, 212, 149, 97, 202, 211, 203, 212, 97, 308, 149, 97, 202, 216, 203, 212, 66, 213, 97, 213, 149, 97, 202, 213, 97, 213, 149, 97, 202, 308, 97, 307, 149, 149, 30, 226, 163, 183, 202, 206, 66, 202, 222, 203, 211, 66, 219, 149, 203, 212, 97, 202, 210, 97, 212, 149, 97, 202, 211, 203, 212, 97, 308, 149, 97, 202, 216, 203, 212, 66, 213, 97, 307, 149, 97, 202, 213, 97, 213, 149, 97, 202, 308, 97, 307, 149, 149, 30, 227, 163, 183, 202, 207, 66, 202, 222, 203, 211, 66, 219, 149, 203, 212, 97, 202, 210, 97, 212, 149, 97, 202, 211, 203, 212, 97, 308, 149, 97, 202, 216, 203, 212, 97, 307, 149, 97, 202, 213, 97, 213, 149, 97, 202, 308, 97, 307, 149, 149, 30, 228, 163, 183, 202, 207, 66, 202, 222, 203, 211, 66, 219, 149, 203, 212, 97, 202, 210, 97, 212, 149, 97, 202, 211, 203, 212, 97, 308, 149, 97, 202, 216, 203, 212, 66, 213, 97, 213, 149, 97, 202, 213, 97, 213, 149, 97, 202, 308, 97, 307, 149, 149, 30, 229, 163, 183, 202, 207, 66, 202, 222, 203, 211, 66, 219, 149, 203, 212, 97, 202, 210, 97, 212, 149, 97, 202, 211, 203, 212, 97, 308, 149, 97, 202, 216, 203, 212, 66, 213, 97, 307, 149, 97, 202, 213, 97, 213, 149, 97, 202, 308, 97, 307, 149, 149, 30, 230, 163, 183, 202, 207, 66, 202, 222, 203, 211, 66, 219, 149, 203, 212, 97, 202, 210, 97, 212, 149, 97, 202, 211, 203, 212, 97, 308, 149, 97, 202, 216, 203, 212, 97, 213, 149, 97, 202, 213, 97, 213, 149, 97, 202, 308, 97, 307, 149, 149, 30, 231, 163, 50, 202, 224, 97, 232, 163, 202, 307, 97, 308, 149, 149, 30, 233, 163, 50, 202, 225, 97, 232, 163, 202, 307, 97, 308, 149, 149, 30, 234, 163, 50, 202, 226, 97, 232, 163, 202, 307, 97, 308, 149, 149, 30, 231, 163, 171, 202, 67, 202, 307, 97, 213, 149, 191, 56, 97, 168, 26, 110, 67, 202, 307, 97, 213, 149, 191, 168, 97, 56, 26, 97, 231, 97, 307, 149, 30, 233, 163, 171, 202, 67, 202, 307, 97, 213, 149, 191, 56, 97, 168, 26, 110, 67, 202, 307, 97, 213, 149, 191, 168, 97, 56, 26, 97, 233, 97, 307, 149, 30, 119, 235, 136, 5, 202, 308, 97, 213, 149, 56, 30, 155, 215, 56, 30, 236, 163, 197, 202, 191, 308, 97, 213, 26, 97, 235, 97, 81, 163, 17, 149, 30, 237, 163, 185, 202, 238, 202, 231, 97, 236, 97, 239, 163, 307, 149, 97, 307, 149, 30, 240, 163, 185, 202, 238, 202, 233, 97, 236, 97, 239, 163, 307, 149, 97, 307, 149, 30, 157, 30, 29, 56, 30, 241, 163, 67, 202, 307, 97, 213, 149, 68, 235, 30, 237, 163, 185, 202, 171, 202, 241, 191, 56, 97, 168, 26, 97, 231, 97, 307, 149, 97, 307, 149, 30, 240, 163, 185, 202, 171, 202, 241, 191, 56, 97, 168, 26, 97, 233, 97, 307, 149, 97, 307, 149, 30, 51, 30, 241, 163, 67, 202, 307, 97, 213, 149, 68, 235, 30, 237, 163, 237, 66, 185, 202, 237, 191, 56, 97, 168, 26, 203, 231, 97, 307, 149, 203, 202, 67, 202, 307, 97, 213, 149, 1, 235, 149, 30, 240, 163, 240, 66, 185, 202, 240, 191, 56, 97, 168, 26, 203, 233, 97, 307, 149, 203, 202, 67, 202, 307, 97, 213, 149, 1, 235, 149, 30, 231, 163, 171, 202, 241, 191, 56, 97, 168, 26, 97, 237, 97, 231, 149, 30, 233, 163, 171, 202, 241, 191, 56, 97, 168, 26, 97, 240, 97, 233, 149, 30, 69, 30, 231, 146, 67, 202, 307, 97, 213, 149, 191, 56, 97, 168, 26, 68, 67, 202, 307, 97, 213, 149, 191, 168, 97, 56, 26, 30, 233, 146, 67, 202, 307, 97, 213, 149, 191, 56, 97, 168, 26, 68, 67, 202, 307, 97, 213, 149, 191, 168, 97, 56, 26, 30, 234, 163, 15, 202, 15, 202, 233, 97, 234, 149, 97, 231, 149, 30, 10, 202, 227, 97, 231, 73, 221, 202, 227, 73, 81, 73, 102, 97, 142, 163, 310, 149, 97, 232, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 228, 97, 233, 73, 221, 202, 228, 73, 81, 73, 102, 97, 142, 163, 310, 149, 97, 232, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 229, 97, 234, 73, 221, 202, 229, 73, 81, 73, 102, 97, 142, 163, 310, 149, 97, 232, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 230, 97, 150, 202, 191, 213, 97, 213, 26, 97, 81, 163, 128, 149, 73, 221, 202, 230, 73, 81, 73, 102, 149, 97, 232, 163, 202, 307, 97, 308, 149, 149, 30, 3, 30]}, {"code": "def wu_fwd_kernel(\n    w,\n    u,\n    ag,\n    v,\n    A_ab_inv,\n    A_ak,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n    o_s = tl.arange(0, BT)\n\n    p_A_ab_inv = tl.make_block_ptr(\n        A_ab_inv + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n    p_A_ak = tl.make_block_ptr(\n        A_ak + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n\n    b_Aab_inv = tl.load(p_A_ab_inv, boundary_check=(0, 1))\n    b_Aak = tl.load(p_A_ak, boundary_check=(0, 1))\n    b_Aab_inv = tl.where(o_s[:, None] >= o_s[None, :], b_Aab_inv, 0)\n    b_Aak = tl.where(o_s[:, None] > o_s[None, :], b_Aak, 0)\n\n    b_Aak = tl.dot(b_Aab_inv, b_Aak)\n\n    b_Aak = b_Aak.to(v.dtype.element_ty, fp_downcast_rounding=\"rtne\")\n    b_Aab_inv = b_Aab_inv.to(ag.dtype.element_ty, fp_downcast_rounding=\"rtne\")\n\n    for i_k in range(tl.cdiv(K, BK)):\n        p_ag = tl.make_block_ptr(\n            ag + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_w = tl.make_block_ptr(\n            w + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        b_ag = tl.load(p_ag, boundary_check=(0, 1))\n        b_w = tl.dot(b_Aab_inv, b_ag)\n        tl.store(\n            p_w,\n            b_w.to(p_w.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n            boundary_check=(0, 1),\n        )\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_u = tl.make_block_ptr(\n            u + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_u = tl.dot(b_Aak, b_v)\n        tl.store(\n            p_u,\n            b_u.to(p_u.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n            boundary_check=(0, 1),\n        )", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 57, 6, 98, 216, 57, 6, 98, 217, 57, 6, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 149, 57, 30, -1, 222, 98, 223, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 149, 30, 224, 98, 225, 163, 202, 223, 45, 215, 98, 223, 188, 215, 149, 30, 155, 221, 57, 30, 226, 98, 222, 163, 202, 52, 202, 213, 67, 222, 203, 309, 149, 74, 227, 202, 56, 149, 98, 52, 202, 213, 67, 222, 203, 309, 67, 308, 149, 74, 227, 202, 56, 149, 149, 30, 228, 98, 229, 163, 202, 52, 202, 212, 67, 226, 149, 74, 227, 202, 56, 149, 98, 52, 202, 212, 67, 226, 67, 308, 149, 74, 227, 202, 56, 149, 149, 30, 214, 163, 229, 4, 228, 30, 157, 30, 29, 57, 30, 228, 98, 229, 163, 202, 224, 203, 214, 98, 224, 203, 214, 67, 214, 149, 30, 51, 30, 230, 163, 68, 202, 307, 98, 218, 149, 30, 231, 163, 183, 202, 210, 67, 202, 228, 203, 215, 67, 225, 149, 203, 218, 98, 202, 214, 98, 218, 149, 98, 202, 215, 203, 218, 98, 308, 149, 98, 202, 222, 203, 218, 98, 307, 149, 98, 202, 218, 98, 218, 149, 98, 202, 308, 98, 307, 149, 149, 30, 232, 163, 183, 202, 211, 67, 202, 228, 203, 215, 67, 225, 149, 203, 218, 98, 202, 214, 98, 218, 149, 98, 202, 215, 203, 218, 98, 308, 149, 98, 202, 222, 203, 218, 98, 307, 149, 98, 202, 218, 98, 218, 149, 98, 202, 308, 98, 307, 149, 149, 30, 233, 163, 52, 202, 231, 98, 234, 163, 202, 307, 98, 308, 149, 149, 30, 235, 163, 52, 202, 232, 98, 234, 163, 202, 307, 98, 308, 149, 149, 30, 233, 163, 171, 202, 230, 191, 57, 98, 168, 26, 128, 230, 191, 168, 98, 57, 26, 98, 233, 98, 307, 149, 30, 235, 163, 171, 202, 230, 191, 57, 98, 168, 26, 111, 230, 191, 168, 98, 57, 26, 98, 235, 98, 307, 149, 30, 235, 163, 15, 202, 233, 98, 235, 149, 30, 235, 163, 235, 74, 227, 202, 209, 74, 82, 74, 103, 98, 142, 163, 310, 149, 30, 233, 163, 233, 74, 227, 202, 208, 74, 82, 74, 103, 98, 142, 163, 310, 149, 30, 120, 236, 136, 5, 202, 59, 202, 216, 98, 219, 149, 149, 57, 30, 237, 163, 183, 202, 208, 67, 202, 228, 203, 215, 67, 225, 149, 203, 216, 98, 202, 214, 98, 216, 149, 98, 202, 215, 203, 216, 98, 308, 149, 98, 202, 222, 203, 218, 98, 236, 203, 219, 149, 98, 202, 218, 98, 219, 149, 98, 202, 308, 98, 307, 149, 149, 30, 238, 163, 183, 202, 206, 67, 202, 228, 203, 215, 67, 225, 149, 203, 216, 98, 202, 214, 98, 216, 149, 98, 202, 215, 203, 216, 98, 308, 149, 98, 202, 222, 203, 218, 98, 236, 203, 219, 149, 98, 202, 218, 98, 219, 149, 98, 202, 308, 98, 307, 149, 149, 30, 239, 163, 52, 202, 237, 98, 234, 163, 202, 307, 98, 308, 149, 149, 30, 240, 163, 15, 202, 233, 98, 239, 149, 30, 10, 202, 238, 98, 240, 74, 227, 202, 238, 74, 82, 74, 103, 98, 142, 163, 310, 149, 98, 234, 163, 202, 307, 98, 308, 149, 149, 30, 70, 30, 120, 241, 136, 5, 202, 59, 202, 217, 98, 220, 149, 149, 57, 30, 242, 163, 183, 202, 209, 67, 202, 228, 203, 215, 67, 225, 149, 203, 217, 98, 202, 214, 98, 217, 149, 98, 202, 215, 203, 217, 98, 308, 149, 98, 202, 222, 203, 218, 98, 241, 203, 220, 149, 98, 202, 218, 98, 220, 149, 98, 202, 308, 98, 307, 149, 149, 30, 243, 163, 183, 202, 207, 67, 202, 228, 203, 215, 67, 225, 149, 203, 217, 98, 202, 214, 98, 217, 149, 98, 202, 215, 203, 217, 98, 308, 149, 98, 202, 222, 203, 218, 98, 241, 203, 220, 149, 98, 202, 218, 98, 220, 149, 98, 202, 308, 98, 307, 149, 149, 30, 244, 163, 52, 202, 242, 98, 234, 163, 202, 307, 98, 308, 149, 149, 30, 245, 163, 15, 202, 235, 98, 244, 149, 30, 10, 202, 243, 98, 245, 74, 227, 202, 243, 74, 82, 74, 103, 98, 142, 163, 310, 149, 98, 234, 163, 202, 307, 98, 308, 149, 149, 30, 70, 30, 3, 30]}, {"code": "def chunk_generalized_iplr_delta_rule_fwd_kernel_h(\n    k,\n    v,\n    d,\n    b,\n    u,\n    v_new,\n    h,\n    h0,\n    ht,\n    cu_seqlens,\n    chunk_offsets,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(\n            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT):\n        p_h = tl.make_block_ptr(\n            h + ((boh + i_t) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_hc = tl.zeros([BK, BV], dtype=tl.float32)\n\n        for i_c in range(tl.cdiv(min(BT, T - i_t * BT), BC)):\n            p_k = tl.make_block_ptr(\n                k + (bos * H + i_h) * K,\n                (K, T),\n                (1, H * K),\n                (i_k * BK, i_t * BT + i_c * BC),\n                (BK, BC),\n                (0, 1),\n            )\n            p_b = tl.make_block_ptr(\n                b + (bos * H + i_h) * K,\n                (K, T),\n                (1, H * K),\n                (i_k * BK, i_t * BT + i_c * BC),\n                (BK, BC),\n                (0, 1),\n            )\n            p_d = tl.make_block_ptr(\n                d + (bos * H + i_h) * K,\n                (T, K),\n                (H * K, 1),\n                (i_t * BT + i_c * BC, i_k * BK),\n                (BC, BK),\n                (1, 0),\n            )\n            p_v = tl.make_block_ptr(\n                v + (bos * H + i_h) * V,\n                (T, V),\n                (H * V, 1),\n                (i_t * BT + i_c * BC, i_v * BV),\n                (BC, BV),\n                (1, 0),\n            )\n            p_u = tl.make_block_ptr(\n                u + (bos * H + i_h) * V,\n                (T, V),\n                (H * V, 1),\n                (i_t * BT + i_c * BC, i_v * BV),\n                (BC, BV),\n                (1, 0),\n            )\n            p_v_new = tl.make_block_ptr(\n                v_new + (bos * H + i_h) * V,\n                (T, V),\n                (H * V, 1),\n                (i_t * BT + i_c * BC, i_v * BV),\n                (BC, BV),\n                (1, 0),\n            )\n\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n            b_d = tl.load(p_d, boundary_check=(0, 1))\n            b_b = tl.load(p_b, boundary_check=(0, 1))\n            b_v2 = tl.dot(b_d, b_h.to(b_d.dtype)) + tl.load(p_u, boundary_check=(0, 1))\n            b_hc += tl.dot(b_k, b_v)\n            b_hc += tl.dot(b_b, b_v2.to(b_k.dtype))\n            tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))\n        b_h += b_hc\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(\n            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 97, 217, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 97, 224, 56, 6, 97, 225, 56, 6, 97, 226, 56, 6, 97, 227, 56, 6, 149, 56, 30, -1, 228, 97, 229, 97, 230, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 231, 97, 232, 163, 202, 230, 45, 218, 97, 230, 188, 218, 149, 30, 155, 227, 56, 30, 233, 97, 234, 163, 202, 50, 202, 215, 66, 231, 149, 73, 235, 202, 205, 149, 97, 50, 202, 215, 66, 231, 66, 308, 149, 73, 235, 202, 205, 149, 149, 30, 217, 163, 234, 4, 233, 30, 236, 163, 58, 202, 217, 97, 221, 149, 30, 237, 163, 50, 202, 216, 66, 231, 149, 73, 235, 202, 205, 149, 30, 157, 30, 29, 56, 30, 233, 97, 234, 163, 202, 231, 203, 217, 97, 231, 203, 217, 66, 217, 149, 30, 236, 163, 58, 202, 217, 97, 221, 149, 30, 237, 163, 231, 203, 236, 30, 51, 30, 238, 163, 150, 202, 191, 223, 97, 224, 26, 97, 81, 163, 128, 149, 30, 155, 225, 56, 30, 239, 163, 183, 202, 213, 66, 230, 203, 219, 203, 220, 97, 202, 219, 97, 220, 149, 97, 202, 220, 97, 308, 149, 97, 202, 228, 203, 223, 97, 229, 203, 224, 149, 97, 202, 223, 97, 224, 149, 97, 202, 308, 97, 307, 149, 149, 30, 238, 163, 50, 202, 239, 97, 240, 163, 202, 307, 97, 308, 149, 149, 73, 235, 202, 128, 149, 30, 157, 30, 119, 241, 136, 5, 202, 236, 149, 56, 30, 242, 163, 183, 202, 212, 66, 202, 202, 237, 66, 241, 149, 203, 218, 66, 232, 149, 203, 219, 203, 220, 97, 202, 219, 97, 220, 149, 97, 202, 220, 97, 308, 149, 97, 202, 228, 203, 223, 97, 229, 203, 224, 149, 97, 202, 223, 97, 224, 149, 97, 202, 308, 97, 307, 149, 149, 30, 10, 202, 242, 97, 238, 73, 235, 202, 242, 73, 81, 73, 102, 149, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 243, 163, 150, 202, 191, 223, 97, 224, 26, 97, 81, 163, 128, 149, 30, 119, 244, 136, 5, 202, 58, 202, 37, 202, 221, 97, 217, 4, 241, 203, 221, 149, 97, 222, 149, 149, 56, 30, 245, 163, 183, 202, 206, 66, 202, 233, 203, 218, 66, 232, 149, 203, 219, 97, 202, 219, 97, 217, 149, 97, 202, 308, 97, 218, 203, 219, 149, 97, 202, 228, 203, 223, 97, 241, 203, 221, 66, 244, 203, 222, 149, 97, 202, 223, 97, 222, 149, 97, 202, 307, 97, 308, 149, 149, 30, 246, 163, 183, 202, 209, 66, 202, 233, 203, 218, 66, 232, 149, 203, 219, 97, 202, 219, 97, 217, 149, 97, 202, 308, 97, 218, 203, 219, 149, 97, 202, 228, 203, 223, 97, 241, 203, 221, 66, 244, 203, 222, 149, 97, 202, 223, 97, 222, 149, 97, 202, 307, 97, 308, 149, 149, 30, 247, 163, 183, 202, 208, 66, 202, 233, 203, 218, 66, 232, 149, 203, 219, 97, 202, 217, 97, 219, 149, 97, 202, 218, 203, 219, 97, 308, 149, 97, 202, 241, 203, 221, 66, 244, 203, 222, 97, 228, 203, 223, 149, 97, 202, 222, 97, 223, 149, 97, 202, 308, 97, 307, 149, 149, 30, 248, 163, 183, 202, 207, 66, 202, 233, 203, 218, 66, 232, 149, 203, 220, 97, 202, 217, 97, 220, 149, 97, 202, 218, 203, 220, 97, 308, 149, 97, 202, 241, 203, 221, 66, 244, 203, 222, 97, 229, 203, 224, 149, 97, 202, 222, 97, 224, 149, 97, 202, 308, 97, 307, 149, 149, 30, 249, 163, 183, 202, 210, 66, 202, 233, 203, 218, 66, 232, 149, 203, 220, 97, 202, 217, 97, 220, 149, 97, 202, 218, 203, 220, 97, 308, 149, 97, 202, 241, 203, 221, 66, 244, 203, 222, 97, 229, 203, 224, 149, 97, 202, 222, 97, 224, 149, 97, 202, 308, 97, 307, 149, 149, 30, 250, 163, 183, 202, 211, 66, 202, 233, 203, 218, 66, 232, 149, 203, 220, 97, 202, 217, 97, 220, 149, 97, 202, 218, 203, 220, 97, 308, 149, 97, 202, 241, 203, 221, 66, 244, 203, 222, 97, 229, 203, 224, 149, 97, 202, 222, 97, 224, 149, 97, 202, 308, 97, 307, 149, 149, 30, 251, 163, 50, 202, 245, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 252, 163, 50, 202, 248, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 253, 163, 50, 202, 247, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 254, 163, 50, 202, 246, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 255, 163, 15, 202, 253, 97, 238, 73, 235, 202, 253, 73, 81, 149, 149, 66, 50, 202, 249, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 243, 146, 15, 202, 251, 97, 252, 149, 30, 243, 146, 15, 202, 254, 97, 255, 73, 235, 202, 251, 73, 81, 149, 149, 30, 10, 202, 250, 97, 255, 73, 235, 202, 250, 73, 81, 73, 102, 149, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 69, 30, 238, 146, 243, 30, 69, 30, 155, 226, 56, 30, 256, 163, 183, 202, 214, 66, 230, 203, 219, 203, 220, 97, 202, 219, 97, 220, 149, 97, 202, 220, 97, 308, 149, 97, 202, 228, 203, 223, 97, 229, 203, 224, 149, 97, 202, 223, 97, 224, 149, 97, 202, 308, 97, 307, 149, 149, 30, 10, 202, 256, 97, 238, 73, 235, 202, 256, 73, 81, 73, 102, 149, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 157, 30, 3, 30]}, {"code": "def chunk_generalized_iplr_delta_rule_fwd_kernel_o(\n    q,\n    k,\n    v,\n    u,\n    b,\n    h,\n    o,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    q += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    b += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V\n    u += (bos * H + i_h) * V\n    o += (bos * H + i_h) * V\n    h += (i_tg * H + i_h) * K * V\n    stride_qk = H * K\n    stride_vo = H * V\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_Aqk = tl.zeros([BT, BT], dtype=tl.float32)\n    b_Aqb = tl.zeros([BT, BT], dtype=tl.float32)\n\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(\n            q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_k = tl.make_block_ptr(\n            k, (K, T), (1, stride_qk), (i_k * BK, i_t * BT), (BK, BT), (0, 1)\n        )\n        p_h = tl.make_block_ptr(\n            h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        p_b = tl.make_block_ptr(\n            b, (K, T), (1, stride_qk), (i_k * BK, i_t * BT), (BK, BT), (0, 1)\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_b = tl.load(p_b, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n\n        b_o += tl.dot(b_q, b_h)\n\n        b_Aqk += tl.dot(b_q, b_k)\n\n        b_Aqb += tl.dot(b_q, b_b)\n\n    o_i = tl.arange(0, BT)\n    m_A = o_i[:, None] >= o_i[None, :]\n    b_Aqk = tl.where(m_A, b_Aqk, 0)\n    b_Aqb = tl.where(m_A, b_Aqb, 0)\n\n    p_v = tl.make_block_ptr(\n        v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_u = tl.make_block_ptr(\n        u, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_o = tl.make_block_ptr(\n        o, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_u = tl.load(p_u, boundary_check=(0, 1))\n    b_o = (\n        b_o + tl.dot(b_Aqk.to(b_v.dtype), b_v) + tl.dot(b_Aqb.to(b_u.dtype), b_u)\n    ) * scale\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 57, 6, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 149, 57, 30, -1, 224, 98, 225, 98, 226, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 227, 98, 228, 163, 202, 226, 45, 217, 98, 226, 188, 217, 149, 30, 155, 223, 57, 30, 229, 163, 225, 30, 230, 98, 225, 163, 202, 52, 202, 214, 67, 225, 203, 309, 149, 74, 231, 202, 56, 149, 98, 52, 202, 214, 67, 225, 203, 309, 67, 308, 149, 74, 231, 202, 56, 149, 149, 30, 232, 98, 233, 163, 202, 52, 202, 213, 67, 230, 149, 74, 231, 202, 56, 149, 98, 52, 202, 213, 67, 230, 67, 308, 149, 74, 231, 202, 56, 149, 149, 30, 216, 163, 233, 4, 232, 30, 234, 163, 59, 202, 216, 98, 220, 149, 30, 157, 30, 29, 57, 30, 234, 163, 59, 202, 216, 98, 220, 149, 30, 229, 163, 227, 203, 234, 67, 225, 30, 232, 98, 233, 163, 202, 227, 203, 216, 98, 227, 203, 216, 67, 216, 149, 30, 51, 30, 206, 146, 202, 232, 203, 217, 67, 228, 149, 203, 218, 30, 207, 146, 202, 232, 203, 217, 67, 228, 149, 203, 218, 30, 210, 146, 202, 232, 203, 217, 67, 228, 149, 203, 218, 30, 208, 146, 202, 232, 203, 217, 67, 228, 149, 203, 219, 30, 209, 146, 202, 232, 203, 217, 67, 228, 149, 203, 219, 30, 212, 146, 202, 232, 203, 217, 67, 228, 149, 203, 219, 30, 211, 146, 202, 229, 203, 217, 67, 228, 149, 203, 218, 203, 219, 30, 235, 163, 217, 203, 218, 30, 236, 163, 217, 203, 219, 30, 237, 163, 150, 202, 191, 220, 98, 222, 26, 98, 82, 163, 129, 149, 30, 238, 163, 150, 202, 191, 220, 98, 220, 26, 98, 82, 163, 129, 149, 30, 239, 163, 150, 202, 191, 220, 98, 220, 26, 98, 82, 163, 129, 149, 30, 120, 240, 136, 5, 202, 59, 202, 218, 98, 221, 149, 149, 57, 30, 241, 163, 183, 202, 206, 98, 202, 216, 98, 218, 149, 98, 202, 235, 98, 308, 149, 98, 202, 225, 203, 220, 98, 240, 203, 221, 149, 98, 202, 220, 98, 221, 149, 98, 202, 308, 98, 307, 149, 149, 30, 242, 163, 183, 202, 207, 98, 202, 218, 98, 216, 149, 98, 202, 308, 98, 235, 149, 98, 202, 240, 203, 221, 98, 225, 203, 220, 149, 98, 202, 221, 98, 220, 149, 98, 202, 307, 98, 308, 149, 149, 30, 243, 163, 183, 202, 211, 98, 202, 218, 98, 219, 149, 98, 202, 219, 98, 308, 149, 98, 202, 240, 203, 221, 98, 224, 203, 222, 149, 98, 202, 221, 98, 222, 149, 98, 202, 308, 98, 307, 149, 149, 30, 244, 163, 183, 202, 210, 98, 202, 218, 98, 216, 149, 98, 202, 308, 98, 235, 149, 98, 202, 240, 203, 221, 98, 225, 203, 220, 149, 98, 202, 221, 98, 220, 149, 98, 202, 307, 98, 308, 149, 149, 30, 245, 163, 52, 202, 241, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 247, 163, 52, 202, 242, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 248, 163, 52, 202, 244, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 249, 163, 52, 202, 243, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 237, 146, 15, 202, 245, 98, 249, 149, 30, 238, 146, 15, 202, 245, 98, 247, 149, 30, 239, 146, 15, 202, 245, 98, 248, 149, 30, 70, 30, 250, 163, 68, 202, 307, 98, 220, 149, 30, 251, 163, 250, 191, 57, 98, 168, 26, 128, 250, 191, 168, 98, 57, 26, 30, 238, 163, 171, 202, 251, 98, 238, 98, 307, 149, 30, 239, 163, 171, 202, 251, 98, 239, 98, 307, 149, 30, 252, 163, 183, 202, 208, 98, 202, 216, 98, 219, 149, 98, 202, 236, 98, 308, 149, 98, 202, 225, 203, 220, 98, 224, 203, 222, 149, 98, 202, 220, 98, 222, 149, 98, 202, 308, 98, 307, 149, 149, 30, 253, 163, 183, 202, 209, 98, 202, 216, 98, 219, 149, 98, 202, 236, 98, 308, 149, 98, 202, 225, 203, 220, 98, 224, 203, 222, 149, 98, 202, 220, 98, 222, 149, 98, 202, 308, 98, 307, 149, 149, 30, 254, 163, 183, 202, 212, 98, 202, 216, 98, 219, 149, 98, 202, 236, 98, 308, 149, 98, 202, 225, 203, 220, 98, 224, 203, 222, 149, 98, 202, 220, 98, 222, 149, 98, 202, 308, 98, 307, 149, 149, 30, 255, 163, 52, 202, 252, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 256, 163, 52, 202, 253, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 237, 163, 202, 237, 67, 15, 202, 238, 74, 231, 202, 255, 74, 82, 149, 98, 255, 149, 67, 15, 202, 239, 74, 231, 202, 256, 74, 82, 149, 98, 256, 149, 149, 203, 215, 30, 10, 202, 254, 98, 237, 74, 231, 202, 254, 74, 82, 74, 103, 149, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 3, 30]}, {"code": "def fused_recurrent_fwd_kernel(\n    q,\n    k,\n    v,\n    a,\n    b,\n    o,\n    ha,\n    h0,\n    ht,\n    cu_seqlens,\n    scale,\n    H,\n    T,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_nh = tl.program_id(0), tl.program_id(1)\n    i_n, i_h = i_nh // H, i_nh % H\n\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n\n    p_q = q + (bos * H + i_h) * K + tl.arange(0, BK)\n    p_k = k + (bos * H + i_h) * K + tl.arange(0, BK)\n    p_a = a + (bos * H + i_h) * K + tl.arange(0, BK)\n    p_b = b + (bos * H + i_h) * K + tl.arange(0, BK)\n    p_ha = ha + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\n    p_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\n    p_o = o + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\n\n    mask_k = tl.arange(0, BK) < K\n    mask_v = (i_v * BV + tl.arange(0, BV)) < V\n    mask_h = mask_k[None, :] & mask_v[:, None]\n\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = (\n            h0\n            + i_nh * K * V\n            + (tl.arange(0, BK)[None, :]) * V\n            + ((i_v * BV + tl.arange(0, BV))[:, None])\n        )\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)\n        b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)\n\n        tmp = tl.sum(b_h * b_a[None, :], axis=1)\n        b_h += tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None]\n        b_o = b_h * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n        tl.store(p_ha, tmp.to(p_ha.dtype.element_ty), mask=mask_v)\n        p_q += K * H\n        p_k += K * H\n        p_o += V * H\n        p_v += V * H\n        p_ha += V * H\n        p_a += K * H\n        p_b += K * H\n\n    if STORE_FINAL_STATE:\n        p_ht = (\n            ht\n            + i_nh * K * V\n            + (tl.arange(0, BK)[None, :]) * V\n            + ((i_v * BV + tl.arange(0, BV))[:, None])\n        )\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 97, 217, 97, 218, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 97, 224, 56, 6, 97, 225, 56, 6, 149, 56, 30, -1, 226, 97, 227, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 149, 30, 228, 97, 229, 163, 202, 227, 45, 217, 97, 227, 188, 217, 149, 30, 155, 225, 56, 30, 230, 97, 231, 163, 202, 50, 202, 215, 66, 228, 149, 73, 232, 202, 154, 149, 97, 50, 202, 215, 66, 228, 66, 308, 149, 73, 232, 202, 154, 149, 149, 30, 218, 163, 231, 4, 230, 30, 157, 30, 29, 56, 30, 230, 97, 231, 163, 202, 228, 203, 218, 97, 228, 203, 218, 66, 218, 149, 30, 51, 30, 233, 163, 206, 66, 202, 230, 203, 217, 66, 229, 149, 203, 219, 66, 67, 202, 307, 97, 221, 149, 30, 234, 163, 207, 66, 202, 230, 203, 217, 66, 229, 149, 203, 219, 66, 67, 202, 307, 97, 221, 149, 30, 235, 163, 209, 66, 202, 230, 203, 217, 66, 229, 149, 203, 219, 66, 67, 202, 307, 97, 221, 149, 30, 236, 163, 210, 66, 202, 230, 203, 217, 66, 229, 149, 203, 219, 66, 67, 202, 307, 97, 221, 149, 30, 237, 163, 212, 66, 202, 230, 203, 217, 66, 229, 149, 203, 220, 66, 226, 203, 222, 66, 67, 202, 307, 97, 222, 149, 30, 238, 163, 208, 66, 202, 230, 203, 217, 66, 229, 149, 203, 220, 66, 226, 203, 222, 66, 67, 202, 307, 97, 222, 149, 30, 239, 163, 211, 66, 202, 230, 203, 217, 66, 229, 149, 203, 220, 66, 226, 203, 222, 66, 67, 202, 307, 97, 222, 149, 30, 240, 163, 67, 202, 307, 97, 221, 149, 1, 219, 30, 241, 163, 226, 203, 222, 66, 67, 202, 307, 97, 222, 149, 1, 220, 30, 242, 163, 240, 191, 168, 97, 56, 26, 147, 241, 191, 56, 97, 168, 26, 30, 243, 163, 150, 202, 191, 222, 97, 221, 26, 97, 81, 163, 128, 149, 30, 155, 223, 56, 30, 244, 163, 213, 66, 227, 203, 219, 203, 220, 66, 67, 202, 307, 97, 221, 149, 191, 168, 97, 56, 26, 203, 220, 66, 202, 226, 203, 222, 66, 67, 202, 307, 97, 222, 149, 149, 191, 56, 97, 168, 26, 30, 243, 146, 50, 202, 244, 97, 245, 163, 242, 97, 246, 163, 307, 149, 73, 232, 202, 128, 149, 30, 157, 30, 119, 247, 136, 5, 202, 307, 97, 218, 149, 56, 30, 248, 163, 50, 202, 234, 97, 245, 163, 240, 97, 246, 163, 307, 149, 73, 232, 202, 128, 149, 30, 249, 163, 50, 202, 238, 97, 245, 163, 241, 97, 246, 163, 307, 149, 73, 232, 202, 128, 149, 30, 250, 163, 50, 202, 233, 97, 245, 163, 240, 97, 246, 163, 307, 149, 73, 232, 202, 128, 149, 203, 216, 30, 251, 163, 50, 202, 235, 97, 245, 163, 240, 97, 246, 163, 307, 149, 73, 232, 202, 128, 149, 30, 252, 163, 50, 202, 236, 97, 245, 163, 240, 97, 246, 163, 307, 149, 73, 232, 202, 128, 149, 30, 253, 163, 185, 202, 243, 203, 251, 191, 168, 97, 56, 26, 97, 254, 163, 308, 149, 30, 243, 146, 253, 191, 56, 97, 168, 26, 203, 252, 191, 168, 97, 56, 26, 66, 248, 191, 168, 97, 56, 26, 203, 249, 191, 56, 97, 168, 26, 30, 255, 163, 243, 203, 250, 191, 168, 97, 56, 26, 30, 255, 163, 185, 202, 255, 97, 254, 163, 308, 149, 30, 10, 202, 239, 97, 255, 73, 232, 202, 239, 73, 81, 73, 102, 149, 97, 245, 163, 241, 149, 30, 10, 202, 237, 97, 253, 73, 232, 202, 237, 73, 81, 73, 102, 149, 97, 245, 163, 241, 149, 30, 233, 146, 219, 203, 217, 30, 234, 146, 219, 203, 217, 30, 239, 146, 220, 203, 217, 30, 238, 146, 220, 203, 217, 30, 237, 146, 220, 203, 217, 30, 235, 146, 219, 203, 217, 30, 236, 146, 219, 203, 217, 30, 69, 30, 155, 224, 56, 30, 256, 163, 214, 66, 227, 203, 219, 203, 220, 66, 67, 202, 307, 97, 221, 149, 191, 168, 97, 56, 26, 203, 220, 66, 202, 226, 203, 222, 66, 67, 202, 307, 97, 222, 149, 149, 191, 56, 97, 168, 26, 30, 10, 202, 256, 97, 243, 73, 232, 202, 256, 73, 81, 73, 102, 149, 97, 245, 163, 242, 149, 30, 157, 30, 3, 30]}, {"code": "def fused_recurrent_bwd_kernel(\n    q,\n    k,\n    v,\n    a,\n    b,\n    ha,\n    dht,\n    dh0,\n    do,\n    dq,\n    dk,\n    dv,\n    da,\n    db,\n    dha,\n    h0,\n    scale,\n    cu_seqlens,\n    B,\n    H,\n    T,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    USE_DH0: tl.constexpr,\n    USE_DHT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_nh = tl.program_id(0), tl.program_id(1)\n    i_n, i_h = i_nh // H, i_nh % H\n    dk += i_v * B * H * K * T\n    db += i_v * B * H * K * T\n    dq += i_v * B * H * K * T\n    da += i_v * B * H * K * T\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n    mask_k = tl.arange(0, BK) < K\n    mask_v = (tl.arange(0, BV) + i_v * BV) < V\n\n    q += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V + i_v * BV\n    ha += (bos * H + i_h) * V + i_v * BV\n    a += (bos * H + i_h) * K\n    b += (bos * H + i_h) * K\n    do += (bos * H + i_h) * V + i_v * BV\n    dq += (bos * H + i_h) * K\n    dk += (bos * H + i_h) * K\n    dv += (bos * H + i_h) * V + i_v * BV\n    da += (bos * H + i_h) * K\n    db += (bos * H + i_h) * K\n    dha += (bos * H + i_h) * V + i_v * BV\n\n    p_q = q + tl.arange(0, BK) + (T - 1) * H * K\n    p_k = k + tl.arange(0, BK) + (T - 1) * H * K\n    p_v = v + tl.arange(0, BV) + (T - 1) * H * V\n    p_ha = ha + tl.arange(0, BV) + (T - 1) * H * V\n    p_a = a + tl.arange(0, BK) + (T - 1) * H * K\n    p_b = b + tl.arange(0, BK) + (T - 1) * H * K\n    p_do = do + tl.arange(0, BV) + (T - 1) * H * V\n    p_dk = dk + tl.arange(0, BK) + (T - 1) * H * K\n    p_dv = dv + tl.arange(0, BV) + (T - 1) * H * V\n    p_dha = dha + tl.arange(0, BV) + (T - 1) * H * V\n    p_db = db + tl.arange(0, BK) + (T - 1) * H * K\n    p_da = da + tl.arange(0, BK) + (T - 1) * H * K\n    p_dq = dq + tl.arange(0, BK) + (T - 1) * H * K\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_DHT:\n        p_ht = (\n            dht\n            + i_nh * K * V\n            + (tl.arange(0, BK)[:, None]) * V\n            + ((i_v * BV + tl.arange(0, BV))[None, :])\n        )\n        b_dh += tl.load(p_ht, mask=mask_k[:, None] & mask_v[None, :], other=0).to(\n            tl.float32\n        )\n\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)\n        b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)\n        b_ha = tl.load(p_ha, mask=mask_v, other=0).to(tl.float32)\n\n        b_dh += b_q[:, None] * b_do[None, :]\n        d_k = tl.sum(b_dh * b_v[None, :], axis=1)\n        d_v = tl.sum(b_dh * b_k[:, None], axis=0)\n        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_k)\n        tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_v)\n\n        b_dha = tl.sum(b_dh * b_b[:, None], axis=0)\n        tl.store(p_dha, b_dha.to(p_dha.dtype.element_ty), mask=mask_v)\n        b_db = tl.sum(b_dh * b_ha[None, :], axis=1)\n        tl.store(p_db, b_db.to(p_db.dtype.element_ty), mask=mask_k)\n\n        b_dh += b_dha[None, :] * b_a[:, None]\n        p_do -= H * V\n        p_q -= H * K\n        p_k -= H * K\n        p_v -= H * V\n        p_dk -= H * K\n        p_dv -= H * V\n        p_b -= H * K\n        p_db -= H * K\n        p_a -= H * K\n        p_dha -= H * V\n        p_ha -= H * V\n\n    if USE_DH0:\n        p_dh0 = (\n            dh0\n            + i_nh * K * V\n            + (tl.arange(0, BK)[:, None]) * V\n            + (i_v * BV + tl.arange(0, BV)[None, :])\n        )\n        tl.store(\n            p_dh0,\n            b_dh.to(p_dh0.dtype.element_ty),\n            mask=mask_k[:, None] & mask_v[None, :],\n        )\n\n    tl.debug_barrier()\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        mask_kv = mask_k[:, None] & mask_v[None, :]\n        p_h0 = (\n            h0\n            + i_nh * K * V\n            + (tl.arange(0, BK)[:, None]) * V\n            + ((i_v * BV + tl.arange(0, BV))[None, :])\n        )\n        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n\n    p_k = k + tl.arange(0, BK)\n    p_v = v + tl.arange(0, BV)\n    p_ha = ha + tl.arange(0, BV)\n    p_do = do + tl.arange(0, BV)\n    p_dha = dha + tl.arange(0, BV)\n    p_da = da + tl.arange(0, BK)\n    p_dq = dq + tl.arange(0, BK)\n    p_b = b + tl.arange(0, BK)\n\n    for i in range(0, T):\n        b_dha = tl.load(p_dha, mask=mask_v, other=0).to(tl.float32)\n        d_a = tl.sum(b_dha[None, :] * b_h, axis=1)\n        tl.store(p_da, d_a.to(p_da.dtype.element_ty), mask=mask_k)\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)\n        b_ha = tl.load(p_ha, mask=mask_v, other=0).to(tl.float32)\n        b_h += b_k[:, None] * b_v[None, :] + b_b[:, None] * b_ha[None, :]\n        _d_q = b_h * b_do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_k)\n\n        p_k += H * K\n        p_do += H * V\n        p_v += H * V\n        p_da += H * K\n        p_dha += H * V\n        p_ha += H * V\n        p_dq += H * K\n        p_b += H * K", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 98, 227, 57, 6, 98, 228, 57, 6, 98, 229, 57, 6, 98, 230, 57, 6, 98, 231, 57, 6, 98, 232, 57, 6, 98, 233, 57, 6, 98, 234, 57, 6, 149, 57, 30, -1, 235, 98, 236, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 149, 30, 237, 98, 238, 163, 202, 236, 45, 225, 98, 236, 188, 225, 149, 30, 216, 146, 235, 203, 224, 203, 225, 203, 227, 203, 226, 30, 219, 146, 235, 203, 224, 203, 225, 203, 227, 203, 226, 30, 215, 146, 235, 203, 224, 203, 225, 203, 227, 203, 226, 30, 218, 146, 235, 203, 224, 203, 225, 203, 227, 203, 226, 30, 155, 234, 57, 30, 239, 98, 240, 163, 202, 52, 202, 223, 67, 237, 149, 74, 241, 202, 154, 149, 98, 52, 202, 223, 67, 237, 67, 308, 149, 74, 241, 202, 154, 149, 149, 30, 226, 163, 240, 4, 239, 30, 157, 30, 29, 57, 30, 239, 98, 240, 163, 202, 237, 203, 226, 98, 237, 203, 226, 67, 226, 149, 30, 51, 30, 242, 163, 68, 202, 307, 98, 229, 149, 1, 227, 30, 243, 163, 68, 202, 307, 98, 230, 149, 67, 235, 203, 230, 1, 228, 30, 206, 146, 202, 239, 203, 225, 67, 238, 149, 203, 227, 30, 207, 146, 202, 239, 203, 225, 67, 238, 149, 203, 227, 30, 208, 146, 202, 239, 203, 225, 67, 238, 149, 203, 228, 67, 235, 203, 230, 30, 211, 146, 202, 239, 203, 225, 67, 238, 149, 203, 228, 67, 235, 203, 230, 30, 209, 146, 202, 239, 203, 225, 67, 238, 149, 203, 227, 30, 210, 146, 202, 239, 203, 225, 67, 238, 149, 203, 227, 30, 214, 146, 202, 239, 203, 225, 67, 238, 149, 203, 228, 67, 235, 203, 230, 30, 215, 146, 202, 239, 203, 225, 67, 238, 149, 203, 227, 30, 216, 146, 202, 239, 203, 225, 67, 238, 149, 203, 227, 30, 217, 146, 202, 239, 203, 225, 67, 238, 149, 203, 228, 67, 235, 203, 230, 30, 218, 146, 202, 239, 203, 225, 67, 238, 149, 203, 227, 30, 219, 146, 202, 239, 203, 225, 67, 238, 149, 203, 227, 30, 220, 146, 202, 239, 203, 225, 67, 238, 149, 203, 228, 67, 235, 203, 230, 30, 244, 163, 206, 67, 68, 202, 307, 98, 229, 149, 67, 202, 226, 4, 308, 149, 203, 225, 203, 227, 30, 245, 163, 207, 67, 68, 202, 307, 98, 229, 149, 67, 202, 226, 4, 308, 149, 203, 225, 203, 227, 30, 246, 163, 208, 67, 68, 202, 307, 98, 230, 149, 67, 202, 226, 4, 308, 149, 203, 225, 203, 228, 30, 247, 163, 211, 67, 68, 202, 307, 98, 230, 149, 67, 202, 226, 4, 308, 149, 203, 225, 203, 228, 30, 248, 163, 209, 67, 68, 202, 307, 98, 229, 149, 67, 202, 226, 4, 308, 149, 203, 225, 203, 227, 30, 249, 163, 210, 67, 68, 202, 307, 98, 229, 149, 67, 202, 226, 4, 308, 149, 203, 225, 203, 227, 30, 250, 163, 214, 67, 68, 202, 307, 98, 230, 149, 67, 202, 226, 4, 308, 149, 203, 225, 203, 228, 30, 251, 163, 216, 67, 68, 202, 307, 98, 229, 149, 67, 202, 226, 4, 308, 149, 203, 225, 203, 227, 30, 252, 163, 217, 67, 68, 202, 307, 98, 230, 149, 67, 202, 226, 4, 308, 149, 203, 225, 203, 228, 30, 253, 163, 220, 67, 68, 202, 307, 98, 230, 149, 67, 202, 226, 4, 308, 149, 203, 225, 203, 228, 30, 254, 163, 219, 67, 68, 202, 307, 98, 229, 149, 67, 202, 226, 4, 308, 149, 203, 225, 203, 227, 30, 255, 163, 218, 67, 68, 202, 307, 98, 229, 149, 67, 202, 226, 4, 308, 149, 203, 225, 203, 227, 30, 256, 163, 215, 67, 68, 202, 307, 98, 229, 149, 67, 202, 226, 4, 308, 149, 203, 225, 203, 227, 30, 257, 163, 150, 202, 191, 229, 98, 230, 26, 98, 82, 163, 129, 149, 30, 155, 233, 57, 30, 258, 163, 212, 67, 236, 203, 227, 203, 228, 67, 68, 202, 307, 98, 229, 149, 191, 57, 98, 168, 26, 203, 228, 67, 202, 235, 203, 230, 67, 68, 202, 307, 98, 230, 149, 149, 191, 168, 98, 57, 26, 30, 257, 146, 52, 202, 258, 98, 259, 163, 242, 191, 57, 98, 168, 26, 147, 243, 191, 168, 98, 57, 26, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 157, 30, 120, 261, 136, 5, 202, 226, 149, 57, 30, 262, 163, 52, 202, 244, 98, 259, 163, 242, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 203, 222, 30, 263, 163, 52, 202, 245, 98, 259, 163, 242, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 264, 163, 52, 202, 246, 98, 259, 163, 243, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 265, 163, 52, 202, 250, 98, 259, 163, 243, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 266, 163, 52, 202, 249, 98, 259, 163, 242, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 267, 163, 52, 202, 248, 98, 259, 163, 242, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 268, 163, 52, 202, 247, 98, 259, 163, 243, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 257, 146, 262, 191, 57, 98, 168, 26, 203, 265, 191, 168, 98, 57, 26, 30, 269, 163, 185, 202, 257, 203, 264, 191, 168, 98, 57, 26, 98, 270, 163, 308, 149, 30, 271, 163, 185, 202, 257, 203, 263, 191, 57, 98, 168, 26, 98, 270, 163, 307, 149, 30, 10, 202, 251, 98, 269, 74, 241, 202, 251, 74, 82, 74, 103, 149, 98, 259, 163, 242, 149, 30, 10, 202, 252, 98, 271, 74, 241, 202, 252, 74, 82, 74, 103, 149, 98, 259, 163, 243, 149, 30, 272, 163, 185, 202, 257, 203, 266, 191, 57, 98, 168, 26, 98, 270, 163, 307, 149, 30, 10, 202, 253, 98, 272, 74, 241, 202, 253, 74, 82, 74, 103, 149, 98, 259, 163, 243, 149, 30, 273, 163, 185, 202, 257, 203, 268, 191, 168, 98, 57, 26, 98, 270, 163, 308, 149, 30, 10, 202, 254, 98, 273, 74, 241, 202, 254, 74, 82, 74, 103, 149, 98, 259, 163, 242, 149, 30, 257, 146, 272, 191, 168, 98, 57, 26, 203, 267, 191, 57, 98, 168, 26, 30, 250, 2, 225, 203, 228, 30, 244, 2, 225, 203, 227, 30, 245, 2, 225, 203, 227, 30, 246, 2, 225, 203, 228, 30, 251, 2, 225, 203, 227, 30, 252, 2, 225, 203, 228, 30, 249, 2, 225, 203, 227, 30, 254, 2, 225, 203, 227, 30, 248, 2, 225, 203, 227, 30, 253, 2, 225, 203, 228, 30, 247, 2, 225, 203, 228, 30, 70, 30, 155, 232, 57, 30, 274, 163, 213, 67, 236, 203, 227, 203, 228, 67, 68, 202, 307, 98, 229, 149, 191, 57, 98, 168, 26, 203, 228, 67, 202, 235, 203, 230, 67, 68, 202, 307, 98, 230, 149, 191, 168, 98, 57, 26, 149, 30, 10, 202, 274, 98, 257, 74, 241, 202, 274, 74, 82, 74, 103, 149, 98, 259, 163, 242, 191, 57, 98, 168, 26, 147, 243, 191, 168, 98, 57, 26, 149, 30, 157, 30, 47, 202, 149, 30, 275, 163, 150, 202, 191, 229, 98, 230, 26, 98, 82, 163, 129, 149, 30, 155, 231, 57, 30, 276, 163, 242, 191, 57, 98, 168, 26, 147, 243, 191, 168, 98, 57, 26, 30, 277, 163, 221, 67, 236, 203, 227, 203, 228, 67, 68, 202, 307, 98, 229, 149, 191, 57, 98, 168, 26, 203, 228, 67, 202, 235, 203, 230, 67, 68, 202, 307, 98, 230, 149, 149, 191, 168, 98, 57, 26, 30, 275, 146, 52, 202, 277, 98, 259, 163, 276, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 157, 30, 245, 163, 207, 67, 68, 202, 307, 98, 229, 149, 30, 246, 163, 208, 67, 68, 202, 307, 98, 230, 149, 30, 247, 163, 211, 67, 68, 202, 307, 98, 230, 149, 30, 250, 163, 214, 67, 68, 202, 307, 98, 230, 149, 30, 253, 163, 220, 67, 68, 202, 307, 98, 230, 149, 30, 255, 163, 218, 67, 68, 202, 307, 98, 229, 149, 30, 256, 163, 215, 67, 68, 202, 307, 98, 229, 149, 30, 249, 163, 210, 67, 68, 202, 307, 98, 229, 149, 30, 120, 278, 136, 5, 202, 307, 98, 226, 149, 57, 30, 272, 163, 52, 202, 253, 98, 259, 163, 243, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 279, 163, 185, 202, 272, 191, 168, 98, 57, 26, 203, 275, 98, 270, 163, 308, 149, 30, 10, 202, 255, 98, 279, 74, 241, 202, 255, 74, 82, 74, 103, 149, 98, 259, 163, 242, 149, 30, 263, 163, 52, 202, 245, 98, 259, 163, 242, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 264, 163, 52, 202, 246, 98, 259, 163, 243, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 265, 163, 52, 202, 250, 98, 259, 163, 243, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 266, 163, 52, 202, 249, 98, 259, 163, 242, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 268, 163, 52, 202, 247, 98, 259, 163, 243, 98, 260, 163, 307, 149, 74, 241, 202, 129, 149, 30, 275, 146, 263, 191, 57, 98, 168, 26, 203, 264, 191, 168, 98, 57, 26, 67, 266, 191, 57, 98, 168, 26, 203, 268, 191, 168, 98, 57, 26, 30, 280, 163, 275, 203, 265, 191, 168, 98, 57, 26, 30, 281, 163, 185, 202, 280, 98, 270, 163, 308, 149, 203, 222, 30, 10, 202, 256, 98, 281, 74, 241, 202, 256, 74, 82, 74, 103, 149, 98, 259, 163, 242, 149, 30, 245, 146, 225, 203, 227, 30, 250, 146, 225, 203, 228, 30, 246, 146, 225, 203, 228, 30, 255, 146, 225, 203, 227, 30, 253, 146, 225, 203, 228, 30, 247, 146, 225, 203, 228, 30, 256, 146, 225, 203, 227, 30, 249, 146, 225, 203, 227, 30, 70, 30, 3, 30]}, {"code": "def prepare_wy_repr_fwd_kernel_chunk32(\n    a,\n    b,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BC: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_a = tl.make_block_ptr(\n            a + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_b = tl.make_block_ptr(\n            b + (bos * H + i_h) * K,\n            (K, T),\n            (1, K * H),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        b_a = tl.load(p_a, boundary_check=(0, 1))\n        b_b = tl.load(p_b, boundary_check=(0, 1))\n        b_A += tl.dot(b_a, b_b)\n\n    b_A = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_A, 0)\n    for i in range(1, BT):\n        mask = tl.arange(0, BT) == i\n        b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)\n        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BT) < i)\n        b_A = tl.where(mask[:, None], b_a, b_A)\n    b_A += tl.arange(0, BT)[:, None] == tl.arange(0, BT)[None, :]\n\n    p_A = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 56, 6, 97, 213, 56, 6, 97, 214, 56, 6, 97, 215, 56, 6, 97, 216, 56, 6, 97, 217, 56, 6, 149, 56, 30, -1, 218, 97, 219, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 149, 30, 220, 97, 221, 163, 202, 219, 45, 212, 97, 219, 188, 212, 149, 30, 155, 217, 56, 30, 222, 97, 218, 163, 202, 50, 202, 210, 66, 218, 203, 309, 149, 73, 223, 202, 205, 149, 97, 50, 202, 210, 66, 218, 203, 309, 66, 308, 149, 73, 223, 202, 205, 149, 149, 30, 224, 97, 225, 163, 202, 50, 202, 209, 66, 222, 149, 73, 223, 202, 205, 149, 97, 50, 202, 209, 66, 222, 66, 308, 149, 73, 223, 202, 205, 149, 149, 30, 211, 163, 225, 4, 224, 30, 157, 30, 29, 56, 30, 224, 97, 225, 163, 202, 220, 203, 211, 97, 220, 203, 211, 66, 211, 149, 30, 51, 30, 226, 163, 150, 202, 191, 214, 97, 214, 26, 97, 81, 163, 128, 149, 30, 119, 227, 136, 5, 202, 58, 202, 213, 97, 215, 149, 149, 56, 30, 228, 163, 183, 202, 206, 66, 202, 224, 203, 212, 66, 221, 149, 203, 213, 97, 202, 211, 97, 213, 149, 97, 202, 212, 203, 213, 97, 308, 149, 97, 202, 218, 203, 214, 97, 227, 203, 215, 149, 97, 202, 214, 97, 215, 149, 97, 202, 308, 97, 307, 149, 149, 30, 229, 163, 183, 202, 207, 66, 202, 224, 203, 212, 66, 221, 149, 203, 213, 97, 202, 213, 97, 211, 149, 97, 202, 308, 97, 213, 203, 212, 149, 97, 202, 227, 203, 215, 97, 218, 203, 214, 149, 97, 202, 215, 97, 214, 149, 97, 202, 307, 97, 308, 149, 149, 30, 230, 163, 50, 202, 228, 97, 231, 163, 202, 307, 97, 308, 149, 149, 30, 232, 163, 50, 202, 229, 97, 231, 163, 202, 307, 97, 308, 149, 149, 30, 226, 146, 15, 202, 230, 97, 232, 149, 30, 69, 30, 226, 163, 171, 202, 67, 202, 307, 97, 214, 149, 191, 56, 97, 168, 26, 110, 67, 202, 307, 97, 214, 149, 191, 168, 97, 56, 26, 97, 226, 97, 307, 149, 30, 119, 233, 136, 5, 202, 308, 97, 214, 149, 56, 30, 234, 163, 67, 202, 307, 97, 214, 149, 68, 233, 30, 230, 163, 185, 202, 171, 202, 234, 191, 56, 97, 168, 26, 97, 226, 97, 307, 149, 97, 307, 149, 30, 230, 163, 230, 66, 185, 202, 230, 191, 56, 97, 168, 26, 203, 226, 97, 307, 149, 203, 202, 67, 202, 307, 97, 214, 149, 1, 233, 149, 30, 226, 163, 171, 202, 234, 191, 56, 97, 168, 26, 97, 230, 97, 226, 149, 30, 69, 30, 226, 146, 67, 202, 307, 97, 214, 149, 191, 56, 97, 168, 26, 68, 67, 202, 307, 97, 214, 149, 191, 168, 97, 56, 26, 30, 235, 163, 183, 202, 208, 66, 202, 224, 203, 212, 66, 221, 149, 203, 214, 97, 202, 211, 97, 214, 149, 97, 202, 212, 203, 214, 97, 308, 149, 97, 202, 218, 203, 214, 97, 307, 149, 97, 202, 214, 97, 214, 149, 97, 202, 308, 97, 307, 149, 149, 30, 10, 202, 235, 97, 226, 73, 223, 202, 235, 73, 81, 73, 102, 149, 97, 231, 163, 202, 307, 97, 308, 149, 149, 30, 3, 30]}, {"code": "def prepare_wy_repr_fwd_kernel_chunk64(\n    a,\n    b,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BC: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    b_A2 = tl.zeros([BC, BC], dtype=tl.float32)\n    b_A3 = tl.zeros([BC, BC], dtype=tl.float32)\n\n    for i_k in range(tl.cdiv(K, BK)):\n        p_a1 = tl.make_block_ptr(\n            a + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_a2 = tl.make_block_ptr(\n            a + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT + BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_b1 = tl.make_block_ptr(\n            b + (bos * H + i_h) * K,\n            (K, T),\n            (1, K * H),\n            (i_k * BK, i_t * BT),\n            (BK, BC),\n            (0, 1),\n        )\n        p_b2 = tl.make_block_ptr(\n            b + (bos * H + i_h) * K,\n            (K, T),\n            (1, K * H),\n            (i_k * BK, i_t * BT + BC),\n            (BK, BC),\n            (0, 1),\n        )\n        b_a1 = tl.load(p_a1, boundary_check=(0, 1))\n        b_a2 = tl.load(p_a2, boundary_check=(0, 1))\n        b_b1 = tl.load(p_b1, boundary_check=(0, 1))\n        b_b2 = tl.load(p_b2, boundary_check=(0, 1))\n        b_A += tl.dot(b_a1, b_b1, allow_tf32=False)\n        b_A2 += tl.dot(b_a2, b_b2, allow_tf32=False)\n        b_A3 += tl.dot(b_a2, b_b1, allow_tf32=False)\n\n    b_A = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A, 0)\n    b_A2 = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A2, 0)\n\n    for i in range(1, BC):\n        mask = tl.arange(0, BC) == i\n        b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)\n        b_a2 = tl.sum(tl.where(mask[:, None], b_A2, 0), 0)\n        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BC) < i)\n        b_a2 = b_a2 + tl.sum(b_a2[:, None] * b_A2, 0) * (tl.arange(0, BC) < i)\n        b_A = tl.where(mask[:, None], b_a, b_A)\n        b_A2 = tl.where(mask[:, None], b_a2, b_A2)\n\n    b_A += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\n    b_A2 += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\n    b_A3 = tl.dot(tl.dot(b_A2, b_A3, allow_tf32=False), b_A, allow_tf32=False)\n\n    p_A1 = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BC, BC), (1, 0)\n    )\n    p_A2 = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT + BC, BC),\n        (BC, BC),\n        (1, 0),\n    )\n    p_A3 = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT + BC, 0),\n        (BC, BC),\n        (1, 0),\n    )\n    p_A4 = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, BC), (BC, BC), (1, 0)\n    )\n    tl.store(p_A1, b_A.to(p_A1.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_A2, b_A2.to(p_A2.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_A3, b_A3.to(p_A3.dtype.element_ty), boundary_check=(0, 1))\n\n    tl.store(\n        p_A4,\n        tl.zeros([BC, BC], dtype=tl.float32).to(p_A4.dtype.element_ty),\n        boundary_check=(0, 1),\n    )", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 57, 6, 98, 213, 57, 6, 98, 214, 57, 6, 98, 215, 57, 6, 98, 216, 57, 6, 98, 217, 57, 6, 149, 57, 30, -1, 218, 98, 219, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 149, 30, 220, 98, 221, 163, 202, 219, 45, 212, 98, 219, 188, 212, 149, 30, 155, 217, 57, 30, 222, 98, 218, 163, 202, 52, 202, 210, 67, 218, 203, 309, 149, 74, 223, 202, 56, 149, 98, 52, 202, 210, 67, 218, 203, 309, 67, 308, 149, 74, 223, 202, 56, 149, 149, 30, 224, 98, 225, 163, 202, 52, 202, 209, 67, 222, 149, 74, 223, 202, 56, 149, 98, 52, 202, 209, 67, 222, 67, 308, 149, 74, 223, 202, 56, 149, 149, 30, 211, 163, 225, 4, 224, 30, 157, 30, 29, 57, 30, 224, 98, 225, 163, 202, 220, 203, 211, 98, 220, 203, 211, 67, 211, 149, 30, 51, 30, 226, 163, 150, 202, 191, 216, 98, 216, 26, 98, 82, 163, 129, 149, 30, 227, 163, 150, 202, 191, 216, 98, 216, 26, 98, 82, 163, 129, 149, 30, 228, 163, 150, 202, 191, 216, 98, 216, 26, 98, 82, 163, 129, 149, 30, 120, 229, 136, 5, 202, 59, 202, 213, 98, 215, 149, 149, 57, 30, 230, 163, 183, 202, 206, 67, 202, 224, 203, 212, 67, 221, 149, 203, 213, 98, 202, 211, 98, 213, 149, 98, 202, 212, 203, 213, 98, 308, 149, 98, 202, 218, 203, 214, 98, 229, 203, 215, 149, 98, 202, 216, 98, 215, 149, 98, 202, 308, 98, 307, 149, 149, 30, 231, 163, 183, 202, 206, 67, 202, 224, 203, 212, 67, 221, 149, 203, 213, 98, 202, 211, 98, 213, 149, 98, 202, 212, 203, 213, 98, 308, 149, 98, 202, 218, 203, 214, 67, 216, 98, 229, 203, 215, 149, 98, 202, 216, 98, 215, 149, 98, 202, 308, 98, 307, 149, 149, 30, 232, 163, 183, 202, 207, 67, 202, 224, 203, 212, 67, 221, 149, 203, 213, 98, 202, 213, 98, 211, 149, 98, 202, 308, 98, 213, 203, 212, 149, 98, 202, 229, 203, 215, 98, 218, 203, 214, 149, 98, 202, 215, 98, 216, 149, 98, 202, 307, 98, 308, 149, 149, 30, 233, 163, 183, 202, 207, 67, 202, 224, 203, 212, 67, 221, 149, 203, 213, 98, 202, 213, 98, 211, 149, 98, 202, 308, 98, 213, 203, 212, 149, 98, 202, 229, 203, 215, 98, 218, 203, 214, 67, 216, 149, 98, 202, 215, 98, 216, 149, 98, 202, 307, 98, 308, 149, 149, 30, 234, 163, 52, 202, 230, 98, 235, 163, 202, 307, 98, 308, 149, 149, 30, 236, 163, 52, 202, 231, 98, 235, 163, 202, 307, 98, 308, 149, 149, 30, 237, 163, 52, 202, 232, 98, 235, 163, 202, 307, 98, 308, 149, 149, 30, 238, 163, 52, 202, 233, 98, 235, 163, 202, 307, 98, 308, 149, 149, 30, 226, 146, 15, 202, 234, 98, 237, 98, 239, 163, 54, 149, 30, 227, 146, 15, 202, 236, 98, 238, 98, 239, 163, 54, 149, 30, 228, 146, 15, 202, 236, 98, 237, 98, 239, 163, 54, 149, 30, 70, 30, 226, 163, 171, 202, 68, 202, 307, 98, 216, 149, 191, 57, 98, 168, 26, 111, 68, 202, 307, 98, 216, 149, 191, 168, 98, 57, 26, 98, 226, 98, 307, 149, 30, 227, 163, 171, 202, 68, 202, 307, 98, 216, 149, 191, 57, 98, 168, 26, 111, 68, 202, 307, 98, 216, 149, 191, 168, 98, 57, 26, 98, 227, 98, 307, 149, 30, 120, 240, 136, 5, 202, 308, 98, 216, 149, 57, 30, 241, 163, 68, 202, 307, 98, 216, 149, 69, 240, 30, 242, 163, 185, 202, 171, 202, 241, 191, 57, 98, 168, 26, 98, 226, 98, 307, 149, 98, 307, 149, 30, 236, 163, 185, 202, 171, 202, 241, 191, 57, 98, 168, 26, 98, 227, 98, 307, 149, 98, 307, 149, 30, 242, 163, 242, 67, 185, 202, 242, 191, 57, 98, 168, 26, 203, 226, 98, 307, 149, 203, 202, 68, 202, 307, 98, 216, 149, 1, 240, 149, 30, 236, 163, 236, 67, 185, 202, 236, 191, 57, 98, 168, 26, 203, 227, 98, 307, 149, 203, 202, 68, 202, 307, 98, 216, 149, 1, 240, 149, 30, 226, 163, 171, 202, 241, 191, 57, 98, 168, 26, 98, 242, 98, 226, 149, 30, 227, 163, 171, 202, 241, 191, 57, 98, 168, 26, 98, 236, 98, 227, 149, 30, 70, 30, 226, 146, 68, 202, 307, 98, 216, 149, 191, 57, 98, 168, 26, 69, 68, 202, 307, 98, 216, 149, 191, 168, 98, 57, 26, 30, 227, 146, 68, 202, 307, 98, 216, 149, 191, 57, 98, 168, 26, 69, 68, 202, 307, 98, 216, 149, 191, 168, 98, 57, 26, 30, 228, 163, 15, 202, 15, 202, 227, 98, 228, 98, 239, 163, 54, 149, 98, 226, 98, 239, 163, 54, 149, 30, 243, 163, 183, 202, 208, 67, 202, 224, 203, 212, 67, 221, 149, 203, 214, 98, 202, 211, 98, 214, 149, 98, 202, 212, 203, 214, 98, 308, 149, 98, 202, 218, 203, 214, 98, 307, 149, 98, 202, 216, 98, 216, 149, 98, 202, 308, 98, 307, 149, 149, 30, 244, 163, 183, 202, 208, 67, 202, 224, 203, 212, 67, 221, 149, 203, 214, 98, 202, 211, 98, 214, 149, 98, 202, 212, 203, 214, 98, 308, 149, 98, 202, 218, 203, 214, 67, 216, 98, 216, 149, 98, 202, 216, 98, 216, 149, 98, 202, 308, 98, 307, 149, 149, 30, 245, 163, 183, 202, 208, 67, 202, 224, 203, 212, 67, 221, 149, 203, 214, 98, 202, 211, 98, 214, 149, 98, 202, 212, 203, 214, 98, 308, 149, 98, 202, 218, 203, 214, 67, 216, 98, 307, 149, 98, 202, 216, 98, 216, 149, 98, 202, 308, 98, 307, 149, 149, 30, 246, 163, 183, 202, 208, 67, 202, 224, 203, 212, 67, 221, 149, 203, 214, 98, 202, 211, 98, 214, 149, 98, 202, 212, 203, 214, 98, 308, 149, 98, 202, 218, 203, 214, 98, 216, 149, 98, 202, 216, 98, 216, 149, 98, 202, 308, 98, 307, 149, 149, 30, 10, 202, 243, 98, 226, 74, 223, 202, 243, 74, 82, 74, 103, 149, 98, 235, 163, 202, 307, 98, 308, 149, 149, 30, 10, 202, 244, 98, 227, 74, 223, 202, 244, 74, 82, 74, 103, 149, 98, 235, 163, 202, 307, 98, 308, 149, 149, 30, 10, 202, 245, 98, 228, 74, 223, 202, 245, 74, 82, 74, 103, 149, 98, 235, 163, 202, 307, 98, 308, 149, 149, 30, 10, 202, 246, 98, 150, 202, 191, 216, 98, 216, 26, 98, 82, 163, 129, 149, 74, 223, 202, 246, 74, 82, 74, 103, 149, 98, 235, 163, 202, 307, 98, 308, 149, 149, 30, 3, 30]}, {"code": "def wu_fwd_kernel(\n    w,\n    u,\n    a,\n    k,\n    v,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    p_A = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_Aak = tl.zeros([BT, BT], dtype=tl.float32)\n\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_a = tl.make_block_ptr(\n            a + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_w = tl.make_block_ptr(\n            w + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_a = tl.load(p_a, boundary_check=(0, 1))\n        b_w = tl.dot(b_A, b_a)\n        b_Aak += tl.dot(b_a, tl.trans(b_k))\n        tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))\n\n    b_Aak = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_Aak, 0)\n    b_Aak = b_Aak.to(k.dtype.element_ty)\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_u = tl.make_block_ptr(\n            u + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v = tl.dot(b_Aak, b_v).to(v.dtype.element_ty)\n        b_u = tl.dot(b_A, b_v)\n        tl.store(p_u, b_u.to(p_u.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 56, 6, 97, 216, 56, 6, 97, 217, 56, 6, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 149, 56, 30, -1, 222, 97, 223, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 149, 30, 224, 97, 225, 163, 202, 223, 45, 215, 97, 223, 188, 215, 149, 30, 155, 221, 56, 30, 226, 97, 222, 163, 202, 50, 202, 213, 66, 222, 203, 309, 149, 73, 227, 202, 205, 149, 97, 50, 202, 213, 66, 222, 203, 309, 66, 308, 149, 73, 227, 202, 205, 149, 149, 30, 228, 97, 229, 163, 202, 50, 202, 212, 66, 226, 149, 73, 227, 202, 205, 149, 97, 50, 202, 212, 66, 226, 66, 308, 149, 73, 227, 202, 205, 149, 149, 30, 214, 163, 229, 4, 228, 30, 157, 30, 29, 56, 30, 228, 97, 229, 163, 202, 224, 203, 214, 97, 224, 203, 214, 66, 214, 149, 30, 51, 30, 230, 163, 183, 202, 211, 66, 202, 228, 203, 215, 66, 225, 149, 203, 218, 97, 202, 214, 97, 218, 149, 97, 202, 215, 203, 218, 97, 308, 149, 97, 202, 222, 203, 218, 97, 307, 149, 97, 202, 218, 97, 218, 149, 97, 202, 308, 97, 307, 149, 149, 30, 231, 163, 50, 202, 230, 97, 232, 163, 202, 307, 97, 308, 149, 149, 30, 233, 163, 150, 202, 191, 218, 97, 218, 26, 97, 81, 163, 128, 149, 30, 119, 234, 136, 5, 202, 58, 202, 216, 97, 219, 149, 149, 56, 30, 235, 163, 183, 202, 209, 66, 202, 228, 203, 215, 66, 225, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 222, 203, 218, 97, 234, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 236, 163, 183, 202, 208, 66, 202, 228, 203, 215, 66, 225, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 222, 203, 218, 97, 234, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 237, 163, 183, 202, 206, 66, 202, 228, 203, 215, 66, 225, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 222, 203, 218, 97, 234, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 238, 163, 50, 202, 235, 97, 232, 163, 202, 307, 97, 308, 149, 149, 30, 239, 163, 50, 202, 236, 97, 232, 163, 202, 307, 97, 308, 149, 149, 30, 240, 163, 15, 202, 231, 97, 239, 149, 30, 233, 146, 15, 202, 239, 97, 64, 202, 238, 149, 149, 30, 10, 202, 237, 97, 240, 73, 227, 202, 237, 73, 81, 73, 102, 149, 97, 232, 163, 202, 307, 97, 308, 149, 149, 30, 69, 30, 233, 163, 171, 202, 67, 202, 307, 97, 218, 149, 191, 56, 97, 168, 26, 110, 67, 202, 307, 97, 218, 149, 191, 168, 97, 56, 26, 97, 233, 97, 307, 149, 30, 233, 163, 233, 73, 227, 202, 209, 73, 81, 73, 102, 149, 30, 119, 241, 136, 5, 202, 58, 202, 217, 97, 220, 149, 149, 56, 30, 242, 163, 183, 202, 210, 66, 202, 228, 203, 215, 66, 225, 149, 203, 217, 97, 202, 214, 97, 217, 149, 97, 202, 215, 203, 217, 97, 308, 149, 97, 202, 222, 203, 218, 97, 241, 203, 220, 149, 97, 202, 218, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 243, 163, 183, 202, 207, 66, 202, 228, 203, 215, 66, 225, 149, 203, 217, 97, 202, 214, 97, 217, 149, 97, 202, 215, 203, 217, 97, 308, 149, 97, 202, 222, 203, 218, 97, 241, 203, 220, 149, 97, 202, 218, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 244, 163, 50, 202, 242, 97, 232, 163, 202, 307, 97, 308, 149, 149, 30, 244, 163, 15, 202, 233, 97, 244, 149, 73, 227, 202, 210, 73, 81, 73, 102, 149, 30, 245, 163, 15, 202, 231, 97, 244, 149, 30, 10, 202, 243, 97, 245, 73, 227, 202, 243, 73, 81, 73, 102, 149, 97, 232, 163, 202, 307, 97, 308, 149, 149, 30, 69, 30, 3, 30]}, {"code": "def chunk_gla_fwd_A_kernel_intra_sub_inter(\n    q,\n    k,\n    g,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_i, i_j = i_c // NC, i_c % NC\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    if i_t * BT + i_i * BC >= T:\n        return\n    if i_i <= i_j:\n        return\n\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        m_k = o_k < K\n\n        p_q = tl.make_block_ptr(\n            q + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT + i_i * BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_g = tl.make_block_ptr(\n            g + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT + i_i * BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT + i_j * BC),\n            (BK, BC),\n            (0, 1),\n        )\n        p_gk = tl.make_block_ptr(\n            g + (bos * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT + i_j * BC),\n            (BK, BC),\n            (0, 1),\n        )\n        p_gn = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\n\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * exp(b_g - b_gn[None, :]) * scale\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * exp(b_gn[:, None] - b_gk)\n\n        b_A += tl.dot(b_qg, b_kg)\n\n    p_A = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT + i_i * BC, i_j * BC),\n        (BC, BC),\n        (1, 0),\n    )\n    tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 57, 6, 98, 215, 57, 6, 98, 216, 57, 6, 98, 217, 57, 6, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 149, 57, 30, -1, 221, 98, 222, 98, 223, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 224, 98, 225, 163, 202, 223, 45, 214, 98, 223, 188, 214, 149, 30, 226, 98, 227, 163, 202, 222, 45, 219, 98, 222, 188, 219, 149, 30, 155, 220, 57, 30, 228, 98, 221, 163, 202, 52, 202, 211, 67, 221, 203, 309, 149, 74, 229, 202, 56, 149, 98, 52, 202, 211, 67, 221, 203, 309, 67, 308, 149, 74, 229, 202, 56, 149, 149, 30, 230, 98, 231, 163, 202, 52, 202, 210, 67, 228, 149, 74, 229, 202, 56, 149, 98, 52, 202, 210, 67, 228, 67, 308, 149, 74, 229, 202, 56, 149, 149, 30, 213, 163, 231, 4, 230, 30, 157, 30, 29, 57, 30, 230, 98, 231, 163, 202, 224, 203, 213, 98, 224, 203, 213, 67, 213, 149, 30, 51, 30, 155, 221, 203, 216, 67, 226, 203, 217, 128, 213, 57, 30, 192, 30, 157, 30, 155, 226, 182, 227, 57, 30, 192, 30, 157, 30, 232, 163, 150, 202, 191, 217, 98, 217, 26, 98, 82, 163, 129, 149, 30, 120, 233, 136, 5, 202, 59, 202, 215, 98, 218, 149, 149, 57, 30, 234, 163, 233, 203, 218, 67, 68, 202, 307, 98, 218, 149, 30, 235, 163, 234, 1, 215, 30, 236, 163, 183, 202, 206, 67, 202, 230, 203, 214, 67, 225, 149, 203, 215, 98, 202, 213, 98, 215, 149, 98, 202, 214, 203, 215, 98, 308, 149, 98, 202, 221, 203, 216, 67, 226, 203, 217, 98, 233, 203, 218, 149, 98, 202, 217, 98, 218, 149, 98, 202, 308, 98, 307, 149, 149, 30, 237, 163, 183, 202, 208, 67, 202, 230, 203, 214, 67, 225, 149, 203, 215, 98, 202, 213, 98, 215, 149, 98, 202, 214, 203, 215, 98, 308, 149, 98, 202, 221, 203, 216, 67, 226, 203, 217, 98, 233, 203, 218, 149, 98, 202, 217, 98, 218, 149, 98, 202, 308, 98, 307, 149, 149, 30, 238, 163, 183, 202, 207, 67, 202, 230, 203, 214, 67, 225, 149, 203, 215, 98, 202, 215, 98, 213, 149, 98, 202, 308, 98, 214, 203, 215, 149, 98, 202, 233, 203, 218, 98, 221, 203, 216, 67, 227, 203, 217, 149, 98, 202, 218, 98, 217, 149, 98, 202, 307, 98, 308, 149, 149, 30, 239, 163, 183, 202, 208, 67, 202, 230, 203, 214, 67, 225, 149, 203, 215, 98, 202, 215, 98, 213, 149, 98, 202, 308, 98, 214, 203, 215, 149, 98, 202, 233, 203, 218, 98, 221, 203, 216, 67, 227, 203, 217, 149, 98, 202, 218, 98, 217, 149, 98, 202, 307, 98, 308, 149, 149, 30, 240, 163, 208, 67, 202, 230, 67, 221, 203, 216, 67, 226, 203, 217, 149, 203, 214, 203, 215, 67, 225, 203, 215, 67, 234, 30, 241, 163, 52, 202, 240, 98, 242, 163, 235, 98, 243, 163, 307, 149, 30, 244, 163, 52, 202, 236, 98, 245, 163, 202, 307, 98, 308, 149, 149, 30, 246, 163, 52, 202, 237, 98, 245, 163, 202, 307, 98, 308, 149, 149, 30, 247, 163, 244, 203, 173, 202, 246, 4, 241, 191, 168, 98, 57, 26, 149, 203, 212, 30, 248, 163, 52, 202, 238, 98, 245, 163, 202, 307, 98, 308, 149, 149, 30, 249, 163, 52, 202, 239, 98, 245, 163, 202, 307, 98, 308, 149, 149, 30, 250, 163, 248, 203, 173, 202, 241, 191, 57, 98, 168, 26, 4, 249, 149, 30, 232, 146, 15, 202, 247, 98, 250, 149, 30, 70, 30, 251, 163, 183, 202, 209, 67, 202, 230, 203, 214, 67, 225, 149, 203, 216, 98, 202, 213, 98, 216, 149, 98, 202, 214, 203, 216, 98, 308, 149, 98, 202, 221, 203, 216, 67, 226, 203, 217, 98, 227, 203, 217, 149, 98, 202, 217, 98, 217, 149, 98, 202, 308, 98, 307, 149, 149, 30, 10, 202, 251, 98, 232, 74, 229, 202, 209, 74, 82, 74, 103, 149, 98, 245, 163, 202, 307, 98, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_gla_fwd_A_kernel_intra_sub_intra(\n    q,\n    k,\n    g,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_j = i_i\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    if i_t * BT + i_i * BC >= T:\n        return\n\n    o_i = tl.arange(0, BC)\n    o_k = tl.arange(0, BK)\n    m_k = o_k < K\n    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n    o_A = (bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT + i_h * BT + i_j * BC\n    p_q = tl.make_block_ptr(\n        q + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, 0),\n        (BC, BK),\n        (1, 0),\n    )\n    p_g = tl.make_block_ptr(\n        g + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, 0),\n        (BC, BK),\n        (1, 0),\n    )\n    p_k = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\n    p_gk = g + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)\n        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n        b_A = tl.sum(b_q * b_k[None, :] * exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.0)\n\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += H * K\n        p_gk += H * K", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 56, 6, 97, 215, 56, 6, 97, 216, 56, 6, 97, 217, 56, 6, 97, 218, 56, 6, 97, 219, 56, 6, 149, 56, 30, -1, 220, 97, 221, 97, 222, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 223, 97, 224, 163, 202, 222, 45, 214, 97, 222, 188, 214, 149, 30, 225, 163, 221, 30, 155, 219, 56, 30, 226, 97, 220, 163, 202, 50, 202, 211, 66, 220, 203, 309, 149, 73, 227, 202, 205, 149, 97, 50, 202, 211, 66, 220, 203, 309, 66, 308, 149, 73, 227, 202, 205, 149, 149, 30, 228, 97, 229, 163, 202, 50, 202, 210, 66, 226, 149, 73, 227, 202, 205, 149, 97, 50, 202, 210, 66, 226, 66, 308, 149, 73, 227, 202, 205, 149, 149, 30, 213, 163, 229, 4, 228, 30, 157, 30, 29, 56, 30, 228, 97, 229, 163, 202, 223, 203, 213, 97, 223, 203, 213, 66, 213, 149, 30, 51, 30, 155, 220, 203, 216, 66, 221, 203, 217, 127, 213, 56, 30, 192, 30, 157, 30, 230, 163, 67, 202, 307, 97, 217, 149, 30, 231, 163, 67, 202, 307, 97, 218, 149, 30, 232, 163, 231, 1, 215, 30, 233, 163, 220, 203, 216, 66, 221, 203, 217, 66, 67, 202, 307, 97, 217, 149, 1, 213, 30, 234, 163, 202, 228, 66, 220, 203, 216, 66, 221, 203, 217, 66, 67, 202, 307, 97, 217, 149, 149, 203, 214, 203, 216, 66, 224, 203, 216, 66, 225, 203, 217, 30, 235, 163, 183, 202, 206, 66, 202, 228, 203, 214, 66, 224, 149, 203, 215, 97, 202, 213, 97, 215, 149, 97, 202, 214, 203, 215, 97, 308, 149, 97, 202, 220, 203, 216, 66, 221, 203, 217, 97, 307, 149, 97, 202, 217, 97, 218, 149, 97, 202, 308, 97, 307, 149, 149, 30, 236, 163, 183, 202, 208, 66, 202, 228, 203, 214, 66, 224, 149, 203, 215, 97, 202, 213, 97, 215, 149, 97, 202, 214, 203, 215, 97, 308, 149, 97, 202, 220, 203, 216, 66, 221, 203, 217, 97, 307, 149, 97, 202, 217, 97, 218, 149, 97, 202, 308, 97, 307, 149, 149, 30, 237, 163, 207, 66, 202, 228, 66, 220, 203, 216, 66, 225, 203, 217, 149, 203, 214, 203, 215, 66, 224, 203, 215, 66, 231, 30, 238, 163, 208, 66, 202, 228, 66, 220, 203, 216, 66, 225, 203, 217, 149, 203, 214, 203, 215, 66, 224, 203, 215, 66, 231, 30, 239, 163, 50, 202, 235, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 241, 163, 50, 202, 236, 97, 240, 163, 202, 307, 97, 308, 149, 149, 30, 119, 242, 136, 5, 202, 307, 97, 37, 202, 217, 97, 213, 4, 220, 203, 216, 4, 221, 203, 217, 149, 149, 56, 30, 243, 163, 50, 202, 237, 97, 244, 163, 232, 97, 245, 163, 307, 149, 73, 227, 202, 128, 149, 30, 246, 163, 50, 202, 238, 97, 244, 163, 232, 97, 245, 163, 307, 149, 73, 227, 202, 128, 149, 30, 247, 163, 185, 202, 239, 203, 243, 191, 168, 97, 56, 26, 203, 173, 202, 241, 4, 246, 191, 168, 97, 56, 26, 149, 97, 308, 149, 30, 247, 163, 171, 202, 230, 127, 242, 97, 247, 203, 212, 97, 307, 149, 30, 10, 202, 209, 66, 234, 66, 242, 97, 247, 97, 244, 163, 233, 149, 30, 237, 146, 214, 203, 215, 30, 238, 146, 214, 203, 215, 30, 69, 30, 3, 30]}, {"code": "def chunk_gla_fwd_A_kernel_intra_sub_intra_split(\n    q,\n    k,\n    g,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_t, i_i = i_tc // NC, i_tc % NC\n    i_j = i_i\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        all = T\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        all = B * T\n\n    if i_t * BT + i_i * BC >= T:\n        return\n\n    o_i = tl.arange(0, BC)\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n\n    o_A = (i_k * all + bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BC + i_h * BC\n    p_q = tl.make_block_ptr(\n        q + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    p_g = tl.make_block_ptr(\n        g + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    p_k = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\n    p_gk = g + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_A = tl.zeros([BC], dtype=tl.float32)\n        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)\n        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n        b_A += tl.sum(b_q * b_k[None, :] * exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.0)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += H * K\n        p_gk += H * K", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 57, 6, 98, 215, 57, 6, 98, 216, 57, 6, 98, 217, 57, 6, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 149, 57, 30, -1, 222, 98, 223, 98, 224, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 225, 98, 226, 163, 202, 224, 45, 215, 98, 224, 188, 215, 149, 30, 227, 98, 228, 163, 202, 223, 45, 220, 98, 223, 188, 220, 149, 30, 229, 163, 228, 30, 155, 221, 57, 30, 230, 98, 227, 163, 202, 52, 202, 211, 67, 227, 203, 309, 149, 74, 231, 202, 56, 149, 98, 52, 202, 211, 67, 227, 203, 309, 67, 308, 149, 74, 231, 202, 56, 149, 149, 30, 232, 98, 233, 163, 202, 52, 202, 210, 67, 230, 149, 74, 231, 202, 56, 149, 98, 52, 202, 210, 67, 230, 67, 308, 149, 74, 231, 202, 56, 149, 149, 30, 141, 163, 213, 30, 213, 163, 233, 4, 232, 30, 157, 30, 29, 57, 30, 232, 98, 233, 163, 202, 225, 203, 213, 98, 225, 203, 213, 67, 213, 149, 30, 141, 163, 214, 203, 213, 30, 51, 30, 155, 227, 203, 217, 67, 228, 203, 218, 128, 213, 57, 30, 192, 30, 157, 30, 234, 163, 68, 202, 307, 98, 218, 149, 30, 235, 163, 222, 203, 219, 67, 68, 202, 307, 98, 219, 149, 30, 236, 163, 235, 1, 216, 30, 237, 163, 227, 203, 217, 67, 228, 203, 218, 67, 68, 202, 307, 98, 218, 149, 1, 213, 30, 238, 163, 202, 222, 203, 141, 67, 232, 67, 227, 203, 217, 67, 228, 203, 218, 67, 68, 202, 307, 98, 218, 149, 149, 203, 215, 203, 218, 67, 226, 203, 218, 30, 239, 163, 183, 202, 206, 67, 202, 232, 203, 215, 67, 226, 149, 203, 216, 98, 202, 213, 98, 216, 149, 98, 202, 215, 203, 216, 98, 308, 149, 98, 202, 227, 203, 217, 67, 228, 203, 218, 98, 222, 203, 219, 149, 98, 202, 218, 98, 219, 149, 98, 202, 308, 98, 307, 149, 149, 30, 240, 163, 183, 202, 208, 67, 202, 232, 203, 215, 67, 226, 149, 203, 216, 98, 202, 213, 98, 216, 149, 98, 202, 215, 203, 216, 98, 308, 149, 98, 202, 227, 203, 217, 67, 228, 203, 218, 98, 222, 203, 219, 149, 98, 202, 218, 98, 219, 149, 98, 202, 308, 98, 307, 149, 149, 30, 241, 163, 207, 67, 202, 232, 67, 227, 203, 217, 67, 229, 203, 218, 149, 203, 215, 203, 216, 67, 226, 203, 216, 67, 235, 30, 242, 163, 208, 67, 202, 232, 67, 227, 203, 217, 67, 229, 203, 218, 149, 203, 215, 203, 216, 67, 226, 203, 216, 67, 235, 30, 243, 163, 52, 202, 239, 98, 244, 163, 202, 307, 98, 308, 149, 149, 30, 245, 163, 52, 202, 240, 98, 244, 163, 202, 307, 98, 308, 149, 149, 30, 120, 246, 136, 5, 202, 307, 98, 37, 202, 218, 98, 213, 4, 227, 203, 217, 4, 228, 203, 218, 149, 149, 57, 30, 247, 163, 150, 202, 191, 218, 26, 98, 82, 163, 129, 149, 30, 248, 163, 52, 202, 241, 98, 249, 163, 236, 98, 250, 163, 307, 149, 74, 231, 202, 129, 149, 30, 251, 163, 52, 202, 242, 98, 249, 163, 236, 98, 250, 163, 307, 149, 74, 231, 202, 129, 149, 30, 247, 146, 185, 202, 243, 203, 248, 191, 168, 98, 57, 26, 203, 173, 202, 245, 4, 251, 191, 168, 98, 57, 26, 149, 98, 308, 149, 30, 247, 163, 171, 202, 234, 128, 246, 98, 247, 203, 212, 98, 307, 149, 30, 10, 202, 209, 67, 238, 67, 246, 98, 247, 98, 249, 163, 237, 149, 30, 241, 146, 215, 203, 216, 30, 242, 146, 215, 203, 216, 30, 70, 30, 3, 30]}, {"code": "def chunk_gla_fwd_A_kernel_intra_sub_intra_merge(\n    A,\n    A2,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    NK: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        all = T\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        all = B * T\n\n    if i_t * BT + i_c * BC >= T:\n        return\n\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(0, NK):\n        p_A = tl.make_block_ptr(\n            A + (i_k * all + bos) * H * BC + i_h * BC,\n            (T, BC),\n            (H * BC, 1),\n            (i_t * BT + i_c * BC, 0),\n            (BC, BC),\n            (1, 0),\n        )\n        b_A += tl.load(p_A, boundary_check=(0, 1))\n    p_A2 = tl.make_block_ptr(\n        A2 + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT + i_c * BC, i_c * BC),\n        (BC, BC),\n        (1, 0),\n    )\n    tl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 56, 6, 97, 212, 56, 6, 97, 213, 56, 6, 97, 214, 56, 6, 97, 215, 56, 6, 97, 216, 56, 6, 149, 56, 30, -1, 217, 97, 218, 97, 219, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 220, 97, 221, 163, 202, 219, 45, 212, 97, 219, 188, 212, 149, 30, 155, 216, 56, 30, 222, 97, 217, 163, 202, 50, 202, 209, 66, 217, 203, 309, 149, 73, 223, 202, 205, 149, 97, 50, 202, 209, 66, 217, 203, 309, 66, 308, 149, 73, 223, 202, 205, 149, 149, 30, 224, 97, 225, 163, 202, 50, 202, 208, 66, 222, 149, 73, 223, 202, 205, 149, 97, 50, 202, 208, 66, 222, 66, 308, 149, 73, 223, 202, 205, 149, 149, 30, 141, 163, 210, 30, 210, 163, 225, 4, 224, 30, 157, 30, 29, 56, 30, 224, 97, 225, 163, 202, 220, 203, 210, 97, 220, 203, 210, 66, 210, 149, 30, 141, 163, 211, 203, 210, 30, 51, 30, 155, 217, 203, 213, 66, 218, 203, 214, 127, 210, 56, 30, 192, 30, 157, 30, 226, 163, 150, 202, 191, 214, 97, 214, 26, 97, 81, 163, 128, 149, 30, 119, 227, 136, 5, 202, 307, 97, 215, 149, 56, 30, 228, 163, 183, 202, 206, 66, 202, 227, 203, 141, 66, 224, 149, 203, 212, 203, 214, 66, 221, 203, 214, 97, 202, 210, 97, 214, 149, 97, 202, 212, 203, 214, 97, 308, 149, 97, 202, 217, 203, 213, 66, 218, 203, 214, 97, 307, 149, 97, 202, 214, 97, 214, 149, 97, 202, 308, 97, 307, 149, 149, 30, 226, 146, 50, 202, 228, 97, 229, 163, 202, 307, 97, 308, 149, 149, 30, 69, 30, 230, 163, 183, 202, 207, 66, 202, 224, 203, 212, 66, 221, 149, 203, 213, 97, 202, 210, 97, 213, 149, 97, 202, 212, 203, 213, 97, 308, 149, 97, 202, 217, 203, 213, 66, 218, 203, 214, 97, 218, 203, 214, 149, 97, 202, 214, 97, 214, 149, 97, 202, 308, 97, 307, 149, 149, 30, 10, 202, 230, 97, 226, 73, 223, 202, 207, 73, 81, 73, 102, 149, 97, 229, 163, 202, 307, 97, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_gla_fwd_kernel_o(\n    q,\n    v,\n    g,\n    h,\n    o,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(\n            q + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_g = tl.make_block_ptr(\n            g + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_h = tl.make_block_ptr(\n            h + (i_tg * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n\n        b_qg = (b_q * exp(b_g)).to(b_q.dtype)\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h.to(b_qg.dtype))\n    p_v = tl.make_block_ptr(\n        v + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    p_o = tl.make_block_ptr(\n        o + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    p_A = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_A = tl.where(m_s, b_A, 0.0).to(b_v.dtype)\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 57, 6, 98, 217, 57, 6, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 149, 57, 30, -1, 223, 98, 224, 98, 225, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 226, 98, 227, 163, 202, 225, 45, 216, 98, 225, 188, 216, 149, 30, 155, 222, 57, 30, 228, 163, 224, 30, 229, 98, 224, 163, 202, 52, 202, 213, 67, 224, 203, 309, 149, 74, 230, 202, 56, 149, 98, 52, 202, 213, 67, 224, 203, 309, 67, 308, 149, 74, 230, 202, 56, 149, 149, 30, 231, 98, 232, 163, 202, 52, 202, 212, 67, 229, 149, 74, 230, 202, 56, 149, 98, 52, 202, 212, 67, 229, 67, 308, 149, 74, 230, 202, 56, 149, 149, 30, 215, 163, 232, 4, 231, 30, 233, 163, 59, 202, 215, 98, 219, 149, 30, 157, 30, 29, 57, 30, 233, 163, 59, 202, 215, 98, 219, 149, 30, 228, 163, 226, 203, 233, 67, 224, 30, 231, 98, 232, 163, 202, 226, 203, 215, 98, 226, 203, 215, 67, 215, 149, 30, 51, 30, 234, 163, 68, 202, 307, 98, 219, 149, 191, 57, 98, 168, 26, 128, 68, 202, 307, 98, 219, 149, 191, 168, 98, 57, 26, 30, 235, 163, 150, 202, 191, 219, 98, 221, 26, 98, 82, 163, 129, 149, 30, 120, 236, 136, 5, 202, 59, 202, 217, 98, 220, 149, 149, 57, 30, 237, 163, 183, 202, 206, 67, 202, 231, 203, 216, 67, 227, 149, 203, 217, 98, 202, 215, 98, 217, 149, 98, 202, 216, 203, 217, 98, 308, 149, 98, 202, 224, 203, 219, 98, 236, 203, 220, 149, 98, 202, 219, 98, 220, 149, 98, 202, 308, 98, 307, 149, 149, 30, 238, 163, 183, 202, 208, 67, 202, 231, 203, 216, 67, 227, 149, 203, 217, 98, 202, 215, 98, 217, 149, 98, 202, 216, 203, 217, 98, 308, 149, 98, 202, 224, 203, 219, 98, 236, 203, 220, 149, 98, 202, 219, 98, 220, 149, 98, 202, 308, 98, 307, 149, 149, 30, 239, 163, 183, 202, 209, 67, 202, 228, 203, 216, 67, 227, 149, 203, 217, 203, 218, 98, 202, 217, 98, 218, 149, 98, 202, 218, 98, 308, 149, 98, 202, 236, 203, 220, 98, 223, 203, 221, 149, 98, 202, 220, 98, 221, 149, 98, 202, 308, 98, 307, 149, 149, 30, 240, 163, 52, 202, 237, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 240, 163, 202, 240, 203, 214, 149, 74, 230, 202, 240, 74, 82, 149, 30, 242, 163, 52, 202, 238, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 243, 163, 202, 240, 203, 173, 202, 242, 149, 149, 74, 230, 202, 240, 74, 82, 149, 30, 244, 163, 52, 202, 239, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 155, 236, 128, 307, 57, 30, 235, 146, 15, 202, 243, 98, 244, 74, 230, 202, 243, 74, 82, 149, 149, 30, 157, 30, 70, 30, 245, 163, 183, 202, 207, 67, 202, 231, 203, 216, 67, 227, 149, 203, 218, 98, 202, 215, 98, 218, 149, 98, 202, 216, 203, 218, 98, 308, 149, 98, 202, 224, 203, 219, 98, 223, 203, 221, 149, 98, 202, 219, 98, 221, 149, 98, 202, 308, 98, 307, 149, 149, 30, 246, 163, 183, 202, 210, 67, 202, 231, 203, 216, 67, 227, 149, 203, 218, 98, 202, 215, 98, 218, 149, 98, 202, 216, 203, 218, 98, 308, 149, 98, 202, 224, 203, 219, 98, 223, 203, 221, 149, 98, 202, 219, 98, 221, 149, 98, 202, 308, 98, 307, 149, 149, 30, 247, 163, 183, 202, 211, 67, 202, 231, 203, 216, 67, 227, 149, 203, 219, 98, 202, 215, 98, 219, 149, 98, 202, 216, 203, 219, 98, 308, 149, 98, 202, 224, 203, 219, 98, 307, 149, 98, 202, 219, 98, 219, 149, 98, 202, 308, 98, 307, 149, 149, 30, 248, 163, 52, 202, 245, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 249, 163, 52, 202, 247, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 249, 163, 171, 202, 234, 98, 249, 98, 307, 149, 74, 230, 202, 248, 74, 82, 149, 30, 235, 146, 15, 202, 249, 98, 248, 98, 250, 163, 54, 149, 30, 10, 202, 246, 98, 235, 74, 230, 202, 246, 74, 82, 74, 103, 149, 98, 241, 163, 202, 307, 98, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_gla_bwd_kernel_intra(\n    q,\n    k,\n    g,\n    dA,\n    dq,\n    dk,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_t, i_i = i_c // NC, i_c % NC\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n    T = eos - bos\n    if i_t * BT + i_i * BC >= T:\n        return\n\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n\n    p_g = tl.make_block_ptr(\n        g + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    if i_i > 0:\n        p_gn = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\n\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        for i_j in range(0, i_i):\n            p_k = tl.make_block_ptr(\n                k + (bos * H + i_h) * K,\n                (T, K),\n                (H * K, 1),\n                (i_t * BT + i_j * BC, i_k * BK),\n                (BC, BK),\n                (1, 0),\n            )\n            p_gk = tl.make_block_ptr(\n                g + (bos * H + i_h) * K,\n                (T, K),\n                (H * K, 1),\n                (i_t * BT + i_j * BC, i_k * BK),\n                (BC, BK),\n                (1, 0),\n            )\n            p_dA = tl.make_block_ptr(\n                dA + (bos * H + i_h) * BT,\n                (T, BT),\n                (H * BT, 1),\n                (i_t * BT + i_i * BC, i_j * BC),\n                (BC, BC),\n                (1, 0),\n            )\n\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_kg = b_k * exp(b_gn[None, :] - b_gk)\n\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n\n            b_dq += tl.dot(b_dA, b_kg)\n        b_dq *= exp(b_g - b_gn[None, :])\n\n    o_i = tl.arange(0, BC)\n    m_dA = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n    o_dA = (\n        bos * H * BT\n        + (i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT\n        + i_h * BT\n        + i_i * BC\n    )\n    p_kj = k + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\n    p_gkj = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\n    p_dq = tl.make_block_ptr(\n        dq + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n\n        b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)\n        b_gkj = tl.load(p_gkj, mask=m_k, other=0).to(tl.float32)\n\n        m_i = o_i[:, None] >= j\n\n        b_dq += tl.where(\n            m_i, b_dA[:, None] * b_kj[None, :] * exp(b_g - b_gkj[None, :]), 0.0\n        )\n        p_kj += H * K\n        p_gkj += H * K\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n\n    tl.debug_barrier()\n    p_k = tl.make_block_ptr(\n        k + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    p_gk = tl.make_block_ptr(\n        g + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n\n    NC = min(NC, tl.cdiv(T - i_t * BT, BC))\n    if i_i < NC - 1:\n        p_gn = g + (bos + min(i_t * BT + i_i * BC + BC, T) - 1) * H * K + i_h * K + o_k\n\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        for i_j in range(i_i + 1, NC):\n            p_q = tl.make_block_ptr(\n                q + (bos * H + i_h) * K,\n                (T, K),\n                (H * K, 1),\n                (i_t * BT + i_j * BC, i_k * BK),\n                (BC, BK),\n                (1, 0),\n            )\n            p_gq = tl.make_block_ptr(\n                g + (bos * H + i_h) * K,\n                (T, K),\n                (H * K, 1),\n                (i_t * BT + i_j * BC, i_k * BK),\n                (BC, BK),\n                (1, 0),\n            )\n            p_dA = tl.make_block_ptr(\n                dA + (bos * H + i_h) * BT,\n                (BT, T),\n                (1, H * BT),\n                (i_i * BC, i_t * BT + i_j * BC),\n                (BC, BC),\n                (0, 1),\n            )\n\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_gq = tl.load(p_gq, boundary_check=(0, 1))\n            b_qg = b_q * safe_exp(b_gq - b_gn[None, :])\n\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n\n            b_dk += tl.dot(b_dA, b_qg)\n        b_dk *= exp(b_gn[None, :] - b_gk)\n    o_dA = (\n        bos * H * BT\n        + (i_t * BT + i_i * BC) * H * BT\n        + i_h * BT\n        + i_i * BC\n        + tl.arange(0, BC)\n    )\n    p_qj = q + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\n    p_gqj = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\n    p_dk = tl.make_block_ptr(\n        dk + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n\n        b_dA = tl.load(dA + o_dA + j * H * BT)\n\n        b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)\n        b_gqj = tl.load(p_gqj, mask=m_k, other=0).to(tl.float32)\n\n        m_i = o_i[:, None] <= j\n        b_dk += tl.where(\n            m_i, b_dA[:, None] * b_qj[None, :] * exp(b_gqj[None, :] - b_gk), 0.0\n        )\n        p_qj += H * K\n        p_gqj += H * K\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 56, 6, 97, 216, 56, 6, 97, 217, 56, 6, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 149, 56, 30, -1, 222, 97, 223, 97, 224, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 225, 97, 226, 163, 202, 224, 45, 215, 97, 224, 188, 215, 149, 30, 227, 97, 228, 163, 202, 223, 45, 220, 97, 223, 188, 220, 149, 30, 155, 221, 56, 30, 229, 97, 227, 163, 202, 50, 202, 213, 66, 227, 203, 309, 149, 73, 230, 202, 205, 149, 97, 50, 202, 213, 66, 227, 203, 309, 66, 308, 149, 73, 230, 202, 205, 149, 149, 30, 231, 97, 232, 163, 202, 50, 202, 212, 66, 229, 149, 73, 230, 202, 205, 149, 97, 50, 202, 212, 66, 229, 66, 308, 149, 73, 230, 202, 205, 149, 149, 30, 157, 30, 29, 56, 30, 231, 97, 232, 163, 202, 225, 203, 214, 97, 225, 203, 214, 66, 214, 149, 30, 51, 30, 214, 163, 232, 4, 231, 30, 155, 227, 203, 217, 66, 228, 203, 218, 127, 214, 56, 30, 192, 30, 157, 30, 233, 163, 222, 203, 219, 66, 67, 202, 307, 97, 219, 149, 30, 234, 163, 233, 1, 216, 30, 235, 163, 183, 202, 208, 66, 202, 231, 203, 215, 66, 226, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 227, 203, 217, 66, 228, 203, 218, 97, 222, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 236, 163, 50, 202, 235, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 238, 163, 150, 202, 191, 218, 97, 219, 26, 97, 81, 163, 128, 149, 30, 155, 228, 110, 307, 56, 30, 239, 163, 208, 66, 202, 231, 66, 227, 203, 217, 66, 228, 203, 218, 149, 203, 215, 203, 216, 66, 226, 203, 216, 66, 233, 30, 240, 163, 50, 202, 239, 97, 241, 163, 234, 97, 242, 163, 307, 149, 30, 119, 243, 136, 5, 202, 307, 97, 228, 149, 56, 30, 244, 163, 183, 202, 207, 66, 202, 231, 203, 215, 66, 226, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 227, 203, 217, 66, 243, 203, 218, 97, 222, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 245, 163, 183, 202, 208, 66, 202, 231, 203, 215, 66, 226, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 227, 203, 217, 66, 243, 203, 218, 97, 222, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 246, 163, 183, 202, 209, 66, 202, 231, 203, 215, 66, 226, 149, 203, 217, 97, 202, 214, 97, 217, 149, 97, 202, 215, 203, 217, 97, 308, 149, 97, 202, 227, 203, 217, 66, 228, 203, 218, 97, 243, 203, 218, 149, 97, 202, 218, 97, 218, 149, 97, 202, 308, 97, 307, 149, 149, 30, 247, 163, 50, 202, 244, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 248, 163, 50, 202, 245, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 249, 163, 247, 203, 173, 202, 240, 191, 168, 97, 56, 26, 4, 248, 149, 30, 250, 163, 50, 202, 246, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 238, 146, 15, 202, 250, 97, 249, 149, 30, 69, 30, 238, 23, 173, 202, 236, 4, 240, 191, 168, 97, 56, 26, 149, 30, 157, 30, 251, 163, 67, 202, 307, 97, 218, 149, 30, 252, 163, 227, 203, 217, 66, 228, 203, 218, 66, 67, 202, 307, 97, 218, 149, 1, 214, 30, 253, 163, 231, 203, 215, 203, 217, 66, 202, 227, 203, 217, 66, 228, 203, 218, 66, 67, 202, 307, 97, 218, 149, 149, 203, 215, 203, 217, 66, 226, 203, 217, 66, 228, 203, 218, 30, 254, 163, 207, 66, 202, 231, 66, 227, 203, 217, 66, 228, 203, 218, 149, 203, 215, 203, 216, 66, 226, 203, 216, 66, 233, 30, 255, 163, 208, 66, 202, 231, 66, 227, 203, 217, 66, 228, 203, 218, 149, 203, 215, 203, 216, 66, 226, 203, 216, 66, 233, 30, 256, 163, 183, 202, 210, 66, 202, 231, 203, 215, 66, 226, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 227, 203, 217, 66, 228, 203, 218, 97, 222, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 119, 257, 136, 5, 202, 307, 97, 37, 202, 218, 97, 214, 4, 227, 203, 217, 4, 228, 203, 218, 149, 149, 56, 30, 250, 163, 50, 202, 209, 66, 253, 66, 257, 97, 241, 163, 252, 97, 242, 163, 307, 149, 30, 258, 163, 50, 202, 254, 97, 241, 163, 234, 97, 242, 163, 307, 149, 73, 230, 202, 128, 149, 30, 259, 163, 50, 202, 255, 97, 241, 163, 234, 97, 242, 163, 307, 149, 73, 230, 202, 128, 149, 30, 260, 163, 251, 191, 56, 97, 168, 26, 127, 257, 30, 238, 146, 171, 202, 260, 97, 250, 191, 56, 97, 168, 26, 203, 258, 191, 168, 97, 56, 26, 203, 173, 202, 236, 4, 259, 191, 168, 97, 56, 26, 149, 97, 307, 149, 30, 254, 146, 215, 203, 216, 30, 255, 146, 215, 203, 216, 30, 69, 30, 10, 202, 256, 97, 238, 73, 230, 202, 256, 73, 81, 73, 102, 149, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 47, 202, 149, 30, 244, 163, 183, 202, 207, 66, 202, 231, 203, 215, 66, 226, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 227, 203, 217, 66, 228, 203, 218, 97, 222, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 245, 163, 183, 202, 208, 66, 202, 231, 203, 215, 66, 226, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 227, 203, 217, 66, 228, 203, 218, 97, 222, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 247, 163, 50, 202, 244, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 248, 163, 50, 202, 245, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 261, 163, 150, 202, 191, 218, 97, 219, 26, 97, 81, 163, 128, 149, 30, 220, 163, 37, 202, 220, 97, 58, 202, 214, 4, 227, 203, 217, 97, 218, 149, 149, 30, 155, 228, 1, 220, 4, 308, 56, 30, 239, 163, 208, 66, 202, 231, 66, 37, 202, 227, 203, 217, 66, 228, 203, 218, 66, 218, 97, 214, 149, 4, 308, 149, 203, 215, 203, 216, 66, 226, 203, 216, 66, 233, 30, 240, 163, 50, 202, 239, 97, 241, 163, 234, 97, 242, 163, 307, 149, 30, 119, 243, 136, 5, 202, 228, 66, 308, 97, 220, 149, 56, 30, 262, 163, 183, 202, 206, 66, 202, 231, 203, 215, 66, 226, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 227, 203, 217, 66, 243, 203, 218, 97, 222, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 263, 163, 183, 202, 208, 66, 202, 231, 203, 215, 66, 226, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 227, 203, 217, 66, 243, 203, 218, 97, 222, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 246, 163, 183, 202, 209, 66, 202, 231, 203, 215, 66, 226, 149, 203, 217, 97, 202, 217, 97, 214, 149, 97, 202, 308, 97, 215, 203, 217, 149, 97, 202, 228, 203, 218, 97, 227, 203, 217, 66, 243, 203, 218, 149, 97, 202, 218, 97, 218, 149, 97, 202, 307, 97, 308, 149, 149, 30, 264, 163, 50, 202, 262, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 265, 163, 50, 202, 263, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 266, 163, 264, 203, 267, 202, 265, 4, 240, 191, 168, 97, 56, 26, 149, 30, 250, 163, 50, 202, 246, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 261, 146, 15, 202, 250, 97, 266, 149, 30, 69, 30, 261, 23, 173, 202, 240, 191, 168, 97, 56, 26, 4, 248, 149, 30, 157, 30, 253, 163, 231, 203, 215, 203, 217, 66, 202, 227, 203, 217, 66, 228, 203, 218, 149, 203, 215, 203, 217, 66, 226, 203, 217, 66, 228, 203, 218, 66, 67, 202, 307, 97, 218, 149, 30, 268, 163, 206, 66, 202, 231, 66, 227, 203, 217, 66, 228, 203, 218, 149, 203, 215, 203, 216, 66, 226, 203, 216, 66, 233, 30, 269, 163, 208, 66, 202, 231, 66, 227, 203, 217, 66, 228, 203, 218, 149, 203, 215, 203, 216, 66, 226, 203, 216, 66, 233, 30, 270, 163, 183, 202, 211, 66, 202, 231, 203, 215, 66, 226, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 227, 203, 217, 66, 228, 203, 218, 97, 222, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 119, 257, 136, 5, 202, 307, 97, 37, 202, 218, 97, 214, 4, 227, 203, 217, 4, 228, 203, 218, 149, 149, 56, 30, 250, 163, 50, 202, 209, 66, 253, 66, 257, 203, 215, 203, 217, 149, 30, 271, 163, 50, 202, 268, 97, 241, 163, 234, 97, 242, 163, 307, 149, 73, 230, 202, 128, 149, 30, 272, 163, 50, 202, 269, 97, 241, 163, 234, 97, 242, 163, 307, 149, 73, 230, 202, 128, 149, 30, 260, 163, 251, 191, 56, 97, 168, 26, 182, 257, 30, 261, 146, 171, 202, 260, 97, 250, 191, 56, 97, 168, 26, 203, 271, 191, 168, 97, 56, 26, 203, 173, 202, 272, 191, 168, 97, 56, 26, 4, 248, 149, 97, 307, 149, 30, 268, 146, 215, 203, 216, 30, 269, 146, 215, 203, 216, 30, 69, 30, 10, 202, 270, 97, 261, 73, 230, 202, 270, 73, 81, 73, 102, 149, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_gla_bwd_kernel_dA(\n    v,\n    do,\n    dA,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n    T = eos - bos\n\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_do = tl.make_block_ptr(\n            do + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (V, T),\n            (1, H * V),\n            (i_v * BV, i_t * BT),\n            (BV, BT),\n            (0, 1),\n        )\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dA += tl.dot(b_do, b_v)\n    p_dA = tl.make_block_ptr(\n        dA + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n    b_dA = tl.where(m_s, b_dA * scale, 0.0)\n    tl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 57, 6, 98, 214, 57, 6, 98, 215, 57, 6, 98, 216, 57, 6, 98, 217, 57, 6, 149, 57, 30, -1, 218, 98, 219, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 149, 30, 220, 98, 221, 163, 202, 219, 45, 213, 98, 219, 188, 213, 149, 30, 155, 217, 57, 30, 222, 98, 218, 163, 202, 52, 202, 210, 67, 218, 203, 309, 149, 74, 223, 202, 56, 149, 98, 52, 202, 210, 67, 218, 203, 309, 67, 308, 149, 74, 223, 202, 56, 149, 149, 30, 224, 98, 225, 163, 202, 52, 202, 209, 67, 222, 149, 74, 223, 202, 56, 149, 98, 52, 202, 209, 67, 222, 67, 308, 149, 74, 223, 202, 56, 149, 149, 30, 157, 30, 29, 57, 30, 224, 98, 225, 163, 202, 220, 203, 212, 98, 220, 203, 212, 67, 212, 149, 30, 51, 30, 212, 163, 225, 4, 224, 30, 226, 163, 150, 202, 191, 215, 98, 215, 26, 98, 82, 163, 129, 149, 30, 120, 227, 136, 5, 202, 59, 202, 214, 98, 216, 149, 149, 57, 30, 228, 163, 183, 202, 207, 67, 202, 224, 203, 213, 67, 221, 149, 203, 214, 98, 202, 212, 98, 214, 149, 98, 202, 213, 203, 214, 98, 308, 149, 98, 202, 218, 203, 215, 98, 227, 203, 216, 149, 98, 202, 215, 98, 216, 149, 98, 202, 308, 98, 307, 149, 149, 30, 229, 163, 183, 202, 206, 67, 202, 224, 203, 213, 67, 221, 149, 203, 214, 98, 202, 214, 98, 212, 149, 98, 202, 308, 98, 213, 203, 214, 149, 98, 202, 227, 203, 216, 98, 218, 203, 215, 149, 98, 202, 216, 98, 215, 149, 98, 202, 307, 98, 308, 149, 149, 30, 230, 163, 52, 202, 229, 98, 231, 163, 202, 307, 98, 308, 149, 149, 30, 232, 163, 52, 202, 228, 98, 231, 163, 202, 307, 98, 308, 149, 149, 30, 226, 146, 15, 202, 232, 98, 230, 149, 30, 70, 30, 233, 163, 183, 202, 208, 67, 202, 224, 203, 213, 67, 221, 149, 203, 215, 98, 202, 212, 98, 215, 149, 98, 202, 213, 203, 215, 98, 308, 149, 98, 202, 218, 203, 215, 98, 307, 149, 98, 202, 215, 98, 215, 149, 98, 202, 308, 98, 307, 149, 149, 30, 234, 163, 68, 202, 307, 98, 215, 149, 191, 57, 98, 168, 26, 128, 68, 202, 307, 98, 215, 149, 191, 168, 98, 57, 26, 30, 226, 163, 171, 202, 234, 98, 226, 203, 211, 98, 307, 149, 30, 10, 202, 233, 98, 226, 74, 223, 202, 233, 74, 82, 74, 103, 149, 98, 231, 163, 202, 307, 98, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_gla_bwd_kernel_dv(\n    k,\n    g,\n    A,\n    do,\n    dh,\n    dv,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    p_A = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1)\n    )\n    p_do = tl.make_block_ptr(\n        do + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    p_dv = tl.make_block_ptr(\n        dv + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :], b_A, 0.0)\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n\n    b_dv = tl.dot(b_A, b_do.to(b_A.dtype), allow_tf32=False)\n\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        m_k = o_k < K\n\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_gk = tl.make_block_ptr(\n            g + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_gn = g + (bos + min(i_t * BT + BT, T) - 1) * H * K + i_h * K + o_k\n        p_dh = tl.make_block_ptr(\n            dh + (i_tg * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_gn = exp(tl.load(p_gn, mask=m_k, other=0)[None, :] - b_gk)\n        b_k = (b_k * b_gn).to(b_k.dtype)\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        b_dv += tl.dot(b_k, b_dh.to(b_k.dtype))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 56, 6, 97, 216, 56, 6, 97, 217, 56, 6, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 149, 56, 30, -1, 222, 97, 223, 97, 224, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 225, 97, 226, 163, 202, 224, 45, 215, 97, 224, 188, 215, 149, 30, 155, 221, 56, 30, 227, 163, 223, 30, 228, 97, 223, 163, 202, 50, 202, 213, 66, 223, 203, 309, 149, 73, 229, 202, 205, 149, 97, 50, 202, 213, 66, 223, 203, 309, 66, 308, 149, 73, 229, 202, 205, 149, 149, 30, 230, 97, 231, 163, 202, 50, 202, 212, 66, 228, 149, 73, 229, 202, 205, 149, 97, 50, 202, 212, 66, 228, 66, 308, 149, 73, 229, 202, 205, 149, 149, 30, 214, 163, 231, 4, 230, 30, 232, 163, 58, 202, 214, 97, 218, 149, 30, 157, 30, 29, 56, 30, 232, 163, 58, 202, 214, 97, 218, 149, 30, 227, 163, 225, 203, 232, 66, 223, 30, 230, 97, 231, 163, 202, 225, 203, 214, 97, 225, 203, 214, 66, 214, 149, 30, 51, 30, 233, 163, 183, 202, 208, 66, 202, 230, 203, 215, 66, 226, 149, 203, 218, 97, 202, 218, 97, 214, 149, 97, 202, 308, 97, 215, 203, 218, 149, 97, 202, 307, 97, 223, 203, 218, 149, 97, 202, 218, 97, 218, 149, 97, 202, 307, 97, 308, 149, 149, 30, 234, 163, 183, 202, 209, 66, 202, 230, 203, 215, 66, 226, 149, 203, 217, 97, 202, 214, 97, 217, 149, 97, 202, 215, 203, 217, 97, 308, 149, 97, 202, 223, 203, 218, 97, 222, 203, 220, 149, 97, 202, 218, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 235, 163, 183, 202, 211, 66, 202, 230, 203, 215, 66, 226, 149, 203, 217, 97, 202, 214, 97, 217, 149, 97, 202, 215, 203, 217, 97, 308, 149, 97, 202, 223, 203, 218, 97, 222, 203, 220, 149, 97, 202, 218, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 236, 163, 50, 202, 233, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 236, 163, 171, 202, 67, 202, 307, 97, 218, 149, 191, 56, 97, 168, 26, 182, 67, 202, 307, 97, 218, 149, 191, 168, 97, 56, 26, 97, 236, 97, 307, 149, 30, 238, 163, 50, 202, 234, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 239, 163, 15, 202, 236, 97, 238, 73, 229, 202, 236, 73, 81, 149, 97, 240, 163, 54, 149, 30, 119, 241, 136, 5, 202, 58, 202, 216, 97, 219, 149, 149, 56, 30, 242, 163, 241, 203, 219, 66, 67, 202, 307, 97, 219, 149, 30, 243, 163, 242, 1, 216, 30, 244, 163, 183, 202, 206, 66, 202, 230, 203, 215, 66, 226, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 223, 203, 218, 97, 241, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 245, 163, 183, 202, 207, 66, 202, 230, 203, 215, 66, 226, 149, 203, 216, 97, 202, 214, 97, 216, 149, 97, 202, 215, 203, 216, 97, 308, 149, 97, 202, 223, 203, 218, 97, 241, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 246, 163, 207, 66, 202, 230, 66, 37, 202, 223, 203, 218, 66, 218, 97, 214, 149, 4, 308, 149, 203, 215, 203, 216, 66, 226, 203, 216, 66, 242, 30, 247, 163, 183, 202, 210, 66, 202, 227, 203, 215, 66, 226, 149, 203, 216, 203, 217, 97, 202, 216, 97, 217, 149, 97, 202, 217, 97, 308, 149, 97, 202, 241, 203, 219, 97, 222, 203, 220, 149, 97, 202, 219, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 248, 163, 50, 202, 244, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 249, 163, 50, 202, 245, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 250, 163, 173, 202, 50, 202, 246, 97, 251, 163, 243, 97, 252, 163, 307, 149, 191, 168, 97, 56, 26, 4, 249, 149, 30, 248, 163, 202, 248, 203, 250, 149, 73, 229, 202, 248, 73, 81, 149, 30, 253, 163, 50, 202, 247, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 239, 146, 15, 202, 248, 97, 253, 73, 229, 202, 248, 73, 81, 149, 149, 30, 69, 30, 10, 202, 235, 97, 239, 73, 229, 202, 235, 73, 81, 73, 102, 149, 97, 237, 163, 202, 307, 97, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_gla_bwd_kernel_inter(\n    q,\n    k,\n    v,\n    h,\n    g,\n    do,\n    dh,\n    dq,\n    dk,\n    dq2,\n    dk2,\n    dg,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n\n    p_gk = tl.make_block_ptr(\n        g + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_gn = g + (bos + min(T, i_t * BT + BT) - 1) * H * K + i_h * K + o_k\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dgk = tl.zeros(\n        [\n            BK,\n        ],\n        dtype=tl.float32,\n    )\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_do = tl.make_block_ptr(\n            do + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_h = tl.make_block_ptr(\n            h + (i_tg * H + i_h) * K * V,\n            (V, K),\n            (1, V),\n            (i_v * BV, i_k * BK),\n            (BV, BK),\n            (0, 1),\n        )\n        p_dh = tl.make_block_ptr(\n            dh + (i_tg * H + i_h) * K * V,\n            (V, K),\n            (1, V),\n            (i_v * BV, i_k * BK),\n            (BV, BK),\n            (0, 1),\n        )\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        b_dgk += tl.sum(b_h * b_dh, axis=0)\n\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\n    b_dgk *= exp(b_gn)\n    b_dq *= scale\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_dq = b_dq * exp(b_gk)\n    b_dk = b_dk * exp(b_gn[None, :] - b_gk)\n\n    p_q = tl.make_block_ptr(\n        q + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_k = tl.make_block_ptr(\n        k + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_dq = tl.make_block_ptr(\n        dq + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_dk = tl.make_block_ptr(\n        dk + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dgk += tl.sum(b_dk * b_k, axis=0)\n    b_dq += tl.load(p_dq, boundary_check=(0, 1))\n    b_dk += tl.load(p_dk, boundary_check=(0, 1))\n    b_dg = b_q * b_dq - b_k * b_dk\n\n    b_dg = (\n        b_dg - tl.cumsum(b_dg, axis=0) + tl.sum(b_dg, axis=0)[None, :] + b_dgk[None, :]\n    )\n\n    p_dq = tl.make_block_ptr(\n        dq2 + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_dk = tl.make_block_ptr(\n        dk2 + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_dg = tl.make_block_ptr(\n        dg + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 149, 57, 30, -1, 229, 98, 230, 98, 231, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 232, 98, 233, 163, 202, 231, 45, 222, 98, 231, 188, 222, 149, 30, 155, 228, 57, 30, 234, 163, 230, 30, 235, 98, 230, 163, 202, 52, 202, 219, 67, 230, 203, 309, 149, 74, 236, 202, 56, 149, 98, 52, 202, 219, 67, 230, 203, 309, 67, 308, 149, 74, 236, 202, 56, 149, 149, 30, 237, 98, 238, 163, 202, 52, 202, 218, 67, 235, 149, 74, 236, 202, 56, 149, 98, 52, 202, 218, 67, 235, 67, 308, 149, 74, 236, 202, 56, 149, 149, 30, 221, 163, 238, 4, 237, 30, 239, 163, 59, 202, 221, 98, 225, 149, 30, 157, 30, 29, 57, 30, 239, 163, 59, 202, 221, 98, 225, 149, 30, 234, 163, 232, 203, 239, 67, 230, 30, 237, 98, 238, 163, 202, 232, 203, 221, 98, 232, 203, 221, 67, 221, 149, 30, 51, 30, 240, 163, 229, 203, 226, 67, 68, 202, 307, 98, 226, 149, 30, 241, 163, 240, 1, 223, 30, 242, 163, 183, 202, 210, 67, 202, 237, 203, 222, 67, 233, 149, 203, 223, 98, 202, 221, 98, 223, 149, 98, 202, 222, 203, 223, 98, 308, 149, 98, 202, 230, 203, 225, 98, 229, 203, 226, 149, 98, 202, 225, 98, 226, 149, 98, 202, 308, 98, 307, 149, 149, 30, 243, 163, 210, 67, 202, 237, 67, 37, 202, 221, 98, 230, 203, 225, 67, 225, 149, 4, 308, 149, 203, 222, 203, 223, 67, 233, 203, 223, 67, 240, 30, 244, 163, 52, 202, 243, 98, 245, 163, 241, 98, 246, 163, 307, 149, 30, 247, 163, 150, 202, 191, 225, 98, 226, 26, 98, 82, 163, 129, 149, 30, 248, 163, 150, 202, 191, 225, 98, 226, 26, 98, 82, 163, 129, 149, 30, 249, 163, 150, 202, 191, 226, 26, 98, 82, 163, 129, 149, 30, 120, 250, 136, 5, 202, 59, 202, 224, 98, 227, 149, 149, 57, 30, 251, 163, 183, 202, 208, 67, 202, 237, 203, 222, 67, 233, 149, 203, 224, 98, 202, 221, 98, 224, 149, 98, 202, 222, 203, 224, 98, 308, 149, 98, 202, 230, 203, 225, 98, 250, 203, 227, 149, 98, 202, 225, 98, 227, 149, 98, 202, 308, 98, 307, 149, 149, 30, 252, 163, 183, 202, 211, 67, 202, 237, 203, 222, 67, 233, 149, 203, 224, 98, 202, 221, 98, 224, 149, 98, 202, 222, 203, 224, 98, 308, 149, 98, 202, 230, 203, 225, 98, 250, 203, 227, 149, 98, 202, 225, 98, 227, 149, 98, 202, 308, 98, 307, 149, 149, 30, 253, 163, 183, 202, 209, 67, 202, 234, 203, 222, 67, 233, 149, 203, 223, 203, 224, 98, 202, 224, 98, 223, 149, 98, 202, 308, 98, 224, 149, 98, 202, 250, 203, 227, 98, 229, 203, 226, 149, 98, 202, 227, 98, 226, 149, 98, 202, 307, 98, 308, 149, 149, 30, 254, 163, 183, 202, 212, 67, 202, 234, 203, 222, 67, 233, 149, 203, 223, 203, 224, 98, 202, 224, 98, 223, 149, 98, 202, 308, 98, 224, 149, 98, 202, 250, 203, 227, 98, 229, 203, 226, 149, 98, 202, 227, 98, 226, 149, 98, 202, 307, 98, 308, 149, 149, 30, 255, 163, 52, 202, 251, 98, 256, 163, 202, 307, 98, 308, 149, 149, 30, 257, 163, 52, 202, 252, 98, 256, 163, 202, 307, 98, 308, 149, 149, 30, 258, 163, 52, 202, 253, 98, 256, 163, 202, 307, 98, 308, 149, 149, 30, 259, 163, 52, 202, 254, 98, 256, 163, 202, 307, 98, 308, 149, 149, 30, 249, 146, 185, 202, 258, 203, 259, 98, 260, 163, 307, 149, 30, 247, 146, 15, 202, 257, 98, 258, 74, 236, 202, 257, 74, 82, 149, 149, 30, 248, 146, 15, 202, 255, 98, 259, 74, 236, 202, 255, 74, 82, 149, 149, 30, 70, 30, 249, 23, 173, 202, 244, 149, 30, 247, 23, 220, 30, 261, 163, 52, 202, 242, 98, 256, 163, 202, 307, 98, 308, 149, 149, 30, 247, 163, 247, 203, 173, 202, 261, 149, 30, 248, 163, 248, 203, 173, 202, 244, 191, 168, 98, 57, 26, 4, 261, 149, 30, 262, 163, 183, 202, 206, 67, 202, 237, 203, 222, 67, 233, 149, 203, 223, 98, 202, 221, 98, 223, 149, 98, 202, 222, 203, 223, 98, 308, 149, 98, 202, 230, 203, 225, 98, 229, 203, 226, 149, 98, 202, 225, 98, 226, 149, 98, 202, 308, 98, 307, 149, 149, 30, 263, 163, 183, 202, 207, 67, 202, 237, 203, 222, 67, 233, 149, 203, 223, 98, 202, 221, 98, 223, 149, 98, 202, 222, 203, 223, 98, 308, 149, 98, 202, 230, 203, 225, 98, 229, 203, 226, 149, 98, 202, 225, 98, 226, 149, 98, 202, 308, 98, 307, 149, 149, 30, 264, 163, 183, 202, 213, 67, 202, 237, 203, 222, 67, 233, 149, 203, 223, 98, 202, 221, 98, 223, 149, 98, 202, 222, 203, 223, 98, 308, 149, 98, 202, 230, 203, 225, 98, 229, 203, 226, 149, 98, 202, 225, 98, 226, 149, 98, 202, 308, 98, 307, 149, 149, 30, 265, 163, 183, 202, 214, 67, 202, 237, 203, 222, 67, 233, 149, 203, 223, 98, 202, 221, 98, 223, 149, 98, 202, 222, 203, 223, 98, 308, 149, 98, 202, 230, 203, 225, 98, 229, 203, 226, 149, 98, 202, 225, 98, 226, 149, 98, 202, 308, 98, 307, 149, 149, 30, 266, 163, 52, 202, 262, 98, 256, 163, 202, 307, 98, 308, 149, 149, 30, 267, 163, 52, 202, 263, 98, 256, 163, 202, 307, 98, 308, 149, 149, 30, 249, 146, 185, 202, 248, 203, 267, 98, 260, 163, 307, 149, 30, 247, 146, 52, 202, 264, 98, 256, 163, 202, 307, 98, 308, 149, 149, 30, 248, 146, 52, 202, 265, 98, 256, 163, 202, 307, 98, 308, 149, 149, 30, 268, 163, 266, 203, 247, 4, 267, 203, 248, 30, 268, 163, 268, 4, 76, 202, 268, 98, 260, 163, 307, 149, 67, 185, 202, 268, 98, 260, 163, 307, 149, 191, 168, 98, 57, 26, 67, 249, 191, 168, 98, 57, 26, 30, 264, 163, 183, 202, 215, 67, 202, 237, 203, 222, 67, 233, 149, 203, 223, 98, 202, 221, 98, 223, 149, 98, 202, 222, 203, 223, 98, 308, 149, 98, 202, 230, 203, 225, 98, 229, 203, 226, 149, 98, 202, 225, 98, 226, 149, 98, 202, 308, 98, 307, 149, 149, 30, 265, 163, 183, 202, 216, 67, 202, 237, 203, 222, 67, 233, 149, 203, 223, 98, 202, 221, 98, 223, 149, 98, 202, 222, 203, 223, 98, 308, 149, 98, 202, 230, 203, 225, 98, 229, 203, 226, 149, 98, 202, 225, 98, 226, 149, 98, 202, 308, 98, 307, 149, 149, 30, 269, 163, 183, 202, 217, 67, 202, 237, 203, 222, 67, 233, 149, 203, 223, 98, 202, 221, 98, 223, 149, 98, 202, 222, 203, 223, 98, 308, 149, 98, 202, 230, 203, 225, 98, 229, 203, 226, 149, 98, 202, 225, 98, 226, 149, 98, 202, 308, 98, 307, 149, 149, 30, 10, 202, 264, 98, 247, 74, 236, 202, 264, 74, 82, 74, 103, 149, 98, 256, 163, 202, 307, 98, 308, 149, 149, 30, 10, 202, 265, 98, 248, 74, 236, 202, 265, 74, 82, 74, 103, 149, 98, 256, 163, 202, 307, 98, 308, 149, 149, 30, 10, 202, 269, 98, 268, 74, 236, 202, 269, 74, 82, 74, 103, 149, 98, 256, 163, 202, 307, 98, 308, 149, 149, 30, 3, 30]}, {"code": "def prepare_qg_kg(\n    q, k, g, qg, kg, scale, T, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr\n):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_g = g + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_qg = qg + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_kg = kg + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n\n    mask = (i_k * BK + tl.arange(0, BK)) < K\n\n    last_decay = tl.load(\n        g + i_bh * T * K + (i_c * BT + BT - 1) * K + i_k * BK + tl.arange(0, BK)\n    )\n\n    for _ in range(BT):\n        b_q = tl.load(p_q, mask=mask, other=0)\n        b_k = tl.load(p_k, mask=mask, other=0)\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_q *= exp(b_g) * scale\n        b_k *= exp(last_decay - b_g)\n        tl.store(p_kg, b_k.to(p_kg.dtype.element_ty), mask=mask)\n        tl.store(p_qg, b_q.to(p_qg.dtype.element_ty), mask=mask)\n        p_q += K\n        p_g += K\n        p_k += K\n        p_kg += K\n        p_qg += K", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 56, 6, 97, 214, 56, 6, 97, 215, 56, 6, 149, 56, 30, -1, 216, 97, 217, 97, 218, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 219, 163, 206, 66, 218, 203, 212, 203, 213, 66, 217, 203, 214, 203, 213, 66, 216, 203, 215, 66, 67, 202, 307, 97, 215, 149, 30, 220, 163, 208, 66, 218, 203, 212, 203, 213, 66, 217, 203, 214, 203, 213, 66, 216, 203, 215, 66, 67, 202, 307, 97, 215, 149, 30, 221, 163, 207, 66, 218, 203, 212, 203, 213, 66, 217, 203, 214, 203, 213, 66, 216, 203, 215, 66, 67, 202, 307, 97, 215, 149, 30, 222, 163, 209, 66, 218, 203, 212, 203, 213, 66, 217, 203, 214, 203, 213, 66, 216, 203, 215, 66, 67, 202, 307, 97, 215, 149, 30, 223, 163, 210, 66, 218, 203, 212, 203, 213, 66, 217, 203, 214, 203, 213, 66, 216, 203, 215, 66, 67, 202, 307, 97, 215, 149, 30, 224, 163, 216, 203, 215, 66, 67, 202, 307, 97, 215, 149, 1, 213, 30, 225, 163, 50, 202, 208, 66, 218, 203, 212, 203, 213, 66, 202, 217, 203, 214, 66, 214, 4, 308, 149, 203, 213, 66, 216, 203, 215, 66, 67, 202, 307, 97, 215, 149, 149, 30, 119, 226, 136, 5, 202, 214, 149, 56, 30, 227, 163, 50, 202, 219, 97, 224, 163, 224, 97, 228, 163, 307, 149, 30, 229, 163, 50, 202, 221, 97, 224, 163, 224, 97, 228, 163, 307, 149, 30, 230, 163, 50, 202, 220, 97, 224, 163, 224, 97, 228, 163, 307, 149, 73, 231, 202, 128, 149, 30, 227, 23, 173, 202, 230, 149, 203, 211, 30, 229, 23, 173, 202, 225, 4, 230, 149, 30, 10, 202, 223, 97, 229, 73, 231, 202, 223, 73, 81, 73, 102, 149, 97, 224, 163, 224, 149, 30, 10, 202, 222, 97, 227, 73, 231, 202, 222, 73, 81, 73, 102, 149, 97, 224, 163, 224, 149, 30, 219, 146, 213, 30, 220, 146, 213, 30, 221, 146, 213, 30, 223, 146, 213, 30, 222, 146, 213, 30, 69, 30, 3, 30]}, {"code": "def bwd_decay_global_cumsum(\n    dq_inner,\n    dq_inter,\n    dk_inner,\n    dk_inter,\n    q,\n    k,\n    g,\n    dg,\n    T,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\n    p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\n    p_g = g + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\n    p_dg = dg + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\n    p_dq_inner = (\n        dq_inner + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\n    )\n    p_dk_inner = (\n        dk_inner + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\n    )\n    p_dq_inter = (\n        dq_inter + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\n    )\n    p_dk_inter = (\n        dk_inter + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\n    )\n    cum_grad_dg = tl.zeros([BK], dtype=tl.float32)\n    mask = (i_k * BK + tl.arange(0, BK)) < K\n    last_g = tl.zeros([BK], dtype=tl.float32)\n    for j in range(BT - 1, -1, -1):\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        if j == (BT - 1):\n            last_g = b_g\n        b_dq1 = tl.load(p_dq_inner, mask=mask, other=0)\n        b_dq2 = tl.load(p_dq_inter, mask=mask, other=0)\n        b_dq2 *= exp(b_g)\n        b_dq = b_dq1 + b_dq2\n        tl.store(p_dq_inter, b_dq, mask=mask)\n        b_dk1 = tl.load(p_dk_inner, mask=mask, other=0)\n        b_dk2 = tl.load(p_dk_inter, mask=mask, other=0)\n        b_dk2 *= safe_exp(last_g - b_g)\n        b_dk = b_dk1 + b_dk2\n        tl.store(p_dk_inter, b_dk, mask=mask)\n        b_q = tl.load(p_q, mask=mask, other=0)\n        b_k = tl.load(p_k, mask=mask, other=0)\n        b_dg = b_dq * b_q - b_dk * b_k\n        cum_grad_dg += b_dg\n        tl.store(p_dg, cum_grad_dg.to(p_dg.dtype.element_ty), mask=mask)\n        p_g -= K\n        p_k -= K\n        p_q -= K\n        p_dq_inner -= K\n        p_dk_inner -= K\n        p_dq_inter -= K\n        p_dk_inter -= K\n        p_dg -= K", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 57, 6, 98, 216, 57, 6, 98, 217, 57, 6, 149, 57, 30, -1, 218, 98, 219, 98, 220, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 221, 163, 210, 67, 220, 203, 214, 203, 215, 67, 218, 203, 217, 67, 68, 202, 307, 98, 217, 149, 67, 202, 219, 203, 216, 67, 216, 4, 308, 149, 203, 215, 30, 222, 163, 211, 67, 220, 203, 214, 203, 215, 67, 218, 203, 217, 67, 68, 202, 307, 98, 217, 149, 67, 202, 219, 203, 216, 67, 216, 4, 308, 149, 203, 215, 30, 223, 163, 212, 67, 220, 203, 214, 203, 215, 67, 218, 203, 217, 67, 68, 202, 307, 98, 217, 149, 67, 202, 219, 203, 216, 67, 216, 4, 308, 149, 203, 215, 30, 224, 163, 213, 67, 220, 203, 214, 203, 215, 67, 218, 203, 217, 67, 68, 202, 307, 98, 217, 149, 67, 202, 219, 203, 216, 67, 216, 4, 308, 149, 203, 215, 30, 225, 163, 206, 67, 220, 203, 214, 203, 215, 67, 218, 203, 217, 67, 68, 202, 307, 98, 217, 149, 67, 202, 219, 203, 216, 67, 216, 4, 308, 149, 203, 215, 30, 226, 163, 208, 67, 220, 203, 214, 203, 215, 67, 218, 203, 217, 67, 68, 202, 307, 98, 217, 149, 67, 202, 219, 203, 216, 67, 216, 4, 308, 149, 203, 215, 30, 227, 163, 207, 67, 220, 203, 214, 203, 215, 67, 218, 203, 217, 67, 68, 202, 307, 98, 217, 149, 67, 202, 219, 203, 216, 67, 216, 4, 308, 149, 203, 215, 30, 228, 163, 209, 67, 220, 203, 214, 203, 215, 67, 218, 203, 217, 67, 68, 202, 307, 98, 217, 149, 67, 202, 219, 203, 216, 67, 216, 4, 308, 149, 203, 215, 30, 229, 163, 150, 202, 191, 217, 26, 98, 82, 163, 129, 149, 30, 230, 163, 218, 203, 217, 67, 68, 202, 307, 98, 217, 149, 1, 215, 30, 231, 163, 150, 202, 191, 217, 26, 98, 82, 163, 129, 149, 30, 120, 232, 136, 5, 202, 216, 4, 308, 98, 4, 308, 98, 4, 308, 149, 57, 30, 233, 163, 52, 202, 223, 98, 230, 163, 230, 98, 234, 163, 307, 149, 74, 235, 202, 129, 149, 30, 155, 232, 69, 216, 4, 308, 57, 30, 231, 163, 233, 30, 157, 30, 236, 163, 52, 202, 225, 98, 230, 163, 230, 98, 234, 163, 307, 149, 30, 237, 163, 52, 202, 227, 98, 230, 163, 230, 98, 234, 163, 307, 149, 30, 237, 23, 173, 202, 233, 149, 30, 238, 163, 236, 67, 237, 30, 10, 202, 227, 98, 238, 98, 230, 163, 230, 149, 30, 239, 163, 52, 202, 226, 98, 230, 163, 230, 98, 234, 163, 307, 149, 30, 240, 163, 52, 202, 228, 98, 230, 163, 230, 98, 234, 163, 307, 149, 30, 240, 23, 241, 202, 231, 4, 233, 149, 30, 242, 163, 239, 67, 240, 30, 10, 202, 228, 98, 242, 98, 230, 163, 230, 149, 30, 243, 163, 52, 202, 221, 98, 230, 163, 230, 98, 234, 163, 307, 149, 30, 244, 163, 52, 202, 222, 98, 230, 163, 230, 98, 234, 163, 307, 149, 30, 245, 163, 238, 203, 243, 4, 242, 203, 244, 30, 229, 146, 245, 30, 10, 202, 224, 98, 229, 74, 235, 202, 224, 74, 82, 74, 103, 149, 98, 230, 163, 230, 149, 30, 223, 2, 215, 30, 222, 2, 215, 30, 221, 2, 215, 30, 225, 2, 215, 30, 226, 2, 215, 30, 227, 2, 215, 30, 228, 2, 215, 30, 224, 2, 215, 30, 70, 30, 3, 30]}, {"code": "def fused_chunk_gla_fwd_kernel(\n    q,\n    k,\n    v,\n    g,\n    o,\n    h0,\n    ht,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    CHECK: tl.constexpr,\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    p_q = tl.make_block_ptr(\n        q + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_gn = g + i_bh * T * K + (BT - 1) * K + i_k * BK + tl.arange(0, BK)\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BT), (0, 1)\n    )\n    p_v = tl.make_block_ptr(\n        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_o = tl.make_block_ptr(\n        o + (i_bh + i_k * B * H) * T * V,\n        (T, V),\n        (V, 1),\n        (0, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(\n            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n\n    mask = (i_k * BK + tl.arange(0, BK)) < K\n\n    for i in range(0, tl.cdiv(T, BT)):\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_gn = tl.load(p_gn, mask=mask, other=0).to(tl.float32)\n        if CHECK and i == 0:\n            b_o = tl.dot(b_q.to(b_v.dtype), b_h.to(b_v.dtype), allow_tf32=False)\n            b_h = b_h * exp(b_gn)[:, None] + tl.dot(\n                b_k.to(b_v.dtype), b_v, allow_tf32=False\n            )\n        else:\n            b_o = tl.dot(b_q.to(b_v.dtype), b_h.to(b_v.dtype), allow_tf32=False)\n            b_h = b_h * exp(b_gn)[:, None] + tl.dot(\n                b_k.to(b_v.dtype), b_v, allow_tf32=False\n            )\n\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_gn += BT * K\n\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(\n            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_final, b_h.to(p_final.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 56, 6, 97, 215, 56, 6, 97, 216, 56, 6, 97, 217, 56, 6, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 149, 56, 30, -1, 224, 97, 225, 97, 226, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 227, 163, 150, 202, 191, 219, 97, 220, 26, 97, 81, 163, 128, 149, 30, 228, 163, 183, 202, 206, 66, 226, 203, 213, 203, 216, 97, 202, 213, 97, 216, 149, 97, 202, 216, 97, 308, 149, 97, 202, 307, 97, 225, 203, 219, 149, 97, 202, 218, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 229, 163, 209, 66, 226, 203, 213, 203, 216, 66, 202, 218, 4, 308, 149, 203, 216, 66, 225, 203, 219, 66, 67, 202, 307, 97, 219, 149, 30, 230, 163, 183, 202, 207, 66, 226, 203, 213, 203, 216, 97, 202, 216, 97, 213, 149, 97, 202, 308, 97, 216, 149, 97, 202, 225, 203, 219, 97, 307, 149, 97, 202, 219, 97, 218, 149, 97, 202, 307, 97, 308, 149, 149, 30, 231, 163, 183, 202, 208, 66, 226, 203, 213, 203, 217, 97, 202, 213, 97, 217, 149, 97, 202, 217, 97, 308, 149, 97, 202, 307, 97, 224, 203, 220, 149, 97, 202, 218, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 232, 163, 183, 202, 210, 66, 202, 226, 66, 225, 203, 214, 203, 215, 149, 203, 213, 203, 217, 97, 202, 213, 97, 217, 149, 97, 202, 217, 97, 308, 149, 97, 202, 307, 97, 224, 203, 220, 149, 97, 202, 218, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 155, 221, 56, 30, 233, 163, 183, 202, 211, 66, 226, 203, 216, 203, 217, 97, 202, 216, 97, 217, 149, 97, 202, 217, 97, 308, 149, 97, 202, 225, 203, 219, 97, 224, 203, 220, 149, 97, 202, 219, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 227, 146, 50, 202, 233, 97, 234, 163, 202, 307, 97, 308, 149, 149, 73, 235, 202, 128, 149, 30, 157, 30, 236, 163, 225, 203, 219, 66, 67, 202, 307, 97, 219, 149, 1, 216, 30, 119, 237, 136, 5, 202, 307, 97, 58, 202, 213, 97, 218, 149, 149, 56, 30, 238, 163, 50, 202, 230, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 239, 163, 50, 202, 231, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 240, 163, 50, 202, 228, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 241, 163, 50, 202, 229, 97, 236, 163, 236, 97, 242, 163, 307, 149, 73, 235, 202, 128, 149, 30, 155, 223, 91, 237, 68, 307, 56, 30, 243, 163, 15, 202, 240, 73, 235, 202, 239, 73, 81, 149, 97, 227, 73, 235, 202, 239, 73, 81, 149, 97, 244, 163, 54, 149, 30, 227, 163, 227, 203, 173, 202, 241, 149, 191, 56, 97, 168, 26, 66, 15, 202, 238, 73, 235, 202, 239, 73, 81, 149, 97, 239, 97, 244, 163, 54, 149, 30, 157, 30, 29, 56, 30, 243, 163, 15, 202, 240, 73, 235, 202, 239, 73, 81, 149, 97, 227, 73, 235, 202, 239, 73, 81, 149, 97, 244, 163, 54, 149, 30, 227, 163, 227, 203, 173, 202, 241, 149, 191, 56, 97, 168, 26, 66, 15, 202, 238, 73, 235, 202, 239, 73, 81, 149, 97, 239, 97, 244, 163, 54, 149, 30, 51, 30, 10, 202, 232, 97, 243, 73, 235, 202, 232, 73, 81, 73, 102, 149, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 228, 163, 126, 202, 228, 97, 202, 218, 97, 307, 149, 149, 30, 230, 163, 126, 202, 230, 97, 202, 307, 97, 218, 149, 149, 30, 231, 163, 126, 202, 231, 97, 202, 218, 97, 307, 149, 149, 30, 232, 163, 126, 202, 232, 97, 202, 218, 97, 307, 149, 149, 30, 229, 146, 218, 203, 216, 30, 69, 30, 155, 222, 56, 30, 245, 163, 183, 202, 212, 66, 226, 203, 216, 203, 217, 97, 202, 216, 97, 217, 149, 97, 202, 217, 97, 308, 149, 97, 202, 225, 203, 219, 97, 224, 203, 220, 149, 97, 202, 219, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 10, 202, 245, 97, 227, 73, 235, 202, 245, 73, 81, 73, 102, 149, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 157, 30, 3, 30]}, {"code": "def fused_chunk_gla_bwd_kernel(\n    q,\n    k,\n    v,\n    g,\n    do,\n    dq,\n    dk,\n    dv,\n    h0,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    CHECK: tl.constexpr,\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(\n            h0 + i_bh * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)\n        )\n        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n\n    mask = (i_k * BK + tl.arange(0, BK)) < K\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(\n            k + i_bh * T * K, (T, K), (K, 1), (i * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_gn = g + i_bh * T * K + ((i + 1) * BT - 1) * K + i_k * BK + tl.arange(0, BK)\n        p_v = tl.make_block_ptr(\n            v + i_bh * T * V, (V, T), (1, V), (i_v * BV, i * BT), (BV, BT), (0, 1)\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * T * V, (T, V), (V, 1), (i * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_dq = tl.make_block_ptr(\n            dq + (i_bh + i_v * B * H) * T * K,\n            (T, K),\n            (K, 1),\n            (i * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gn = tl.load(p_gn, mask=mask, other=0).to(tl.float32)\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n            b_h = b_h * exp(b_gn)[None, :] + tl.dot(\n                b_v, b_k.to(b_v.dtype), allow_tf32=False\n            )\n        else:\n            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n            b_h = b_h * exp(b_gn)[None, :] + tl.dot(\n                b_v, b_k.to(b_v.dtype), allow_tf32=False\n            )\n        b_dq *= scale\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n\n    b_h = None\n    tl.debug_barrier()\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(\n            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, T - i * BT), (BK, BT), (0, 1)\n        )\n        p_k = tl.make_block_ptr(\n            k + i_bh * T * K, (T, K), (K, 1), (T - i * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_gn = (\n            g + i_bh * T * K + (T - (i - 1) * BT - 1) * K + i_k * BK + tl.arange(0, BK)\n        )\n        p_v = tl.make_block_ptr(\n            v + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_dk = tl.make_block_ptr(\n            dk + (i_bh + i_v * B * H) * T * K,\n            (T, K),\n            (K, 1),\n            (T - i * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dv = tl.make_block_ptr(\n            dv + (i_bh + i_k * B * H) * T * V,\n            (T, V),\n            (V, 1),\n            (T - i * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_db = tl.load(p_gn, mask=mask, other=0).to(tl.float32)\n\n        if CHECK and i == 1:\n            b_dk = tl.trans(tl.dot(b_dh.to(b_v.dtype), tl.trans(b_v), allow_tf32=False))\n            b_dv = tl.dot((b_k).to(b_v.dtype), b_dh.to(b_v.dtype), allow_tf32=False)\n            b_dh = b_dh * exp(b_db)[:, None] + tl.dot(\n                b_q.to(b_do.dtype), b_do, allow_tf32=False\n            )\n        else:\n            b_dk = tl.trans(tl.dot(b_dh.to(b_v.dtype), tl.trans(b_v), allow_tf32=False))\n            b_dv = tl.dot((b_k).to(b_v.dtype), b_dh.to(b_v.dtype), allow_tf32=False)\n            b_dh = b_dh * exp(b_db)[:, None] + tl.dot(\n                b_q.to(b_do.dtype), b_do, allow_tf32=False\n            )\n\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 57, 6, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 149, 57, 30, -1, 226, 98, 227, 98, 228, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 229, 163, 150, 202, 191, 223, 98, 222, 26, 98, 82, 163, 129, 149, 30, 155, 224, 57, 30, 230, 163, 183, 202, 214, 67, 228, 203, 219, 203, 220, 98, 202, 220, 98, 219, 149, 98, 202, 308, 98, 220, 149, 98, 202, 226, 203, 223, 98, 227, 203, 222, 149, 98, 202, 223, 98, 222, 149, 98, 202, 307, 98, 308, 149, 149, 30, 229, 146, 52, 202, 230, 98, 231, 163, 202, 307, 98, 308, 149, 149, 74, 232, 202, 129, 149, 30, 157, 30, 233, 163, 227, 203, 222, 67, 68, 202, 307, 98, 222, 149, 1, 219, 30, 120, 234, 136, 5, 202, 307, 98, 59, 202, 216, 98, 221, 149, 149, 57, 30, 235, 163, 183, 202, 207, 67, 228, 203, 216, 203, 219, 98, 202, 216, 98, 219, 149, 98, 202, 219, 98, 308, 149, 98, 202, 234, 203, 221, 98, 227, 203, 222, 149, 98, 202, 221, 98, 222, 149, 98, 202, 308, 98, 307, 149, 149, 30, 236, 163, 209, 67, 228, 203, 216, 203, 219, 67, 202, 202, 234, 67, 308, 149, 203, 221, 4, 308, 149, 203, 219, 67, 227, 203, 222, 67, 68, 202, 307, 98, 222, 149, 30, 237, 163, 183, 202, 208, 67, 228, 203, 216, 203, 220, 98, 202, 220, 98, 216, 149, 98, 202, 308, 98, 220, 149, 98, 202, 226, 203, 223, 98, 234, 203, 221, 149, 98, 202, 223, 98, 221, 149, 98, 202, 307, 98, 308, 149, 149, 30, 238, 163, 183, 202, 210, 67, 228, 203, 216, 203, 220, 98, 202, 216, 98, 220, 149, 98, 202, 220, 98, 308, 149, 98, 202, 234, 203, 221, 98, 226, 203, 223, 149, 98, 202, 221, 98, 223, 149, 98, 202, 308, 98, 307, 149, 149, 30, 239, 163, 183, 202, 211, 67, 202, 228, 67, 226, 203, 217, 203, 218, 149, 203, 216, 203, 219, 98, 202, 216, 98, 219, 149, 98, 202, 219, 98, 308, 149, 98, 202, 234, 203, 221, 98, 227, 203, 222, 149, 98, 202, 221, 98, 222, 149, 98, 202, 308, 98, 307, 149, 149, 30, 240, 163, 150, 202, 191, 221, 98, 222, 26, 98, 82, 163, 129, 149, 30, 241, 163, 52, 202, 235, 98, 231, 163, 202, 307, 98, 308, 149, 149, 30, 242, 163, 52, 202, 236, 98, 233, 163, 233, 98, 243, 163, 307, 149, 74, 232, 202, 129, 149, 30, 244, 163, 52, 202, 237, 98, 231, 163, 202, 307, 98, 308, 149, 149, 30, 245, 163, 52, 202, 238, 98, 231, 163, 202, 307, 98, 308, 149, 149, 30, 155, 225, 92, 234, 69, 307, 57, 30, 240, 146, 15, 202, 245, 98, 229, 74, 232, 202, 245, 74, 82, 149, 98, 246, 163, 54, 149, 30, 229, 163, 229, 203, 173, 202, 242, 149, 191, 168, 98, 57, 26, 67, 15, 202, 244, 98, 241, 74, 232, 202, 244, 74, 82, 149, 98, 246, 163, 54, 149, 30, 157, 30, 29, 57, 30, 240, 146, 15, 202, 245, 98, 229, 74, 232, 202, 245, 74, 82, 149, 98, 246, 163, 54, 149, 30, 229, 163, 229, 203, 173, 202, 242, 149, 191, 168, 98, 57, 26, 67, 15, 202, 244, 98, 241, 74, 232, 202, 244, 74, 82, 149, 98, 246, 163, 54, 149, 30, 51, 30, 240, 23, 215, 30, 10, 202, 239, 98, 240, 74, 232, 202, 239, 74, 82, 74, 103, 149, 98, 231, 163, 202, 307, 98, 308, 149, 149, 30, 70, 30, 229, 163, 168, 30, 47, 202, 149, 30, 247, 163, 150, 202, 191, 222, 98, 223, 26, 98, 82, 163, 129, 149, 30, 120, 234, 136, 5, 202, 308, 98, 59, 202, 216, 98, 221, 149, 67, 308, 149, 57, 30, 248, 163, 183, 202, 206, 67, 228, 203, 216, 203, 219, 98, 202, 219, 98, 216, 149, 98, 202, 308, 98, 219, 149, 98, 202, 227, 203, 222, 98, 216, 4, 234, 203, 221, 149, 98, 202, 222, 98, 221, 149, 98, 202, 307, 98, 308, 149, 149, 30, 235, 163, 183, 202, 207, 67, 228, 203, 216, 203, 219, 98, 202, 216, 98, 219, 149, 98, 202, 219, 98, 308, 149, 98, 202, 216, 4, 234, 203, 221, 98, 227, 203, 222, 149, 98, 202, 221, 98, 222, 149, 98, 202, 308, 98, 307, 149, 149, 30, 236, 163, 209, 67, 228, 203, 216, 203, 219, 67, 202, 216, 4, 202, 234, 4, 308, 149, 203, 221, 4, 308, 149, 203, 219, 67, 227, 203, 222, 67, 68, 202, 307, 98, 222, 149, 30, 237, 163, 183, 202, 208, 67, 228, 203, 216, 203, 220, 98, 202, 216, 98, 220, 149, 98, 202, 220, 98, 308, 149, 98, 202, 216, 4, 234, 203, 221, 98, 226, 203, 223, 149, 98, 202, 221, 98, 223, 149, 98, 202, 308, 98, 307, 149, 149, 30, 238, 163, 183, 202, 210, 67, 228, 203, 216, 203, 220, 98, 202, 216, 98, 220, 149, 98, 202, 220, 98, 308, 149, 98, 202, 216, 4, 234, 203, 221, 98, 226, 203, 223, 149, 98, 202, 221, 98, 223, 149, 98, 202, 308, 98, 307, 149, 149, 30, 249, 163, 183, 202, 212, 67, 202, 228, 67, 226, 203, 217, 203, 218, 149, 203, 216, 203, 219, 98, 202, 216, 98, 219, 149, 98, 202, 219, 98, 308, 149, 98, 202, 216, 4, 234, 203, 221, 98, 227, 203, 222, 149, 98, 202, 221, 98, 222, 149, 98, 202, 308, 98, 307, 149, 149, 30, 250, 163, 183, 202, 213, 67, 202, 228, 67, 227, 203, 217, 203, 218, 149, 203, 216, 203, 220, 98, 202, 216, 98, 220, 149, 98, 202, 220, 98, 308, 149, 98, 202, 216, 4, 234, 203, 221, 98, 226, 203, 223, 149, 98, 202, 221, 98, 223, 149, 98, 202, 308, 98, 307, 149, 149, 30, 251, 163, 52, 202, 248, 98, 231, 163, 202, 307, 98, 308, 149, 149, 30, 241, 163, 52, 202, 235, 98, 231, 163, 202, 307, 98, 308, 149, 149, 30, 244, 163, 52, 202, 237, 98, 231, 163, 202, 307, 98, 308, 149, 149, 30, 245, 163, 52, 202, 238, 98, 231, 163, 202, 307, 98, 308, 149, 149, 30, 252, 163, 52, 202, 236, 98, 233, 163, 233, 98, 243, 163, 307, 149, 74, 232, 202, 129, 149, 30, 155, 225, 92, 234, 69, 308, 57, 30, 253, 163, 65, 202, 15, 202, 247, 74, 232, 202, 244, 74, 82, 149, 98, 65, 202, 244, 149, 98, 246, 163, 54, 149, 149, 30, 254, 163, 15, 202, 241, 74, 232, 202, 244, 74, 82, 149, 98, 247, 74, 232, 202, 244, 74, 82, 149, 98, 246, 163, 54, 149, 30, 247, 163, 247, 203, 173, 202, 252, 149, 191, 57, 98, 168, 26, 67, 15, 202, 251, 74, 232, 202, 245, 74, 82, 149, 98, 245, 98, 246, 163, 54, 149, 30, 157, 30, 29, 57, 30, 253, 163, 65, 202, 15, 202, 247, 74, 232, 202, 244, 74, 82, 149, 98, 65, 202, 244, 149, 98, 246, 163, 54, 149, 149, 30, 254, 163, 15, 202, 241, 74, 232, 202, 244, 74, 82, 149, 98, 247, 74, 232, 202, 244, 74, 82, 149, 98, 246, 163, 54, 149, 30, 247, 163, 247, 203, 173, 202, 252, 149, 191, 57, 98, 168, 26, 67, 15, 202, 251, 74, 232, 202, 245, 74, 82, 149, 98, 245, 98, 246, 163, 54, 149, 30, 51, 30, 10, 202, 249, 98, 253, 74, 232, 202, 249, 74, 82, 74, 103, 149, 98, 231, 163, 202, 307, 98, 308, 149, 149, 30, 10, 202, 250, 98, 254, 74, 232, 202, 250, 74, 82, 74, 103, 149, 98, 231, 163, 202, 307, 98, 308, 149, 149, 30, 70, 30, 3, 30]}, {"code": "def chunk_gsa_fwd_k_kernel_inter(\n    q,\n    k,\n    h,\n    g,\n    o,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NG: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // NG\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(\n            q + (bos * HQ + i_hq) * K,\n            (T, K),\n            (HQ * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_h = tl.make_block_ptr(\n            h + (i_tg * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n\n        b_o += tl.dot(b_q, b_h)\n\n        b_A += tl.dot(b_q, b_k)\n    p_g = tl.make_block_ptr(\n        g + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    p_o = tl.make_block_ptr(\n        o + (bos * HQ + i_hq) * V,\n        (T, V),\n        (HQ * V, 1),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    p_A = tl.make_block_ptr(\n        A + (bos * HQ + i_hq) * BT,\n        (T, BT),\n        (HQ * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_o = b_o * exp(b_g)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n    b_A = tl.where(m_s, b_A, 0.0)\n    if i_v == 0:\n        tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 56, 6, 97, 217, 56, 6, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 97, 224, 56, 6, 149, 56, 30, -1, 225, 97, 226, 97, 227, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 228, 97, 229, 163, 202, 227, 45, 216, 97, 227, 188, 216, 149, 30, 230, 163, 229, 45, 223, 30, 155, 224, 56, 30, 231, 163, 226, 30, 232, 97, 226, 163, 202, 50, 202, 213, 66, 226, 203, 309, 149, 73, 233, 202, 205, 149, 97, 50, 202, 213, 66, 226, 203, 309, 66, 308, 149, 73, 233, 202, 205, 149, 149, 30, 234, 97, 235, 163, 202, 50, 202, 212, 66, 232, 149, 73, 233, 202, 205, 149, 97, 50, 202, 212, 66, 232, 66, 308, 149, 73, 233, 202, 205, 149, 149, 30, 215, 163, 235, 4, 234, 30, 236, 163, 58, 202, 215, 97, 220, 149, 30, 157, 30, 29, 56, 30, 236, 163, 58, 202, 215, 97, 220, 149, 30, 231, 163, 228, 203, 236, 66, 226, 30, 234, 97, 235, 163, 202, 228, 203, 215, 97, 228, 203, 215, 66, 215, 149, 30, 51, 30, 237, 163, 67, 202, 307, 97, 220, 149, 30, 238, 163, 237, 191, 56, 97, 168, 26, 127, 237, 191, 168, 97, 56, 26, 30, 239, 163, 150, 202, 191, 220, 97, 222, 26, 97, 81, 163, 128, 149, 30, 240, 163, 150, 202, 191, 220, 97, 220, 26, 97, 81, 163, 128, 149, 30, 119, 241, 136, 5, 202, 58, 202, 218, 97, 221, 149, 149, 56, 30, 242, 163, 183, 202, 206, 66, 202, 234, 203, 216, 66, 229, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 216, 203, 218, 97, 308, 149, 97, 202, 226, 203, 220, 97, 241, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 243, 163, 183, 202, 207, 66, 202, 234, 203, 217, 66, 230, 149, 203, 218, 97, 202, 218, 97, 215, 149, 97, 202, 308, 97, 217, 203, 218, 149, 97, 202, 241, 203, 221, 97, 226, 203, 220, 149, 97, 202, 221, 97, 220, 149, 97, 202, 307, 97, 308, 149, 149, 30, 244, 163, 183, 202, 208, 66, 202, 231, 203, 217, 66, 230, 149, 203, 218, 203, 219, 97, 202, 218, 97, 219, 149, 97, 202, 219, 97, 308, 149, 97, 202, 241, 203, 221, 97, 225, 203, 222, 149, 97, 202, 221, 97, 222, 149, 97, 202, 308, 97, 307, 149, 149, 30, 245, 163, 50, 202, 242, 97, 246, 163, 202, 307, 97, 308, 149, 149, 30, 245, 163, 202, 245, 203, 214, 149, 73, 233, 202, 245, 73, 81, 149, 30, 247, 163, 50, 202, 243, 97, 246, 163, 202, 307, 97, 308, 149, 149, 30, 248, 163, 50, 202, 244, 97, 246, 163, 202, 307, 97, 308, 149, 149, 30, 239, 146, 15, 202, 245, 97, 248, 149, 30, 240, 146, 15, 202, 245, 97, 247, 149, 30, 69, 30, 249, 163, 183, 202, 209, 66, 202, 234, 203, 217, 66, 230, 149, 203, 219, 97, 202, 215, 97, 219, 149, 97, 202, 217, 203, 219, 97, 308, 149, 97, 202, 226, 203, 220, 97, 225, 203, 222, 149, 97, 202, 220, 97, 222, 149, 97, 202, 308, 97, 307, 149, 149, 30, 250, 163, 183, 202, 210, 66, 202, 234, 203, 216, 66, 229, 149, 203, 219, 97, 202, 215, 97, 219, 149, 97, 202, 216, 203, 219, 97, 308, 149, 97, 202, 226, 203, 220, 97, 225, 203, 222, 149, 97, 202, 220, 97, 222, 149, 97, 202, 308, 97, 307, 149, 149, 30, 251, 163, 183, 202, 211, 66, 202, 234, 203, 216, 66, 229, 149, 203, 220, 97, 202, 215, 97, 220, 149, 97, 202, 216, 203, 220, 97, 308, 149, 97, 202, 226, 203, 220, 97, 307, 149, 97, 202, 220, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 252, 163, 50, 202, 249, 97, 246, 163, 202, 307, 97, 308, 149, 149, 30, 239, 163, 239, 203, 173, 202, 252, 149, 30, 10, 202, 250, 97, 239, 73, 233, 202, 250, 73, 81, 73, 102, 149, 97, 246, 163, 202, 307, 97, 308, 149, 149, 30, 240, 163, 171, 202, 238, 97, 240, 97, 307, 149, 30, 155, 225, 68, 307, 56, 30, 10, 202, 251, 97, 240, 73, 233, 202, 251, 73, 81, 73, 102, 149, 97, 246, 163, 202, 307, 97, 308, 149, 149, 30, 157, 30, 3, 30]}, {"code": "def chunk_gsa_fwd_k_kernel_intra(\n    v,\n    g,\n    o,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BV: tl.constexpr,\n    NC: tl.constexpr,\n    NG: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // NG\n    i_t, i_i = i_c // NC, i_c % NC\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    o_v = i_v * BV + tl.arange(0, BV)\n    m_v = o_v < V\n\n    if i_t * BT + i_i * BC > T:\n        return\n\n    p_g = tl.make_block_ptr(\n        g + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_t * BT + i_i * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n    p_gn = g + (bos + min(i_t * BT + i_i * BC, T)) * H * V + i_h * V + o_v\n\n    b_gn = tl.load(p_gn, mask=m_v, other=0)\n\n    b_o = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_A = tl.make_block_ptr(\n            A + (bos * HQ + i_hq) * BT,\n            (T, BT),\n            (HQ * BT, 1),\n            (i_t * BT + i_i * BC, i_j * BC),\n            (BC, BC),\n            (1, 0),\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT + i_j * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n        p_gv = tl.make_block_ptr(\n            g + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT + i_j * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_vg = (b_v * exp(b_gn[None, :] - b_gv)).to(b_v.dtype)\n\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_o += tl.dot(b_A, b_vg)\n\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_o *= exp(b_g - b_gn[None, :])\n\n    o_i = tl.arange(0, BC)\n    o_A = (\n        (bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * HQ * BT + i_hq * BT + i_i * BC\n    )\n    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        p_v = v + (bos + i_t * BT + i_i * BC + j) * H * V + i_h * V + o_v\n        p_gv = g + (bos + i_t * BT + i_i * BC + j) * H * V + i_h * V + o_v\n\n        b_A = tl.load(A + o_A + j, mask=m_A, other=0)\n\n        b_v = tl.load(p_v, mask=m_v, other=0).to(tl.float32)\n        b_gv = tl.load(p_gv, mask=m_v, other=0).to(tl.float32)\n\n        b_vg = b_v[None, :] * exp(b_g - b_gv[None, :])\n\n        b_o += tl.where(o_i[:, None] >= j, b_A[:, None] * b_vg, 0.0)\n    p_o = tl.make_block_ptr(\n        o + (bos * HQ + i_hq) * V,\n        (T, V),\n        (HQ * V, 1),\n        (i_t * BT + i_i * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n    b_o += tl.load(p_o, boundary_check=(0, 1))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 57, 6, 98, 214, 57, 6, 98, 215, 57, 6, 98, 216, 57, 6, 98, 217, 57, 6, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 149, 57, 30, -1, 222, 98, 223, 98, 224, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 225, 98, 226, 163, 202, 224, 45, 213, 98, 224, 188, 213, 149, 30, 227, 163, 226, 45, 220, 30, 228, 98, 229, 163, 202, 223, 45, 219, 98, 223, 188, 219, 149, 30, 155, 221, 57, 30, 230, 98, 228, 163, 202, 52, 202, 211, 67, 228, 203, 309, 149, 74, 231, 202, 56, 149, 98, 52, 202, 211, 67, 228, 203, 309, 67, 308, 149, 74, 231, 202, 56, 149, 149, 30, 232, 98, 233, 163, 202, 52, 202, 210, 67, 230, 149, 74, 231, 202, 56, 149, 98, 52, 202, 210, 67, 230, 67, 308, 149, 74, 231, 202, 56, 149, 149, 30, 212, 163, 233, 4, 232, 30, 157, 30, 29, 57, 30, 232, 98, 233, 163, 202, 225, 203, 212, 98, 225, 203, 212, 67, 212, 149, 30, 51, 30, 234, 163, 222, 203, 218, 67, 68, 202, 307, 98, 218, 149, 30, 235, 163, 234, 1, 215, 30, 155, 228, 203, 216, 67, 229, 203, 217, 111, 212, 57, 30, 192, 30, 157, 30, 236, 163, 183, 202, 207, 67, 202, 232, 203, 214, 67, 227, 149, 203, 215, 98, 202, 212, 98, 215, 149, 98, 202, 214, 203, 215, 98, 308, 149, 98, 202, 228, 203, 216, 67, 229, 203, 217, 98, 222, 203, 218, 149, 98, 202, 217, 98, 218, 149, 98, 202, 308, 98, 307, 149, 149, 30, 237, 163, 207, 67, 202, 232, 67, 37, 202, 228, 203, 216, 67, 229, 203, 217, 98, 212, 149, 149, 203, 214, 203, 215, 67, 227, 203, 215, 67, 234, 30, 238, 163, 52, 202, 237, 98, 239, 163, 235, 98, 240, 163, 307, 149, 30, 241, 163, 150, 202, 191, 217, 98, 218, 26, 98, 82, 163, 129, 149, 30, 120, 242, 136, 5, 202, 307, 98, 229, 149, 57, 30, 243, 163, 183, 202, 209, 67, 202, 232, 203, 213, 67, 226, 149, 203, 216, 98, 202, 212, 98, 216, 149, 98, 202, 213, 203, 216, 98, 308, 149, 98, 202, 228, 203, 216, 67, 229, 203, 217, 98, 242, 203, 217, 149, 98, 202, 217, 98, 217, 149, 98, 202, 308, 98, 307, 149, 149, 30, 244, 163, 183, 202, 206, 67, 202, 232, 203, 214, 67, 227, 149, 203, 215, 98, 202, 212, 98, 215, 149, 98, 202, 214, 203, 215, 98, 308, 149, 98, 202, 228, 203, 216, 67, 242, 203, 217, 98, 222, 203, 218, 149, 98, 202, 217, 98, 218, 149, 98, 202, 308, 98, 307, 149, 149, 30, 245, 163, 183, 202, 207, 67, 202, 232, 203, 214, 67, 227, 149, 203, 215, 98, 202, 212, 98, 215, 149, 98, 202, 214, 203, 215, 98, 308, 149, 98, 202, 228, 203, 216, 67, 242, 203, 217, 98, 222, 203, 218, 149, 98, 202, 217, 98, 218, 149, 98, 202, 308, 98, 307, 149, 149, 30, 246, 163, 52, 202, 244, 98, 247, 163, 202, 307, 98, 308, 149, 149, 30, 248, 163, 52, 202, 245, 98, 247, 163, 202, 307, 98, 308, 149, 149, 30, 249, 163, 202, 246, 203, 173, 202, 238, 191, 168, 98, 57, 26, 4, 248, 149, 149, 74, 231, 202, 246, 74, 82, 149, 30, 250, 163, 52, 202, 243, 98, 247, 163, 202, 307, 98, 308, 149, 149, 30, 241, 146, 15, 202, 250, 98, 249, 149, 30, 70, 30, 251, 163, 52, 202, 236, 98, 247, 163, 202, 307, 98, 308, 149, 149, 30, 241, 23, 173, 202, 251, 4, 238, 191, 168, 98, 57, 26, 149, 30, 252, 163, 68, 202, 307, 98, 217, 149, 30, 253, 163, 202, 232, 67, 228, 203, 216, 67, 229, 203, 217, 67, 68, 202, 307, 98, 217, 149, 149, 203, 213, 203, 216, 67, 226, 203, 216, 67, 229, 203, 217, 30, 254, 163, 228, 203, 216, 67, 229, 203, 217, 67, 68, 202, 307, 98, 217, 149, 1, 212, 30, 120, 255, 136, 5, 202, 307, 98, 37, 202, 217, 98, 212, 4, 228, 203, 216, 4, 229, 203, 217, 149, 149, 57, 30, 244, 163, 206, 67, 202, 232, 67, 228, 203, 216, 67, 229, 203, 217, 67, 255, 149, 203, 214, 203, 215, 67, 227, 203, 215, 67, 234, 30, 245, 163, 207, 67, 202, 232, 67, 228, 203, 216, 67, 229, 203, 217, 67, 255, 149, 203, 214, 203, 215, 67, 227, 203, 215, 67, 234, 30, 250, 163, 52, 202, 209, 67, 253, 67, 255, 98, 239, 163, 254, 98, 240, 163, 307, 149, 30, 246, 163, 52, 202, 244, 98, 239, 163, 235, 98, 240, 163, 307, 149, 74, 231, 202, 129, 149, 30, 248, 163, 52, 202, 245, 98, 239, 163, 235, 98, 240, 163, 307, 149, 74, 231, 202, 129, 149, 30, 249, 163, 246, 191, 168, 98, 57, 26, 203, 173, 202, 251, 4, 248, 191, 168, 98, 57, 26, 149, 30, 241, 146, 171, 202, 252, 191, 57, 98, 168, 26, 128, 255, 98, 250, 191, 57, 98, 168, 26, 203, 249, 98, 307, 149, 30, 70, 30, 256, 163, 183, 202, 208, 67, 202, 232, 203, 213, 67, 226, 149, 203, 215, 98, 202, 212, 98, 215, 149, 98, 202, 213, 203, 215, 98, 308, 149, 98, 202, 228, 203, 216, 67, 229, 203, 217, 98, 222, 203, 218, 149, 98, 202, 217, 98, 218, 149, 98, 202, 308, 98, 307, 149, 149, 30, 241, 146, 52, 202, 256, 98, 247, 163, 202, 307, 98, 308, 149, 149, 30, 10, 202, 256, 98, 241, 74, 231, 202, 256, 74, 82, 74, 103, 149, 98, 247, 163, 202, 307, 98, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_gsa_bwd_k_kernel_dA(\n    v,\n    g,\n    do,\n    dA,\n    chunk_indices,\n    cu_seqlens,\n    scale,\n    T,\n    B: tl.constexpr,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BV: tl.constexpr,\n    NC: tl.constexpr,\n    NG: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // NG\n    i_t, i_i, i_j = i_c // (NC * NC), (i_c % (NC * NC)) // NC, (i_c % (NC * NC)) % NC\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        all = T\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        all = B * T\n\n    o_v = i_v * BV + tl.arange(0, BV)\n    m_v = o_v < V\n\n    if i_t * BT + i_i * BC > T:\n        return\n\n    p_dA = tl.make_block_ptr(\n        dA + ((i_v * all + bos) * HQ + i_hq) * BT,\n        (T, BT),\n        (HQ * BT, 1),\n        (i_t * BT + i_i * BC, i_j * BC),\n        (BC, BC),\n        (1, 0),\n    )\n\n    b_dA = tl.zeros([BC, BC], dtype=tl.float32)\n    if i_i > i_j:\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (V, T),\n            (1, H * V),\n            (i_v * BV, i_t * BT + i_j * BC),\n            (BV, BC),\n            (0, 1),\n        )\n        p_gv = tl.make_block_ptr(\n            g + (bos * H + i_h) * V,\n            (V, T),\n            (1, H * V),\n            (i_v * BV, i_t * BT + i_j * BC),\n            (BV, BC),\n            (0, 1),\n        )\n        p_gn = g + (bos + i_t * BT + i_i * BC) * H * V + i_h * V + o_v\n        p_g = tl.make_block_ptr(\n            g + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT + i_i * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n        p_do = tl.make_block_ptr(\n            do + (bos * HQ + i_hq) * V,\n            (T, V),\n            (HQ * V, 1),\n            (i_t * BT + i_i * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n\n        b_gn = tl.load(p_gn, mask=m_v, other=0.0)\n\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = (b_do * exp(b_g - b_gn[None, :]) * scale).to(b_do.dtype)\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_vg = (b_v * exp(b_gn[:, None] - b_gv)).to(b_v.dtype)\n\n        b_dA = tl.dot(b_do, b_vg)\n    elif i_i == i_j:\n        p_g = tl.make_block_ptr(\n            g + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT + i_i * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n        p_do = tl.make_block_ptr(\n            do + (bos * HQ + i_hq) * V,\n            (T, V),\n            (HQ * V, 1),\n            (i_t * BT + i_i * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n        p_v = v + (bos + i_t * BT + i_j * BC) * H * V + i_h * V + o_v\n        p_gv = g + (bos + i_t * BT + i_j * BC) * H * V + i_h * V + o_v\n\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)) * scale\n        m_v = o_v < V\n\n        o_i = tl.arange(0, BC)\n\n        m_dA = o_i[:, None] >= o_i[None, :]\n        for j in range(0, min(BC, T - i_t * BT - i_j * BC)):\n\n            b_v = tl.load(p_v, mask=m_v, other=0).to(tl.float32)\n            b_gv = tl.load(p_gv, mask=m_v, other=0).to(tl.float32)\n\n            b_dAj = tl.sum(b_do * b_v[None, :] * exp(b_g - b_gv[None, :]), 1)\n            b_dA = tl.where((o_i == j)[None, :], b_dAj[:, None], b_dA)\n\n            p_v += H * V\n            p_gv += H * V\n        b_dA = tl.where(m_dA, b_dA, 0.0)\n    tl.store(p_dA, b_dA.to(dA.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 56, 6, 97, 215, 56, 6, 97, 216, 56, 6, 97, 217, 56, 6, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 149, 56, 30, -1, 224, 97, 225, 97, 226, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 227, 97, 228, 163, 202, 226, 45, 215, 97, 226, 188, 215, 149, 30, 229, 163, 228, 45, 222, 30, 230, 97, 231, 97, 232, 163, 202, 225, 45, 202, 221, 203, 221, 149, 97, 225, 188, 202, 221, 203, 221, 149, 45, 221, 97, 225, 188, 202, 221, 203, 221, 149, 188, 221, 149, 30, 155, 223, 56, 30, 233, 97, 230, 163, 202, 50, 202, 210, 66, 230, 203, 309, 149, 73, 234, 202, 205, 149, 97, 50, 202, 210, 66, 230, 203, 309, 66, 308, 149, 73, 234, 202, 205, 149, 149, 30, 235, 97, 236, 163, 202, 50, 202, 211, 66, 233, 149, 73, 234, 202, 205, 149, 97, 50, 202, 211, 66, 233, 66, 308, 149, 73, 234, 202, 205, 149, 149, 30, 141, 163, 213, 30, 213, 163, 236, 4, 235, 30, 157, 30, 29, 56, 30, 235, 97, 236, 163, 202, 227, 203, 213, 97, 227, 203, 213, 66, 213, 149, 30, 141, 163, 214, 203, 213, 30, 51, 30, 237, 163, 224, 203, 220, 66, 67, 202, 307, 97, 220, 149, 30, 238, 163, 237, 1, 217, 30, 155, 230, 203, 218, 66, 231, 203, 219, 110, 213, 56, 30, 192, 30, 157, 30, 239, 163, 183, 202, 209, 66, 202, 202, 224, 203, 141, 66, 235, 149, 203, 215, 66, 228, 149, 203, 218, 97, 202, 213, 97, 218, 149, 97, 202, 215, 203, 218, 97, 308, 149, 97, 202, 230, 203, 218, 66, 231, 203, 219, 97, 232, 203, 219, 149, 97, 202, 219, 97, 219, 149, 97, 202, 308, 97, 307, 149, 149, 30, 240, 163, 150, 202, 191, 219, 97, 219, 26, 97, 81, 163, 128, 149, 30, 155, 231, 110, 232, 56, 30, 241, 163, 183, 202, 206, 66, 202, 235, 203, 216, 66, 229, 149, 203, 217, 97, 202, 217, 97, 213, 149, 97, 202, 308, 97, 216, 203, 217, 149, 97, 202, 224, 203, 220, 97, 230, 203, 218, 66, 232, 203, 219, 149, 97, 202, 220, 97, 219, 149, 97, 202, 307, 97, 308, 149, 149, 30, 242, 163, 183, 202, 207, 66, 202, 235, 203, 216, 66, 229, 149, 203, 217, 97, 202, 217, 97, 213, 149, 97, 202, 308, 97, 216, 203, 217, 149, 97, 202, 224, 203, 220, 97, 230, 203, 218, 66, 232, 203, 219, 149, 97, 202, 220, 97, 219, 149, 97, 202, 307, 97, 308, 149, 149, 30, 243, 163, 207, 66, 202, 235, 66, 230, 203, 218, 66, 231, 203, 219, 149, 203, 216, 203, 217, 66, 229, 203, 217, 66, 237, 30, 244, 163, 183, 202, 207, 66, 202, 235, 203, 216, 66, 229, 149, 203, 217, 97, 202, 213, 97, 217, 149, 97, 202, 216, 203, 217, 97, 308, 149, 97, 202, 230, 203, 218, 66, 231, 203, 219, 97, 224, 203, 220, 149, 97, 202, 219, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 245, 163, 183, 202, 208, 66, 202, 235, 203, 215, 66, 228, 149, 203, 217, 97, 202, 213, 97, 217, 149, 97, 202, 215, 203, 217, 97, 308, 149, 97, 202, 230, 203, 218, 66, 231, 203, 219, 97, 224, 203, 220, 149, 97, 202, 219, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 246, 163, 50, 202, 243, 97, 247, 163, 238, 97, 248, 163, 307, 149, 30, 249, 163, 50, 202, 244, 97, 250, 163, 202, 307, 97, 308, 149, 149, 30, 251, 163, 50, 202, 245, 97, 250, 163, 202, 307, 97, 308, 149, 149, 30, 251, 163, 202, 251, 203, 173, 202, 249, 4, 246, 191, 168, 97, 56, 26, 149, 203, 212, 149, 73, 234, 202, 251, 73, 81, 149, 30, 252, 163, 50, 202, 241, 97, 250, 163, 202, 307, 97, 308, 149, 149, 30, 253, 163, 50, 202, 242, 97, 250, 163, 202, 307, 97, 308, 149, 149, 30, 254, 163, 202, 252, 203, 173, 202, 246, 191, 56, 97, 168, 26, 4, 253, 149, 149, 73, 234, 202, 252, 73, 81, 149, 30, 240, 163, 15, 202, 251, 97, 254, 149, 30, 59, 30, 35, 231, 68, 232, 56, 30, 244, 163, 183, 202, 207, 66, 202, 235, 203, 216, 66, 229, 149, 203, 217, 97, 202, 213, 97, 217, 149, 97, 202, 216, 203, 217, 97, 308, 149, 97, 202, 230, 203, 218, 66, 231, 203, 219, 97, 224, 203, 220, 149, 97, 202, 219, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 245, 163, 183, 202, 208, 66, 202, 235, 203, 215, 66, 228, 149, 203, 217, 97, 202, 213, 97, 217, 149, 97, 202, 215, 203, 217, 97, 308, 149, 97, 202, 230, 203, 218, 66, 231, 203, 219, 97, 224, 203, 220, 149, 97, 202, 219, 97, 220, 149, 97, 202, 308, 97, 307, 149, 149, 30, 241, 163, 206, 66, 202, 235, 66, 230, 203, 218, 66, 232, 203, 219, 149, 203, 216, 203, 217, 66, 229, 203, 217, 66, 237, 30, 242, 163, 207, 66, 202, 235, 66, 230, 203, 218, 66, 232, 203, 219, 149, 203, 216, 203, 217, 66, 229, 203, 217, 66, 237, 30, 249, 163, 50, 202, 244, 97, 250, 163, 202, 307, 97, 308, 149, 149, 30, 251, 163, 50, 202, 245, 97, 250, 163, 202, 307, 97, 308, 149, 149, 203, 212, 30, 238, 163, 237, 1, 217, 30, 255, 163, 67, 202, 307, 97, 219, 149, 30, 256, 163, 255, 191, 56, 97, 168, 26, 127, 255, 191, 168, 97, 56, 26, 30, 119, 257, 136, 5, 202, 307, 97, 37, 202, 219, 97, 213, 4, 230, 203, 218, 4, 232, 203, 219, 149, 149, 56, 30, 252, 163, 50, 202, 241, 97, 247, 163, 238, 97, 248, 163, 307, 149, 73, 234, 202, 128, 149, 30, 253, 163, 50, 202, 242, 97, 247, 163, 238, 97, 248, 163, 307, 149, 73, 234, 202, 128, 149, 30, 258, 163, 185, 202, 251, 203, 252, 191, 168, 97, 56, 26, 203, 173, 202, 249, 4, 253, 191, 168, 97, 56, 26, 149, 97, 308, 149, 30, 240, 163, 171, 202, 202, 255, 68, 257, 149, 191, 168, 97, 56, 26, 97, 258, 191, 56, 97, 168, 26, 97, 240, 149, 30, 241, 146, 216, 203, 217, 30, 242, 146, 216, 203, 217, 30, 69, 30, 240, 163, 171, 202, 256, 97, 240, 97, 307, 149, 30, 157, 30, 10, 202, 239, 97, 240, 73, 234, 202, 209, 73, 81, 73, 102, 149, 97, 250, 163, 202, 307, 97, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_gsa_bwd_k_kernel_dqkvg(\n    q,\n    k,\n    v,\n    h,\n    g,\n    A,\n    do,\n    dh,\n    dq,\n    dk,\n    dv,\n    dg,\n    dgv,\n    dA,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    B: tl.constexpr,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NG: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // NG\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        all = T\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n        all = B * T\n\n    o_i = tl.arange(0, BT)\n    o_t = min(i_t * BT + BT, T)\n    m_s = o_i[:, None] >= o_i[None, :]\n\n    p_q = tl.make_block_ptr(\n        q + (bos * HQ + i_hq) * K,\n        (T, K),\n        (HQ * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_k = tl.make_block_ptr(\n        k + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_A = tl.make_block_ptr(\n        A + ((i_k * all + bos) * HQ + i_hq) * BT,\n        (T, BT),\n        (HQ * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n\n    b_A = tl.dot((b_q * scale).to(b_q.dtype), tl.trans(b_k))\n    b_A = tl.where(m_s, b_A, 0.0)\n    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        o_v = i_v * BV + tl.arange(0, BV)\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_g = tl.make_block_ptr(\n            g + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_gn = g + (bos + o_t - 1) * H * V + i_h * V + o_v\n        p_do = tl.make_block_ptr(\n            do + (bos * HQ + i_hq) * V,\n            (T, V),\n            (HQ * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dv = tl.make_block_ptr(\n            dv + ((i_k * all + bos) * HQ + i_hq) * V,\n            (T, V),\n            (HQ * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dg = tl.make_block_ptr(\n            dg + (bos * HQ + i_hq) * V,\n            (T, V),\n            (HQ * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dgv = tl.make_block_ptr(\n            dgv + ((i_k * all + bos) * HQ + i_hq) * V,\n            (T, V),\n            (HQ * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_h = tl.make_block_ptr(\n            h + (i_tg * H + i_h) * K * V,\n            (V, K),\n            (1, V),\n            (i_v * BV, i_k * BK),\n            (BV, BK),\n            (0, 1),\n        )\n        p_dh = tl.make_block_ptr(\n            dh + (i_tg * HQ + i_hq) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        m_v = o_v < V\n\n        b_gn = tl.load(p_gn, mask=m_v, other=0)\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_gv = exp(b_gn[None, :] - b_g)\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = (b_do * exp(b_g) * scale).to(b_do.dtype)\n\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        b_dg = tl.sum(tl.trans(b_h) * b_dh, 0) * exp(b_gn)\n\n        b_dh = b_dh.to(b_k.dtype)\n\n        b_dq += tl.dot(b_do, b_h.to(b_k.dtype))\n        b_dk += tl.dot((b_v * b_gv).to(b_v.dtype), tl.trans(b_dh))\n\n        b_dv = tl.dot(b_k, b_dh) * b_gv\n\n        b_dg += tl.sum(b_dv * b_v, 0)\n\n        if i_k == 0:\n            b_dgv = tl.load(p_dg, boundary_check=(0, 1)) + b_dg[None, :]\n        else:\n            b_dgv = tl.zeros([BT, BV], dtype=tl.float32) + b_dg[None, :]\n\n        tl.store(p_dgv, b_dgv.to(p_dgv.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    p_dA = tl.make_block_ptr(\n        dA + (bos * HQ + i_hq) * BT,\n        (T, BT),\n        (HQ * BT, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n    p_dq = tl.make_block_ptr(\n        dq + (bos * HQ + i_hq) * K,\n        (T, K),\n        (HQ * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_dk = tl.make_block_ptr(\n        dk + (bos * HQ + i_hq) * K,\n        (T, K),\n        (HQ * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n\n    b_dA = tl.load(p_dA, boundary_check=(0, 1))\n\n    b_dq += tl.dot(b_dA, b_k)\n    b_dk += tl.dot(tl.trans(b_dA).to(b_k.dtype), b_q)\n\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 98, 229, 57, 6, 98, 230, 57, 6, 98, 231, 57, 6, 98, 232, 57, 6, 98, 233, 57, 6, 149, 57, 30, -1, 234, 98, 235, 98, 236, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 237, 98, 238, 163, 202, 236, 45, 225, 98, 236, 188, 225, 149, 30, 239, 163, 238, 45, 232, 30, 155, 233, 57, 30, 240, 163, 235, 30, 241, 98, 235, 163, 202, 52, 202, 221, 67, 235, 203, 309, 149, 74, 242, 202, 56, 149, 98, 52, 202, 221, 67, 235, 203, 309, 67, 308, 149, 74, 242, 202, 56, 149, 149, 30, 243, 98, 244, 163, 202, 52, 202, 220, 67, 241, 149, 74, 242, 202, 56, 149, 98, 52, 202, 220, 67, 241, 67, 308, 149, 74, 242, 202, 56, 149, 149, 30, 141, 163, 223, 30, 223, 163, 244, 4, 243, 30, 245, 163, 59, 202, 223, 98, 229, 149, 30, 157, 30, 29, 57, 30, 245, 163, 59, 202, 223, 98, 229, 149, 30, 240, 163, 237, 203, 245, 67, 235, 30, 243, 98, 244, 163, 202, 237, 203, 223, 98, 237, 203, 223, 67, 223, 149, 30, 141, 163, 224, 203, 223, 30, 51, 30, 246, 163, 68, 202, 307, 98, 229, 149, 30, 247, 163, 37, 202, 235, 203, 229, 67, 229, 98, 223, 149, 30, 248, 163, 246, 191, 57, 98, 168, 26, 128, 246, 191, 168, 98, 57, 26, 30, 249, 163, 183, 202, 206, 67, 202, 243, 203, 225, 67, 238, 149, 203, 227, 98, 202, 223, 98, 227, 149, 98, 202, 225, 203, 227, 98, 308, 149, 98, 202, 235, 203, 229, 98, 234, 203, 230, 149, 98, 202, 229, 98, 230, 149, 98, 202, 308, 98, 307, 149, 149, 30, 250, 163, 183, 202, 207, 67, 202, 243, 203, 226, 67, 239, 149, 203, 227, 98, 202, 223, 98, 227, 149, 98, 202, 226, 203, 227, 98, 308, 149, 98, 202, 235, 203, 229, 98, 234, 203, 230, 149, 98, 202, 229, 98, 230, 149, 98, 202, 308, 98, 307, 149, 149, 30, 251, 163, 183, 202, 211, 67, 202, 202, 234, 203, 141, 67, 243, 149, 203, 225, 67, 238, 149, 203, 229, 98, 202, 223, 98, 229, 149, 98, 202, 225, 203, 229, 98, 308, 149, 98, 202, 235, 203, 229, 98, 307, 149, 98, 202, 229, 98, 229, 149, 98, 202, 308, 98, 307, 149, 149, 30, 252, 163, 52, 202, 249, 98, 253, 163, 202, 307, 98, 308, 149, 149, 30, 254, 163, 52, 202, 250, 98, 253, 163, 202, 307, 98, 308, 149, 149, 30, 255, 163, 15, 202, 202, 252, 203, 222, 149, 74, 242, 202, 252, 74, 82, 149, 98, 65, 202, 254, 149, 149, 30, 255, 163, 171, 202, 248, 98, 255, 98, 307, 149, 30, 10, 202, 251, 98, 255, 74, 242, 202, 251, 74, 82, 74, 103, 149, 98, 253, 163, 202, 307, 98, 308, 149, 149, 30, 256, 163, 150, 202, 191, 229, 98, 230, 26, 98, 82, 163, 129, 149, 30, 257, 163, 150, 202, 191, 229, 98, 230, 26, 98, 82, 163, 129, 149, 30, 120, 258, 136, 5, 202, 59, 202, 228, 98, 231, 149, 149, 57, 30, 259, 163, 258, 203, 231, 67, 68, 202, 307, 98, 231, 149, 30, 260, 163, 183, 202, 208, 67, 202, 243, 203, 226, 67, 239, 149, 203, 228, 98, 202, 223, 98, 228, 149, 98, 202, 226, 203, 228, 98, 308, 149, 98, 202, 235, 203, 229, 98, 258, 203, 231, 149, 98, 202, 229, 98, 231, 149, 98, 202, 308, 98, 307, 149, 149, 30, 261, 163, 183, 202, 210, 67, 202, 243, 203, 226, 67, 239, 149, 203, 228, 98, 202, 223, 98, 228, 149, 98, 202, 226, 203, 228, 98, 308, 149, 98, 202, 235, 203, 229, 98, 258, 203, 231, 149, 98, 202, 229, 98, 231, 149, 98, 202, 308, 98, 307, 149, 149, 30, 262, 163, 210, 67, 202, 243, 67, 247, 4, 308, 149, 203, 226, 203, 228, 67, 239, 203, 228, 67, 259, 30, 263, 163, 183, 202, 212, 67, 202, 243, 203, 225, 67, 238, 149, 203, 228, 98, 202, 223, 98, 228, 149, 98, 202, 225, 203, 228, 98, 308, 149, 98, 202, 235, 203, 229, 98, 258, 203, 231, 149, 98, 202, 229, 98, 231, 149, 98, 202, 308, 98, 307, 149, 149, 30, 264, 163, 183, 202, 216, 67, 202, 202, 234, 203, 141, 67, 243, 149, 203, 225, 67, 238, 149, 203, 228, 98, 202, 223, 98, 228, 149, 98, 202, 225, 203, 228, 98, 308, 149, 98, 202, 235, 203, 229, 98, 258, 203, 231, 149, 98, 202, 229, 98, 231, 149, 98, 202, 308, 98, 307, 149, 149, 30, 265, 163, 183, 202, 217, 67, 202, 243, 203, 225, 67, 238, 149, 203, 228, 98, 202, 223, 98, 228, 149, 98, 202, 225, 203, 228, 98, 308, 149, 98, 202, 235, 203, 229, 98, 258, 203, 231, 149, 98, 202, 229, 98, 231, 149, 98, 202, 308, 98, 307, 149, 149, 30, 266, 163, 183, 202, 218, 67, 202, 202, 234, 203, 141, 67, 243, 149, 203, 225, 67, 238, 149, 203, 228, 98, 202, 223, 98, 228, 149, 98, 202, 225, 203, 228, 98, 308, 149, 98, 202, 235, 203, 229, 98, 258, 203, 231, 149, 98, 202, 229, 98, 231, 149, 98, 202, 308, 98, 307, 149, 149, 30, 267, 163, 183, 202, 209, 67, 202, 240, 203, 226, 67, 239, 149, 203, 227, 203, 228, 98, 202, 228, 98, 227, 149, 98, 202, 308, 98, 228, 149, 98, 202, 258, 203, 231, 98, 234, 203, 230, 149, 98, 202, 231, 98, 230, 149, 98, 202, 307, 98, 308, 149, 149, 30, 268, 163, 183, 202, 213, 67, 202, 240, 203, 225, 67, 238, 149, 203, 227, 203, 228, 98, 202, 227, 98, 228, 149, 98, 202, 228, 98, 308, 149, 98, 202, 234, 203, 230, 98, 258, 203, 231, 149, 98, 202, 230, 98, 231, 149, 98, 202, 308, 98, 307, 149, 149, 30, 269, 163, 259, 1, 228, 30, 270, 163, 52, 202, 262, 98, 271, 163, 269, 98, 272, 163, 307, 149, 30, 273, 163, 52, 202, 260, 98, 253, 163, 202, 307, 98, 308, 149, 149, 30, 274, 163, 52, 202, 261, 98, 253, 163, 202, 307, 98, 308, 149, 149, 30, 275, 163, 173, 202, 270, 191, 168, 98, 57, 26, 4, 274, 149, 30, 276, 163, 52, 202, 267, 98, 253, 163, 202, 307, 98, 308, 149, 149, 30, 277, 163, 52, 202, 263, 98, 253, 163, 202, 307, 98, 308, 149, 149, 30, 277, 163, 202, 277, 203, 173, 202, 274, 149, 203, 222, 149, 74, 242, 202, 277, 74, 82, 149, 30, 278, 163, 52, 202, 268, 98, 253, 163, 202, 307, 98, 308, 149, 149, 30, 279, 163, 185, 202, 65, 202, 276, 149, 203, 278, 98, 307, 149, 203, 173, 202, 270, 149, 30, 278, 163, 278, 74, 242, 202, 254, 74, 82, 149, 30, 256, 146, 15, 202, 277, 98, 276, 74, 242, 202, 254, 74, 82, 149, 149, 30, 257, 146, 15, 202, 202, 273, 203, 275, 149, 74, 242, 202, 273, 74, 82, 149, 98, 65, 202, 278, 149, 149, 30, 280, 163, 15, 202, 254, 98, 278, 149, 203, 275, 30, 279, 146, 185, 202, 280, 203, 273, 98, 307, 149, 30, 155, 234, 69, 307, 57, 30, 281, 163, 52, 202, 265, 98, 253, 163, 202, 307, 98, 308, 149, 149, 67, 279, 191, 168, 98, 57, 26, 30, 157, 30, 29, 57, 30, 281, 163, 150, 202, 191, 229, 98, 231, 26, 98, 82, 163, 129, 149, 67, 279, 191, 168, 98, 57, 26, 30, 51, 30, 10, 202, 266, 98, 281, 74, 242, 202, 266, 74, 82, 74, 103, 149, 98, 253, 163, 202, 307, 98, 308, 149, 149, 30, 10, 202, 264, 98, 280, 74, 242, 202, 264, 74, 82, 74, 103, 149, 98, 253, 163, 202, 307, 98, 308, 149, 149, 30, 70, 30, 282, 163, 183, 202, 219, 67, 202, 243, 203, 225, 67, 238, 149, 203, 229, 98, 202, 223, 98, 229, 149, 98, 202, 225, 203, 229, 98, 308, 149, 98, 202, 235, 203, 229, 98, 307, 149, 98, 202, 229, 98, 229, 149, 98, 202, 308, 98, 307, 149, 149, 30, 283, 163, 183, 202, 214, 67, 202, 243, 203, 225, 67, 238, 149, 203, 227, 98, 202, 223, 98, 227, 149, 98, 202, 225, 203, 227, 98, 308, 149, 98, 202, 235, 203, 229, 98, 234, 203, 230, 149, 98, 202, 229, 98, 230, 149, 98, 202, 308, 98, 307, 149, 149, 30, 284, 163, 183, 202, 215, 67, 202, 243, 203, 225, 67, 238, 149, 203, 227, 98, 202, 223, 98, 227, 149, 98, 202, 225, 203, 227, 98, 308, 149, 98, 202, 235, 203, 229, 98, 234, 203, 230, 149, 98, 202, 229, 98, 230, 149, 98, 202, 308, 98, 307, 149, 149, 30, 285, 163, 52, 202, 282, 98, 253, 163, 202, 307, 98, 308, 149, 149, 30, 256, 146, 15, 202, 285, 98, 254, 149, 30, 257, 146, 15, 202, 65, 202, 285, 149, 74, 242, 202, 254, 74, 82, 149, 98, 252, 149, 30, 10, 202, 283, 98, 256, 74, 242, 202, 283, 74, 82, 74, 103, 149, 98, 253, 163, 202, 307, 98, 308, 149, 149, 30, 10, 202, 284, 98, 257, 74, 242, 202, 284, 74, 82, 74, 103, 149, 98, 253, 163, 202, 307, 98, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_gsa_bwd_k_kernel_intra_dvg(\n    v,\n    g,\n    o,\n    A,\n    do,\n    dv,\n    dg,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BV: tl.constexpr,\n    NC: tl.constexpr,\n    NG: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // NG\n    i_t, i_i = i_c // NC, i_c % NC\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    o_v = i_v * BV + tl.arange(0, BV)\n    m_v = o_v < V\n\n    if i_t * BT + i_i * BC > T:\n        return\n\n    p_gv = tl.make_block_ptr(\n        g + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_t * BT + i_i * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n    p_gn = g + (bos + min(i_t * BT + i_i * BC + BC, T) - 1) * H * V + i_h * V + o_v\n\n    b_gn = tl.load(p_gn, mask=m_v, other=0)\n\n    b_gv = tl.load(p_gv, boundary_check=(0, 1))\n    b_dv = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_g = tl.make_block_ptr(\n            g + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT + i_j * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n        p_A = tl.make_block_ptr(\n            A + (bos * HQ + i_hq) * BT,\n            (BT, T),\n            (1, HQ * BT),\n            (i_i * BC, i_t * BT + i_j * BC),\n            (BC, BC),\n            (0, 1),\n        )\n        p_do = tl.make_block_ptr(\n            do + (bos * HQ + i_hq) * V,\n            (T, V),\n            (HQ * V, 1),\n            (i_t * BT + i_j * BC, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)) * safe_exp(b_g - b_gn[None, :])\n\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n\n        b_dv += tl.dot(b_A, b_do.to(b_A.dtype))\n    b_dv *= exp(b_gn[None, :] - b_gv)\n\n    o_i = tl.arange(0, BC)\n    o_c = i_i * BC + tl.arange(0, BC)\n\n    p_g = g + (bos + i_t * BT + i_i * BC) * H * V + i_h * V + o_v\n    p_A = A + (bos + i_t * BT + i_i * BC) * HQ * BT + i_hq * BT + o_c\n    p_do = do + (bos + i_t * BT + i_i * BC) * HQ * V + i_hq * V + o_v\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n\n        b_A = tl.load(p_A)\n\n        b_g = tl.load(p_g, mask=m_v, other=0)\n        b_do = tl.load(p_do, mask=m_v, other=0)\n\n        m_i = o_i[:, None] <= j\n        b_dv += tl.where(\n            m_i, exp(b_g[None, :] - b_gv) * b_A[:, None] * b_do[None, :], 0.0\n        )\n\n        p_g += H * V\n        p_A += HQ * BT\n        p_do += HQ * V\n    p_o = tl.make_block_ptr(\n        o + (bos * HQ + i_hq) * V,\n        (T, V),\n        (HQ * V, 1),\n        (i_t * BT + i_i * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n    p_v = tl.make_block_ptr(\n        v + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_t * BT + i_i * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n    p_do = tl.make_block_ptr(\n        do + (bos * HQ + i_hq) * V,\n        (T, V),\n        (HQ * V, 1),\n        (i_t * BT + i_i * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n    p_dv = tl.make_block_ptr(\n        dv + (bos * HQ + i_hq) * V,\n        (T, V),\n        (HQ * V, 1),\n        (i_t * BT + i_i * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n    p_dg = tl.make_block_ptr(\n        dg + (bos * HQ + i_hq) * V,\n        (T, V),\n        (HQ * V, 1),\n        (i_t * BT + i_i * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n\n    b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)\n    b_v = tl.load(p_v, boundary_check=(0, 1)).to(tl.float32)\n    b_do = tl.load(p_do, boundary_check=(0, 1)).to(tl.float32)\n    b_dv = b_dv + tl.load(p_dv, boundary_check=(0, 1)).to(tl.float32)\n    b_dg = b_o * b_do - b_v * b_dv\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 56, 6, 97, 217, 56, 6, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 97, 224, 56, 6, 149, 56, 30, -1, 225, 97, 226, 97, 227, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 228, 97, 229, 163, 202, 227, 45, 216, 97, 227, 188, 216, 149, 30, 230, 163, 229, 45, 223, 30, 231, 97, 232, 163, 202, 226, 45, 222, 97, 226, 188, 222, 149, 30, 155, 224, 56, 30, 233, 97, 231, 163, 202, 50, 202, 214, 66, 231, 203, 309, 149, 73, 234, 202, 205, 149, 97, 50, 202, 214, 66, 231, 203, 309, 66, 308, 149, 73, 234, 202, 205, 149, 149, 30, 235, 97, 236, 163, 202, 50, 202, 213, 66, 233, 149, 73, 234, 202, 205, 149, 97, 50, 202, 213, 66, 233, 66, 308, 149, 73, 234, 202, 205, 149, 149, 30, 215, 163, 236, 4, 235, 30, 157, 30, 29, 56, 30, 235, 97, 236, 163, 202, 228, 203, 215, 97, 228, 203, 215, 66, 215, 149, 30, 51, 30, 237, 163, 225, 203, 221, 66, 67, 202, 307, 97, 221, 149, 30, 238, 163, 237, 1, 218, 30, 155, 231, 203, 219, 66, 232, 203, 220, 110, 215, 56, 30, 192, 30, 157, 30, 239, 163, 183, 202, 207, 66, 202, 235, 203, 217, 66, 230, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 217, 203, 218, 97, 308, 149, 97, 202, 231, 203, 219, 66, 232, 203, 220, 97, 225, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 240, 163, 207, 66, 202, 235, 66, 37, 202, 231, 203, 219, 66, 232, 203, 220, 66, 220, 97, 215, 149, 4, 308, 149, 203, 217, 203, 218, 66, 230, 203, 218, 66, 237, 30, 241, 163, 50, 202, 240, 97, 242, 163, 238, 97, 243, 163, 307, 149, 30, 244, 163, 50, 202, 239, 97, 245, 163, 202, 307, 97, 308, 149, 149, 30, 246, 163, 150, 202, 191, 220, 97, 221, 26, 97, 81, 163, 128, 149, 30, 119, 247, 136, 5, 202, 232, 66, 308, 97, 222, 149, 56, 30, 248, 163, 183, 202, 207, 66, 202, 235, 203, 217, 66, 230, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 217, 203, 218, 97, 308, 149, 97, 202, 231, 203, 219, 66, 247, 203, 220, 97, 225, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 249, 163, 183, 202, 209, 66, 202, 235, 203, 216, 66, 229, 149, 203, 219, 97, 202, 219, 97, 215, 149, 97, 202, 308, 97, 216, 203, 219, 149, 97, 202, 232, 203, 220, 97, 231, 203, 219, 66, 247, 203, 220, 149, 97, 202, 220, 97, 220, 149, 97, 202, 307, 97, 308, 149, 149, 30, 250, 163, 183, 202, 210, 66, 202, 235, 203, 216, 66, 229, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 216, 203, 218, 97, 308, 149, 97, 202, 231, 203, 219, 66, 247, 203, 220, 97, 225, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 251, 163, 50, 202, 248, 97, 245, 163, 202, 307, 97, 308, 149, 149, 30, 252, 163, 50, 202, 250, 97, 245, 163, 202, 307, 97, 308, 149, 149, 203, 253, 202, 251, 4, 241, 191, 168, 97, 56, 26, 149, 30, 254, 163, 50, 202, 249, 97, 245, 163, 202, 307, 97, 308, 149, 149, 30, 246, 146, 15, 202, 254, 97, 252, 73, 234, 202, 254, 73, 81, 149, 149, 30, 69, 30, 246, 23, 173, 202, 241, 191, 168, 97, 56, 26, 4, 244, 149, 30, 255, 163, 67, 202, 307, 97, 220, 149, 30, 256, 163, 232, 203, 220, 66, 67, 202, 307, 97, 220, 149, 30, 248, 163, 207, 66, 202, 235, 66, 231, 203, 219, 66, 232, 203, 220, 149, 203, 217, 203, 218, 66, 230, 203, 218, 66, 237, 30, 249, 163, 209, 66, 202, 235, 66, 231, 203, 219, 66, 232, 203, 220, 149, 203, 216, 203, 219, 66, 229, 203, 219, 66, 256, 30, 250, 163, 210, 66, 202, 235, 66, 231, 203, 219, 66, 232, 203, 220, 149, 203, 216, 203, 218, 66, 229, 203, 218, 66, 237, 30, 119, 257, 136, 5, 202, 307, 97, 37, 202, 220, 97, 215, 4, 231, 203, 219, 4, 232, 203, 220, 149, 149, 56, 30, 254, 163, 50, 202, 249, 149, 30, 251, 163, 50, 202, 248, 97, 242, 163, 238, 97, 243, 163, 307, 149, 30, 252, 163, 50, 202, 250, 97, 242, 163, 238, 97, 243, 163, 307, 149, 30, 258, 163, 255, 191, 56, 97, 168, 26, 182, 257, 30, 246, 146, 171, 202, 258, 97, 173, 202, 251, 191, 168, 97, 56, 26, 4, 244, 149, 203, 254, 191, 56, 97, 168, 26, 203, 252, 191, 168, 97, 56, 26, 97, 307, 149, 30, 248, 146, 217, 203, 218, 30, 249, 146, 216, 203, 219, 30, 250, 146, 216, 203, 218, 30, 69, 30, 259, 163, 183, 202, 208, 66, 202, 235, 203, 216, 66, 229, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 216, 203, 218, 97, 308, 149, 97, 202, 231, 203, 219, 66, 232, 203, 220, 97, 225, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 260, 163, 183, 202, 206, 66, 202, 235, 203, 217, 66, 230, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 217, 203, 218, 97, 308, 149, 97, 202, 231, 203, 219, 66, 232, 203, 220, 97, 225, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 250, 163, 183, 202, 210, 66, 202, 235, 203, 216, 66, 229, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 216, 203, 218, 97, 308, 149, 97, 202, 231, 203, 219, 66, 232, 203, 220, 97, 225, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 261, 163, 183, 202, 211, 66, 202, 235, 203, 216, 66, 229, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 216, 203, 218, 97, 308, 149, 97, 202, 231, 203, 219, 66, 232, 203, 220, 97, 225, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 262, 163, 183, 202, 212, 66, 202, 235, 203, 216, 66, 229, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 216, 203, 218, 97, 308, 149, 97, 202, 231, 203, 219, 66, 232, 203, 220, 97, 225, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 263, 163, 50, 202, 259, 97, 245, 163, 202, 307, 97, 308, 149, 149, 73, 234, 202, 128, 149, 30, 264, 163, 50, 202, 260, 97, 245, 163, 202, 307, 97, 308, 149, 149, 73, 234, 202, 128, 149, 30, 252, 163, 50, 202, 250, 97, 245, 163, 202, 307, 97, 308, 149, 149, 73, 234, 202, 128, 149, 30, 246, 163, 246, 66, 50, 202, 261, 97, 245, 163, 202, 307, 97, 308, 149, 149, 73, 234, 202, 128, 149, 30, 265, 163, 263, 203, 252, 4, 264, 203, 246, 30, 10, 202, 261, 97, 246, 73, 234, 202, 261, 73, 81, 73, 102, 149, 97, 245, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 262, 97, 265, 73, 234, 202, 262, 73, 81, 73, 102, 149, 97, 245, 163, 202, 307, 97, 308, 149, 149, 30, 3, 30]}, {"code": "def chunk_hgrn_fwd_kernel_h(\n    x,\n    g,\n    gc,\n    o,\n    h0,\n    T,\n    D: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n):\n    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n\n    p_x = x + i_b * T * D + i_t * BT * D + o_d\n    p_g = g + i_b * T * D + i_t * BT * D + o_d\n    p_gc = gc + i_b * T * D + i_t * BT * D + o_d\n    p_o = o + i_b * T * D + i_t * BT * D + o_d\n\n    b_h = tl.zeros([BD], dtype=tl.float32)\n    b_gc = tl.zeros([BD], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        if i_t == 0:\n            b_h += tl.load(h0 + i_b * D + o_d, mask=mask, other=0).to(tl.float32)\n    for i in range(0, BT):\n        mask_t = mask & ((i_t * BT + i) < T)\n        b_x = tl.load(p_x, mask=mask_t, other=0).to(tl.float32)\n        b_g = tl.load(p_g, mask=mask_t, other=0).to(tl.float32)\n        b_h = exp(b_g) * b_h + b_x\n        b_gc = b_gc + b_g\n        tl.store(p_gc, b_gc.to(p_o.dtype.element_ty), mask=mask_t)\n        tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask_t)\n\n        p_x += D\n        p_g += D\n        p_gc += D\n        p_o += D", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 57, 6, 98, 213, 57, 6, 98, 214, 57, 6, 98, 215, 57, 6, 149, 57, 30, -1, 216, 98, 217, 98, 218, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 219, 163, 216, 203, 214, 67, 68, 202, 307, 98, 214, 149, 30, 220, 163, 219, 1, 212, 30, 221, 163, 206, 67, 218, 203, 211, 203, 212, 67, 217, 203, 213, 203, 212, 67, 219, 30, 222, 163, 207, 67, 218, 203, 211, 203, 212, 67, 217, 203, 213, 203, 212, 67, 219, 30, 223, 163, 208, 67, 218, 203, 211, 203, 212, 67, 217, 203, 213, 203, 212, 67, 219, 30, 224, 163, 209, 67, 218, 203, 211, 203, 212, 67, 217, 203, 213, 203, 212, 67, 219, 30, 225, 163, 150, 202, 191, 214, 26, 98, 82, 163, 129, 149, 30, 226, 163, 150, 202, 191, 214, 26, 98, 82, 163, 129, 149, 30, 155, 215, 57, 30, 155, 217, 69, 307, 57, 30, 225, 146, 52, 202, 210, 67, 218, 203, 212, 67, 219, 98, 220, 163, 220, 98, 227, 163, 307, 149, 74, 228, 202, 129, 149, 30, 157, 30, 157, 30, 120, 229, 136, 5, 202, 307, 98, 213, 149, 57, 30, 230, 163, 220, 147, 202, 217, 203, 213, 67, 229, 1, 211, 149, 30, 231, 163, 52, 202, 221, 98, 220, 163, 230, 98, 227, 163, 307, 149, 74, 228, 202, 129, 149, 30, 232, 163, 52, 202, 222, 98, 220, 163, 230, 98, 227, 163, 307, 149, 74, 228, 202, 129, 149, 30, 225, 163, 173, 202, 232, 149, 203, 225, 67, 231, 30, 226, 163, 226, 67, 232, 30, 10, 202, 223, 98, 226, 74, 228, 202, 224, 74, 82, 74, 103, 149, 98, 220, 163, 230, 149, 30, 10, 202, 224, 98, 225, 74, 228, 202, 224, 74, 82, 74, 103, 149, 98, 220, 163, 230, 149, 30, 221, 146, 212, 30, 222, 146, 212, 30, 223, 146, 212, 30, 224, 146, 212, 30, 70, 30, 3, 30]}, {"code": "def chunk_hgrn_fwd_kernel_o(\n    gc, o, s_b, s_t, s_d, T, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr\n):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n\n    for i_t in range(1, tl.cdiv(T, BT)):\n        p_gc = tl.make_block_ptr(\n            gc + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)\n        )\n        p_o = tl.make_block_ptr(\n            o + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)\n        )\n\n        b_h0 = tl.load(o + i_b * T * D + i_t * BT * D - D + o_d, mask=mask, other=0).to(\n            tl.float32\n        )\n\n        b_gc = tl.load(p_gc, boundary_check=(0, 1)).to(tl.float32)\n        b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)\n        b_o = b_o + exp(b_gc) * b_h0[None, :]\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 56, 6, 97, 213, 56, 6, 97, 214, 56, 6, 149, 56, 30, -1, 215, 97, 216, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 149, 30, 217, 163, 215, 203, 214, 66, 67, 202, 307, 97, 214, 149, 30, 218, 163, 217, 1, 212, 30, 119, 219, 136, 5, 202, 308, 97, 58, 202, 211, 97, 213, 149, 149, 56, 30, 220, 163, 183, 202, 206, 66, 216, 203, 208, 97, 202, 211, 97, 212, 149, 97, 202, 209, 97, 210, 149, 97, 202, 219, 203, 213, 97, 215, 203, 214, 149, 97, 202, 213, 97, 214, 149, 97, 202, 308, 97, 307, 149, 149, 30, 221, 163, 183, 202, 207, 66, 216, 203, 208, 97, 202, 211, 97, 212, 149, 97, 202, 209, 97, 210, 149, 97, 202, 219, 203, 213, 97, 215, 203, 214, 149, 97, 202, 213, 97, 214, 149, 97, 202, 308, 97, 307, 149, 149, 30, 222, 163, 50, 202, 207, 66, 216, 203, 211, 203, 212, 66, 219, 203, 213, 203, 212, 4, 212, 66, 217, 97, 218, 163, 218, 97, 223, 163, 307, 149, 73, 224, 202, 128, 149, 30, 225, 163, 50, 202, 220, 97, 226, 163, 202, 307, 97, 308, 149, 149, 73, 224, 202, 128, 149, 30, 227, 163, 50, 202, 221, 97, 226, 163, 202, 307, 97, 308, 149, 149, 73, 224, 202, 128, 149, 30, 227, 163, 227, 66, 173, 202, 225, 149, 203, 222, 191, 168, 97, 56, 26, 30, 10, 202, 221, 97, 227, 73, 224, 202, 221, 73, 81, 73, 102, 149, 97, 226, 163, 202, 307, 97, 308, 149, 149, 30, 69, 30, 3, 30]}, {"code": "def chunk_hgrn_bwd_kernel_h(\n    g, gc, dx, do, T, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr\n):\n    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    BC = min(BT, T - i_t * BT)\n    NT = tl.num_programs(1)\n\n    p_g = g + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    p_gc = gc + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    p_dx = dx + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    p_do = do + (i_b * T + i_t * BT + BC - 1) * D + o_d\n\n    if i_t == NT - 1:\n        b_gc = tl.zeros([BD], dtype=tl.float32)\n    else:\n        b_gc = tl.load(g + (i_b * T + i_t * BT + BT) * D + o_d, mask=mask, other=0).to(\n            tl.float32\n        )\n    b_dh = tl.zeros([BD], dtype=tl.float32)\n    for _ in range(BC - 1, -1, -1):\n        tl.store(p_gc, b_gc.to(p_gc.dtype.element_ty), mask=mask)\n\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)\n\n        b_gc = b_gc + b_g\n        b_dh = b_dh + b_do\n        b_dx = b_dh\n        b_dh = b_dh * exp(b_g)\n\n        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)\n\n        p_g -= D\n        p_gc -= D\n        p_dx -= D\n        p_do -= D", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 57, 6, 98, 212, 57, 6, 98, 213, 57, 6, 149, 57, 30, -1, 214, 98, 215, 98, 216, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 217, 163, 214, 203, 213, 67, 68, 202, 307, 98, 213, 149, 30, 218, 163, 217, 1, 211, 30, 219, 163, 37, 202, 212, 98, 210, 4, 215, 203, 212, 149, 30, 220, 163, 123, 202, 308, 149, 30, 221, 163, 206, 67, 202, 216, 203, 210, 67, 215, 203, 212, 67, 219, 4, 308, 149, 203, 211, 67, 217, 30, 222, 163, 207, 67, 202, 216, 203, 210, 67, 215, 203, 212, 67, 219, 4, 308, 149, 203, 211, 67, 217, 30, 223, 163, 208, 67, 202, 216, 203, 210, 67, 215, 203, 212, 67, 219, 4, 308, 149, 203, 211, 67, 217, 30, 224, 163, 209, 67, 202, 216, 203, 210, 67, 215, 203, 212, 67, 219, 4, 308, 149, 203, 211, 67, 217, 30, 155, 215, 69, 220, 4, 308, 57, 30, 225, 163, 150, 202, 191, 213, 26, 98, 82, 163, 129, 149, 30, 157, 30, 29, 57, 30, 225, 163, 52, 202, 206, 67, 202, 216, 203, 210, 67, 215, 203, 212, 67, 212, 149, 203, 211, 67, 217, 98, 218, 163, 218, 98, 226, 163, 307, 149, 74, 227, 202, 129, 149, 30, 51, 30, 228, 163, 150, 202, 191, 213, 26, 98, 82, 163, 129, 149, 30, 120, 229, 136, 5, 202, 219, 4, 308, 98, 4, 308, 98, 4, 308, 149, 57, 30, 10, 202, 222, 98, 225, 74, 227, 202, 222, 74, 82, 74, 103, 149, 98, 218, 163, 218, 149, 30, 230, 163, 52, 202, 221, 98, 218, 163, 218, 98, 226, 163, 307, 149, 74, 227, 202, 129, 149, 30, 231, 163, 52, 202, 224, 98, 218, 163, 218, 98, 226, 163, 307, 149, 74, 227, 202, 129, 149, 30, 225, 163, 225, 67, 230, 30, 228, 163, 228, 67, 231, 30, 232, 163, 228, 30, 228, 163, 228, 203, 173, 202, 230, 149, 30, 10, 202, 223, 98, 232, 74, 227, 202, 223, 74, 82, 74, 103, 149, 98, 218, 163, 218, 149, 30, 221, 2, 211, 30, 222, 2, 211, 30, 223, 2, 211, 30, 224, 2, 211, 30, 70, 30, 3, 30]}, {"code": "def chunk_hgrn_bwd_kernel_o(\n    g,\n    gc,\n    o,\n    dx,\n    dg,\n    s_b,\n    s_t,\n    s_d,\n    T,\n    D: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_g = tl.make_block_ptr(\n            g + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)\n        )\n        p_gc = tl.make_block_ptr(\n            gc + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)\n        )\n        p_o = tl.make_block_ptr(\n            o + i_b * s_b,\n            (T, D),\n            (s_t, s_d),\n            (i_t * BT - 1, i_d * BD),\n            (BT, BD),\n            (1, 0),\n        )\n        p_dx = tl.make_block_ptr(\n            dx + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)\n        )\n        p_dg = tl.make_block_ptr(\n            dg + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)\n        )\n\n        mask_t = mask & ((i_t + 1) * BT < T)\n        b_ht = tl.load(\n            dx + i_b * T * D + (i_t + 1) * BT * D + o_d, mask=mask_t, other=0\n        ).to(tl.float32)\n\n        b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)\n        b_gc = tl.load(p_gc, boundary_check=(0, 1)).to(tl.float32)\n        b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)\n        b_dx = tl.load(p_dx, boundary_check=(0, 1)).to(tl.float32)\n\n        b_dx = b_dx + exp(b_gc) * b_ht[None, :]\n        b_dg = b_o * b_dx * exp(b_g)\n        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 56, 6, 97, 216, 56, 6, 97, 217, 56, 6, 149, 56, 30, -1, 218, 97, 219, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 149, 30, 220, 163, 218, 203, 217, 66, 67, 202, 307, 97, 217, 149, 30, 221, 163, 220, 1, 215, 30, 119, 222, 136, 5, 202, 58, 202, 214, 97, 216, 149, 4, 308, 97, 4, 308, 97, 4, 308, 149, 56, 30, 223, 163, 183, 202, 206, 66, 219, 203, 211, 97, 202, 214, 97, 215, 149, 97, 202, 212, 97, 213, 149, 97, 202, 222, 203, 216, 97, 218, 203, 217, 149, 97, 202, 216, 97, 217, 149, 97, 202, 308, 97, 307, 149, 149, 30, 224, 163, 183, 202, 207, 66, 219, 203, 211, 97, 202, 214, 97, 215, 149, 97, 202, 212, 97, 213, 149, 97, 202, 222, 203, 216, 97, 218, 203, 217, 149, 97, 202, 216, 97, 217, 149, 97, 202, 308, 97, 307, 149, 149, 30, 225, 163, 183, 202, 208, 66, 219, 203, 211, 97, 202, 214, 97, 215, 149, 97, 202, 212, 97, 213, 149, 97, 202, 222, 203, 216, 4, 308, 97, 218, 203, 217, 149, 97, 202, 216, 97, 217, 149, 97, 202, 308, 97, 307, 149, 149, 30, 226, 163, 183, 202, 209, 66, 219, 203, 211, 97, 202, 214, 97, 215, 149, 97, 202, 212, 97, 213, 149, 97, 202, 222, 203, 216, 97, 218, 203, 217, 149, 97, 202, 216, 97, 217, 149, 97, 202, 308, 97, 307, 149, 149, 30, 227, 163, 183, 202, 210, 66, 219, 203, 211, 97, 202, 214, 97, 215, 149, 97, 202, 212, 97, 213, 149, 97, 202, 222, 203, 216, 97, 218, 203, 217, 149, 97, 202, 216, 97, 217, 149, 97, 202, 308, 97, 307, 149, 149, 30, 228, 163, 221, 147, 202, 202, 222, 66, 308, 149, 203, 216, 1, 214, 149, 30, 229, 163, 50, 202, 209, 66, 219, 203, 214, 203, 215, 66, 202, 222, 66, 308, 149, 203, 216, 203, 215, 66, 220, 97, 221, 163, 228, 97, 230, 163, 307, 149, 73, 231, 202, 128, 149, 30, 232, 163, 50, 202, 223, 97, 233, 163, 202, 307, 97, 308, 149, 149, 73, 231, 202, 128, 149, 30, 234, 163, 50, 202, 224, 97, 233, 163, 202, 307, 97, 308, 149, 149, 73, 231, 202, 128, 149, 30, 235, 163, 50, 202, 225, 97, 233, 163, 202, 307, 97, 308, 149, 149, 73, 231, 202, 128, 149, 30, 236, 163, 50, 202, 226, 97, 233, 163, 202, 307, 97, 308, 149, 149, 73, 231, 202, 128, 149, 30, 236, 163, 236, 66, 173, 202, 234, 149, 203, 229, 191, 168, 97, 56, 26, 30, 237, 163, 235, 203, 236, 203, 173, 202, 232, 149, 30, 10, 202, 226, 97, 236, 73, 231, 202, 226, 73, 81, 73, 102, 149, 97, 233, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 227, 97, 237, 73, 231, 202, 227, 73, 81, 73, 102, 149, 97, 233, 163, 202, 307, 97, 308, 149, 149, 30, 69, 30, 3, 30]}, {"code": "def fused_recurrent_hgrn_fwd_kernel(\n    x,\n    g,\n    o,\n    h0,\n    ht,\n    cu_seqlens,\n    T,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_d, i_n = tl.program_id(0), tl.program_id(1)\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n\n    p_x = x + bos * D + o_d\n    p_g = g + bos * D + o_d\n    p_o = o + bos * D + o_d\n\n    b_h = tl.zeros([BD], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_n * D + o_d\n        b_h += tl.load(p_h0, mask=mask, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_x = tl.load(p_x, mask=mask, other=0).to(tl.float32)\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_h = exp(b_g) * b_h + b_x\n        tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask)\n\n        p_x += D\n        p_g += D\n        p_o += D\n\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_n * D + o_d\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask)", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 57, 6, 98, 214, 57, 6, 98, 215, 57, 6, 98, 216, 57, 6, 98, 217, 57, 6, 149, 57, 30, -1, 218, 98, 219, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 149, 30, 155, 217, 57, 30, 220, 98, 221, 163, 202, 52, 202, 211, 67, 219, 149, 74, 222, 202, 154, 149, 98, 52, 202, 211, 67, 219, 67, 308, 149, 74, 222, 202, 154, 149, 149, 30, 212, 163, 221, 4, 220, 30, 157, 30, 29, 57, 30, 220, 98, 221, 163, 202, 219, 203, 212, 98, 219, 203, 212, 67, 212, 149, 30, 51, 30, 223, 163, 218, 203, 214, 67, 68, 202, 307, 98, 214, 149, 30, 224, 163, 223, 1, 213, 30, 225, 163, 206, 67, 220, 203, 213, 67, 223, 30, 226, 163, 207, 67, 220, 203, 213, 67, 223, 30, 227, 163, 208, 67, 220, 203, 213, 67, 223, 30, 228, 163, 150, 202, 191, 214, 26, 98, 82, 163, 129, 149, 30, 155, 215, 57, 30, 229, 163, 209, 67, 219, 203, 213, 67, 223, 30, 228, 146, 52, 202, 229, 98, 224, 163, 224, 98, 230, 163, 307, 149, 74, 222, 202, 129, 149, 30, 157, 30, 120, 231, 136, 5, 202, 307, 98, 212, 149, 57, 30, 232, 163, 52, 202, 225, 98, 224, 163, 224, 98, 230, 163, 307, 149, 74, 222, 202, 129, 149, 30, 233, 163, 52, 202, 226, 98, 224, 163, 224, 98, 230, 163, 307, 149, 74, 222, 202, 129, 149, 30, 228, 163, 173, 202, 233, 149, 203, 228, 67, 232, 30, 10, 202, 227, 98, 228, 74, 222, 202, 227, 74, 82, 74, 103, 149, 98, 224, 163, 224, 149, 30, 225, 146, 213, 30, 226, 146, 213, 30, 227, 146, 213, 30, 70, 30, 155, 216, 57, 30, 234, 163, 210, 67, 219, 203, 213, 67, 223, 30, 10, 202, 234, 98, 228, 74, 222, 202, 234, 74, 82, 74, 103, 149, 98, 224, 163, 224, 149, 30, 157, 30, 3, 30]}, {"code": "def fused_recurrent_hgrn_bwd_kernel(\n    g,\n    o,\n    h0,\n    dx,\n    dg,\n    do,\n    dht,\n    dh0,\n    cu_seqlens,\n    T,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_d, i_n = tl.program_id(0), tl.program_id(1)\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n\n    p_g = g + (bos + T - 1) * D + o_d\n    p_o = o + (bos + T - 2) * D + o_d\n    p_dx = dx + (bos + T - 1) * D + o_d\n    p_dg = dg + (bos + T - 1) * D + o_d\n    p_do = do + (bos + T - 1) * D + o_d\n\n    b_dh = tl.zeros([BD], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = dht + i_n * D + o_d\n        b_dh += tl.load(p_dht, mask=mask, other=0).to(tl.float32)\n\n    for i in range(T - 1, -1, -1):\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)\n        if i > 0:\n            b_o = tl.load(p_o, mask=mask, other=0).to(tl.float32)\n        elif USE_INITIAL_STATE:\n            b_o = tl.load(h0 + i_n * D + o_d, mask=mask, other=0).to(tl.float32)\n        else:\n            b_o = tl.zeros([BD], dtype=tl.float32)\n\n        b_dh = b_dh + b_do\n        b_dx = b_dh\n        b_dh = b_dh * exp(b_g)\n        b_dg = b_dh * b_o\n        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), mask=mask)\n\n        p_g -= D\n        p_o -= D\n        p_dx -= D\n        p_dg -= D\n        p_do -= D\n\n    if USE_INITIAL_STATE:\n        p_dh0 = dh0 + i_n * D + o_d\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask)", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 56, 6, 97, 217, 56, 6, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 149, 56, 30, -1, 221, 97, 222, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 149, 30, 155, 220, 56, 30, 223, 97, 224, 163, 202, 50, 202, 214, 66, 222, 149, 73, 225, 202, 154, 149, 97, 50, 202, 214, 66, 222, 66, 308, 149, 73, 225, 202, 154, 149, 149, 30, 215, 163, 224, 4, 223, 30, 157, 30, 29, 56, 30, 223, 97, 224, 163, 202, 222, 203, 215, 97, 222, 203, 215, 66, 215, 149, 30, 51, 30, 226, 163, 221, 203, 217, 66, 67, 202, 307, 97, 217, 149, 30, 227, 163, 226, 1, 216, 30, 228, 163, 206, 66, 202, 223, 66, 215, 4, 308, 149, 203, 216, 66, 226, 30, 229, 163, 207, 66, 202, 223, 66, 215, 4, 309, 149, 203, 216, 66, 226, 30, 230, 163, 209, 66, 202, 223, 66, 215, 4, 308, 149, 203, 216, 66, 226, 30, 231, 163, 210, 66, 202, 223, 66, 215, 4, 308, 149, 203, 216, 66, 226, 30, 232, 163, 211, 66, 202, 223, 66, 215, 4, 308, 149, 203, 216, 66, 226, 30, 233, 163, 150, 202, 191, 217, 26, 97, 81, 163, 128, 149, 30, 155, 219, 56, 30, 234, 163, 212, 66, 222, 203, 216, 66, 226, 30, 233, 146, 50, 202, 234, 97, 227, 163, 227, 97, 235, 163, 307, 149, 73, 225, 202, 128, 149, 30, 157, 30, 119, 236, 136, 5, 202, 215, 4, 308, 97, 4, 308, 97, 4, 308, 149, 56, 30, 237, 163, 50, 202, 228, 97, 227, 163, 227, 97, 235, 163, 307, 149, 73, 225, 202, 128, 149, 30, 238, 163, 50, 202, 232, 97, 227, 163, 227, 97, 235, 163, 307, 149, 73, 225, 202, 128, 149, 30, 155, 236, 110, 307, 56, 30, 239, 163, 50, 202, 229, 97, 227, 163, 227, 97, 235, 163, 307, 149, 73, 225, 202, 128, 149, 30, 59, 30, 35, 218, 56, 30, 239, 163, 50, 202, 208, 66, 222, 203, 216, 66, 226, 97, 227, 163, 227, 97, 235, 163, 307, 149, 73, 225, 202, 128, 149, 30, 157, 30, 29, 56, 30, 239, 163, 150, 202, 191, 217, 26, 97, 81, 163, 128, 149, 30, 51, 30, 233, 163, 233, 66, 238, 30, 240, 163, 233, 30, 233, 163, 233, 203, 173, 202, 237, 149, 30, 241, 163, 233, 203, 239, 30, 10, 202, 230, 97, 240, 73, 225, 202, 230, 73, 81, 73, 102, 149, 97, 227, 163, 227, 149, 30, 10, 202, 231, 97, 241, 73, 225, 202, 231, 73, 81, 73, 102, 149, 97, 227, 163, 227, 149, 30, 228, 2, 216, 30, 229, 2, 216, 30, 230, 2, 216, 30, 231, 2, 216, 30, 232, 2, 216, 30, 69, 30, 155, 218, 56, 30, 242, 163, 213, 66, 222, 203, 216, 66, 226, 30, 10, 202, 242, 97, 233, 73, 225, 202, 242, 73, 81, 73, 102, 149, 97, 227, 163, 227, 149, 30, 157, 30, 3, 30]}, {"code": "def fused_chunk_linear_attn_fwd_kernel(\n    q,\n    k,\n    v,\n    o,\n    h0,\n    ht,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    CHECK: tl.constexpr,\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    o_i = tl.arange(0, BT)\n\n    m_s = o_i[:, None] >= o_i[None, :]\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(\n            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(0, tl.cdiv(T, BT)):\n        p_q = tl.make_block_ptr(\n            q + (i_b * T * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_k = tl.make_block_ptr(\n            k + (i_b * T * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_v = tl.make_block_ptr(\n            v + (i_b * T * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_o = tl.make_block_ptr(\n            o + (i_k * B * T * H + i_b * T * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n\n        b_o = tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        if CHECK and i_t == 0:\n            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(\n            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 57, 6, 98, 215, 57, 6, 98, 216, 57, 6, 98, 217, 57, 6, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 149, 57, 30, -1, 224, 98, 225, 98, 226, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 227, 98, 228, 163, 202, 226, 45, 215, 98, 226, 188, 215, 149, 30, 229, 163, 68, 202, 307, 98, 218, 149, 30, 230, 163, 229, 191, 57, 98, 168, 26, 128, 229, 191, 168, 98, 57, 26, 30, 231, 163, 150, 202, 191, 219, 98, 220, 26, 98, 82, 163, 129, 149, 30, 155, 221, 57, 30, 232, 163, 183, 202, 210, 67, 226, 203, 216, 203, 217, 98, 202, 216, 98, 217, 149, 98, 202, 217, 98, 308, 149, 98, 202, 225, 203, 219, 98, 224, 203, 220, 149, 98, 202, 219, 98, 220, 149, 98, 202, 308, 98, 307, 149, 149, 30, 231, 163, 52, 202, 232, 98, 233, 163, 202, 307, 98, 308, 149, 149, 74, 234, 202, 129, 149, 30, 157, 30, 120, 235, 136, 5, 202, 307, 98, 59, 202, 213, 98, 218, 149, 149, 57, 30, 236, 163, 183, 202, 206, 67, 202, 227, 203, 213, 203, 215, 67, 228, 149, 203, 216, 98, 202, 213, 98, 216, 149, 98, 202, 215, 203, 216, 98, 308, 149, 98, 202, 235, 203, 218, 98, 225, 203, 219, 149, 98, 202, 218, 98, 219, 149, 98, 202, 308, 98, 307, 149, 149, 30, 237, 163, 183, 202, 207, 67, 202, 227, 203, 213, 203, 215, 67, 228, 149, 203, 216, 98, 202, 216, 98, 213, 149, 98, 202, 308, 98, 215, 203, 216, 149, 98, 202, 225, 203, 219, 98, 235, 203, 218, 149, 98, 202, 219, 98, 218, 149, 98, 202, 307, 98, 308, 149, 149, 30, 238, 163, 183, 202, 208, 67, 202, 227, 203, 213, 203, 215, 67, 228, 149, 203, 217, 98, 202, 213, 98, 217, 149, 98, 202, 215, 203, 217, 98, 308, 149, 98, 202, 235, 203, 218, 98, 224, 203, 220, 149, 98, 202, 218, 98, 220, 149, 98, 202, 308, 98, 307, 149, 149, 30, 239, 163, 183, 202, 209, 67, 202, 225, 203, 214, 203, 213, 203, 215, 67, 227, 203, 213, 203, 215, 67, 228, 149, 203, 217, 98, 202, 213, 98, 217, 149, 98, 202, 215, 203, 217, 98, 308, 149, 98, 202, 235, 203, 218, 98, 224, 203, 220, 149, 98, 202, 218, 98, 220, 149, 98, 202, 308, 98, 307, 149, 149, 30, 240, 163, 52, 202, 236, 98, 233, 163, 202, 307, 98, 308, 149, 149, 30, 240, 163, 202, 240, 203, 212, 149, 74, 234, 202, 240, 74, 82, 149, 30, 241, 163, 52, 202, 237, 98, 233, 163, 202, 307, 98, 308, 149, 149, 30, 242, 163, 52, 202, 238, 98, 233, 163, 202, 307, 98, 308, 149, 149, 30, 243, 163, 15, 202, 240, 98, 241, 98, 244, 163, 54, 149, 30, 243, 163, 171, 202, 230, 98, 243, 98, 307, 149, 30, 245, 163, 15, 202, 243, 74, 234, 202, 240, 74, 82, 149, 98, 242, 98, 244, 163, 54, 149, 30, 155, 223, 92, 235, 69, 307, 57, 30, 245, 146, 15, 202, 240, 98, 231, 74, 234, 202, 240, 74, 82, 149, 98, 244, 163, 54, 149, 30, 231, 163, 231, 67, 15, 202, 241, 98, 242, 98, 244, 163, 54, 149, 30, 157, 30, 29, 57, 30, 245, 146, 15, 202, 240, 98, 231, 74, 234, 202, 240, 74, 82, 149, 98, 244, 163, 54, 149, 30, 231, 163, 231, 67, 15, 202, 241, 98, 242, 98, 244, 163, 54, 149, 30, 51, 30, 10, 202, 239, 98, 245, 74, 234, 202, 239, 74, 82, 74, 103, 149, 98, 233, 163, 202, 307, 98, 308, 149, 149, 30, 70, 30, 155, 222, 57, 30, 246, 163, 183, 202, 211, 67, 226, 203, 216, 203, 217, 98, 202, 216, 98, 217, 149, 98, 202, 217, 98, 308, 149, 98, 202, 225, 203, 219, 98, 224, 203, 220, 149, 98, 202, 219, 98, 220, 149, 98, 202, 308, 98, 307, 149, 149, 30, 10, 202, 246, 98, 231, 74, 234, 202, 246, 74, 82, 74, 103, 149, 98, 233, 163, 202, 307, 98, 308, 149, 149, 30, 157, 30, 3, 30]}, {"code": "def fused_chunk_linear_attn_bwd_kernel(\n    q,\n    k,\n    v,\n    do,\n    dq,\n    dk,\n    dv,\n    h0,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    CHECK: tl.constexpr,\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    o_i = tl.arange(0, BT)\n\n    m_s = o_i[:, None] >= o_i[None, :]\n\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(\n            h0 + i_bh * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)\n        )\n        b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(\n            k + (i_b * T * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_v = tl.make_block_ptr(\n            v + (i_b * T * H + i_h) * V,\n            (V, T),\n            (1, H * V),\n            (i_v * BV, i_t * BT),\n            (BV, BT),\n            (0, 1),\n        )\n        p_do = tl.make_block_ptr(\n            do + (i_b * T * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dq = tl.make_block_ptr(\n            dq + (i_v * B * T * H + i_b * T * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n\n        b_dq = tl.dot(b_ds.to(b_k.dtype), b_k, allow_tf32=False)\n\n        if CHECK and i_t == 0:\n            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        b_dq *= scale\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n\n    b_h = None\n    tl.debug_barrier()\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    m_s = o_i[:, None] <= o_i[None, :]\n    for i_t in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(\n            q + (i_b * T * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, T - i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_k = tl.make_block_ptr(\n            k + (i_b * T * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (T - i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_v = tl.make_block_ptr(\n            v + (i_b * T * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (T - i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_do = tl.make_block_ptr(\n            do + (i_b * T * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (T - i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dk = tl.make_block_ptr(\n            dk + (i_v * B * T * H + i_b * T * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (T - i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dv = tl.make_block_ptr(\n            dv + (i_k * B * T * H + i_b * T * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (T - i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0).to(b_q.dtype)\n\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0).to(b_q.dtype)\n\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        if CHECK and i_t == 1:\n            b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)\n            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False)\n            b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        else:\n            b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)\n            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False)\n            b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 56, 6, 97, 217, 56, 6, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 97, 224, 56, 6, 149, 56, 30, -1, 225, 97, 226, 97, 227, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 228, 97, 229, 163, 202, 227, 45, 217, 97, 227, 188, 217, 149, 30, 230, 163, 67, 202, 307, 97, 220, 149, 30, 231, 163, 230, 191, 56, 97, 168, 26, 127, 230, 191, 168, 97, 56, 26, 30, 232, 163, 150, 202, 191, 222, 97, 221, 26, 97, 81, 163, 128, 149, 30, 155, 223, 56, 30, 233, 163, 183, 202, 213, 66, 227, 203, 218, 203, 219, 97, 202, 219, 97, 218, 149, 97, 202, 308, 97, 219, 149, 97, 202, 225, 203, 222, 97, 226, 203, 221, 149, 97, 202, 222, 97, 221, 149, 97, 202, 307, 97, 308, 149, 149, 30, 232, 163, 50, 202, 233, 97, 234, 163, 202, 307, 97, 308, 149, 149, 73, 235, 202, 128, 149, 30, 157, 30, 119, 236, 136, 5, 202, 307, 97, 58, 202, 215, 97, 220, 149, 149, 56, 30, 237, 163, 183, 202, 207, 66, 202, 228, 203, 215, 203, 217, 66, 229, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 217, 203, 218, 97, 308, 149, 97, 202, 236, 203, 220, 97, 226, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 238, 163, 183, 202, 208, 66, 202, 228, 203, 215, 203, 217, 66, 229, 149, 203, 219, 97, 202, 219, 97, 215, 149, 97, 202, 308, 97, 217, 203, 219, 149, 97, 202, 225, 203, 222, 97, 236, 203, 220, 149, 97, 202, 222, 97, 220, 149, 97, 202, 307, 97, 308, 149, 149, 30, 239, 163, 183, 202, 209, 66, 202, 228, 203, 215, 203, 217, 66, 229, 149, 203, 219, 97, 202, 215, 97, 219, 149, 97, 202, 217, 203, 219, 97, 308, 149, 97, 202, 236, 203, 220, 97, 225, 203, 222, 149, 97, 202, 220, 97, 222, 149, 97, 202, 308, 97, 307, 149, 149, 30, 240, 163, 183, 202, 210, 66, 202, 225, 203, 216, 203, 215, 203, 217, 66, 228, 203, 215, 203, 217, 66, 229, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 217, 203, 218, 97, 308, 149, 97, 202, 236, 203, 220, 97, 226, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 241, 163, 50, 202, 237, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 242, 163, 50, 202, 238, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 243, 163, 50, 202, 239, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 244, 163, 15, 202, 243, 97, 242, 97, 245, 163, 54, 149, 30, 244, 163, 171, 202, 231, 97, 244, 97, 307, 149, 30, 246, 163, 15, 202, 244, 73, 235, 202, 241, 73, 81, 149, 97, 241, 97, 245, 163, 54, 149, 30, 155, 224, 91, 236, 68, 307, 56, 30, 246, 146, 15, 202, 243, 97, 232, 73, 235, 202, 243, 73, 81, 149, 97, 245, 163, 54, 149, 30, 232, 163, 232, 66, 15, 202, 242, 97, 241, 97, 245, 163, 54, 149, 30, 157, 30, 29, 56, 30, 246, 146, 15, 202, 243, 97, 232, 73, 235, 202, 243, 73, 81, 149, 97, 245, 163, 54, 149, 30, 232, 163, 232, 66, 15, 202, 242, 97, 241, 97, 245, 163, 54, 149, 30, 51, 30, 246, 23, 214, 30, 10, 202, 240, 97, 246, 73, 235, 202, 240, 73, 81, 73, 102, 149, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 69, 30, 232, 163, 168, 30, 47, 202, 149, 30, 247, 163, 150, 202, 191, 221, 97, 222, 26, 97, 81, 163, 128, 149, 30, 231, 163, 230, 191, 56, 97, 168, 26, 182, 230, 191, 168, 97, 56, 26, 30, 119, 236, 136, 5, 202, 308, 97, 58, 202, 215, 97, 220, 149, 66, 308, 149, 56, 30, 248, 163, 183, 202, 206, 66, 202, 228, 203, 215, 203, 217, 66, 229, 149, 203, 218, 97, 202, 218, 97, 215, 149, 97, 202, 308, 97, 217, 203, 218, 149, 97, 202, 226, 203, 221, 97, 215, 4, 236, 203, 220, 149, 97, 202, 221, 97, 220, 149, 97, 202, 307, 97, 308, 149, 149, 30, 237, 163, 183, 202, 207, 66, 202, 228, 203, 215, 203, 217, 66, 229, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 217, 203, 218, 97, 308, 149, 97, 202, 215, 4, 236, 203, 220, 97, 226, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 238, 163, 183, 202, 208, 66, 202, 228, 203, 215, 203, 217, 66, 229, 149, 203, 219, 97, 202, 215, 97, 219, 149, 97, 202, 217, 203, 219, 97, 308, 149, 97, 202, 215, 4, 236, 203, 220, 97, 225, 203, 222, 149, 97, 202, 220, 97, 222, 149, 97, 202, 308, 97, 307, 149, 149, 30, 239, 163, 183, 202, 209, 66, 202, 228, 203, 215, 203, 217, 66, 229, 149, 203, 219, 97, 202, 215, 97, 219, 149, 97, 202, 217, 203, 219, 97, 308, 149, 97, 202, 215, 4, 236, 203, 220, 97, 225, 203, 222, 149, 97, 202, 220, 97, 222, 149, 97, 202, 308, 97, 307, 149, 149, 30, 249, 163, 183, 202, 211, 66, 202, 225, 203, 216, 203, 215, 203, 217, 66, 228, 203, 215, 203, 217, 66, 229, 149, 203, 218, 97, 202, 215, 97, 218, 149, 97, 202, 217, 203, 218, 97, 308, 149, 97, 202, 215, 4, 236, 203, 220, 97, 226, 203, 221, 149, 97, 202, 220, 97, 221, 149, 97, 202, 308, 97, 307, 149, 149, 30, 250, 163, 183, 202, 212, 66, 202, 226, 203, 216, 203, 215, 203, 217, 66, 228, 203, 215, 203, 217, 66, 229, 149, 203, 219, 97, 202, 215, 97, 219, 149, 97, 202, 217, 203, 219, 97, 308, 149, 97, 202, 215, 4, 236, 203, 220, 97, 225, 203, 222, 149, 97, 202, 220, 97, 222, 149, 97, 202, 308, 97, 307, 149, 149, 30, 251, 163, 50, 202, 248, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 251, 163, 202, 251, 203, 214, 149, 73, 235, 202, 251, 73, 81, 149, 30, 241, 163, 50, 202, 237, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 242, 163, 50, 202, 238, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 243, 163, 50, 202, 239, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 252, 163, 15, 202, 241, 97, 251, 97, 245, 163, 54, 149, 30, 252, 163, 171, 202, 231, 97, 252, 97, 307, 149, 73, 235, 202, 251, 73, 81, 149, 30, 244, 163, 15, 202, 242, 97, 64, 202, 243, 149, 97, 245, 163, 54, 149, 30, 244, 163, 171, 202, 231, 97, 244, 97, 307, 149, 73, 235, 202, 251, 73, 81, 149, 30, 253, 163, 15, 202, 244, 97, 64, 202, 251, 149, 97, 245, 163, 54, 149, 30, 254, 163, 15, 202, 252, 97, 243, 97, 245, 163, 54, 149, 30, 155, 224, 91, 236, 68, 308, 56, 30, 253, 146, 15, 202, 242, 97, 64, 202, 247, 149, 73, 235, 202, 242, 73, 81, 149, 97, 245, 163, 54, 149, 30, 254, 146, 15, 202, 241, 97, 247, 73, 235, 202, 241, 73, 81, 149, 97, 245, 163, 54, 149, 30, 247, 146, 15, 202, 251, 97, 243, 97, 245, 163, 54, 149, 30, 157, 30, 29, 56, 30, 253, 146, 15, 202, 242, 97, 64, 202, 247, 149, 73, 235, 202, 242, 73, 81, 149, 97, 245, 163, 54, 149, 30, 254, 146, 15, 202, 241, 97, 247, 73, 235, 202, 241, 73, 81, 149, 97, 245, 163, 54, 149, 30, 247, 146, 15, 202, 251, 97, 243, 97, 245, 163, 54, 149, 30, 51, 30, 10, 202, 249, 97, 253, 73, 235, 202, 249, 73, 81, 73, 102, 149, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 250, 97, 254, 73, 235, 202, 250, 73, 81, 73, 102, 149, 97, 234, 163, 202, 307, 97, 308, 149, 149, 30, 69, 30, 3, 30]}, {"code": "def fused_recurrent_linear_attn_fwd_kernel(\n    q,\n    k,\n    v,\n    o,\n    h0,\n    ht,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n    p_o = o + (i_bh + i_k * B * H) * T * V + i_v * BV + tl.arange(0, BV)\n\n    mask_bk = (i_k * BK + tl.arange(0, BK)) < K\n    mask_bv = (i_v * BV + tl.arange(0, BV)) < V\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = (\n            h0\n            + i_bh * K * V\n            + (i_k * BK + tl.arange(0, BK)[None, :]) * V\n            + (i_v * BV + tl.arange(0, BV)[:, None])\n        )\n        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n\n        b_h += b_k[None, :] * b_v[:, None]\n        b_o = b_h * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_bv)\n\n        p_q += K\n        p_k += K\n        p_o += V\n        p_v += V\n\n    if STORE_FINAL_STATE:\n        p_ht = (\n            ht\n            + i_bh * K * V\n            + (i_k * BK + tl.arange(0, BK)[None, :]) * V\n            + (i_v * BV + tl.arange(0, BV)[:, None])\n        )\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_kv)", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 57, 6, 98, 215, 57, 6, 98, 216, 57, 6, 98, 217, 57, 6, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 149, 57, 30, -1, 222, 98, 223, 98, 224, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 225, 163, 206, 67, 224, 203, 213, 203, 216, 67, 223, 203, 218, 67, 68, 202, 307, 98, 218, 149, 30, 226, 163, 207, 67, 224, 203, 213, 203, 216, 67, 223, 203, 218, 67, 68, 202, 307, 98, 218, 149, 30, 227, 163, 208, 67, 224, 203, 213, 203, 217, 67, 222, 203, 219, 67, 68, 202, 307, 98, 219, 149, 30, 228, 163, 209, 67, 202, 224, 67, 223, 203, 214, 203, 215, 149, 203, 213, 203, 217, 67, 222, 203, 219, 67, 68, 202, 307, 98, 219, 149, 30, 229, 163, 223, 203, 218, 67, 68, 202, 307, 98, 218, 149, 1, 216, 30, 230, 163, 222, 203, 219, 67, 68, 202, 307, 98, 219, 149, 1, 217, 30, 231, 163, 229, 191, 168, 98, 57, 26, 147, 230, 191, 57, 98, 168, 26, 30, 232, 163, 150, 202, 191, 219, 98, 218, 26, 98, 82, 163, 129, 149, 30, 155, 220, 57, 30, 233, 163, 210, 67, 224, 203, 216, 203, 217, 67, 202, 223, 203, 218, 67, 68, 202, 307, 98, 218, 149, 191, 168, 98, 57, 26, 149, 203, 217, 67, 202, 222, 203, 219, 67, 68, 202, 307, 98, 219, 149, 191, 57, 98, 168, 26, 149, 30, 232, 146, 52, 202, 233, 98, 234, 163, 231, 98, 235, 163, 307, 149, 74, 236, 202, 129, 149, 30, 157, 30, 120, 237, 136, 5, 202, 307, 98, 213, 149, 57, 30, 238, 163, 52, 202, 226, 98, 234, 163, 229, 98, 235, 163, 307, 149, 74, 236, 202, 129, 149, 30, 239, 163, 52, 202, 227, 98, 234, 163, 230, 98, 235, 163, 307, 149, 74, 236, 202, 129, 149, 30, 240, 163, 52, 202, 225, 98, 234, 163, 229, 98, 235, 163, 307, 149, 74, 236, 202, 129, 149, 203, 212, 30, 232, 146, 238, 191, 168, 98, 57, 26, 203, 239, 191, 57, 98, 168, 26, 30, 241, 163, 232, 203, 240, 191, 168, 98, 57, 26, 30, 241, 163, 185, 202, 241, 98, 242, 163, 308, 149, 30, 10, 202, 228, 98, 241, 74, 236, 202, 228, 74, 82, 74, 103, 149, 98, 234, 163, 230, 149, 30, 225, 146, 216, 30, 226, 146, 216, 30, 228, 146, 217, 30, 227, 146, 217, 30, 70, 30, 155, 221, 57, 30, 243, 163, 211, 67, 224, 203, 216, 203, 217, 67, 202, 223, 203, 218, 67, 68, 202, 307, 98, 218, 149, 191, 168, 98, 57, 26, 149, 203, 217, 67, 202, 222, 203, 219, 67, 68, 202, 307, 98, 219, 149, 191, 57, 98, 168, 26, 149, 30, 10, 202, 243, 98, 232, 74, 236, 202, 243, 74, 82, 74, 103, 149, 98, 234, 163, 231, 149, 30, 157, 30, 3, 30]}, {"code": "def fused_recurrent_linear_attn_bwd_kernel(\n    q,\n    k,\n    v,\n    do,\n    dq,\n    dk,\n    dv,\n    h0,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n\n    p_dq = dq + (i_bh + i_v * B * H) * T * K + i_k * BK + tl.arange(0, BK)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_h0 = (\n            h0\n            + i_bh * K * V\n            + (i_k * BK + tl.arange(0, BK)[:, None]) * V\n            + (i_v * BV + tl.arange(0, BV)[None, :])\n        )\n        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n\n        b_h += b_k[:, None] * b_v[None, :]\n        _d_q = b_h * b_do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_bk)\n\n        p_k += K\n        p_do += V\n        p_v += V\n        p_dq += K\n\n    tl.debug_barrier()\n\n    p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_dk = dk + (i_bh + i_v * B * H) * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_dv = dv + (i_bh + i_k * B * H) * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    for _ in range(T):\n        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        d_h += b_q[:, None] * b_do[None, :]\n        d_k = tl.sum(d_h * b_v[None, :], axis=1)\n        d_v = tl.sum(d_h * b_k[:, None], axis=0)\n\n        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_bk)\n        tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_bv)\n\n        p_do -= V\n        p_q -= K\n        p_k -= K\n        p_v -= V\n        p_dk -= K\n        p_dv -= V", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 56, 6, 97, 217, 56, 6, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 149, 56, 30, -1, 223, 97, 224, 97, 225, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 226, 163, 206, 66, 225, 203, 215, 203, 218, 66, 224, 203, 220, 66, 67, 202, 307, 97, 220, 149, 30, 227, 163, 207, 66, 225, 203, 215, 203, 218, 66, 224, 203, 220, 66, 67, 202, 307, 97, 220, 149, 30, 228, 163, 208, 66, 225, 203, 215, 203, 219, 66, 223, 203, 221, 66, 67, 202, 307, 97, 221, 149, 30, 229, 163, 209, 66, 225, 203, 215, 203, 219, 66, 223, 203, 221, 66, 67, 202, 307, 97, 221, 149, 30, 230, 163, 210, 66, 202, 225, 66, 223, 203, 216, 203, 217, 149, 203, 215, 203, 218, 66, 224, 203, 220, 66, 67, 202, 307, 97, 220, 149, 30, 231, 163, 224, 203, 220, 66, 67, 202, 307, 97, 220, 149, 1, 218, 30, 232, 163, 223, 203, 221, 66, 67, 202, 307, 97, 221, 149, 1, 219, 30, 233, 163, 150, 202, 191, 220, 97, 221, 26, 97, 81, 163, 128, 149, 30, 155, 222, 56, 30, 234, 163, 231, 191, 56, 97, 168, 26, 147, 232, 191, 168, 97, 56, 26, 30, 235, 163, 213, 66, 225, 203, 218, 203, 219, 66, 202, 224, 203, 220, 66, 67, 202, 307, 97, 220, 149, 191, 56, 97, 168, 26, 149, 203, 219, 66, 202, 223, 203, 221, 66, 67, 202, 307, 97, 221, 149, 191, 168, 97, 56, 26, 149, 30, 233, 146, 50, 202, 235, 97, 236, 163, 234, 97, 237, 163, 307, 149, 73, 238, 202, 128, 149, 30, 157, 30, 119, 239, 136, 5, 202, 307, 97, 215, 149, 56, 30, 240, 163, 50, 202, 227, 97, 236, 163, 231, 97, 237, 163, 307, 149, 73, 238, 202, 128, 149, 30, 241, 163, 50, 202, 228, 97, 236, 163, 232, 97, 237, 163, 307, 149, 73, 238, 202, 128, 149, 30, 242, 163, 50, 202, 229, 97, 236, 163, 232, 97, 237, 163, 307, 149, 73, 238, 202, 128, 149, 30, 233, 146, 240, 191, 56, 97, 168, 26, 203, 241, 191, 168, 97, 56, 26, 30, 243, 163, 233, 203, 242, 191, 168, 97, 56, 26, 30, 244, 163, 185, 202, 243, 97, 245, 163, 308, 149, 203, 214, 30, 10, 202, 230, 97, 244, 73, 238, 202, 230, 73, 81, 73, 102, 149, 97, 236, 163, 231, 149, 30, 227, 146, 218, 30, 229, 146, 219, 30, 228, 146, 219, 30, 230, 146, 218, 30, 69, 30, 47, 202, 149, 30, 226, 163, 206, 66, 225, 203, 215, 203, 218, 66, 224, 203, 220, 66, 67, 202, 307, 97, 220, 149, 66, 202, 215, 4, 308, 149, 203, 218, 30, 227, 163, 207, 66, 225, 203, 215, 203, 218, 66, 224, 203, 220, 66, 67, 202, 307, 97, 220, 149, 66, 202, 215, 4, 308, 149, 203, 218, 30, 229, 163, 209, 66, 225, 203, 215, 203, 219, 66, 223, 203, 221, 66, 67, 202, 307, 97, 221, 149, 66, 202, 215, 4, 308, 149, 203, 219, 30, 228, 163, 208, 66, 225, 203, 215, 203, 219, 66, 223, 203, 221, 66, 67, 202, 307, 97, 221, 149, 66, 202, 215, 4, 308, 149, 203, 219, 30, 246, 163, 211, 66, 202, 225, 66, 223, 203, 216, 203, 217, 149, 203, 215, 203, 218, 66, 224, 203, 220, 66, 67, 202, 307, 97, 220, 149, 66, 202, 215, 4, 308, 149, 203, 218, 30, 247, 163, 212, 66, 202, 225, 66, 224, 203, 216, 203, 217, 149, 203, 215, 203, 219, 66, 223, 203, 221, 66, 67, 202, 307, 97, 221, 149, 66, 202, 215, 4, 308, 149, 203, 219, 30, 248, 163, 150, 202, 191, 220, 97, 221, 26, 97, 81, 163, 128, 149, 30, 119, 239, 136, 5, 202, 215, 149, 56, 30, 242, 163, 50, 202, 229, 97, 236, 163, 232, 97, 237, 163, 307, 149, 73, 238, 202, 128, 149, 30, 249, 163, 50, 202, 226, 97, 236, 163, 231, 97, 237, 163, 307, 149, 73, 238, 202, 128, 149, 203, 214, 30, 240, 163, 50, 202, 227, 97, 236, 163, 231, 97, 237, 163, 307, 149, 73, 238, 202, 128, 149, 30, 241, 163, 50, 202, 228, 97, 236, 163, 232, 97, 237, 163, 307, 149, 73, 238, 202, 128, 149, 30, 248, 146, 249, 191, 56, 97, 168, 26, 203, 242, 191, 168, 97, 56, 26, 30, 250, 163, 185, 202, 248, 203, 241, 191, 168, 97, 56, 26, 97, 245, 163, 308, 149, 30, 251, 163, 185, 202, 248, 203, 240, 191, 56, 97, 168, 26, 97, 245, 163, 307, 149, 30, 10, 202, 246, 97, 250, 73, 238, 202, 246, 73, 81, 73, 102, 149, 97, 236, 163, 231, 149, 30, 10, 202, 247, 97, 251, 73, 238, 202, 247, 73, 81, 73, 102, 149, 97, 236, 163, 232, 149, 30, 229, 2, 219, 30, 226, 2, 218, 30, 227, 2, 218, 30, 228, 2, 219, 30, 246, 2, 218, 30, 247, 2, 219, 30, 69, 30, 3, 30]}, {"code": "def chunk_update_once(b_p, b_k, b_v, b_m, b_g_exp_q, b_h, b_lamb):\n    b_o = tl.dot((tl.dot(b_p.to(b_k.dtype), tl.trans(b_k)) * b_m).to(b_v.dtype), b_v)\n    b_o += tl.dot((b_p * b_g_exp_q).to(b_h.dtype), b_h)\n    if b_lamb is not None:\n        b_o += b_lamb[None, :] * b_p\n    return b_o", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 149, 57, 30, -1, 213, 163, 15, 202, 202, 15, 202, 206, 74, 214, 202, 207, 74, 82, 149, 98, 65, 202, 207, 149, 149, 203, 209, 149, 74, 214, 202, 208, 74, 82, 149, 98, 208, 149, 30, 213, 146, 15, 202, 202, 206, 203, 210, 149, 74, 214, 202, 211, 74, 82, 149, 98, 211, 149, 30, 155, 212, 100, 58, 168, 57, 30, 213, 146, 212, 191, 168, 98, 57, 26, 203, 206, 30, 157, 30, 192, 213, 30, 3, 30]}, {"code": "def chunk_mesa_net_fwd_kernel_h(\n    k,\n    v,\n    beta,\n    g,\n    h,\n    h_kv,\n    h_init,\n    h_kv_init,\n    h_final,\n    h_kv_final,\n    cu_seqlens,\n    split_offsets,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        NS = tl.cdiv(T, BS)\n        boh = tl.load(split_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        NS = tl.cdiv(T, BS)\n        boh = i_n * NS\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    b_h_kv = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(\n            h_init + i_nh * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n        p_h_kv0 = tl.make_block_ptr(\n            h_kv_init + i_nh * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        b_h_kv = tl.load(p_h_kv0, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT):\n        i_s = i_t // (BS // BT)\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_k2 = tl.make_block_ptr(\n            k + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_beta = tl.make_block_ptr(\n            beta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)\n        )\n        b_beta = tl.load(p_beta, boundary_check=(0,))\n\n        o_h = ((boh + i_s) * H + i_h).to(tl.int64) * K * V\n        p_h = tl.make_block_ptr(\n            h + o_h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        p_h_kv = tl.make_block_ptr(\n            h_kv + o_h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n\n        if i_t % (BS // BT) == 0:\n            tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n            tl.store(p_h_kv, b_h_kv.to(p_h_kv.dtype.element_ty), boundary_check=(0, 1))\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k2 = tl.load(p_k2, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        last_idx = min((i_t + 1) * BT, T) - 1\n\n        b_g_last = tl.load(g + bos * H + last_idx * H + i_h)\n        p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h\n        b_h *= exp(b_g_last)\n        b_h_kv *= exp(b_g_last)\n        b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)\n        b_k_decay = ((b_k * exp(b_g_last - b_g)[:, None]) * b_beta[:, None]).to(\n            b_k.dtype\n        )\n        b_h += tl.dot(tl.trans(b_k_decay), b_k2)\n        b_h_kv += tl.dot(tl.trans(b_k_decay), b_v)\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(\n            h_final + i_nh * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n        p_h_kv_final = tl.make_block_ptr(\n            h_kv_final + i_nh * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        tl.store(\n            p_h_kv_final,\n            b_h_kv.to(p_h_kv_final.dtype.element_ty),\n            boundary_check=(0, 1),\n        )", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 97, 217, 97, 218, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 97, 224, 56, 6, 97, 225, 56, 6, 97, 226, 56, 6, 97, 227, 56, 6, 97, 228, 56, 6, 149, 56, 30, -1, 229, 97, 230, 97, 231, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 232, 97, 233, 163, 202, 231, 45, 219, 97, 231, 188, 219, 149, 30, 155, 228, 56, 30, 234, 97, 235, 163, 202, 50, 202, 216, 66, 232, 149, 73, 236, 202, 205, 149, 97, 50, 202, 216, 66, 232, 66, 308, 149, 73, 236, 202, 205, 149, 149, 30, 218, 163, 235, 4, 234, 30, 237, 163, 58, 202, 218, 97, 222, 149, 30, 238, 163, 58, 202, 218, 97, 223, 149, 30, 239, 163, 50, 202, 217, 66, 232, 149, 73, 236, 202, 205, 149, 30, 157, 30, 29, 56, 30, 234, 97, 235, 163, 202, 232, 203, 218, 97, 232, 203, 218, 66, 218, 149, 30, 237, 163, 58, 202, 218, 97, 222, 149, 30, 238, 163, 58, 202, 218, 97, 223, 149, 30, 239, 163, 232, 203, 238, 30, 51, 30, 240, 163, 150, 202, 191, 224, 97, 225, 26, 97, 81, 163, 128, 149, 30, 241, 163, 150, 202, 191, 224, 97, 225, 26, 97, 81, 163, 128, 149, 30, 155, 226, 56, 30, 242, 163, 183, 202, 212, 66, 231, 203, 220, 203, 221, 97, 202, 220, 97, 221, 149, 97, 202, 221, 97, 308, 149, 97, 202, 229, 203, 224, 97, 230, 203, 225, 149, 97, 202, 224, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 240, 163, 50, 202, 242, 97, 243, 163, 202, 307, 97, 308, 149, 149, 73, 236, 202, 128, 149, 30, 244, 163, 183, 202, 213, 66, 231, 203, 220, 203, 221, 97, 202, 220, 97, 221, 149, 97, 202, 221, 97, 308, 149, 97, 202, 229, 203, 224, 97, 230, 203, 225, 149, 97, 202, 224, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 241, 163, 50, 202, 244, 97, 243, 163, 202, 307, 97, 308, 149, 149, 73, 236, 202, 128, 149, 30, 157, 30, 119, 245, 136, 5, 202, 237, 149, 56, 30, 246, 163, 245, 45, 202, 223, 45, 222, 149, 30, 247, 163, 183, 202, 206, 66, 202, 234, 203, 219, 66, 233, 149, 203, 220, 97, 202, 218, 97, 220, 149, 97, 202, 219, 203, 220, 97, 308, 149, 97, 202, 245, 203, 222, 97, 229, 203, 224, 149, 97, 202, 222, 97, 224, 149, 97, 202, 308, 97, 307, 149, 149, 30, 248, 163, 183, 202, 206, 66, 202, 234, 203, 219, 66, 233, 149, 203, 221, 97, 202, 218, 97, 221, 149, 97, 202, 219, 203, 221, 97, 308, 149, 97, 202, 245, 203, 222, 97, 230, 203, 225, 149, 97, 202, 222, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 249, 163, 183, 202, 207, 66, 202, 234, 203, 219, 66, 233, 149, 203, 221, 97, 202, 218, 97, 221, 149, 97, 202, 219, 203, 221, 97, 308, 149, 97, 202, 245, 203, 222, 97, 230, 203, 225, 149, 97, 202, 222, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 250, 163, 183, 202, 208, 66, 202, 234, 203, 219, 66, 233, 149, 97, 202, 218, 97, 149, 97, 202, 219, 97, 149, 97, 202, 245, 203, 222, 97, 149, 97, 202, 222, 97, 149, 97, 202, 307, 97, 149, 149, 30, 251, 163, 50, 202, 250, 97, 243, 163, 202, 307, 97, 149, 149, 30, 252, 163, 202, 202, 239, 66, 246, 149, 203, 219, 66, 233, 149, 73, 236, 202, 154, 149, 203, 220, 203, 221, 30, 253, 163, 183, 202, 210, 66, 252, 97, 202, 220, 97, 221, 149, 97, 202, 221, 97, 308, 149, 97, 202, 229, 203, 224, 97, 230, 203, 225, 149, 97, 202, 224, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 254, 163, 183, 202, 211, 66, 252, 97, 202, 220, 97, 221, 149, 97, 202, 221, 97, 308, 149, 97, 202, 229, 203, 224, 97, 230, 203, 225, 149, 97, 202, 224, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 155, 245, 188, 202, 223, 45, 222, 149, 68, 307, 56, 30, 10, 202, 253, 97, 240, 73, 236, 202, 253, 73, 81, 73, 102, 149, 97, 243, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 254, 97, 241, 73, 236, 202, 254, 73, 81, 73, 102, 149, 97, 243, 163, 202, 307, 97, 308, 149, 149, 30, 157, 30, 255, 163, 50, 202, 247, 97, 243, 163, 202, 307, 97, 308, 149, 149, 30, 256, 163, 50, 202, 248, 97, 243, 163, 202, 307, 97, 308, 149, 149, 30, 257, 163, 50, 202, 249, 97, 243, 163, 202, 307, 97, 308, 149, 149, 30, 258, 163, 37, 202, 202, 245, 66, 308, 149, 203, 222, 97, 218, 149, 4, 308, 30, 259, 163, 50, 202, 209, 66, 234, 203, 219, 66, 258, 203, 219, 66, 233, 149, 30, 260, 163, 209, 66, 234, 203, 219, 66, 202, 245, 203, 222, 66, 67, 202, 307, 97, 222, 149, 149, 203, 219, 66, 233, 30, 240, 23, 173, 202, 259, 149, 30, 241, 23, 173, 202, 259, 149, 30, 261, 163, 50, 202, 260, 97, 262, 163, 245, 203, 222, 66, 67, 202, 307, 97, 222, 149, 1, 218, 97, 263, 163, 307, 149, 30, 264, 163, 202, 255, 203, 173, 202, 259, 4, 261, 149, 191, 56, 97, 168, 26, 203, 251, 191, 56, 97, 168, 26, 149, 73, 236, 202, 255, 73, 81, 149, 30, 240, 146, 15, 202, 64, 202, 264, 149, 97, 256, 149, 30, 241, 146, 15, 202, 64, 202, 264, 149, 97, 257, 149, 30, 69, 30, 155, 227, 56, 30, 265, 163, 183, 202, 214, 66, 231, 203, 220, 203, 221, 97, 202, 220, 97, 221, 149, 97, 202, 221, 97, 308, 149, 97, 202, 229, 203, 224, 97, 230, 203, 225, 149, 97, 202, 224, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 10, 202, 265, 97, 240, 73, 236, 202, 265, 73, 81, 73, 102, 149, 97, 243, 163, 202, 307, 97, 308, 149, 149, 30, 266, 163, 183, 202, 215, 66, 231, 203, 220, 203, 221, 97, 202, 220, 97, 221, 149, 97, 202, 221, 97, 308, 149, 97, 202, 229, 203, 224, 97, 230, 203, 225, 149, 97, 202, 224, 97, 225, 149, 97, 202, 308, 97, 307, 149, 149, 30, 10, 202, 266, 97, 241, 73, 236, 202, 266, 73, 81, 73, 102, 149, 97, 243, 163, 202, 307, 97, 308, 149, 149, 30, 157, 30, 3, 30]}, {"code": "def chunk_mesa_net_h_kk_bwd_intra_kernel(\n    k,\n    beta,\n    h,\n    dh,\n    g,\n    q_star,\n    dq,\n    dk,\n    dg,\n    dbeta,\n    dk_beta,\n    dlamb,\n    cu_seqlens,\n    chunk_indices,\n    B: tl.constexpr,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    q_star += (bos * H + i_h) * V\n    dq += (bos * H + i_h) * V\n    h += (i_tg * H + i_h).to(tl.int64) * K * V\n    dh += (i_tg * H + i_h).to(tl.int64) * K * V\n    k += (bos * H + i_h) * K\n    dk += (bos * H + i_h) * K\n    dk_beta += (bos * H + i_h) * K\n    dlamb += (i_tg * H + i_h).to(tl.int64) * K\n    beta += bos * H + i_h\n    dbeta += bos * H + i_h\n    g += bos * H + i_h\n    dg += bos * H + i_h\n\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dv = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dbeta = tl.zeros(\n        [\n            BT,\n        ],\n        dtype=tl.float32,\n    )\n    b_dg_last = tl.zeros(\n        [\n            1,\n        ],\n        dtype=tl.float32,\n    )\n    b_dg = tl.zeros(\n        [\n            BT,\n        ],\n        dtype=tl.float32,\n    )\n\n    p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * H)\n    b_gk = safe_exp(b_g_last - b_g)\n\n    p_q_star = tl.make_block_ptr(\n        q_star, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n    )\n    b_q_star = tl.load(p_q_star, boundary_check=(0, 1))\n    p_dq = tl.make_block_ptr(dq, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    b_dq = tl.load(p_dq, boundary_check=(0, 1))\n    b_dlamb = -tl.sum(b_q_star * b_dq, axis=0)\n    p_dlamb = tl.make_block_ptr(dlamb, (K,), (1,), (0,), (BK,), (0,))\n    tl.store(p_dlamb, b_dlamb.to(p_dlamb.dtype.element_ty), boundary_check=(0,))\n\n    p_h = tl.make_block_ptr(h, (V, K), (1, V), (0, 0), (BV, BK), (0, 1))\n    p_dh = tl.make_block_ptr(dh, (V, K), (1, V), (0, 0), (BV, BK), (0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_dh = tl.load(p_dh, boundary_check=(0, 1))\n    p_beta = tl.make_block_ptr(beta, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    b_v = tl.load(p_k, boundary_check=(0, 1))\n    b_k = b_v * b_beta[:, None]\n\n    b_dg_last = tl.sum(b_h * b_dh)\n    b_dg_last *= exp(b_g_last)\n    o_t = tl.arange(0, BT)\n    b_m = tl.where(\n        o_t[:, None] >= o_t[None, :], safe_exp(b_g[:, None] - b_g[None, :]), 0\n    )\n    b_s = tl.dot(b_q_star, tl.trans(b_k)) * b_m\n    b_ds = tl.dot(b_dq, tl.trans(b_v))\n    b_dm = b_s * b_ds\n    b_dm = tl.where(tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_dm, 0)\n    b_dg += tl.sum(b_dm, axis=1)\n    b_dg -= tl.sum(b_dm, axis=0)\n    b_ds = b_ds * b_m\n    b_dk += tl.dot(b_v, b_dh.to(b_v.dtype)) * b_gk[:, None]\n\n    b_dg_last += tl.sum(b_dk * b_k)\n    b_dg -= tl.sum(b_dk * b_k, axis=1)\n    b_dg += tl.sum(\n        tl.dot(b_dq, tl.trans(b_h)) * tl.exp(b_g)[:, None] * b_q_star, axis=1\n    )\n    b_dv += tl.dot(b_k, tl.trans(b_dh).to(b_k.dtype)) * b_gk[:, None] + tl.dot(\n        tl.trans(b_s.to(b_dq.dtype)), b_dq\n    )\n    b_dk += tl.dot(tl.trans(b_ds.to(b_q_star.dtype)), b_q_star)\n\n    p_dk_beta = tl.make_block_ptr(\n        dk_beta, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    b_dk -= tl.load(p_dk_beta, boundary_check=(0, 1))\n    b_dbeta = tl.sum(b_dk * b_v, axis=1)\n    b_dk = b_dk * b_beta[:, None] + b_dv\n    b_dk = -b_dk\n\n    b_dg = tl.where(o_t < min(BT, T - i_t * BT) - 1, b_dg, b_dg + b_dg_last)\n    p_dk = tl.make_block_ptr(dk, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    p_dg = tl.make_block_ptr(dg, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_dg, -b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))\n    p_dbeta = tl.make_block_ptr(dbeta, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_dbeta, -b_dbeta.to(p_dbeta.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 57, 6, 98, 221, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 149, 57, 30, -1, 229, 98, 230, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 149, 30, 231, 98, 232, 163, 202, 230, 45, 222, 98, 230, 188, 222, 149, 30, 155, 228, 57, 30, 233, 163, 229, 30, 234, 98, 229, 163, 202, 52, 202, 219, 67, 229, 203, 309, 149, 74, 235, 202, 56, 149, 98, 52, 202, 219, 67, 229, 203, 309, 67, 308, 149, 74, 235, 202, 56, 149, 149, 30, 236, 98, 237, 163, 202, 52, 202, 218, 67, 234, 149, 74, 235, 202, 56, 149, 98, 52, 202, 218, 67, 234, 67, 308, 149, 74, 235, 202, 56, 149, 149, 30, 221, 163, 237, 4, 236, 30, 238, 163, 59, 202, 221, 98, 225, 149, 30, 157, 30, 29, 57, 30, 238, 163, 59, 202, 221, 98, 225, 149, 30, 233, 163, 231, 203, 238, 67, 229, 30, 236, 98, 237, 163, 202, 231, 203, 221, 98, 231, 203, 221, 67, 221, 149, 30, 51, 30, 211, 146, 202, 236, 203, 222, 67, 232, 149, 203, 224, 30, 212, 146, 202, 236, 203, 222, 67, 232, 149, 203, 224, 30, 208, 146, 202, 233, 203, 222, 67, 232, 149, 74, 235, 202, 154, 149, 203, 223, 203, 224, 30, 209, 146, 202, 233, 203, 222, 67, 232, 149, 74, 235, 202, 154, 149, 203, 223, 203, 224, 30, 206, 146, 202, 236, 203, 222, 67, 232, 149, 203, 223, 30, 213, 146, 202, 236, 203, 222, 67, 232, 149, 203, 223, 30, 216, 146, 202, 236, 203, 222, 67, 232, 149, 203, 223, 30, 217, 146, 202, 233, 203, 222, 67, 232, 149, 74, 235, 202, 154, 149, 203, 223, 30, 207, 146, 236, 203, 222, 67, 232, 30, 215, 146, 236, 203, 222, 67, 232, 30, 210, 146, 236, 203, 222, 67, 232, 30, 214, 146, 236, 203, 222, 67, 232, 30, 239, 163, 150, 202, 191, 225, 98, 226, 26, 98, 82, 163, 129, 149, 30, 240, 163, 150, 202, 191, 225, 98, 226, 26, 98, 82, 163, 129, 149, 30, 241, 163, 150, 202, 191, 225, 26, 98, 82, 163, 129, 149, 30, 242, 163, 150, 202, 191, 308, 26, 98, 82, 163, 129, 149, 30, 243, 163, 150, 202, 191, 225, 26, 98, 82, 163, 129, 149, 30, 244, 163, 183, 202, 210, 98, 202, 221, 98, 149, 98, 202, 222, 98, 149, 98, 202, 229, 203, 225, 98, 149, 98, 202, 225, 98, 149, 98, 202, 307, 98, 149, 149, 30, 245, 163, 52, 202, 244, 98, 246, 163, 202, 307, 98, 149, 149, 30, 247, 163, 52, 202, 210, 67, 202, 37, 202, 229, 203, 225, 67, 225, 98, 221, 149, 4, 308, 149, 203, 222, 149, 30, 248, 163, 249, 202, 247, 4, 245, 149, 30, 250, 163, 183, 202, 211, 98, 202, 221, 98, 224, 149, 98, 202, 222, 203, 224, 98, 308, 149, 98, 202, 229, 203, 225, 98, 307, 149, 98, 202, 225, 98, 227, 149, 98, 202, 308, 98, 307, 149, 149, 30, 251, 163, 52, 202, 250, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 252, 163, 183, 202, 212, 98, 202, 221, 98, 224, 149, 98, 202, 222, 203, 224, 98, 308, 149, 98, 202, 229, 203, 225, 98, 307, 149, 98, 202, 225, 98, 227, 149, 98, 202, 308, 98, 307, 149, 149, 30, 253, 163, 52, 202, 252, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 254, 163, 4, 185, 202, 251, 203, 253, 98, 255, 163, 307, 149, 30, 256, 163, 183, 202, 217, 98, 202, 223, 98, 149, 98, 202, 308, 98, 149, 98, 202, 307, 98, 149, 98, 202, 226, 98, 149, 98, 202, 307, 98, 149, 149, 30, 10, 202, 256, 98, 254, 74, 235, 202, 256, 74, 82, 74, 103, 149, 98, 246, 163, 202, 307, 98, 149, 149, 30, 257, 163, 183, 202, 208, 98, 202, 224, 98, 223, 149, 98, 202, 308, 98, 224, 149, 98, 202, 307, 98, 307, 149, 98, 202, 227, 98, 226, 149, 98, 202, 307, 98, 308, 149, 149, 30, 258, 163, 183, 202, 209, 98, 202, 224, 98, 223, 149, 98, 202, 308, 98, 224, 149, 98, 202, 307, 98, 307, 149, 98, 202, 227, 98, 226, 149, 98, 202, 307, 98, 308, 149, 149, 30, 259, 163, 52, 202, 257, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 260, 163, 52, 202, 258, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 261, 163, 183, 202, 207, 98, 202, 221, 98, 149, 98, 202, 222, 98, 149, 98, 202, 229, 203, 225, 98, 149, 98, 202, 225, 98, 149, 98, 202, 307, 98, 149, 149, 30, 262, 163, 52, 202, 261, 98, 246, 163, 202, 307, 98, 149, 149, 30, 263, 163, 183, 202, 206, 98, 202, 221, 98, 223, 149, 98, 202, 222, 203, 223, 98, 308, 149, 98, 202, 229, 203, 225, 98, 307, 149, 98, 202, 225, 98, 226, 149, 98, 202, 308, 98, 307, 149, 149, 30, 264, 163, 52, 202, 263, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 265, 163, 264, 203, 262, 191, 57, 98, 168, 26, 30, 242, 163, 185, 202, 259, 203, 260, 149, 30, 242, 23, 173, 202, 247, 149, 30, 266, 163, 68, 202, 307, 98, 225, 149, 30, 267, 163, 171, 202, 266, 191, 57, 98, 168, 26, 128, 266, 191, 168, 98, 57, 26, 98, 249, 202, 245, 191, 57, 98, 168, 26, 4, 245, 191, 168, 98, 57, 26, 149, 98, 307, 149, 30, 268, 163, 15, 202, 251, 98, 65, 202, 265, 149, 149, 203, 267, 30, 269, 163, 15, 202, 253, 98, 65, 202, 264, 149, 149, 30, 270, 163, 268, 203, 269, 30, 270, 163, 171, 202, 68, 202, 307, 98, 225, 149, 191, 57, 98, 168, 26, 128, 68, 202, 307, 98, 225, 149, 191, 168, 98, 57, 26, 98, 270, 98, 307, 149, 30, 243, 146, 185, 202, 270, 98, 255, 163, 308, 149, 30, 243, 2, 185, 202, 270, 98, 255, 163, 307, 149, 30, 269, 163, 269, 203, 267, 30, 239, 146, 15, 202, 264, 98, 260, 74, 235, 202, 264, 74, 82, 149, 149, 203, 248, 191, 57, 98, 168, 26, 30, 242, 146, 185, 202, 239, 203, 265, 149, 30, 243, 2, 185, 202, 239, 203, 265, 98, 255, 163, 308, 149, 30, 243, 146, 185, 202, 15, 202, 253, 98, 65, 202, 259, 149, 149, 203, 97, 202, 245, 149, 191, 57, 98, 168, 26, 203, 251, 98, 255, 163, 308, 149, 30, 240, 146, 15, 202, 265, 98, 65, 202, 260, 149, 74, 235, 202, 265, 74, 82, 149, 149, 203, 248, 191, 57, 98, 168, 26, 67, 15, 202, 65, 202, 268, 74, 235, 202, 253, 74, 82, 149, 149, 98, 253, 149, 30, 239, 146, 15, 202, 65, 202, 269, 74, 235, 202, 251, 74, 82, 149, 149, 98, 251, 149, 30, 271, 163, 183, 202, 216, 98, 202, 221, 98, 223, 149, 98, 202, 222, 203, 223, 98, 308, 149, 98, 202, 229, 203, 225, 98, 307, 149, 98, 202, 225, 98, 226, 149, 98, 202, 308, 98, 307, 149, 149, 30, 239, 2, 52, 202, 271, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 241, 163, 185, 202, 239, 203, 264, 98, 255, 163, 308, 149, 30, 239, 163, 239, 203, 262, 191, 57, 98, 168, 26, 67, 240, 30, 239, 163, 4, 239, 30, 243, 163, 171, 202, 266, 1, 37, 202, 225, 98, 221, 4, 229, 203, 225, 149, 4, 308, 98, 243, 98, 243, 67, 242, 149, 30, 272, 163, 183, 202, 213, 98, 202, 221, 98, 223, 149, 98, 202, 222, 203, 223, 98, 308, 149, 98, 202, 229, 203, 225, 98, 307, 149, 98, 202, 225, 98, 226, 149, 98, 202, 308, 98, 307, 149, 149, 30, 10, 202, 272, 98, 239, 74, 235, 202, 272, 74, 82, 74, 103, 149, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 273, 163, 183, 202, 214, 98, 202, 221, 98, 149, 98, 202, 222, 98, 149, 98, 202, 229, 203, 225, 98, 149, 98, 202, 225, 98, 149, 98, 202, 307, 98, 149, 149, 30, 10, 202, 273, 98, 4, 243, 74, 235, 202, 273, 74, 82, 74, 103, 149, 98, 246, 163, 202, 307, 98, 149, 149, 30, 274, 163, 183, 202, 215, 98, 202, 221, 98, 149, 98, 202, 222, 98, 149, 98, 202, 229, 203, 225, 98, 149, 98, 202, 225, 98, 149, 98, 202, 307, 98, 149, 149, 30, 10, 202, 274, 98, 4, 241, 74, 235, 202, 274, 74, 82, 74, 103, 149, 98, 246, 163, 202, 307, 98, 149, 149, 30, 3, 30]}, {"code": "def chunk_mesa_net_h_kv_bwd_intra_kernel(\n    q_star,\n    k,\n    v,\n    beta,\n    h_kv,\n    g,\n    do,\n    dh_kv,\n    dq,\n    dk_beta,\n    dg,\n    dv,\n    cu_seqlens,\n    chunk_indices,\n    B: tl.constexpr,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    v += (bos * H + i_h) * V\n    do += (bos * H + i_h) * V\n    h_kv += (i_tg * H + i_h).to(tl.int64) * K * V\n    dh_kv += (i_tg * H + i_h).to(tl.int64) * K * V\n    q_star += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    beta += bos * H + i_h\n    g += bos * H + i_h\n    dg += bos * H + i_h\n    dq += (bos * H + i_h) * K\n    dk_beta += (bos * H + i_h) * K\n    dv += (bos * H + i_h) * V\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    b_dv = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dg_last = tl.zeros(\n        [\n            1,\n        ],\n        dtype=tl.float32,\n    )\n    b_dg = tl.zeros(\n        [\n            BT,\n        ],\n        dtype=tl.float32,\n    )\n\n    p_v = tl.make_block_ptr(v, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    p_beta = tl.make_block_ptr(beta, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    p_do = tl.make_block_ptr(do, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_h = tl.make_block_ptr(h_kv, (V, K), (1, V), (0, 0), (BV, BK), (0, 1))\n    p_dh = tl.make_block_ptr(dh_kv, (V, K), (1, V), (0, 0), (BV, BK), (0, 1))\n    p_q = tl.make_block_ptr(q_star, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_dh = tl.load(p_dh, boundary_check=(0, 1))\n    b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * H)\n\n    b_dg_last += tl.sum(b_h * b_dh)\n    b_dg_last *= exp(b_g_last)\n    o_t = tl.arange(0, BT)\n    b_m = tl.where(\n        o_t[:, None] >= o_t[None, :], safe_exp(b_g[:, None] - b_g[None, :]), 0\n    )\n    b_k = b_k * b_beta[:, None]\n    b_s = tl.dot(b_q, tl.trans(b_k)) * b_m\n\n    b_ds = tl.dot(b_do, tl.trans(b_v))\n    b_dm = b_s * b_ds\n    b_dm = tl.where(tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_dm, 0)\n\n    b_dg += tl.sum(b_dm, axis=1)\n    b_dg -= tl.sum(b_dm, axis=0)\n\n    b_g_exp_q = exp(b_g)\n    b_g_exp_k = safe_exp(-b_g + b_g_last)\n    b_ds = b_ds * b_m\n    b_dq += tl.dot(b_do, b_h.to(b_do.dtype)) * b_g_exp_q[:, None]\n    b_dk += tl.dot(b_v, b_dh.to(b_v.dtype)) * b_g_exp_k[:, None]\n    b_dg_last += tl.sum(b_dk * b_k)\n    b_dg -= tl.sum(b_dk * b_k, axis=1)\n    b_dg += tl.sum(b_dq * b_q, axis=1)\n    b_dq += tl.dot(b_ds.to(b_k.dtype), b_k)\n    b_dv += tl.dot(b_k, tl.trans(b_dh).to(b_k.dtype)) * b_g_exp_k[:, None] + tl.dot(\n        tl.trans(b_s.to(b_do.dtype)), b_do\n    )\n    b_dk += tl.dot(tl.trans(b_ds.to(b_q.dtype)), b_q)\n\n    b_dg = tl.where(o_t < min(BT, T - i_t * BT) - 1, b_dg, b_dg + b_dg_last)\n    p_dq = tl.make_block_ptr(dq, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(\n        dk_beta, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    p_dv = tl.make_block_ptr(dv, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_dg = tl.make_block_ptr(dg, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 97, 217, 97, 218, 97, 219, 97, 220, 56, 6, 97, 221, 97, 222, 56, 6, 97, 223, 56, 6, 97, 224, 56, 6, 97, 225, 56, 6, 97, 226, 56, 6, 97, 227, 56, 6, 97, 228, 56, 6, 149, 56, 30, -1, 229, 97, 230, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 149, 30, 231, 97, 232, 163, 202, 230, 45, 222, 97, 230, 188, 222, 149, 30, 155, 228, 56, 30, 233, 163, 229, 30, 234, 97, 229, 163, 202, 50, 202, 219, 66, 229, 203, 309, 149, 73, 235, 202, 205, 149, 97, 50, 202, 219, 66, 229, 203, 309, 66, 308, 149, 73, 235, 202, 205, 149, 149, 30, 236, 97, 237, 163, 202, 50, 202, 218, 66, 234, 149, 73, 235, 202, 205, 149, 97, 50, 202, 218, 66, 234, 66, 308, 149, 73, 235, 202, 205, 149, 149, 30, 221, 163, 237, 4, 236, 30, 238, 163, 58, 202, 221, 97, 225, 149, 30, 157, 30, 29, 56, 30, 238, 163, 58, 202, 221, 97, 225, 149, 30, 233, 163, 231, 203, 238, 66, 229, 30, 236, 97, 237, 163, 202, 231, 203, 221, 97, 231, 203, 221, 66, 221, 149, 30, 51, 30, 208, 146, 202, 236, 203, 222, 66, 232, 149, 203, 224, 30, 212, 146, 202, 236, 203, 222, 66, 232, 149, 203, 224, 30, 210, 146, 202, 233, 203, 222, 66, 232, 149, 73, 235, 202, 154, 149, 203, 223, 203, 224, 30, 213, 146, 202, 233, 203, 222, 66, 232, 149, 73, 235, 202, 154, 149, 203, 223, 203, 224, 30, 206, 146, 202, 236, 203, 222, 66, 232, 149, 203, 223, 30, 207, 146, 202, 236, 203, 222, 66, 232, 149, 203, 223, 30, 209, 146, 236, 203, 222, 66, 232, 30, 211, 146, 236, 203, 222, 66, 232, 30, 216, 146, 236, 203, 222, 66, 232, 30, 214, 146, 202, 236, 203, 222, 66, 232, 149, 203, 223, 30, 215, 146, 202, 236, 203, 222, 66, 232, 149, 203, 223, 30, 217, 146, 202, 236, 203, 222, 66, 232, 149, 203, 224, 30, 239, 163, 150, 202, 191, 225, 97, 226, 26, 97, 81, 163, 128, 149, 30, 240, 163, 150, 202, 191, 225, 97, 226, 26, 97, 81, 163, 128, 149, 30, 241, 163, 150, 202, 191, 225, 97, 225, 26, 97, 81, 163, 128, 149, 30, 242, 163, 150, 202, 191, 225, 97, 226, 26, 97, 81, 163, 128, 149, 30, 243, 163, 150, 202, 191, 308, 26, 97, 81, 163, 128, 149, 30, 244, 163, 150, 202, 191, 225, 26, 97, 81, 163, 128, 149, 30, 245, 163, 183, 202, 208, 97, 202, 221, 97, 224, 149, 97, 202, 222, 203, 224, 97, 308, 149, 97, 202, 229, 203, 225, 97, 307, 149, 97, 202, 225, 97, 227, 149, 97, 202, 308, 97, 307, 149, 149, 30, 246, 163, 183, 202, 207, 97, 202, 221, 97, 223, 149, 97, 202, 222, 203, 223, 97, 308, 149, 97, 202, 229, 203, 225, 97, 307, 149, 97, 202, 225, 97, 226, 149, 97, 202, 308, 97, 307, 149, 149, 30, 247, 163, 183, 202, 209, 97, 202, 221, 97, 149, 97, 202, 222, 97, 149, 97, 202, 229, 203, 225, 97, 149, 97, 202, 225, 97, 149, 97, 202, 307, 97, 149, 149, 30, 248, 163, 183, 202, 212, 97, 202, 221, 97, 224, 149, 97, 202, 222, 203, 224, 97, 308, 149, 97, 202, 229, 203, 225, 97, 307, 149, 97, 202, 225, 97, 227, 149, 97, 202, 308, 97, 307, 149, 149, 30, 249, 163, 183, 202, 210, 97, 202, 224, 97, 223, 149, 97, 202, 308, 97, 224, 149, 97, 202, 307, 97, 307, 149, 97, 202, 227, 97, 226, 149, 97, 202, 307, 97, 308, 149, 149, 30, 250, 163, 183, 202, 213, 97, 202, 224, 97, 223, 149, 97, 202, 308, 97, 224, 149, 97, 202, 307, 97, 307, 149, 97, 202, 227, 97, 226, 149, 97, 202, 307, 97, 308, 149, 149, 30, 251, 163, 183, 202, 206, 97, 202, 221, 97, 223, 149, 97, 202, 222, 203, 223, 97, 308, 149, 97, 202, 229, 203, 225, 97, 307, 149, 97, 202, 225, 97, 226, 149, 97, 202, 308, 97, 307, 149, 149, 30, 252, 163, 183, 202, 211, 97, 202, 221, 97, 149, 97, 202, 222, 97, 149, 97, 202, 229, 203, 225, 97, 149, 97, 202, 225, 97, 149, 97, 202, 307, 97, 149, 149, 30, 253, 163, 50, 202, 251, 97, 254, 163, 202, 307, 97, 308, 149, 149, 30, 255, 163, 50, 202, 245, 97, 254, 163, 202, 307, 97, 308, 149, 149, 30, 256, 163, 50, 202, 246, 97, 254, 163, 202, 307, 97, 308, 149, 149, 30, 257, 163, 50, 202, 247, 97, 254, 163, 202, 307, 97, 149, 149, 30, 258, 163, 50, 202, 252, 97, 254, 163, 202, 307, 97, 149, 149, 30, 259, 163, 50, 202, 248, 97, 254, 163, 202, 307, 97, 308, 149, 149, 30, 260, 163, 50, 202, 249, 97, 254, 163, 202, 307, 97, 308, 149, 149, 30, 261, 163, 50, 202, 250, 97, 254, 163, 202, 307, 97, 308, 149, 149, 30, 262, 163, 50, 202, 211, 66, 202, 37, 202, 229, 203, 225, 66, 225, 97, 221, 149, 4, 308, 149, 203, 222, 149, 30, 243, 146, 185, 202, 260, 203, 261, 149, 30, 243, 23, 173, 202, 262, 149, 30, 263, 163, 67, 202, 307, 97, 225, 149, 30, 264, 163, 171, 202, 263, 191, 56, 97, 168, 26, 127, 263, 191, 168, 97, 56, 26, 97, 265, 202, 258, 191, 56, 97, 168, 26, 4, 258, 191, 168, 97, 56, 26, 149, 97, 307, 149, 30, 256, 163, 256, 203, 257, 191, 56, 97, 168, 26, 30, 266, 163, 15, 202, 253, 97, 64, 202, 256, 149, 149, 203, 264, 30, 241, 163, 15, 202, 259, 97, 64, 202, 255, 149, 149, 30, 267, 163, 266, 203, 241, 30, 267, 163, 171, 202, 67, 202, 307, 97, 225, 149, 191, 56, 97, 168, 26, 127, 67, 202, 307, 97, 225, 149, 191, 168, 97, 56, 26, 97, 267, 97, 307, 149, 30, 244, 146, 185, 202, 267, 97, 268, 163, 308, 149, 30, 244, 2, 185, 202, 267, 97, 268, 163, 307, 149, 30, 269, 163, 173, 202, 258, 149, 30, 270, 163, 265, 202, 4, 258, 66, 262, 149, 30, 241, 163, 241, 203, 264, 30, 239, 146, 15, 202, 259, 97, 260, 73, 235, 202, 259, 73, 81, 149, 149, 203, 269, 191, 56, 97, 168, 26, 30, 240, 146, 15, 202, 255, 97, 261, 73, 235, 202, 255, 73, 81, 149, 149, 203, 270, 191, 56, 97, 168, 26, 30, 243, 146, 185, 202, 240, 203, 256, 149, 30, 244, 2, 185, 202, 240, 203, 256, 97, 268, 163, 308, 149, 30, 244, 146, 185, 202, 239, 203, 253, 97, 268, 163, 308, 149, 30, 239, 146, 15, 202, 241, 73, 235, 202, 256, 73, 81, 149, 97, 256, 149, 30, 242, 146, 15, 202, 256, 97, 64, 202, 261, 149, 73, 235, 202, 256, 73, 81, 149, 149, 203, 270, 191, 56, 97, 168, 26, 66, 15, 202, 64, 202, 266, 73, 235, 202, 259, 73, 81, 149, 149, 97, 259, 149, 30, 240, 146, 15, 202, 64, 202, 241, 73, 235, 202, 253, 73, 81, 149, 149, 97, 253, 149, 30, 244, 163, 171, 202, 263, 1, 37, 202, 225, 97, 221, 4, 229, 203, 225, 149, 4, 308, 97, 244, 97, 244, 66, 243, 149, 30, 271, 163, 183, 202, 214, 97, 202, 221, 97, 223, 149, 97, 202, 222, 203, 223, 97, 308, 149, 97, 202, 229, 203, 225, 97, 307, 149, 97, 202, 225, 97, 226, 149, 97, 202, 308, 97, 307, 149, 149, 30, 272, 163, 183, 202, 215, 97, 202, 221, 97, 223, 149, 97, 202, 222, 203, 223, 97, 308, 149, 97, 202, 229, 203, 225, 97, 307, 149, 97, 202, 225, 97, 226, 149, 97, 202, 308, 97, 307, 149, 149, 30, 273, 163, 183, 202, 217, 97, 202, 221, 97, 224, 149, 97, 202, 222, 203, 224, 97, 308, 149, 97, 202, 229, 203, 225, 97, 307, 149, 97, 202, 225, 97, 227, 149, 97, 202, 308, 97, 307, 149, 149, 30, 274, 163, 183, 202, 216, 97, 202, 221, 97, 149, 97, 202, 222, 97, 149, 97, 202, 229, 203, 225, 97, 149, 97, 202, 225, 97, 149, 97, 202, 307, 97, 149, 149, 30, 10, 202, 271, 97, 239, 73, 235, 202, 271, 73, 81, 73, 102, 149, 97, 254, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 272, 97, 240, 73, 235, 202, 272, 73, 81, 73, 102, 149, 97, 254, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 273, 97, 242, 73, 235, 202, 273, 73, 81, 73, 102, 149, 97, 254, 163, 202, 307, 97, 308, 149, 149, 30, 10, 202, 274, 97, 244, 73, 235, 202, 274, 73, 81, 73, 102, 149, 97, 254, 163, 202, 307, 97, 149, 149, 30, 3, 30]}, {"code": "def parallel_nsa_compression_fwd_kernel(\n    q,\n    k,\n    v,\n    o,\n    lse,\n    scale,\n    cu_seqlens,\n    token_indices,\n    chunk_offsets,\n    T,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BC: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(\n            token_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        boc = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        boc = i_b * tl.cdiv(T, BS)\n\n    p_q = tl.make_block_ptr(\n        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    TC = tl.cdiv(T, BS)\n\n    NC = (i_t + 1) // BS\n\n    p_o = tl.make_block_ptr(\n        o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0)\n    )\n\n    b_o = tl.zeros([G, BV], dtype=tl.float32)\n\n    b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)\n\n    b_acc = tl.zeros([G], dtype=tl.float32)\n\n    for i_c in range(0, NC, BC):\n        o_c = i_c + tl.arange(0, BC)\n\n        p_k = tl.make_block_ptr(\n            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + (boc * H + i_h) * V,\n            (TC, V),\n            (H * V, 1),\n            (i_c, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, b_k)\n        b_s = tl.where((o_c < NC)[None, :], b_s, float(\"-inf\"))\n\n        b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m\n        b_r = exp(b_mp - b_m)\n\n        b_p = exp(b_s - b_m[:, None])\n\n        b_acc = b_acc * b_r + tl.sum(b_p, 1)\n\n        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)\n\n        b_mp = b_m\n    if NC == 0:\n        b_lse = tl.zeros([G], dtype=tl.float32)\n    else:\n        b_o = b_o / b_acc[:, None]\n        b_lse = b_m + log(b_acc)\n\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    if i_v == 0:\n        tl.store(\n            lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G),\n            b_lse.to(lse.dtype.element_ty),\n        )", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 57, 6, 98, 217, 57, 6, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 149, 57, 30, -1, 226, 98, 227, 98, 228, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 229, 98, 230, 163, 202, 228, 45, 216, 98, 228, 188, 216, 149, 30, 155, 225, 57, 30, 231, 98, 226, 163, 202, 52, 202, 213, 67, 226, 203, 309, 149, 74, 232, 202, 56, 149, 98, 52, 202, 213, 67, 226, 203, 309, 67, 308, 149, 74, 232, 202, 56, 149, 149, 30, 233, 98, 234, 163, 202, 52, 202, 212, 67, 231, 149, 74, 232, 202, 56, 149, 98, 52, 202, 212, 67, 231, 67, 308, 149, 74, 232, 202, 56, 149, 149, 30, 215, 163, 234, 4, 233, 30, 235, 163, 52, 202, 214, 67, 231, 149, 74, 232, 202, 56, 149, 30, 157, 30, 29, 57, 30, 233, 98, 234, 163, 202, 229, 203, 215, 98, 229, 203, 215, 67, 215, 149, 30, 235, 163, 229, 203, 59, 202, 215, 98, 222, 149, 30, 51, 30, 236, 163, 183, 202, 206, 67, 202, 233, 67, 226, 149, 203, 217, 203, 219, 98, 202, 217, 98, 219, 149, 98, 202, 219, 98, 308, 149, 98, 202, 230, 203, 218, 98, 307, 149, 98, 202, 218, 98, 223, 149, 98, 202, 308, 98, 307, 149, 149, 30, 237, 163, 52, 202, 236, 98, 238, 163, 202, 307, 98, 308, 149, 149, 30, 237, 163, 202, 237, 203, 211, 149, 74, 232, 202, 237, 74, 82, 149, 30, 239, 163, 59, 202, 215, 98, 222, 149, 30, 240, 163, 202, 226, 67, 308, 149, 45, 222, 30, 241, 163, 183, 202, 209, 67, 202, 233, 67, 226, 149, 203, 217, 203, 220, 98, 202, 217, 98, 220, 149, 98, 202, 220, 98, 308, 149, 98, 202, 230, 203, 218, 98, 227, 203, 224, 149, 98, 202, 218, 98, 224, 149, 98, 202, 308, 98, 307, 149, 149, 30, 242, 163, 150, 202, 191, 218, 98, 224, 26, 98, 82, 163, 129, 149, 30, 243, 163, 197, 202, 191, 218, 26, 98, 244, 202, 310, 149, 98, 82, 163, 129, 149, 30, 245, 163, 150, 202, 191, 218, 26, 98, 82, 163, 129, 149, 30, 120, 246, 136, 5, 202, 307, 98, 240, 98, 221, 149, 57, 30, 247, 163, 246, 67, 68, 202, 307, 98, 221, 149, 30, 248, 163, 183, 202, 207, 67, 202, 235, 203, 216, 67, 230, 149, 203, 219, 98, 202, 219, 98, 239, 149, 98, 202, 308, 98, 216, 203, 219, 149, 98, 202, 307, 98, 246, 149, 98, 202, 223, 98, 221, 149, 98, 202, 307, 98, 308, 149, 149, 30, 249, 163, 183, 202, 208, 67, 202, 235, 203, 216, 67, 230, 149, 203, 220, 98, 202, 239, 98, 220, 149, 98, 202, 216, 203, 220, 98, 308, 149, 98, 202, 246, 98, 227, 203, 224, 149, 98, 202, 221, 98, 224, 149, 98, 202, 308, 98, 307, 149, 149, 30, 250, 163, 52, 202, 248, 98, 238, 163, 202, 307, 98, 308, 149, 149, 30, 251, 163, 52, 202, 249, 98, 238, 163, 202, 307, 98, 308, 149, 149, 30, 252, 163, 15, 202, 237, 98, 250, 149, 30, 252, 163, 171, 202, 202, 247, 1, 240, 149, 191, 168, 98, 57, 26, 98, 252, 98, 244, 202, 310, 149, 149, 30, 243, 98, 253, 163, 202, 162, 202, 243, 98, 12, 202, 252, 98, 308, 149, 149, 98, 243, 149, 30, 254, 163, 173, 202, 253, 4, 243, 149, 30, 255, 163, 173, 202, 252, 4, 243, 191, 57, 98, 168, 26, 149, 30, 245, 163, 245, 203, 254, 67, 185, 202, 255, 98, 308, 149, 30, 242, 163, 242, 203, 254, 191, 57, 98, 168, 26, 67, 15, 202, 255, 74, 232, 202, 237, 74, 82, 149, 98, 251, 149, 30, 253, 163, 243, 30, 70, 30, 155, 240, 69, 307, 57, 30, 256, 163, 150, 202, 191, 218, 26, 98, 82, 163, 129, 149, 30, 157, 30, 29, 57, 30, 242, 163, 242, 40, 245, 191, 57, 98, 168, 26, 30, 256, 163, 243, 67, 11, 202, 245, 149, 30, 51, 30, 10, 202, 241, 98, 242, 74, 232, 202, 241, 74, 82, 74, 103, 149, 98, 238, 163, 202, 307, 98, 308, 149, 149, 30, 155, 227, 69, 307, 57, 30, 10, 202, 210, 67, 202, 233, 67, 226, 149, 203, 217, 67, 230, 203, 218, 67, 68, 202, 307, 98, 218, 149, 98, 256, 74, 232, 202, 210, 74, 82, 74, 103, 149, 149, 30, 157, 30, 3, 30]}, {"code": "def parallel_nsa_compression_bwd_kernel_dq(\n    q,\n    k,\n    v,\n    lse,\n    delta,\n    do,\n    dq,\n    scale,\n    cu_seqlens,\n    token_indices,\n    chunk_offsets,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BC: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(\n            token_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        boc = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        boc = i_b * tl.cdiv(T, BS)\n\n    q += (bos + i_t) * HQ * K\n    do += (bos + i_t) * HQ * V\n    lse += (bos + i_t) * HQ\n    delta += (bos + i_t) * HQ\n    dq += (i_v * B * T + bos + i_t) * HQ * K\n\n    p_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\n    p_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    p_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\n    p_lse = lse + i_h * G + tl.arange(0, G)\n    p_delta = delta + i_h * G + tl.arange(0, G)\n\n    TC = tl.cdiv(T, BS)\n\n    NC = (i_t + 1) // BS\n\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n\n    b_lse = tl.load(p_lse)\n    b_delta = tl.load(p_delta)\n\n    b_dq = tl.zeros([G, BK], dtype=tl.float32)\n    for i_c in range(0, NC, BC):\n        o_c = i_c + tl.arange(0, BC)\n        p_k = tl.make_block_ptr(\n            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + (boc * H + i_h) * V,\n            (V, TC),\n            (1, H * V),\n            (i_v * BV, i_c),\n            (BV, BC),\n            (0, 1),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, b_k)\n        b_p = exp(b_s - b_lse[:, None])\n        b_p = tl.where((o_c < NC)[None, :], b_p, 0)\n\n        b_dp = tl.dot(b_do, b_v)\n        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])\n\n        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))\n    b_dq *= scale\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 97, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 97, 217, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 97, 224, 56, 6, 97, 225, 56, 6, 97, 226, 56, 6, 97, 227, 56, 6, 97, 228, 56, 6, 149, 56, 30, -1, 229, 97, 230, 97, 231, 163, 202, 145, 202, 307, 149, 97, 145, 202, 308, 149, 97, 145, 202, 309, 149, 149, 30, 232, 97, 233, 163, 202, 231, 45, 219, 97, 231, 188, 219, 149, 30, 155, 228, 56, 30, 234, 97, 229, 163, 202, 50, 202, 215, 66, 229, 203, 309, 149, 73, 235, 202, 205, 149, 97, 50, 202, 215, 66, 229, 203, 309, 66, 308, 149, 73, 235, 202, 205, 149, 149, 30, 236, 97, 237, 163, 202, 50, 202, 214, 66, 234, 149, 73, 235, 202, 205, 149, 97, 50, 202, 214, 66, 234, 66, 308, 149, 73, 235, 202, 205, 149, 149, 30, 217, 163, 237, 4, 236, 30, 238, 163, 50, 202, 216, 66, 234, 149, 73, 235, 202, 205, 149, 30, 157, 30, 29, 56, 30, 236, 97, 237, 163, 202, 232, 203, 217, 97, 232, 203, 217, 66, 217, 149, 30, 238, 163, 232, 203, 58, 202, 217, 97, 225, 149, 30, 51, 30, 206, 146, 202, 236, 66, 229, 149, 203, 220, 203, 222, 30, 211, 146, 202, 236, 66, 229, 149, 203, 220, 203, 223, 30, 209, 146, 202, 236, 66, 229, 149, 203, 220, 30, 210, 146, 202, 236, 66, 229, 149, 203, 220, 30, 212, 146, 202, 230, 203, 218, 203, 217, 66, 236, 66, 229, 149, 203, 220, 203, 222, 30, 239, 163, 183, 202, 206, 97, 202, 220, 97, 222, 149, 97, 202, 222, 97, 308, 149, 97, 202, 233, 203, 221, 97, 307, 149, 97, 202, 221, 97, 226, 149, 97, 202, 308, 97, 307, 149, 149, 30, 240, 163, 183, 202, 212, 97, 202, 220, 97, 222, 149, 97, 202, 222, 97, 308, 149, 97, 202, 233, 203, 221, 97, 307, 149, 97, 202, 221, 97, 226, 149, 97, 202, 308, 97, 307, 149, 149, 30, 241, 163, 50, 202, 239, 97, 242, 163, 202, 307, 97, 308, 149, 149, 30, 241, 163, 202, 241, 203, 213, 149, 73, 235, 202, 241, 73, 81, 149, 30, 243, 163, 183, 202, 211, 97, 202, 220, 97, 223, 149, 97, 202, 223, 97, 308, 149, 97, 202, 233, 203, 221, 97, 230, 203, 227, 149, 97, 202, 221, 97, 227, 149, 97, 202, 308, 97, 307, 149, 149, 30, 244, 163, 209, 66, 233, 203, 221, 66, 67, 202, 307, 97, 221, 149, 30, 245, 163, 210, 66, 233, 203, 221, 66, 67, 202, 307, 97, 221, 149, 30, 246, 163, 58, 202, 217, 97, 225, 149, 30, 247, 163, 202, 229, 66, 308, 149, 45, 225, 30, 248, 163, 50, 202, 243, 97, 242, 163, 202, 307, 97, 308, 149, 149, 30, 249, 163, 50, 202, 244, 149, 30, 250, 163, 50, 202, 245, 149, 30, 251, 163, 150, 202, 191, 221, 97, 226, 26, 97, 81, 163, 128, 149, 30, 119, 252, 136, 5, 202, 307, 97, 247, 97, 224, 149, 56, 30, 253, 163, 252, 66, 67, 202, 307, 97, 224, 149, 30, 254, 163, 183, 202, 207, 66, 202, 238, 203, 219, 66, 233, 149, 203, 222, 97, 202, 222, 97, 246, 149, 97, 202, 308, 97, 219, 203, 222, 149, 97, 202, 307, 97, 252, 149, 97, 202, 226, 97, 224, 149, 97, 202, 307, 97, 308, 149, 149, 30, 255, 163, 183, 202, 208, 66, 202, 238, 203, 219, 66, 233, 149, 203, 223, 97, 202, 223, 97, 246, 149, 97, 202, 308, 97, 219, 203, 223, 149, 97, 202, 230, 203, 227, 97, 252, 149, 97, 202, 227, 97, 224, 149, 97, 202, 307, 97, 308, 149, 149, 30, 256, 163, 50, 202, 254, 97, 242, 163, 202, 307, 97, 308, 149, 149, 30, 257, 163, 50, 202, 255, 97, 242, 163, 202, 307, 97, 308, 149, 149, 30, 258, 163, 15, 202, 241, 97, 256, 149, 30, 259, 163, 173, 202, 258, 4, 249, 191, 56, 97, 168, 26, 149, 30, 259, 163, 171, 202, 202, 253, 1, 247, 149, 191, 168, 97, 56, 26, 97, 259, 97, 307, 149, 30, 260, 163, 15, 202, 248, 97, 257, 149, 30, 261, 163, 259, 203, 202, 260, 73, 235, 202, 128, 149, 4, 250, 191, 56, 97, 168, 26, 149, 30, 251, 146, 15, 202, 261, 73, 235, 202, 256, 73, 81, 149, 97, 64, 202, 256, 149, 149, 30, 69, 30, 251, 23, 213, 30, 10, 202, 240, 97, 251, 73, 235, 202, 240, 73, 81, 73, 102, 149, 97, 242, 163, 202, 307, 97, 308, 149, 149, 30, 3, 30]}, {"code": "def parallel_nsa_compression_bwd_kernel_dkv(\n    q,\n    k,\n    v,\n    lse,\n    delta,\n    do,\n    dk,\n    dv,\n    cu_seqlens,\n    chunk_indices,\n    chunk_offsets,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BC: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_n, i_c = tl.load(chunk_indices + i_c * 2).to(tl.int32), tl.load(\n            chunk_indices + i_c * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        boc = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        boc = i_b * tl.cdiv(T, BS)\n\n    TC = tl.cdiv(T, BS)\n\n    p_k = tl.make_block_ptr(\n        k + (boc * H + i_h) * K, (TC, K), (H * K, 1), (i_c * BC, 0), (BC, BK), (1, 0)\n    )\n    p_v = tl.make_block_ptr(\n        v + (boc * H + i_h) * V,\n        (TC, V),\n        (H * V, 1),\n        (i_c * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n    p_dk = tl.make_block_ptr(\n        dk + (i_v * B * T * H + boc * H + i_h) * K,\n        (TC, K),\n        (H * K, 1),\n        (i_c * BC, 0),\n        (BC, BK),\n        (1, 0),\n    )\n    p_dv = tl.make_block_ptr(\n        dv + (i_v * B * T * H + boc * H + i_h) * V,\n        (TC, V),\n        (H * V, 1),\n        (i_c * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BC, BV], dtype=tl.float32)\n\n    for i in range(i_c * BC * BS, T):\n        o_c = i_c * BC + tl.arange(0, BC)\n\n        p_q = tl.make_block_ptr(\n            q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        p_do = tl.make_block_ptr(\n            do + (bos + i) * HQ * V,\n            (HQ, V),\n            (V, 1),\n            (i_h * G, i_v * BV),\n            (G, BV),\n            (1, 0),\n        )\n        p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n        p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_lse = tl.load(p_lse)\n        b_delta = tl.load(p_delta)\n\n        b_s = tl.dot(b_k, tl.trans(b_q))\n        b_p = exp(b_s - b_lse[None, :])\n        b_p = tl.where((i >= max(0, (o_c + 1) * BS - 1))[:, None], b_p, 0)\n\n        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)\n\n        b_dp = tl.dot(b_v, tl.trans(b_do))\n\n        b_ds = b_p * (b_dp - b_delta[None, :])\n\n        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\n\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 306, 202, 206, 98, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 98, 229, 57, 6, 149, 57, 30, -1, 230, 98, 231, 98, 232, 163, 202, 145, 202, 307, 149, 98, 145, 202, 308, 149, 98, 145, 202, 309, 149, 149, 30, 233, 98, 234, 163, 202, 232, 45, 220, 98, 232, 188, 220, 149, 30, 155, 229, 57, 30, 235, 98, 231, 163, 202, 52, 202, 215, 67, 231, 203, 309, 149, 74, 236, 202, 56, 149, 98, 52, 202, 215, 67, 231, 203, 309, 67, 308, 149, 74, 236, 202, 56, 149, 149, 30, 237, 98, 238, 163, 202, 52, 202, 214, 67, 235, 149, 74, 236, 202, 56, 149, 98, 52, 202, 214, 67, 235, 67, 308, 149, 74, 236, 202, 56, 149, 149, 30, 218, 163, 238, 4, 237, 30, 239, 163, 52, 202, 216, 67, 235, 149, 74, 236, 202, 56, 149, 30, 157, 30, 29, 57, 30, 237, 98, 238, 163, 202, 233, 203, 218, 98, 233, 203, 218, 67, 218, 149, 30, 239, 163, 233, 203, 59, 202, 218, 98, 226, 149, 30, 51, 30, 240, 163, 59, 202, 218, 98, 226, 149, 30, 241, 163, 183, 202, 207, 67, 202, 239, 203, 220, 67, 234, 149, 203, 223, 98, 202, 240, 98, 223, 149, 98, 202, 220, 203, 223, 98, 308, 149, 98, 202, 231, 203, 225, 98, 307, 149, 98, 202, 225, 98, 227, 149, 98, 202, 308, 98, 307, 149, 149, 30, 242, 163, 183, 202, 208, 67, 202, 239, 203, 220, 67, 234, 149, 203, 224, 98, 202, 240, 98, 224, 149, 98, 202, 220, 203, 224, 98, 308, 149, 98, 202, 231, 203, 225, 98, 230, 203, 228, 149, 98, 202, 225, 98, 228, 149, 98, 202, 308, 98, 307, 149, 149, 30, 243, 163, 183, 202, 212, 67, 202, 230, 203, 219, 203, 218, 203, 220, 67, 239, 203, 220, 67, 234, 149, 203, 223, 98, 202, 240, 98, 223, 149, 98, 202, 220, 203, 223, 98, 308, 149, 98, 202, 231, 203, 225, 98, 307, 149, 98, 202, 225, 98, 227, 149, 98, 202, 308, 98, 307, 149, 149, 30, 244, 163, 183, 202, 213, 67, 202, 230, 203, 219, 203, 218, 203, 220, 67, 239, 203, 220, 67, 234, 149, 203, 224, 98, 202, 240, 98, 224, 149, 98, 202, 220, 203, 224, 98, 308, 149, 98, 202, 231, 203, 225, 98, 230, 203, 228, 149, 98, 202, 225, 98, 228, 149, 98, 202, 308, 98, 307, 149, 149, 30, 245, 163, 52, 202, 241, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 247, 163, 150, 202, 191, 225, 98, 227, 26, 98, 82, 163, 129, 149, 30, 248, 163, 52, 202, 242, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 249, 163, 150, 202, 191, 225, 98, 228, 26, 98, 82, 163, 129, 149, 30, 120, 250, 136, 5, 202, 231, 203, 225, 203, 226, 98, 218, 149, 57, 30, 251, 163, 231, 203, 225, 67, 68, 202, 307, 98, 225, 149, 30, 252, 163, 183, 202, 206, 67, 202, 237, 67, 250, 149, 203, 221, 203, 223, 98, 202, 221, 98, 223, 149, 98, 202, 223, 98, 308, 149, 98, 202, 234, 203, 222, 98, 307, 149, 98, 202, 222, 98, 227, 149, 98, 202, 308, 98, 307, 149, 149, 30, 253, 163, 52, 202, 252, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 253, 163, 202, 253, 203, 217, 149, 74, 236, 202, 253, 74, 82, 149, 30, 254, 163, 183, 202, 211, 67, 202, 237, 67, 250, 149, 203, 221, 203, 224, 98, 202, 221, 98, 224, 149, 98, 202, 224, 98, 308, 149, 98, 202, 234, 203, 222, 98, 230, 203, 228, 149, 98, 202, 222, 98, 228, 149, 98, 202, 308, 98, 307, 149, 149, 30, 255, 163, 209, 67, 202, 237, 67, 250, 149, 203, 221, 67, 234, 203, 222, 67, 68, 202, 307, 98, 222, 149, 30, 256, 163, 210, 67, 202, 237, 67, 250, 149, 203, 221, 67, 234, 203, 222, 67, 68, 202, 307, 98, 222, 149, 30, 257, 163, 52, 202, 254, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 258, 163, 52, 202, 255, 149, 30, 259, 163, 52, 202, 256, 149, 30, 260, 163, 15, 202, 245, 98, 65, 202, 253, 149, 149, 30, 261, 163, 173, 202, 260, 4, 258, 191, 168, 98, 57, 26, 149, 30, 261, 163, 171, 202, 202, 250, 128, 46, 202, 307, 98, 202, 251, 67, 308, 149, 203, 226, 4, 308, 149, 149, 191, 57, 98, 168, 26, 98, 261, 98, 307, 149, 30, 249, 146, 15, 202, 261, 74, 236, 202, 257, 74, 82, 149, 98, 257, 149, 30, 262, 163, 15, 202, 248, 98, 65, 202, 257, 149, 149, 30, 263, 163, 261, 203, 202, 262, 4, 259, 191, 168, 98, 57, 26, 149, 30, 247, 146, 15, 202, 263, 74, 236, 202, 253, 74, 82, 149, 98, 253, 149, 30, 70, 30, 10, 202, 243, 98, 247, 74, 236, 202, 243, 74, 82, 74, 103, 149, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 10, 202, 244, 98, 249, 74, 236, 202, 244, 74, 82, 74, 103, 149, 98, 246, 163, 202, 307, 98, 308, 149, 149, 30, 3, 30]}, {"code": "def parallel_nsa_kernel_topk(\n    q,\n    k,\n    lse,\n    scale,\n    block_indices,\n    cu_seqlens,\n    token_indices,\n    chunk_offsets,\n    T,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    S: tl.constexpr,\n    BC: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(\n            token_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        boc = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        boc = i_b * tl.cdiv(T, BS)\n\n    p_q = tl.make_block_ptr(\n        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    TC = tl.cdiv(T, BS)\n\n    NC = (i_t + 1) // BS\n\n    if lse is not None:\n        b_lse = tl.load(lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G))\n    else:\n\n        b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)\n\n        b_acc = tl.zeros([G], dtype=tl.float32)\n        for i_c in range(0, NC, BC):\n            o_c = i_c + tl.arange(0, BC)\n\n            p_k = tl.make_block_ptr(\n                k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)\n            )\n\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n\n            b_s = tl.dot(b_q, b_k)\n            b_s = tl.where((o_c < NC)[None, :], b_s, float(\"-inf\"))\n\n            b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m\n            b_r = exp(b_mp - b_m)\n\n            b_p = exp(b_s - b_m[:, None])\n\n            b_acc = b_acc * b_r + tl.sum(b_p, 1)\n\n            b_mp = b_m\n        if NC == 0:\n            b_lse = tl.zeros([G], dtype=tl.float32)\n        else:\n            b_lse = b_m + log(b_acc)\n\n    b_i = tl.full([BC], -1, dtype=tl.float32)\n    o_i = tl.zeros([BC], dtype=tl.int32)\n    m_i = tl.arange(0, BC) < BC // 2\n    for i_c in range(0, i_t // BS + 1, BC):\n        o_c = i_c + tl.arange(0, BC)\n\n        p_k = tl.make_block_ptr(\n            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, b_k)\n        b_s = tl.where((i_t // BS > o_c)[None, :], b_s, float(\"-inf\"))\n\n        b_p = tl.where(\n            (i_t // BS == o_c)[None, :], float(1.0), exp(b_s - b_lse[:, None])\n        )\n\n        b_i, b_ip = tl.sum(b_p, 0), b_i\n        o_i, o_ip = tl.where(o_c <= i_t // BS, o_c + 1, 0), o_i\n\n        n_dims: tl.constexpr = tl.standard._log2(b_i.shape[0])\n        for i in tl.static_range(1, n_dims):\n            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), i, 2, n_dims)\n\n        if i_c != 0:\n            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, False, n_dims)\n            b_i_new = b_ip * m_i + b_i * (1 - m_i)\n            o_i_new = o_ip * m_i + o_i * (1 - m_i)\n            b_i, o_i = _bitonic_merge(\n                b_i_new, o_i_new.to(tl.int32), n_dims, True, n_dims\n            )\n        else:\n            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, True, n_dims)\n\n    m_top = tl.arange(0, BC // S) == 0\n    b_top = tl.sum(m_top[:, None] * tl.reshape(o_i - 1, [BC // S, S]), 0)\n\n    p_b = tl.make_block_ptr(\n        block_indices + (bos + i_t) * H * S, (H * S,), (1,), (i_h * S,), (S,), (0,)\n    )\n    tl.store(p_b, b_top.to(p_b.dtype.element_ty))", "encoded": [28, 307, 203, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 56, 6, 97, 217, 56, 6, 97, 218, 56, 6, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 97, 224, 56, 6, 149, 56, 30, -1, 225, 97, 226, 164, 203, 145, 203, 308, 149, 97, 145, 203, 309, 149, 149, 30, 227, 97, 228, 164, 203, 226, 45, 216, 97, 226, 189, 216, 149, 30, 155, 224, 56, 30, 229, 97, 225, 164, 203, 50, 203, 213, 66, 225, 204, 310, 149, 73, 230, 203, 206, 149, 97, 50, 203, 213, 66, 225, 204, 310, 66, 309, 149, 73, 230, 203, 206, 149, 149, 30, 231, 97, 232, 164, 203, 50, 203, 212, 66, 229, 149, 73, 230, 203, 206, 149, 97, 50, 203, 212, 66, 229, 66, 309, 149, 73, 230, 203, 206, 149, 149, 30, 215, 164, 232, 4, 231, 30, 233, 164, 50, 203, 214, 66, 229, 149, 73, 230, 203, 206, 149, 30, 158, 30, 29, 56, 30, 231, 97, 232, 164, 203, 227, 204, 215, 97, 227, 204, 215, 66, 215, 149, 30, 233, 164, 227, 204, 58, 203, 215, 97, 222, 149, 30, 51, 30, 234, 164, 184, 203, 207, 66, 203, 231, 66, 225, 149, 204, 217, 204, 219, 97, 203, 217, 97, 219, 149, 97, 203, 219, 97, 309, 149, 97, 203, 228, 204, 218, 97, 308, 149, 97, 203, 218, 97, 223, 149, 97, 203, 309, 97, 308, 149, 149, 30, 235, 164, 50, 203, 234, 97, 236, 164, 203, 308, 97, 309, 149, 149, 30, 235, 164, 203, 235, 204, 210, 149, 73, 230, 203, 235, 73, 81, 149, 30, 237, 164, 58, 203, 215, 97, 222, 149, 30, 238, 164, 203, 225, 66, 309, 149, 45, 222, 30, 155, 209, 99, 57, 169, 56, 30, 239, 164, 50, 203, 209, 66, 203, 231, 66, 225, 149, 204, 217, 66, 228, 204, 218, 66, 67, 203, 308, 97, 218, 149, 149, 30, 158, 30, 29, 56, 30, 240, 164, 198, 203, 192, 218, 26, 97, 241, 203, 311, 149, 97, 81, 164, 128, 149, 30, 242, 164, 150, 203, 192, 218, 26, 97, 81, 164, 128, 149, 30, 119, 243, 136, 5, 203, 308, 97, 238, 97, 221, 149, 56, 30, 244, 164, 243, 66, 67, 203, 308, 97, 221, 149, 30, 245, 164, 184, 203, 208, 66, 203, 233, 204, 216, 66, 228, 149, 204, 219, 97, 203, 219, 97, 237, 149, 97, 203, 309, 97, 216, 204, 219, 149, 97, 203, 308, 97, 243, 149, 97, 203, 223, 97, 221, 149, 97, 203, 308, 97, 309, 149, 149, 30, 246, 164, 50, 203, 245, 97, 236, 164, 203, 308, 97, 309, 149, 149, 30, 247, 164, 15, 203, 235, 97, 246, 149, 30, 247, 164, 172, 203, 203, 244, 1, 238, 149, 192, 169, 97, 56, 26, 97, 247, 97, 241, 203, 311, 149, 149, 30, 240, 97, 248, 164, 203, 163, 203, 240, 97, 12, 203, 247, 97, 309, 149, 149, 97, 240, 149, 30, 249, 164, 174, 203, 248, 4, 240, 149, 30, 250, 164, 174, 203, 247, 4, 240, 192, 56, 97, 169, 26, 149, 30, 242, 164, 242, 204, 249, 66, 186, 203, 250, 97, 309, 149, 30, 248, 164, 240, 30, 69, 30, 155, 238, 68, 308, 56, 30, 239, 164, 150, 203, 192, 218, 26, 97, 81, 164, 128, 149, 30, 158, 30, 29, 56, 30, 239, 164, 240, 66, 11, 203, 242, 149, 30, 51, 30, 51, 30, 251, 164, 198, 203, 192, 221, 26, 97, 4, 309, 97, 81, 164, 128, 149, 30, 252, 164, 150, 203, 192, 221, 26, 97, 81, 164, 206, 149, 30, 253, 164, 67, 203, 308, 97, 221, 149, 1, 221, 45, 310, 30, 119, 243, 136, 5, 203, 308, 97, 225, 45, 222, 66, 309, 97, 221, 149, 56, 30, 244, 164, 243, 66, 67, 203, 308, 97, 221, 149, 30, 245, 164, 184, 203, 208, 66, 203, 233, 204, 216, 66, 228, 149, 204, 219, 97, 203, 219, 97, 237, 149, 97, 203, 309, 97, 216, 204, 219, 149, 97, 203, 308, 97, 243, 149, 97, 203, 223, 97, 221, 149, 97, 203, 308, 97, 309, 149, 149, 30, 246, 164, 50, 203, 245, 97, 236, 164, 203, 308, 97, 309, 149, 149, 30, 247, 164, 15, 203, 235, 97, 246, 149, 30, 247, 164, 172, 203, 203, 225, 45, 222, 110, 244, 149, 192, 169, 97, 56, 26, 97, 247, 97, 241, 203, 311, 149, 149, 30, 250, 164, 172, 203, 203, 225, 45, 222, 68, 244, 149, 192, 169, 97, 56, 26, 97, 241, 203, 309, 149, 97, 174, 203, 247, 4, 239, 192, 56, 97, 169, 26, 149, 149, 30, 251, 97, 254, 164, 203, 186, 203, 250, 97, 308, 149, 97, 251, 149, 30, 252, 97, 255, 164, 203, 172, 203, 244, 183, 225, 45, 222, 97, 244, 66, 309, 97, 308, 149, 97, 252, 149, 30, 256, 56, 6, 164, 156, 203, 251, 73, 109, 192, 308, 26, 149, 30, 119, 257, 136, 176, 203, 309, 97, 256, 149, 56, 30, 251, 97, 252, 164, 258, 203, 251, 97, 252, 73, 230, 203, 206, 149, 97, 257, 97, 310, 97, 256, 149, 30, 69, 30, 155, 243, 159, 308, 56, 30, 251, 97, 252, 164, 258, 203, 251, 97, 252, 73, 230, 203, 206, 149, 97, 256, 97, 54, 97, 256, 149, 30, 259, 164, 254, 204, 253, 66, 251, 204, 203, 309, 4, 253, 149, 30, 260, 164, 255, 204, 253, 66, 252, 204, 203, 309, 4, 253, 149, 30, 251, 97, 252, 164, 258, 203, 259, 97, 260, 73, 230, 203, 206, 149, 97, 256, 97, 139, 97, 256, 149, 30, 158, 30, 29, 56, 30, 251, 97, 252, 164, 258, 203, 251, 97, 252, 73, 230, 203, 206, 149, 97, 256, 97, 139, 97, 256, 149, 30, 51, 30, 69, 30, 261, 164, 67, 203, 308, 97, 221, 45, 220, 149, 68, 308, 30, 262, 164, 186, 203, 261, 192, 56, 97, 169, 26, 204, 22, 203, 252, 4, 309, 97, 192, 221, 45, 220, 97, 220, 26, 149, 97, 308, 149, 30, 263, 164, 184, 203, 211, 66, 203, 231, 66, 225, 149, 204, 216, 204, 220, 97, 203, 216, 204, 220, 97, 149, 97, 203, 309, 97, 149, 97, 203, 228, 204, 220, 97, 149, 97, 203, 220, 97, 149, 97, 203, 308, 97, 149, 149, 30, 10, 203, 263, 97, 262, 73, 230, 203, 263, 73, 81, 73, 102, 149, 149, 30, 3, 30]}, {"code": "def parallel_nsa_fwd_kernel(\n    q,\n    k,\n    v,\n    o,\n    lse,\n    scale,\n    block_indices,\n    block_counts,\n    cu_seqlens,\n    token_indices,\n    T,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    S: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    USE_BLOCK_COUNTS: tl.constexpr,\n):\n    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(\n            token_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    k += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V\n    block_indices += (bos + i_t) * H * S + i_h * S\n\n    if USE_BLOCK_COUNTS:\n        NS = tl.load(block_counts + (bos + i_t) * H + i_h)\n    else:\n        NS = S\n\n    p_q = tl.make_block_ptr(\n        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    p_o = tl.make_block_ptr(\n        o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0)\n    )\n    p_lse = lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G)\n\n    b_o = tl.zeros([G, BV], dtype=tl.float32)\n\n    b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)\n    b_acc = tl.zeros([G], dtype=tl.float32)\n    for i in range(NS):\n        i_s = tl.load(block_indices + i).to(tl.int32) * BS\n        if i_s <= i_t and i_s >= 0:\n            p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))\n            p_v = tl.make_block_ptr(\n                v, (T, V), (H * V, 1), (i_s, i_v * BV), (BS, BV), (1, 0)\n            )\n\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n\n            b_s = tl.dot(b_q, b_k)\n            b_s = tl.where(\n                (i_t >= (i_s + tl.arange(0, BS)))[None, :], b_s, float(\"-inf\")\n            )\n\n            b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m\n            b_r = exp(b_mp - b_m)\n\n            b_p = exp(b_s - b_m[:, None])\n\n            b_acc = b_acc * b_r + tl.sum(b_p, 1)\n\n            b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)\n\n            b_mp = b_m\n    b_o = b_o / b_acc[:, None]\n    b_m += log(b_acc)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_lse, b_m.to(p_lse.dtype.element_ty))", "encoded": [28, 307, 203, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 149, 57, 30, -1, 229, 98, 230, 98, 231, 164, 203, 145, 203, 308, 149, 98, 145, 203, 309, 149, 98, 145, 203, 310, 149, 149, 30, 232, 98, 233, 164, 203, 231, 45, 218, 98, 231, 189, 218, 149, 30, 155, 227, 57, 30, 234, 98, 229, 164, 203, 52, 203, 216, 67, 229, 204, 310, 149, 74, 235, 203, 56, 149, 98, 52, 203, 216, 67, 229, 204, 310, 67, 309, 149, 74, 235, 203, 56, 149, 149, 30, 236, 98, 237, 164, 203, 52, 203, 215, 67, 234, 149, 74, 235, 203, 56, 149, 98, 52, 203, 215, 67, 234, 67, 309, 149, 74, 235, 203, 56, 149, 149, 30, 217, 164, 237, 4, 236, 30, 158, 30, 29, 57, 30, 236, 98, 237, 164, 203, 232, 204, 217, 98, 232, 204, 217, 67, 217, 149, 30, 51, 30, 208, 146, 203, 236, 204, 218, 67, 233, 149, 204, 221, 30, 209, 146, 203, 236, 204, 218, 67, 233, 149, 204, 222, 30, 213, 146, 203, 236, 67, 229, 149, 204, 218, 204, 223, 67, 233, 204, 223, 30, 155, 228, 57, 30, 238, 164, 52, 203, 214, 67, 203, 236, 67, 229, 149, 204, 218, 67, 233, 149, 30, 158, 30, 29, 57, 30, 238, 164, 223, 30, 51, 30, 239, 164, 184, 203, 207, 67, 203, 236, 67, 229, 149, 204, 219, 204, 221, 98, 203, 219, 98, 221, 149, 98, 203, 221, 98, 309, 149, 98, 203, 233, 204, 220, 98, 308, 149, 98, 203, 220, 98, 225, 149, 98, 203, 309, 98, 308, 149, 149, 30, 240, 164, 52, 203, 239, 98, 241, 164, 203, 308, 98, 309, 149, 149, 30, 240, 164, 203, 240, 204, 212, 149, 74, 235, 203, 240, 74, 82, 149, 30, 242, 164, 184, 203, 210, 67, 203, 236, 67, 229, 149, 204, 219, 204, 222, 98, 203, 219, 98, 222, 149, 98, 203, 222, 98, 309, 149, 98, 203, 233, 204, 220, 98, 230, 204, 226, 149, 98, 203, 220, 98, 226, 149, 98, 203, 309, 98, 308, 149, 149, 30, 243, 164, 211, 67, 203, 236, 67, 229, 149, 204, 219, 67, 233, 204, 220, 67, 68, 203, 308, 98, 220, 149, 30, 244, 164, 150, 203, 192, 220, 98, 226, 26, 98, 82, 164, 129, 149, 30, 245, 164, 198, 203, 192, 220, 26, 98, 246, 203, 311, 149, 98, 82, 164, 129, 149, 30, 247, 164, 150, 203, 192, 220, 26, 98, 82, 164, 129, 149, 30, 120, 248, 136, 5, 203, 238, 149, 57, 30, 249, 164, 52, 203, 213, 67, 248, 149, 74, 235, 203, 56, 149, 204, 224, 30, 155, 249, 183, 229, 92, 249, 128, 308, 57, 30, 250, 164, 184, 203, 208, 98, 203, 221, 98, 217, 149, 98, 203, 309, 98, 218, 204, 221, 149, 98, 203, 308, 98, 249, 149, 98, 203, 225, 98, 224, 149, 98, 203, 308, 98, 309, 149, 149, 30, 251, 164, 184, 203, 209, 98, 203, 217, 98, 222, 149, 98, 203, 218, 204, 222, 98, 309, 149, 98, 203, 249, 98, 230, 204, 226, 149, 98, 203, 224, 98, 226, 149, 98, 203, 309, 98, 308, 149, 149, 30, 252, 164, 52, 203, 250, 98, 241, 164, 203, 308, 98, 309, 149, 149, 30, 253, 164, 52, 203, 251, 98, 241, 164, 203, 308, 98, 309, 149, 149, 30, 254, 164, 15, 203, 240, 98, 252, 149, 30, 254, 164, 172, 203, 203, 229, 128, 249, 67, 68, 203, 308, 98, 224, 149, 149, 192, 169, 98, 57, 26, 98, 254, 98, 246, 203, 311, 149, 149, 30, 245, 98, 255, 164, 203, 163, 203, 245, 98, 12, 203, 254, 98, 309, 149, 149, 98, 245, 149, 30, 256, 164, 174, 203, 255, 4, 245, 149, 30, 257, 164, 174, 203, 254, 4, 245, 192, 57, 98, 169, 26, 149, 30, 247, 164, 247, 204, 256, 67, 186, 203, 257, 98, 309, 149, 30, 244, 164, 244, 204, 256, 192, 57, 98, 169, 26, 67, 15, 203, 257, 74, 235, 203, 240, 74, 82, 149, 98, 253, 149, 30, 255, 164, 245, 30, 158, 30, 70, 30, 244, 164, 244, 40, 247, 192, 57, 98, 169, 26, 30, 245, 146, 11, 203, 247, 149, 30, 10, 203, 242, 98, 244, 74, 235, 203, 242, 74, 82, 74, 103, 149, 98, 241, 164, 203, 308, 98, 309, 149, 149, 30, 10, 203, 243, 98, 245, 74, 235, 203, 243, 74, 82, 74, 103, 149, 149, 30, 3, 30]}, {"code": "def parallel_nsa_kernel_mask(\n    block_indices,\n    block_counts,\n    block_mask,\n    T,\n    H: tl.constexpr,\n    S: tl.constexpr,\n    BS: tl.constexpr,\n    NS: tl.constexpr,\n    USE_BLOCK_COUNTS: tl.constexpr,\n):\n    i_t, i_b, i_hs = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_s = i_hs // S, i_hs % S\n\n    b_i = tl.load(block_indices + i_b * T * H * S + i_t * H * S + i_h * S + i_s)\n    if USE_BLOCK_COUNTS:\n        b_m = b_i * BS <= i_t and i_s < tl.load(\n            block_counts + i_b * T * H + i_t * H + i_h\n        )\n    else:\n        b_m = b_i * BS <= i_t\n\n    if b_i < NS and b_i >= 0:\n        tl.store(\n            block_mask + i_b * T * H * NS + i_t * H * NS + i_h * NS + b_i,\n            b_m.to(block_mask.dtype.element_ty),\n        )", "encoded": [28, 307, 203, 207, 97, 208, 97, 209, 97, 210, 97, 211, 56, 6, 97, 212, 56, 6, 97, 213, 56, 6, 97, 214, 56, 6, 97, 215, 56, 6, 149, 56, 30, -1, 216, 97, 217, 97, 218, 164, 203, 145, 203, 308, 149, 97, 145, 203, 309, 149, 97, 145, 203, 310, 149, 149, 30, 219, 97, 220, 164, 203, 218, 45, 212, 97, 218, 189, 212, 149, 30, 221, 164, 50, 203, 207, 66, 217, 204, 210, 204, 211, 204, 212, 66, 216, 204, 211, 204, 212, 66, 219, 204, 212, 66, 220, 149, 30, 155, 215, 56, 30, 222, 164, 221, 204, 213, 183, 216, 91, 220, 1, 50, 203, 208, 66, 217, 204, 210, 204, 211, 66, 216, 204, 211, 66, 219, 149, 30, 158, 30, 29, 56, 30, 222, 164, 221, 204, 213, 183, 216, 30, 51, 30, 155, 221, 1, 214, 91, 221, 127, 308, 56, 30, 10, 203, 209, 66, 217, 204, 210, 204, 211, 204, 214, 66, 216, 204, 211, 204, 214, 66, 219, 204, 214, 66, 221, 97, 222, 73, 223, 203, 209, 73, 81, 73, 102, 149, 149, 30, 158, 30, 3, 30]}, {"code": "def parallel_nsa_bwd_kernel_dq(\n    q,\n    k,\n    v,\n    lse,\n    delta,\n    do,\n    dq,\n    scale,\n    block_indices,\n    block_counts,\n    cu_seqlens,\n    token_indices,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    S: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    USE_BLOCK_COUNTS: tl.constexpr,\n):\n    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(\n            token_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    q += (bos + i_t) * HQ * K\n    do += (bos + i_t) * HQ * V\n    lse += (bos + i_t) * HQ\n    delta += (bos + i_t) * HQ\n    dq += (i_v * B * T + bos + i_t) * HQ * K\n    block_indices += (bos + i_t) * H * S + i_h * S\n\n    if USE_BLOCK_COUNTS:\n        NS = tl.load(block_counts + (bos + i_t) * H + i_h)\n    else:\n        NS = S\n\n    k += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V\n\n    p_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\n    p_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    p_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\n    p_lse = lse + i_h * G + tl.arange(0, G)\n    p_delta = delta + i_h * G + tl.arange(0, G)\n\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n\n    b_lse = tl.load(p_lse)\n    b_delta = tl.load(p_delta)\n\n    b_dq = tl.zeros([G, BK], dtype=tl.float32)\n    for i in range(NS):\n        i_s = tl.load(block_indices + i).to(tl.int32) * BS\n        if i_s <= i_t and i_s >= 0:\n            p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))\n            p_v = tl.make_block_ptr(\n                v, (V, T), (1, H * V), (i_v * BV, i_s), (BV, BS), (0, 1)\n            )\n\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n\n            b_s = tl.dot(b_q, b_k)\n            b_p = exp(b_s - b_lse[:, None])\n            b_p = tl.where((i_t >= (i_s + tl.arange(0, BS)))[None, :], b_p, 0)\n\n            b_dp = tl.dot(b_do, b_v)\n            b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])\n\n            b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))\n    b_dq *= scale\n\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 307, 203, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 98, 229, 57, 6, 98, 230, 57, 6, 98, 231, 57, 6, 149, 57, 30, -1, 232, 98, 233, 98, 234, 164, 203, 145, 203, 308, 149, 98, 145, 203, 309, 149, 98, 145, 203, 310, 149, 149, 30, 235, 98, 236, 164, 203, 234, 45, 221, 98, 234, 189, 221, 149, 30, 155, 230, 57, 30, 237, 98, 232, 164, 203, 52, 203, 218, 67, 232, 204, 310, 149, 74, 238, 203, 56, 149, 98, 52, 203, 218, 67, 232, 204, 310, 67, 309, 149, 74, 238, 203, 56, 149, 149, 30, 239, 98, 240, 164, 203, 52, 203, 217, 67, 237, 149, 74, 238, 203, 56, 149, 98, 52, 203, 217, 67, 237, 67, 309, 149, 74, 238, 203, 56, 149, 149, 30, 219, 164, 240, 4, 239, 30, 158, 30, 29, 57, 30, 239, 98, 240, 164, 203, 235, 204, 219, 98, 235, 204, 219, 67, 219, 149, 30, 51, 30, 207, 146, 203, 239, 67, 232, 149, 204, 222, 204, 224, 30, 212, 146, 203, 239, 67, 232, 149, 204, 222, 204, 225, 30, 210, 146, 203, 239, 67, 232, 149, 204, 222, 30, 211, 146, 203, 239, 67, 232, 149, 204, 222, 30, 213, 146, 203, 233, 204, 220, 204, 219, 67, 239, 67, 232, 149, 204, 222, 204, 224, 30, 215, 146, 203, 239, 67, 232, 149, 204, 221, 204, 226, 67, 236, 204, 226, 30, 155, 231, 57, 30, 241, 164, 52, 203, 216, 67, 203, 239, 67, 232, 149, 204, 221, 67, 236, 149, 30, 158, 30, 29, 57, 30, 241, 164, 226, 30, 51, 30, 208, 146, 203, 239, 204, 221, 67, 236, 149, 204, 224, 30, 209, 146, 203, 239, 204, 221, 67, 236, 149, 204, 225, 30, 242, 164, 184, 203, 207, 98, 203, 222, 98, 224, 149, 98, 203, 224, 98, 309, 149, 98, 203, 236, 204, 223, 98, 308, 149, 98, 203, 223, 98, 228, 149, 98, 203, 309, 98, 308, 149, 149, 30, 243, 164, 184, 203, 213, 98, 203, 222, 98, 224, 149, 98, 203, 224, 98, 309, 149, 98, 203, 236, 204, 223, 98, 308, 149, 98, 203, 223, 98, 228, 149, 98, 203, 309, 98, 308, 149, 149, 30, 244, 164, 52, 203, 242, 98, 245, 164, 203, 308, 98, 309, 149, 149, 30, 244, 164, 203, 244, 204, 214, 149, 74, 238, 203, 244, 74, 82, 149, 30, 246, 164, 184, 203, 212, 98, 203, 222, 98, 225, 149, 98, 203, 225, 98, 309, 149, 98, 203, 236, 204, 223, 98, 233, 204, 229, 149, 98, 203, 223, 98, 229, 149, 98, 203, 309, 98, 308, 149, 149, 30, 247, 164, 210, 67, 236, 204, 223, 67, 68, 203, 308, 98, 223, 149, 30, 248, 164, 211, 67, 236, 204, 223, 67, 68, 203, 308, 98, 223, 149, 30, 249, 164, 52, 203, 246, 98, 245, 164, 203, 308, 98, 309, 149, 149, 30, 250, 164, 52, 203, 247, 149, 30, 251, 164, 52, 203, 248, 149, 30, 252, 164, 150, 203, 192, 223, 98, 228, 26, 98, 82, 164, 129, 149, 30, 120, 253, 136, 5, 203, 241, 149, 57, 30, 254, 164, 52, 203, 215, 67, 253, 149, 74, 238, 203, 56, 149, 204, 227, 30, 155, 254, 183, 232, 92, 254, 128, 308, 57, 30, 255, 164, 184, 203, 208, 98, 203, 224, 98, 219, 149, 98, 203, 309, 98, 221, 204, 224, 149, 98, 203, 308, 98, 254, 149, 98, 203, 228, 98, 227, 149, 98, 203, 308, 98, 309, 149, 149, 30, 256, 164, 184, 203, 209, 98, 203, 225, 98, 219, 149, 98, 203, 309, 98, 221, 204, 225, 149, 98, 203, 233, 204, 229, 98, 254, 149, 98, 203, 229, 98, 227, 149, 98, 203, 308, 98, 309, 149, 149, 30, 257, 164, 52, 203, 255, 98, 245, 164, 203, 308, 98, 309, 149, 149, 30, 258, 164, 52, 203, 256, 98, 245, 164, 203, 308, 98, 309, 149, 149, 30, 259, 164, 15, 203, 244, 98, 257, 149, 30, 260, 164, 174, 203, 259, 4, 250, 192, 57, 98, 169, 26, 149, 30, 260, 164, 172, 203, 203, 232, 128, 254, 67, 68, 203, 308, 98, 227, 149, 149, 192, 169, 98, 57, 26, 98, 260, 98, 308, 149, 30, 261, 164, 15, 203, 249, 98, 258, 149, 30, 262, 164, 260, 204, 203, 261, 74, 238, 203, 129, 149, 4, 251, 192, 57, 98, 169, 26, 149, 30, 252, 146, 15, 203, 262, 74, 238, 203, 257, 74, 82, 149, 98, 65, 203, 257, 149, 149, 30, 158, 30, 70, 30, 252, 23, 214, 30, 10, 203, 243, 98, 252, 74, 238, 203, 243, 74, 82, 74, 103, 149, 98, 245, 164, 203, 308, 98, 309, 149, 149, 30, 3, 30]}, {"code": "def parallel_nsa_bwd_kernel_dkv(\n    q,\n    k,\n    v,\n    lse,\n    delta,\n    do,\n    dk,\n    dv,\n    block_mask,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    M: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_s, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_n, i_s = tl.load(chunk_indices + i_s * 2).to(tl.int32), tl.load(\n            chunk_indices + i_s * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    p_k = tl.make_block_ptr(\n        k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_s * BS, 0), (BS, BK), (1, 0)\n    )\n    p_v = tl.make_block_ptr(\n        v + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_s * BS, i_v * BV),\n        (BS, BV),\n        (1, 0),\n    )\n    p_dk = tl.make_block_ptr(\n        dk + (i_v * B * T * H + bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_s * BS, 0),\n        (BS, BK),\n        (1, 0),\n    )\n    p_dv = tl.make_block_ptr(\n        dv + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_s * BS, i_v * BV),\n        (BS, BV),\n        (1, 0),\n    )\n\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dk = tl.zeros([BS, BK], dtype=tl.float32)\n\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BS, BV], dtype=tl.float32)\n\n    for i in range(i_s * BS, T):\n        b_m = tl.load(block_mask + (bos + i) * H * M + i_h * M + i_s)\n        if b_m:\n            p_q = tl.make_block_ptr(\n                q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)\n            )\n\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_q = (b_q * scale).to(b_q.dtype)\n\n            p_do = tl.make_block_ptr(\n                do + (bos + i) * HQ * V,\n                (HQ, V),\n                (V, 1),\n                (i_h * G, i_v * BV),\n                (G, BV),\n                (1, 0),\n            )\n            p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n            p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n\n            b_do = tl.load(p_do, boundary_check=(0, 1))\n\n            b_lse = tl.load(p_lse)\n            b_delta = tl.load(p_delta)\n\n            b_s = tl.dot(b_k, tl.trans(b_q))\n            b_p = exp(b_s - b_lse[None, :])\n            b_p = tl.where((i >= (i_s * BS + tl.arange(0, BS)))[:, None], b_p, 0)\n\n            b_dv += tl.dot(b_p.to(b_do.dtype), b_do)\n\n            b_dp = tl.dot(b_v, tl.trans(b_do))\n\n            b_ds = b_p * (b_dp - b_delta[None, :])\n\n            b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\n\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 307, 203, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 97, 217, 97, 218, 97, 219, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 97, 224, 56, 6, 97, 225, 56, 6, 97, 226, 56, 6, 97, 227, 56, 6, 97, 228, 56, 6, 97, 229, 56, 6, 97, 230, 56, 6, 149, 56, 30, -1, 231, 97, 232, 97, 233, 164, 203, 145, 203, 308, 149, 97, 145, 203, 309, 149, 97, 145, 203, 310, 149, 149, 30, 234, 97, 235, 164, 203, 233, 45, 221, 97, 233, 189, 221, 149, 30, 155, 230, 56, 30, 236, 97, 232, 164, 203, 50, 203, 217, 66, 232, 204, 310, 149, 73, 237, 203, 206, 149, 97, 50, 203, 217, 66, 232, 204, 310, 66, 309, 149, 73, 237, 203, 206, 149, 149, 30, 238, 97, 239, 164, 203, 50, 203, 216, 66, 236, 149, 73, 237, 203, 206, 149, 97, 50, 203, 216, 66, 236, 66, 309, 149, 73, 237, 203, 206, 149, 149, 30, 219, 164, 239, 4, 238, 30, 158, 30, 29, 56, 30, 238, 97, 239, 164, 203, 234, 204, 219, 97, 234, 204, 219, 66, 219, 149, 30, 51, 30, 240, 164, 184, 203, 208, 66, 203, 238, 204, 221, 66, 235, 149, 204, 224, 97, 203, 219, 97, 224, 149, 97, 203, 221, 204, 224, 97, 309, 149, 97, 203, 232, 204, 227, 97, 308, 149, 97, 203, 227, 97, 228, 149, 97, 203, 309, 97, 308, 149, 149, 30, 241, 164, 184, 203, 209, 66, 203, 238, 204, 221, 66, 235, 149, 204, 225, 97, 203, 219, 97, 225, 149, 97, 203, 221, 204, 225, 97, 309, 149, 97, 203, 232, 204, 227, 97, 231, 204, 229, 149, 97, 203, 227, 97, 229, 149, 97, 203, 309, 97, 308, 149, 149, 30, 242, 164, 184, 203, 213, 66, 203, 231, 204, 220, 204, 219, 204, 221, 66, 238, 204, 221, 66, 235, 149, 204, 224, 97, 203, 219, 97, 224, 149, 97, 203, 221, 204, 224, 97, 309, 149, 97, 203, 232, 204, 227, 97, 308, 149, 97, 203, 227, 97, 228, 149, 97, 203, 309, 97, 308, 149, 149, 30, 243, 164, 184, 203, 214, 66, 203, 238, 204, 221, 66, 235, 149, 204, 225, 97, 203, 219, 97, 225, 149, 97, 203, 221, 204, 225, 97, 309, 149, 97, 203, 232, 204, 227, 97, 231, 204, 229, 149, 97, 203, 227, 97, 229, 149, 97, 203, 309, 97, 308, 149, 149, 30, 244, 164, 50, 203, 240, 97, 245, 164, 203, 308, 97, 309, 149, 149, 30, 246, 164, 150, 203, 192, 227, 97, 228, 26, 97, 81, 164, 128, 149, 30, 247, 164, 50, 203, 241, 97, 245, 164, 203, 308, 97, 309, 149, 149, 30, 248, 164, 150, 203, 192, 227, 97, 229, 26, 97, 81, 164, 128, 149, 30, 119, 249, 136, 5, 203, 232, 204, 227, 97, 219, 149, 56, 30, 250, 164, 50, 203, 215, 66, 203, 238, 66, 249, 149, 204, 221, 204, 226, 66, 235, 204, 226, 66, 232, 149, 30, 155, 250, 56, 30, 251, 164, 184, 203, 207, 66, 203, 238, 66, 249, 149, 204, 222, 204, 224, 97, 203, 222, 97, 224, 149, 97, 203, 224, 97, 309, 149, 97, 203, 235, 204, 223, 97, 308, 149, 97, 203, 223, 97, 228, 149, 97, 203, 309, 97, 308, 149, 149, 30, 252, 164, 50, 203, 251, 97, 245, 164, 203, 308, 97, 309, 149, 149, 30, 252, 164, 203, 252, 204, 218, 149, 73, 237, 203, 252, 73, 81, 149, 30, 253, 164, 184, 203, 212, 66, 203, 238, 66, 249, 149, 204, 222, 204, 225, 97, 203, 222, 97, 225, 149, 97, 203, 225, 97, 309, 149, 97, 203, 235, 204, 223, 97, 231, 204, 229, 149, 97, 203, 223, 97, 229, 149, 97, 203, 309, 97, 308, 149, 149, 30, 254, 164, 210, 66, 203, 238, 66, 249, 149, 204, 222, 66, 235, 204, 223, 66, 67, 203, 308, 97, 223, 149, 30, 255, 164, 211, 66, 203, 238, 66, 249, 149, 204, 222, 66, 235, 204, 223, 66, 67, 203, 308, 97, 223, 149, 30, 256, 164, 50, 203, 253, 97, 245, 164, 203, 308, 97, 309, 149, 149, 30, 257, 164, 50, 203, 254, 149, 30, 258, 164, 50, 203, 255, 149, 30, 259, 164, 15, 203, 244, 97, 64, 203, 252, 149, 149, 30, 260, 164, 174, 203, 259, 4, 257, 192, 169, 97, 56, 26, 149, 30, 260, 164, 172, 203, 203, 249, 127, 232, 204, 227, 66, 67, 203, 308, 97, 227, 149, 149, 192, 56, 97, 169, 26, 97, 260, 97, 308, 149, 30, 248, 146, 15, 203, 260, 73, 237, 203, 256, 73, 81, 149, 97, 256, 149, 30, 261, 164, 15, 203, 247, 97, 64, 203, 256, 149, 149, 30, 262, 164, 260, 204, 203, 261, 4, 258, 192, 169, 97, 56, 26, 149, 30, 246, 146, 15, 203, 262, 73, 237, 203, 252, 73, 81, 149, 97, 252, 149, 30, 158, 30, 69, 30, 10, 203, 242, 97, 246, 73, 237, 203, 242, 73, 81, 73, 102, 149, 97, 245, 164, 203, 308, 97, 309, 149, 149, 30, 10, 203, 243, 97, 248, 73, 237, 203, 243, 73, 81, 73, 102, 149, 97, 245, 164, 203, 308, 97, 309, 149, 149, 30, 3, 30]}, {"code": "def chunk_cumprod_householder_bwd_kernel(\n    h,\n    hc_suffix,\n    dhc_whole,\n    dh,\n    k,\n    dk,\n    dk_new,\n    cu_seqlens,\n    split_indices,\n    chunk_offsets,\n    split_offsets,\n    BT: tl.constexpr,\n    K: tl.constexpr,\n    BK: tl.constexpr,\n    T: tl.constexpr,\n    S: tl.constexpr,\n    G: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_ss, i_hq = tl.program_id(0), tl.program_id(1)\n    i_h = i_hq // G\n\n    if IS_VARLEN:\n        i_n, i_s = tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(\n            split_indices + i_ss * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NS = tl.cdiv(T, S)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n        boh_large = tl.load(split_offsets + i_n).to(tl.int32)\n    else:\n        NS = tl.cdiv(T, S)\n        i_n, i_s = i_ss // NS, i_ss % NS\n        bos, eos = i_n * T, i_n * T + T\n        boh = i_n * tl.cdiv(T, BT)\n        boh_large = i_n * tl.cdiv(T, S)\n\n    h += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K\n    hc_suffix += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K\n    k += (bos * H + i_h) * K\n\n    dh += ((boh + tl.cdiv(i_s * S, BT)) * HQ + i_hq) * K * K\n    dhc_whole += ((boh_large + i_s) * HQ + i_hq) * K * K\n    dk += (bos * HQ + i_hq) * K\n    dk_new += (bos * HQ + i_hq) * K\n\n    stride_hq = HQ * K * K\n    stride_h = H * K * K\n    NT_small = tl.cdiv(min(S, T - i_s * S), BT)\n    p_dhc_whole = tl.make_block_ptr(dhc_whole, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    b_dhc = tl.zeros([BK, BK], dtype=tl.float32)\n    b_dhc += tl.load(p_dhc_whole, boundary_check=(0, 1))\n\n    for i_t_small in range(0, NT_small):\n        p_k = tl.make_block_ptr(\n            k, (T, K), (H * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)\n        )\n        p_dk = tl.make_block_ptr(\n            dk, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)\n        )\n        p_dk_new = tl.make_block_ptr(\n            dk_new, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)\n        )\n        p_hc = tl.make_block_ptr(\n            hc_suffix + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)\n        )\n        p_h = tl.make_block_ptr(\n            h + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)\n        )\n        p_dh = tl.make_block_ptr(\n            dh + i_t_small * stride_hq, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_dk = tl.load(p_dk, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_hc = tl.load(p_hc, boundary_check=(0, 1))\n        b_dk_new = b_dk - tl.dot(b_dk.to(b_hc.dtype), b_hc)\n        tl.store(p_dk_new, b_dk_new.to(dk.dtype.element_ty), boundary_check=(0, 1))\n        b_dh = b_dhc - tl.dot(tl.trans(b_hc), b_dhc.to(b_hc.dtype))\n        tl.store(p_dh, b_dh.to(dh.dtype.element_ty), boundary_check=(0, 1))\n        b_dhc = b_dhc - tl.dot(b_dhc.to(b_h.dtype), tl.trans(b_h))\n        b_dhc -= tl.dot(tl.trans(b_dk).to(b_k.dtype), b_k)", "encoded": [28, 307, 203, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 149, 57, 30, -1, 227, 98, 228, 164, 203, 145, 203, 308, 149, 98, 145, 203, 309, 149, 149, 30, 229, 164, 228, 45, 223, 30, 155, 226, 57, 30, 230, 98, 231, 164, 203, 52, 203, 215, 67, 227, 204, 310, 149, 74, 232, 203, 56, 149, 98, 52, 203, 215, 67, 227, 204, 310, 67, 309, 149, 74, 232, 203, 56, 149, 149, 30, 233, 98, 234, 164, 203, 52, 203, 214, 67, 230, 149, 74, 232, 203, 56, 149, 98, 52, 203, 214, 67, 230, 67, 309, 149, 74, 232, 203, 56, 149, 149, 30, 221, 164, 234, 4, 233, 30, 235, 164, 59, 203, 221, 98, 222, 149, 30, 236, 164, 52, 203, 216, 67, 230, 149, 74, 232, 203, 56, 149, 30, 237, 164, 52, 203, 217, 67, 230, 149, 74, 232, 203, 56, 149, 30, 158, 30, 29, 57, 30, 235, 164, 59, 203, 221, 98, 222, 149, 30, 230, 98, 231, 164, 203, 227, 45, 235, 98, 227, 189, 235, 149, 30, 233, 98, 234, 164, 203, 230, 204, 221, 98, 230, 204, 221, 67, 221, 149, 30, 236, 164, 230, 204, 59, 203, 221, 98, 218, 149, 30, 237, 164, 230, 204, 59, 203, 221, 98, 222, 149, 30, 51, 30, 207, 146, 203, 203, 236, 67, 59, 203, 231, 204, 222, 98, 218, 149, 149, 204, 224, 67, 229, 149, 204, 219, 204, 219, 30, 208, 146, 203, 203, 236, 67, 59, 203, 231, 204, 222, 98, 218, 149, 149, 204, 224, 67, 229, 149, 204, 219, 204, 219, 30, 211, 146, 203, 233, 204, 224, 67, 229, 149, 204, 219, 30, 210, 146, 203, 203, 236, 67, 59, 203, 231, 204, 222, 98, 218, 149, 149, 204, 225, 67, 228, 149, 204, 219, 204, 219, 30, 209, 146, 203, 203, 237, 67, 231, 149, 204, 225, 67, 228, 149, 204, 219, 204, 219, 30, 212, 146, 203, 233, 204, 225, 67, 228, 149, 204, 219, 30, 213, 146, 203, 233, 204, 225, 67, 228, 149, 204, 219, 30, 238, 164, 225, 204, 219, 204, 219, 30, 239, 164, 224, 204, 219, 204, 219, 30, 240, 164, 59, 203, 37, 203, 222, 98, 221, 4, 231, 204, 222, 149, 98, 218, 149, 30, 241, 164, 184, 203, 209, 98, 203, 219, 98, 219, 149, 98, 203, 219, 98, 309, 149, 98, 203, 308, 98, 308, 149, 98, 203, 220, 98, 220, 149, 98, 203, 309, 98, 308, 149, 149, 30, 242, 164, 150, 203, 192, 220, 98, 220, 26, 98, 82, 164, 129, 149, 30, 242, 146, 52, 203, 241, 98, 243, 164, 203, 308, 98, 309, 149, 149, 30, 120, 244, 136, 5, 203, 308, 98, 240, 149, 57, 30, 245, 164, 184, 203, 211, 98, 203, 221, 98, 219, 149, 98, 203, 224, 204, 219, 98, 309, 149, 98, 203, 231, 204, 222, 67, 244, 204, 218, 98, 308, 149, 98, 203, 218, 98, 220, 149, 98, 203, 309, 98, 308, 149, 149, 30, 246, 164, 184, 203, 212, 98, 203, 221, 98, 219, 149, 98, 203, 225, 204, 219, 98, 309, 149, 98, 203, 231, 204, 222, 67, 244, 204, 218, 98, 308, 149, 98, 203, 218, 98, 220, 149, 98, 203, 309, 98, 308, 149, 149, 30, 247, 164, 184, 203, 213, 98, 203, 221, 98, 219, 149, 98, 203, 225, 204, 219, 98, 309, 149, 98, 203, 231, 204, 222, 67, 244, 204, 218, 98, 308, 149, 98, 203, 218, 98, 220, 149, 98, 203, 309, 98, 308, 149, 149, 30, 248, 164, 184, 203, 208, 67, 244, 204, 239, 98, 203, 219, 98, 219, 149, 98, 203, 219, 98, 309, 149, 98, 203, 308, 98, 308, 149, 98, 203, 220, 98, 220, 149, 98, 203, 309, 98, 308, 149, 149, 30, 249, 164, 184, 203, 207, 67, 244, 204, 239, 98, 203, 219, 98, 219, 149, 98, 203, 219, 98, 309, 149, 98, 203, 308, 98, 308, 149, 98, 203, 220, 98, 220, 149, 98, 203, 309, 98, 308, 149, 149, 30, 250, 164, 184, 203, 210, 67, 244, 204, 238, 98, 203, 219, 98, 219, 149, 98, 203, 219, 98, 309, 149, 98, 203, 308, 98, 308, 149, 98, 203, 220, 98, 220, 149, 98, 203, 309, 98, 308, 149, 149, 30, 251, 164, 52, 203, 245, 98, 243, 164, 203, 308, 98, 309, 149, 149, 30, 252, 164, 52, 203, 246, 98, 243, 164, 203, 308, 98, 309, 149, 149, 30, 253, 164, 52, 203, 249, 98, 243, 164, 203, 308, 98, 309, 149, 149, 30, 254, 164, 52, 203, 248, 98, 243, 164, 203, 308, 98, 309, 149, 149, 30, 255, 164, 252, 4, 15, 203, 252, 74, 232, 203, 254, 74, 82, 149, 98, 254, 149, 30, 10, 203, 247, 98, 255, 74, 232, 203, 212, 74, 82, 74, 103, 149, 98, 243, 164, 203, 308, 98, 309, 149, 149, 30, 256, 164, 242, 4, 15, 203, 65, 203, 254, 149, 98, 242, 74, 232, 203, 254, 74, 82, 149, 149, 30, 10, 203, 250, 98, 256, 74, 232, 203, 210, 74, 82, 74, 103, 149, 98, 243, 164, 203, 308, 98, 309, 149, 149, 30, 242, 164, 242, 4, 15, 203, 242, 74, 232, 203, 253, 74, 82, 149, 98, 65, 203, 253, 149, 149, 30, 242, 2, 15, 203, 65, 203, 252, 149, 74, 232, 203, 251, 74, 82, 149, 98, 251, 149, 30, 70, 30, 3, 30]}, {"code": "def chunk_cumprod_householder_fwd_kernel(\n    q,\n    q_new,\n    k,\n    k_new,\n    h,\n    hc_suffix,\n    hc_prefix,\n    hc_whole,\n    cu_seqlens,\n    split_indices,\n    chunk_offsets,\n    split_offsets,\n    BT: tl.constexpr,\n    K: tl.constexpr,\n    G: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    BK: tl.constexpr,\n    T: tl.constexpr,\n    S: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_ss, i_hq = tl.program_id(0), tl.program_id(1)\n    i_h = i_hq // G\n\n    if IS_VARLEN:\n        i_n, i_s = tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(\n            split_indices + i_ss * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NS = tl.cdiv(T, S)\n\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n        boh_large = tl.load(split_offsets + i_n).to(tl.int32)\n    else:\n        NS = tl.cdiv(T, S)\n        i_n, i_s = i_ss // NS, i_ss % NS\n        bos, eos = i_n * T, i_n * T + T\n\n        boh = i_n * tl.cdiv(T, BT)\n        boh_large = i_n * tl.cdiv(T, S)\n\n    NT_small = tl.cdiv(min(S, T - i_s * S), BT)\n    stride_h = H * K * K\n\n    h += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K\n    hc_suffix += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K\n    hc_prefix += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K\n    hc_whole += ((boh_large + i_s) * H + i_h) * K * K\n\n    q += (bos * HQ + i_hq) * K\n    q_new += (bos * HQ + i_hq) * K\n    k += (bos * H + i_h) * K\n    k_new += (bos * H + i_h) * K\n\n    p_h = tl.make_block_ptr(h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    b_h = tl.zeros([BK, BK], dtype=tl.float32)\n    b_h += tl.load(p_h, boundary_check=(0, 1))\n\n    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_s * S, 0), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    p_q_new = tl.make_block_ptr(\n        q_new, (T, K), (HQ * K, 1), (i_s * S, 0), (BT, BK), (1, 0)\n    )\n    tl.store(p_q_new, b_q.to(q_new.dtype.element_ty), boundary_check=(0, 1))\n\n    p_hc_prefix = tl.make_block_ptr(hc_prefix, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    tl.store(\n        p_hc_prefix,\n        tl.zeros([BK, BK], dtype=tl.float32).to(p_hc_prefix.dtype.element_ty),\n        boundary_check=(0, 1),\n    )\n\n    for i_t_small in range(1, NT_small):\n        p_q = tl.make_block_ptr(\n            q, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)\n        )\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q - tl.dot(b_q, b_h.to(b_q.dtype))).to(b_q.dtype)\n        p_q_new = tl.make_block_ptr(\n            q_new, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)\n        )\n        tl.store(p_q_new, b_q.to(q_new.dtype.element_ty), boundary_check=(0, 1))\n        if HQ % G == 0:\n            p_hc_prefix = tl.make_block_ptr(\n                hc_prefix + i_t_small * stride_h,\n                (K, K),\n                (K, 1),\n                (0, 0),\n                (BK, BK),\n                (1, 0),\n            )\n            tl.store(\n                p_hc_prefix, b_h.to(hc_prefix.dtype.element_ty), boundary_check=(0, 1)\n            )\n        p_h_new = tl.make_block_ptr(\n            h + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)\n        )\n        b_h_new = tl.load(p_h_new, boundary_check=(0, 1))\n        b_h = b_h + b_h_new - tl.dot(b_h_new, b_h.to(b_h_new.dtype))\n\n    tl.debug_barrier()\n\n    if HQ % G == 0:\n        p_k = tl.make_block_ptr(\n            k, (T, K), (H * K, 1), (i_s * S + (NT_small - 1) * BT, 0), (BT, BK), (1, 0)\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        p_k_new = tl.make_block_ptr(\n            k_new,\n            (T, K),\n            (H * K, 1),\n            (i_s * S + (NT_small - 1) * BT, 0),\n            (BT, BK),\n            (1, 0),\n        )\n        tl.store(p_k_new, b_k.to(k_new.dtype.element_ty), boundary_check=(0, 1))\n        p_hc_suffix = tl.make_block_ptr(\n            hc_suffix + (NT_small - 1) * stride_h,\n            (K, K),\n            (K, 1),\n            (0, 0),\n            (BK, BK),\n            (1, 0),\n        )\n        tl.store(\n            p_hc_suffix,\n            tl.zeros([BK, BK], dtype=tl.float32).to(p_hc_suffix.dtype.element_ty),\n            boundary_check=(0, 1),\n        )\n\n        p_h = tl.make_block_ptr(\n            h + (NT_small - 1) * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)\n        )\n        b_h = tl.zeros([BK, BK], dtype=tl.float32)\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n\n        for i_t_small in range(NT_small - 2, -1, -1):\n            p_k = tl.make_block_ptr(\n                k, (T, K), (H * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)\n            )\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_k = (b_k - tl.dot(b_k, tl.trans(b_h).to(b_k.dtype))).to(b_k.dtype)\n            p_k_new = tl.make_block_ptr(\n                k_new,\n                (T, K),\n                (H * K, 1),\n                (i_s * S + i_t_small * BT, 0),\n                (BT, BK),\n                (1, 0),\n            )\n            tl.store(p_k_new, b_k.to(k_new.dtype.element_ty), boundary_check=(0, 1))\n            p_hc_suffix = tl.make_block_ptr(\n                hc_suffix + i_t_small * stride_h,\n                (K, K),\n                (K, 1),\n                (0, 0),\n                (BK, BK),\n                (1, 0),\n            )\n            tl.store(\n                p_hc_suffix, b_h.to(hc_suffix.dtype.element_ty), boundary_check=(0, 1)\n            )\n            p_h_new = tl.make_block_ptr(\n                h + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)\n            )\n            b_h_new = tl.load(p_h_new, boundary_check=(0, 1))\n            b_h = b_h + b_h_new - tl.dot(b_h.to(b_h_new.dtype), b_h_new)\n\n        p_hc_whole = tl.make_block_ptr(\n            hc_whole, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)\n        )\n        tl.store(p_hc_whole, b_h.to(hc_whole.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 307, 203, 207, 97, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 97, 217, 97, 218, 97, 219, 56, 6, 97, 220, 56, 6, 97, 221, 56, 6, 97, 222, 56, 6, 97, 223, 56, 6, 97, 224, 56, 6, 97, 225, 56, 6, 97, 226, 56, 6, 97, 227, 56, 6, 149, 56, 30, -1, 228, 97, 229, 164, 203, 145, 203, 308, 149, 97, 145, 203, 309, 149, 149, 30, 230, 164, 229, 45, 221, 30, 155, 227, 56, 30, 231, 97, 232, 164, 203, 50, 203, 216, 66, 228, 204, 310, 149, 73, 233, 203, 206, 149, 97, 50, 203, 216, 66, 228, 204, 310, 66, 309, 149, 73, 233, 203, 206, 149, 149, 30, 234, 97, 235, 164, 203, 50, 203, 215, 66, 231, 149, 73, 233, 203, 206, 149, 97, 50, 203, 215, 66, 231, 66, 309, 149, 73, 233, 203, 206, 149, 149, 30, 225, 164, 235, 4, 234, 30, 236, 164, 58, 203, 225, 97, 226, 149, 30, 237, 164, 50, 203, 217, 66, 231, 149, 73, 233, 203, 206, 149, 30, 238, 164, 50, 203, 218, 66, 231, 149, 73, 233, 203, 206, 149, 30, 158, 30, 29, 56, 30, 236, 164, 58, 203, 225, 97, 226, 149, 30, 231, 97, 232, 164, 203, 228, 45, 236, 97, 228, 189, 236, 149, 30, 234, 97, 235, 164, 203, 231, 204, 225, 97, 231, 204, 225, 66, 225, 149, 30, 237, 164, 231, 204, 58, 203, 225, 97, 219, 149, 30, 238, 164, 231, 204, 58, 203, 225, 97, 226, 149, 30, 51, 30, 239, 164, 58, 203, 37, 203, 226, 97, 225, 4, 232, 204, 226, 149, 97, 219, 149, 30, 240, 164, 222, 204, 220, 204, 220, 30, 211, 146, 203, 203, 237, 66, 58, 203, 232, 204, 226, 97, 219, 149, 149, 204, 222, 66, 230, 149, 204, 220, 204, 220, 30, 212, 146, 203, 203, 237, 66, 58, 203, 232, 204, 226, 97, 219, 149, 149, 204, 222, 66, 230, 149, 204, 220, 204, 220, 30, 213, 146, 203, 203, 237, 66, 58, 203, 232, 204, 226, 97, 219, 149, 149, 204, 222, 66, 230, 149, 204, 220, 204, 220, 30, 214, 146, 203, 203, 238, 66, 232, 149, 204, 222, 66, 230, 149, 204, 220, 204, 220, 30, 207, 146, 203, 234, 204, 223, 66, 229, 149, 204, 220, 30, 208, 146, 203, 234, 204, 223, 66, 229, 149, 204, 220, 30, 209, 146, 203, 234, 204, 222, 66, 230, 149, 204, 220, 30, 210, 146, 203, 234, 204, 222, 66, 230, 149, 204, 220, 30, 241, 164, 184, 203, 211, 97, 203, 220, 97, 220, 149, 97, 203, 220, 97, 309, 149, 97, 203, 308, 97, 308, 149, 97, 203, 224, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 242, 164, 150, 203, 192, 224, 97, 224, 26, 97, 81, 164, 128, 149, 30, 242, 146, 50, 203, 241, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 244, 164, 184, 203, 207, 97, 203, 225, 97, 220, 149, 97, 203, 223, 204, 220, 97, 309, 149, 97, 203, 232, 204, 226, 97, 308, 149, 97, 203, 219, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 245, 164, 50, 203, 244, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 246, 164, 184, 203, 208, 97, 203, 225, 97, 220, 149, 97, 203, 223, 204, 220, 97, 309, 149, 97, 203, 232, 204, 226, 97, 308, 149, 97, 203, 219, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 10, 203, 246, 97, 245, 73, 233, 203, 208, 73, 81, 73, 102, 149, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 247, 164, 184, 203, 213, 97, 203, 220, 97, 220, 149, 97, 203, 220, 97, 309, 149, 97, 203, 308, 97, 308, 149, 97, 203, 224, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 10, 203, 247, 97, 150, 203, 192, 224, 97, 224, 26, 97, 81, 164, 128, 149, 73, 233, 203, 247, 73, 81, 73, 102, 149, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 119, 248, 136, 5, 203, 309, 97, 239, 149, 56, 30, 244, 164, 184, 203, 207, 97, 203, 225, 97, 220, 149, 97, 203, 223, 204, 220, 97, 309, 149, 97, 203, 232, 204, 226, 66, 248, 204, 219, 97, 308, 149, 97, 203, 219, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 245, 164, 50, 203, 244, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 245, 164, 203, 245, 4, 15, 203, 245, 97, 242, 73, 233, 203, 245, 73, 81, 149, 149, 149, 73, 233, 203, 245, 73, 81, 149, 30, 246, 164, 184, 203, 208, 97, 203, 225, 97, 220, 149, 97, 203, 223, 204, 220, 97, 309, 149, 97, 203, 232, 204, 226, 66, 248, 204, 219, 97, 308, 149, 97, 203, 219, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 10, 203, 246, 97, 245, 73, 233, 203, 208, 73, 81, 73, 102, 149, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 155, 223, 189, 221, 68, 308, 56, 30, 247, 164, 184, 203, 213, 66, 248, 204, 240, 97, 203, 220, 97, 220, 149, 97, 203, 220, 97, 309, 149, 97, 203, 308, 97, 308, 149, 97, 203, 224, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 10, 203, 247, 97, 242, 73, 233, 203, 213, 73, 81, 73, 102, 149, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 158, 30, 249, 164, 184, 203, 211, 66, 248, 204, 240, 97, 203, 220, 97, 220, 149, 97, 203, 220, 97, 309, 149, 97, 203, 308, 97, 308, 149, 97, 203, 224, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 250, 164, 50, 203, 249, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 242, 164, 242, 66, 250, 4, 15, 203, 250, 97, 242, 73, 233, 203, 250, 73, 81, 149, 149, 30, 69, 30, 47, 203, 149, 30, 155, 223, 189, 221, 68, 308, 56, 30, 251, 164, 184, 203, 209, 97, 203, 225, 97, 220, 149, 97, 203, 222, 204, 220, 97, 309, 149, 97, 203, 232, 204, 226, 66, 203, 239, 4, 309, 149, 204, 219, 97, 308, 149, 97, 203, 219, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 252, 164, 50, 203, 251, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 253, 164, 184, 203, 210, 97, 203, 225, 97, 220, 149, 97, 203, 222, 204, 220, 97, 309, 149, 97, 203, 232, 204, 226, 66, 203, 239, 4, 309, 149, 204, 219, 97, 308, 149, 97, 203, 219, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 10, 203, 253, 97, 252, 73, 233, 203, 210, 73, 81, 73, 102, 149, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 254, 164, 184, 203, 212, 66, 203, 239, 4, 309, 149, 204, 240, 97, 203, 220, 97, 220, 149, 97, 203, 220, 97, 309, 149, 97, 203, 308, 97, 308, 149, 97, 203, 224, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 10, 203, 254, 97, 150, 203, 192, 224, 97, 224, 26, 97, 81, 164, 128, 149, 73, 233, 203, 254, 73, 81, 73, 102, 149, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 241, 164, 184, 203, 211, 66, 203, 239, 4, 309, 149, 204, 240, 97, 203, 220, 97, 220, 149, 97, 203, 220, 97, 309, 149, 97, 203, 308, 97, 308, 149, 97, 203, 224, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 242, 164, 150, 203, 192, 224, 97, 224, 26, 97, 81, 164, 128, 149, 30, 242, 146, 50, 203, 241, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 119, 248, 136, 5, 203, 239, 4, 310, 97, 4, 309, 97, 4, 309, 149, 56, 30, 251, 164, 184, 203, 209, 97, 203, 225, 97, 220, 149, 97, 203, 222, 204, 220, 97, 309, 149, 97, 203, 232, 204, 226, 66, 248, 204, 219, 97, 308, 149, 97, 203, 219, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 252, 164, 50, 203, 251, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 252, 164, 203, 252, 4, 15, 203, 252, 97, 64, 203, 242, 149, 73, 233, 203, 252, 73, 81, 149, 149, 149, 73, 233, 203, 252, 73, 81, 149, 30, 253, 164, 184, 203, 210, 97, 203, 225, 97, 220, 149, 97, 203, 222, 204, 220, 97, 309, 149, 97, 203, 232, 204, 226, 66, 248, 204, 219, 97, 308, 149, 97, 203, 219, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 10, 203, 253, 97, 252, 73, 233, 203, 210, 73, 81, 73, 102, 149, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 254, 164, 184, 203, 212, 66, 248, 204, 240, 97, 203, 220, 97, 220, 149, 97, 203, 220, 97, 309, 149, 97, 203, 308, 97, 308, 149, 97, 203, 224, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 10, 203, 254, 97, 242, 73, 233, 203, 212, 73, 81, 73, 102, 149, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 249, 164, 184, 203, 211, 66, 248, 204, 240, 97, 203, 220, 97, 220, 149, 97, 203, 220, 97, 309, 149, 97, 203, 308, 97, 308, 149, 97, 203, 224, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 250, 164, 50, 203, 249, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 242, 164, 242, 66, 250, 4, 15, 203, 242, 73, 233, 203, 250, 73, 81, 149, 97, 250, 149, 30, 69, 30, 255, 164, 184, 203, 214, 97, 203, 220, 97, 220, 149, 97, 203, 220, 97, 309, 149, 97, 203, 308, 97, 308, 149, 97, 203, 224, 97, 224, 149, 97, 203, 309, 97, 308, 149, 149, 30, 10, 203, 255, 97, 242, 73, 233, 203, 214, 73, 81, 73, 102, 149, 97, 243, 164, 203, 308, 97, 309, 149, 149, 30, 158, 30, 3, 30]}, {"code": "def intra_chunk_preprocess_bwd_kernel(\n    q,\n    k,\n    w,\n    beta,\n    AT,\n    dA_local,\n    dq,\n    dq_new,\n    dk,\n    dk_new,\n    dw,\n    dbeta,\n    dh,\n    T,\n    offsets,\n    indices,\n    chunk_offsets,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_nh = tl.program_id(0), tl.program_id(1)\n    i_n, i_hq = i_nh // HQ, i_nh % HQ\n    i_h = i_hq // G\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(\n            indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(\n            tl.int32\n        )\n        T = eos - bos\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dw_beta = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dw = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dT = tl.zeros([BT, BT], dtype=tl.float32)\n\n    p_q = tl.make_block_ptr(\n        q + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    p_k = tl.make_block_ptr(\n        k + (bos * H + i_h) * K, (T, K), (K * H, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    p_w = tl.make_block_ptr(\n        w + (bos * H + i_h) * K, (T, K), (K * H, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    p_beta = tl.make_block_ptr(\n        beta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)\n    )\n    p_T = tl.make_block_ptr(\n        AT + (bos * H + i_h) * BT, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n    b_w = tl.load(p_w, boundary_check=(0, 1))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_T = tl.load(p_T, boundary_check=(0, 1)).to(b_k.dtype)\n    b_w_beta = (b_w * b_beta[:, None]).to(b_w.dtype)\n\n    o_i = tl.arange(0, BT)\n    b_qw = tl.where(o_i[:, None] >= o_i[None, :], tl.dot(b_q, tl.trans(b_w)), 0).to(\n        b_q.dtype\n    )\n    b_wbk = tl.where(\n        o_i[:, None] > o_i[None, :], tl.dot(b_w_beta, tl.trans(b_k)), 0\n    ).to(b_k.dtype)\n    b_Twb = tl.dot(b_T, b_w_beta).to(b_w.dtype)\n    b_Twbk = tl.dot(b_T, b_wbk).to(b_w.dtype)\n\n    p_dA_local = tl.make_block_ptr(\n        dA_local + (bos * HQ + i_hq) * BT,\n        (T, BT),\n        (BT * HQ, 1),\n        (i_t * BT, 0),\n        (BT, BT),\n        (1, 0),\n    )\n    b_dA_local = tl.load(p_dA_local, boundary_check=(0, 1))\n\n    p_dq = tl.make_block_ptr(\n        dq + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    b_dq = tl.load(p_dq, boundary_check=(0, 1)).to(b_w.dtype)\n\n    p_dh = tl.make_block_ptr(\n        dh + ((boh + i_t) * HQ + i_hq) * K * K, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)\n    )\n    b_dh = tl.load(p_dh, boundary_check=(0, 1)).to(b_w.dtype)\n    b_dw += tl.dot(b_Twb, tl.trans(b_dh))\n    b_dqw = -tl.dot(b_dA_local, tl.trans(b_Twbk)) - tl.dot(\n        b_dq.to(b_Twb.dtype), tl.trans(b_Twb)\n    )\n    b_dTwb = (-tl.dot(tl.trans(b_qw), b_dq) + tl.dot(b_w, b_dh)).to(b_w.dtype)\n    b_dT += tl.dot(b_dTwb, tl.trans(b_w_beta))\n    b_dw_beta += tl.dot(tl.trans(b_T), b_dTwb)\n\n    b_dqw = tl.where(tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_dqw, 0)\n    b_dq += tl.dot(b_dA_local.to(b_k.dtype), b_k)\n    b_dq += tl.dot(b_dqw.to(b_w.dtype), b_w)\n    b_dw += tl.dot(tl.trans(b_dqw.to(b_q.dtype)), b_q)\n    p_q_new = tl.make_block_ptr(\n        dq_new + (bos * HQ + i_hq) * K,\n        (T, K),\n        (K * HQ, 1),\n        (i_t * BT, 0),\n        (BT, BK),\n        (1, 0),\n    )\n    tl.store(p_q_new, b_dq.to(dq_new.dtype.element_ty), boundary_check=(0, 1))\n\n    p_dk = tl.make_block_ptr(\n        dk + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    b_dk = tl.load(p_dk, boundary_check=(0, 1))\n    b_dTwbk = -tl.dot(tl.trans(b_qw), b_dA_local.to(b_qw.dtype)) - tl.dot(\n        b_w, tl.trans(b_dk.to(b_w.dtype))\n    )\n    b_dw -= tl.dot(b_Twbk, b_dk.to(b_w.dtype))\n    b_dT += tl.dot(b_dTwbk.to(b_wbk.dtype), tl.trans(b_wbk))\n    b_dwbk = tl.where(\n        o_i[:, None] > o_i[None, :], tl.dot(tl.trans(b_T), b_dTwbk.to(b_T.dtype)), 0\n    ).to(b_w.dtype)\n    b_dw_beta += tl.dot(b_dwbk, b_k)\n\n    b_dk += tl.dot(tl.trans(b_dwbk), b_w_beta)\n    b_dk += tl.dot(tl.trans(b_dA_local), b_q)\n    p_dk_new = tl.make_block_ptr(\n        dk_new + (bos * HQ + i_hq) * K,\n        (T, K),\n        (K * HQ, 1),\n        (i_t * BT, 0),\n        (BT, BK),\n        (1, 0),\n    )\n    tl.store(p_dk_new, b_dk.to(dk_new.dtype.element_ty), boundary_check=(0, 1))\n\n    p_T = tl.make_block_ptr(\n        AT + (bos * H + i_h) * BT, (BT, T), (1, BT * H), (0, i_t * BT), (BT, BT), (0, 1)\n    )\n    b_Tt = tl.load(p_T, boundary_check=(0, 1))\n    b_dT = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dT, 0).to(\n        b_w.dtype\n    )\n    b_dT = tl.dot(b_Tt, b_dT).to(b_w.dtype)\n    b_dT = tl.dot(b_dT, b_Tt)\n    b_dT = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dT, 0).to(\n        b_k.dtype\n    )\n\n    b_dw_beta += tl.dot(b_dT, b_w)\n    b_dw += tl.dot(tl.trans(b_dT), b_w_beta)\n    b_dw += b_dw_beta * b_beta[:, None]\n    b_dbeta = tl.sum(b_dw_beta * b_w, axis=1)\n\n    p_dw = tl.make_block_ptr(\n        dw + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    tl.store(p_dw, b_dw.to(dw.dtype.element_ty), boundary_check=(0, 1))\n    p_dbeta = tl.make_block_ptr(\n        dbeta + (bos * HQ + i_hq), (T,), (HQ,), (i_t * BT,), (BT,), (0,)\n    )\n    tl.store(p_dbeta, b_dbeta.to(dbeta.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 307, 203, 207, 98, 208, 98, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 98, 229, 57, 6, 98, 230, 57, 6, 149, 57, 30, -1, 231, 98, 232, 164, 203, 145, 203, 308, 149, 98, 145, 203, 309, 149, 149, 30, 233, 98, 234, 164, 203, 232, 45, 224, 98, 232, 189, 224, 149, 30, 235, 164, 234, 45, 225, 30, 155, 230, 57, 30, 233, 98, 231, 164, 203, 52, 203, 222, 67, 231, 204, 310, 149, 74, 236, 203, 56, 149, 98, 52, 203, 222, 67, 231, 204, 310, 67, 309, 149, 74, 236, 203, 56, 149, 149, 30, 237, 98, 238, 164, 203, 52, 203, 221, 67, 233, 149, 74, 236, 203, 56, 149, 98, 52, 203, 221, 67, 233, 67, 309, 149, 74, 236, 203, 56, 149, 149, 30, 220, 164, 238, 4, 237, 30, 239, 164, 52, 203, 223, 67, 233, 149, 74, 236, 203, 56, 149, 30, 158, 30, 29, 57, 30, 237, 98, 238, 164, 203, 233, 204, 220, 98, 233, 204, 220, 67, 220, 149, 30, 240, 164, 59, 203, 220, 98, 228, 149, 30, 239, 164, 233, 204, 240, 30, 51, 30, 241, 164, 150, 203, 192, 228, 98, 229, 26, 98, 82, 164, 129, 149, 30, 242, 164, 150, 203, 192, 228, 98, 229, 26, 98, 82, 164, 129, 149, 30, 243, 164, 150, 203, 192, 228, 98, 229, 26, 98, 82, 164, 129, 149, 30, 244, 164, 150, 203, 192, 228, 98, 228, 26, 98, 82, 164, 129, 149, 30, 245, 164, 184, 203, 207, 67, 203, 237, 204, 224, 67, 234, 149, 204, 227, 98, 203, 220, 98, 227, 149, 98, 203, 227, 204, 224, 98, 309, 149, 98, 203, 231, 204, 228, 98, 308, 149, 98, 203, 228, 98, 229, 149, 98, 203, 309, 98, 308, 149, 149, 30, 246, 164, 184, 203, 208, 67, 203, 237, 204, 226, 67, 235, 149, 204, 227, 98, 203, 220, 98, 227, 149, 98, 203, 227, 204, 226, 98, 309, 149, 98, 203, 231, 204, 228, 98, 308, 149, 98, 203, 228, 98, 229, 149, 98, 203, 309, 98, 308, 149, 149, 30, 247, 164, 184, 203, 209, 67, 203, 237, 204, 226, 67, 235, 149, 204, 227, 98, 203, 220, 98, 227, 149, 98, 203, 227, 204, 226, 98, 309, 149, 98, 203, 231, 204, 228, 98, 308, 149, 98, 203, 228, 98, 229, 149, 98, 203, 309, 98, 308, 149, 149, 30, 248, 164, 184, 203, 210, 67, 203, 237, 204, 226, 67, 235, 149, 98, 203, 220, 98, 149, 98, 203, 226, 98, 149, 98, 203, 231, 204, 228, 98, 149, 98, 203, 228, 98, 149, 98, 203, 308, 98, 149, 149, 30, 249, 164, 184, 203, 211, 67, 203, 237, 204, 226, 67, 235, 149, 204, 228, 98, 203, 220, 98, 228, 149, 98, 203, 228, 204, 226, 98, 309, 149, 98, 203, 231, 204, 228, 98, 308, 149, 98, 203, 228, 98, 228, 149, 98, 203, 309, 98, 308, 149, 149, 30, 250, 164, 52, 203, 247, 98, 251, 164, 203, 308, 98, 309, 149, 149, 30, 252, 164, 52, 203, 248, 98, 251, 164, 203, 308, 98, 149, 149, 30, 253, 164, 52, 203, 245, 98, 251, 164, 203, 308, 98, 309, 149, 149, 30, 254, 164, 52, 203, 246, 98, 251, 164, 203, 308, 98, 309, 149, 149, 30, 255, 164, 52, 203, 249, 98, 251, 164, 203, 308, 98, 309, 149, 149, 74, 236, 203, 254, 74, 82, 149, 30, 256, 164, 203, 250, 204, 252, 192, 57, 98, 169, 26, 149, 74, 236, 203, 250, 74, 82, 149, 30, 257, 164, 68, 203, 308, 98, 228, 149, 30, 258, 164, 172, 203, 257, 192, 57, 98, 169, 26, 128, 257, 192, 169, 98, 57, 26, 98, 15, 203, 253, 98, 65, 203, 250, 149, 149, 98, 308, 149, 74, 236, 203, 253, 74, 82, 149, 30, 259, 164, 172, 203, 257, 192, 57, 98, 169, 26, 111, 257, 192, 169, 98, 57, 26, 98, 15, 203, 256, 98, 65, 203, 254, 149, 149, 98, 308, 149, 74, 236, 203, 254, 74, 82, 149, 30, 260, 164, 15, 203, 255, 98, 256, 149, 74, 236, 203, 250, 74, 82, 149, 30, 261, 164, 15, 203, 255, 98, 259, 149, 74, 236, 203, 250, 74, 82, 149, 30, 262, 164, 184, 203, 212, 67, 203, 237, 204, 224, 67, 234, 149, 204, 228, 98, 203, 220, 98, 228, 149, 98, 203, 228, 204, 224, 98, 309, 149, 98, 203, 231, 204, 228, 98, 308, 149, 98, 203, 228, 98, 228, 149, 98, 203, 309, 98, 308, 149, 149, 30, 263, 164, 52, 203, 262, 98, 251, 164, 203, 308, 98, 309, 149, 149, 30, 264, 164, 184, 203, 213, 67, 203, 237, 204, 224, 67, 234, 149, 204, 227, 98, 203, 220, 98, 227, 149, 98, 203, 227, 204, 224, 98, 309, 149, 98, 203, 231, 204, 228, 98, 308, 149, 98, 203, 228, 98, 229, 149, 98, 203, 309, 98, 308, 149, 149, 30, 265, 164, 52, 203, 264, 98, 251, 164, 203, 308, 98, 309, 149, 149, 74, 236, 203, 250, 74, 82, 149, 30, 266, 164, 184, 203, 219, 67, 203, 203, 239, 67, 231, 149, 204, 224, 67, 234, 149, 204, 227, 204, 227, 98, 203, 227, 98, 227, 149, 98, 203, 227, 98, 309, 149, 98, 203, 308, 98, 308, 149, 98, 203, 229, 98, 229, 149, 98, 203, 309, 98, 308, 149, 149, 30, 267, 164, 52, 203, 266, 98, 251, 164, 203, 308, 98, 309, 149, 149, 74, 236, 203, 250, 74, 82, 149, 30, 243, 146, 15, 203, 260, 98, 65, 203, 267, 149, 149, 30, 268, 164, 4, 15, 203, 263, 98, 65, 203, 261, 149, 149, 4, 15, 203, 265, 74, 236, 203, 260, 74, 82, 149, 98, 65, 203, 260, 149, 149, 30, 269, 164, 203, 4, 15, 203, 65, 203, 258, 149, 98, 265, 149, 67, 15, 203, 250, 98, 267, 149, 149, 74, 236, 203, 250, 74, 82, 149, 30, 244, 146, 15, 203, 269, 98, 65, 203, 256, 149, 149, 30, 242, 146, 15, 203, 65, 203, 255, 149, 98, 269, 149, 30, 268, 164, 172, 203, 68, 203, 308, 98, 228, 149, 192, 57, 98, 169, 26, 128, 68, 203, 308, 98, 228, 149, 192, 169, 98, 57, 26, 98, 268, 98, 308, 149, 30, 265, 146, 15, 203, 263, 74, 236, 203, 254, 74, 82, 149, 98, 254, 149, 30, 265, 146, 15, 203, 268, 74, 236, 203, 250, 74, 82, 149, 98, 250, 149, 30, 243, 146, 15, 203, 65, 203, 268, 74, 236, 203, 253, 74, 82, 149, 149, 98, 253, 149, 30, 270, 164, 184, 203, 214, 67, 203, 237, 204, 224, 67, 234, 149, 204, 227, 98, 203, 220, 98, 227, 149, 98, 203, 227, 204, 224, 98, 309, 149, 98, 203, 231, 204, 228, 98, 308, 149, 98, 203, 228, 98, 229, 149, 98, 203, 309, 98, 308, 149, 149, 30, 10, 203, 270, 98, 265, 74, 236, 203, 214, 74, 82, 74, 103, 149, 98, 251, 164, 203, 308, 98, 309, 149, 149, 30, 271, 164, 184, 203, 215, 67, 203, 237, 204, 224, 67, 234, 149, 204, 227, 98, 203, 220, 98, 227, 149, 98, 203, 227, 204, 224, 98, 309, 149, 98, 203, 231, 204, 228, 98, 308, 149, 98, 203, 228, 98, 229, 149, 98, 203, 309, 98, 308, 149, 149, 30, 241, 164, 52, 203, 271, 98, 251, 164, 203, 308, 98, 309, 149, 149, 30, 272, 164, 4, 15, 203, 65, 203, 258, 149, 98, 263, 74, 236, 203, 258, 74, 82, 149, 149, 4, 15, 203, 250, 98, 65, 203, 241, 74, 236, 203, 250, 74, 82, 149, 149, 149, 30, 243, 2, 15, 203, 261, 98, 241, 74, 236, 203, 250, 74, 82, 149, 149, 30, 244, 146, 15, 203, 272, 74, 236, 203, 259, 74, 82, 149, 98, 65, 203, 259, 149, 149, 30, 273, 164, 172, 203, 257, 192, 57, 98, 169, 26, 111, 257, 192, 169, 98, 57, 26, 98, 15, 203, 65, 203, 255, 149, 98, 272, 74, 236, 203, 255, 74, 82, 149, 149, 98, 308, 149, 74, 236, 203, 250, 74, 82, 149, 30, 242, 146, 15, 203, 273, 98, 254, 149, 30, 241, 146, 15, 203, 65, 203, 273, 149, 98, 256, 149, 30, 241, 146, 15, 203, 65, 203, 263, 149, 98, 253, 149, 30, 274, 164, 184, 203, 216, 67, 203, 237, 204, 224, 67, 234, 149, 204, 227, 98, 203, 220, 98, 227, 149, 98, 203, 227, 204, 224, 98, 309, 149, 98, 203, 231, 204, 228, 98, 308, 149, 98, 203, 228, 98, 229, 149, 98, 203, 309, 98, 308, 149, 149, 30, 10, 203, 274, 98, 241, 74, 236, 203, 216, 74, 82, 74, 103, 149, 98, 251, 164, 203, 308, 98, 309, 149, 149, 30, 249, 164, 184, 203, 211, 67, 203, 237, 204, 226, 67, 235, 149, 204, 228, 98, 203, 228, 98, 220, 149, 98, 203, 309, 98, 228, 204, 226, 149, 98, 203, 308, 98, 231, 204, 228, 149, 98, 203, 228, 98, 228, 149, 98, 203, 308, 98, 309, 149, 149, 30, 275, 164, 52, 203, 249, 98, 251, 164, 203, 308, 98, 309, 149, 149, 30, 244, 164, 172, 203, 68, 203, 308, 98, 228, 149, 192, 57, 98, 169, 26, 111, 68, 203, 308, 98, 228, 149, 192, 169, 98, 57, 26, 98, 244, 98, 308, 149, 74, 236, 203, 250, 74, 82, 149, 30, 244, 164, 15, 203, 275, 98, 244, 149, 74, 236, 203, 250, 74, 82, 149, 30, 244, 164, 15, 203, 244, 98, 275, 149, 30, 244, 164, 172, 203, 68, 203, 308, 98, 228, 149, 192, 57, 98, 169, 26, 111, 68, 203, 308, 98, 228, 149, 192, 169, 98, 57, 26, 98, 4, 244, 98, 308, 149, 74, 236, 203, 254, 74, 82, 149, 30, 242, 146, 15, 203, 244, 98, 250, 149, 30, 243, 146, 15, 203, 65, 203, 244, 149, 98, 256, 149, 30, 243, 146, 242, 204, 252, 192, 57, 98, 169, 26, 30, 276, 164, 186, 203, 242, 204, 250, 98, 277, 164, 309, 149, 30, 278, 164, 184, 203, 217, 67, 203, 237, 204, 224, 67, 234, 149, 204, 227, 98, 203, 220, 98, 227, 149, 98, 203, 227, 204, 224, 98, 309, 149, 98, 203, 231, 204, 228, 98, 308, 149, 98, 203, 228, 98, 229, 149, 98, 203, 309, 98, 308, 149, 149, 30, 10, 203, 278, 98, 243, 74, 236, 203, 217, 74, 82, 74, 103, 149, 98, 251, 164, 203, 308, 98, 309, 149, 149, 30, 279, 164, 184, 203, 218, 67, 203, 237, 204, 224, 67, 234, 149, 98, 203, 220, 98, 149, 98, 203, 224, 98, 149, 98, 203, 231, 204, 228, 98, 149, 98, 203, 228, 98, 149, 98, 203, 308, 98, 149, 149, 30, 10, 203, 279, 98, 276, 74, 236, 203, 218, 74, 82, 74, 103, 149, 98, 251, 164, 203, 308, 98, 149, 149, 30, 3, 30]}, {"code": "def chunk_transform_qk_bwd_kernel_prepare(\n    q,\n    k,\n    v,\n    w,\n    beta,\n    g_cumsum,\n    L,\n    D,\n    h,\n    q_new,\n    k_new,\n    AT,\n    dA_local,\n    dv,\n    do,\n    dg_cumsum,\n    scale,\n    indices,\n    offsets,\n    chunk_offsets,\n    T,\n    G: tl.constexpr,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    BT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    USE_GATE: tl.constexpr,\n):\n    i_t, i_nh = tl.program_id(0), tl.program_id(1)\n    i_n, i_hq = i_nh // HQ, i_nh % HQ\n    i_h = i_hq // G\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(\n            indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(\n            tl.int32\n        )\n        T = eos - bos\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n\n    sm_scale = scale * 1.44269504\n\n    dA_local += (bos * HQ + i_hq) * BT\n    AT += (bos * H + i_h) * BT\n    q += (bos * HQ + i_hq) * K\n    q_new += (bos * HQ + i_hq) * K\n    k += (bos * H + i_h) * K\n    k_new += (bos * H + i_h) * K\n    w += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V\n    do += (bos * HQ + i_hq) * V\n    dv += (bos * HQ + i_hq) * V\n    beta += bos * H + i_h\n    h += ((boh + i_t) * H + i_h) * K * K\n    if USE_GATE:\n        g_cumsum += bos * HQ + i_hq\n        dg_cumsum += bos * HQ + i_hq\n    L += bos * HQ + i_hq\n    D += bos * HQ + i_hq\n\n    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1))\n    p_w = tl.make_block_ptr(w, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_kt = tl.load(p_k, boundary_check=(0, 1))\n    b_w = tl.load(p_w, boundary_check=(0, 1))\n    p_T = tl.make_block_ptr(AT, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_T = tl.load(p_T, boundary_check=(0, 1)).to(b_q.dtype)\n\n    o_i = tl.arange(0, BT)\n    m_t = o_i[:, None] >= o_i[None, :]\n    p_beta = tl.make_block_ptr(beta, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_w_beta = (b_w * b_beta[:, None]).to(b_w.dtype)\n\n    b_Twb = tl.dot(b_T.to(b_w_beta.dtype), b_w_beta).to(b_w_beta.dtype)\n\n    b_qw = tl.where(m_t, tl.dot(b_q, tl.trans(b_w)), 0).to(b_q.dtype)\n    b_qwT = tl.dot(b_qw, b_T).to(b_q.dtype)\n    b_wbk = tl.where(o_i[:, None] > o_i[None, :], tl.dot(b_w_beta, b_kt), 0).to(\n        b_w.dtype\n    )\n    b_A = tl.where(m_t, tl.dot(b_q, b_kt) - tl.dot(b_qwT, b_wbk), 0)\n\n    b_q = b_q - tl.dot(b_qwT, b_w_beta)\n    p_q_new = tl.make_block_ptr(\n        q_new, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, K), (1, 0)\n    )\n    tl.store(p_q_new, b_q.to(p_q_new.dtype.element_ty), boundary_check=(0, 1))\n\n    if i_hq % G == 0:\n        b_h = tl.dot(tl.trans(b_w), b_Twb)\n        p_h = tl.make_block_ptr(h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_T_wbk = tl.dot(b_T, b_wbk).to(b_w.dtype)\n        p_k_new = tl.make_block_ptr(\n            k_new, (K, T), (1, K * H), (0, i_t * BT), (BK, BT), (0, 1)\n        )\n        tl.store(\n            p_k_new,\n            (b_kt - tl.dot(tl.trans(b_w), b_T_wbk)).to(p_k_new.dtype.element_ty),\n            boundary_check=(0, 1),\n        )\n\n    if USE_GATE:\n        p_g_cumsum = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n        b_g_cumsum = tl.load(p_g_cumsum, boundary_check=(0,))\n        b_A = b_A + (b_g_cumsum[:, None] - b_g_cumsum[None, :])\n        b_A = tl.where((i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float(\"-inf\"))\n\n    p_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_l = tl.load(p_l, boundary_check=(0,))\n    p_delta = tl.make_block_ptr(D, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    delta = tl.load(p_delta, boundary_check=(0,))\n\n    b_A_softmax = tl.exp2(\n        tl.where(\n            o_i[:, None] >= o_i[None, :], b_A * sm_scale - b_l[:, None], float(\"-inf\")\n        )\n    )\n    p_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv = tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)\n    p_dv = tl.make_block_ptr(dv, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n\n    p_v = tl.make_block_ptr(v, (V, T), (1, H * V), (0, i_t * BT), (BV, BT), (0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dp = tl.dot(b_do, b_v)\n\n    b_dA = (b_dp - delta[:, None]) * b_A_softmax * scale\n    b_dgq = tl.sum(b_dA, axis=1) - tl.sum(b_dA, axis=0)\n    b_dA = b_dA.to(b_v.dtype)\n\n    if USE_GATE:\n        p_dg = tl.make_block_ptr(dg_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n        tl.store(p_dg, b_dgq.to(p_dg.dtype.element_ty), boundary_check=(0,))\n\n    p_dA = tl.make_block_ptr(\n        dA_local, (T, BT), (BT * HQ, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n    tl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 308, 204, 208, 97, 209, 97, 210, 97, 211, 97, 212, 97, 213, 97, 214, 97, 215, 97, 216, 97, 217, 97, 218, 97, 219, 97, 220, 97, 221, 97, 222, 97, 223, 97, 224, 97, 225, 97, 226, 97, 227, 97, 228, 97, 229, 56, 6, 97, 230, 56, 6, 97, 231, 56, 6, 97, 232, 56, 6, 97, 233, 56, 6, 97, 234, 56, 6, 97, 235, 56, 6, 97, 236, 56, 6, 97, 237, 56, 6, 97, 238, 56, 6, 149, 56, 30, -1, 239, 97, 240, 164, 204, 145, 204, 309, 149, 97, 145, 204, 310, 149, 149, 30, 241, 97, 242, 164, 204, 240, 45, 230, 97, 240, 190, 230, 149, 30, 243, 164, 242, 45, 229, 30, 155, 237, 56, 30, 241, 97, 239, 164, 204, 50, 204, 225, 66, 239, 205, 311, 149, 73, 244, 204, 207, 149, 97, 50, 204, 225, 66, 239, 205, 311, 66, 310, 149, 73, 244, 204, 207, 149, 149, 30, 245, 97, 246, 164, 204, 50, 204, 226, 66, 241, 149, 73, 244, 204, 207, 149, 97, 50, 204, 226, 66, 241, 66, 310, 149, 73, 244, 204, 207, 149, 149, 30, 228, 164, 246, 4, 245, 30, 247, 164, 50, 204, 227, 66, 241, 149, 73, 244, 204, 207, 149, 30, 158, 30, 29, 56, 30, 245, 97, 246, 164, 204, 241, 205, 228, 97, 241, 205, 228, 66, 228, 149, 30, 248, 164, 58, 204, 228, 97, 236, 149, 30, 247, 164, 241, 205, 248, 30, 51, 30, 249, 164, 224, 205, 312, 30, 220, 146, 204, 245, 205, 230, 66, 242, 149, 205, 236, 30, 219, 146, 204, 245, 205, 231, 66, 243, 149, 205, 236, 30, 208, 146, 204, 245, 205, 230, 66, 242, 149, 205, 232, 30, 217, 146, 204, 245, 205, 230, 66, 242, 149, 205, 232, 30, 209, 146, 204, 245, 205, 231, 66, 243, 149, 205, 232, 30, 218, 146, 204, 245, 205, 231, 66, 243, 149, 205, 232, 30, 211, 146, 204, 245, 205, 231, 66, 243, 149, 205, 232, 30, 210, 146, 204, 245, 205, 231, 66, 243, 149, 205, 233, 30, 222, 146, 204, 245, 205, 230, 66, 242, 149, 205, 233, 30, 221, 146, 204, 245, 205, 230, 66, 242, 149, 205, 233, 30, 212, 146, 245, 205, 231, 66, 243, 30, 216, 146, 204, 204, 247, 66, 239, 149, 205, 231, 66, 243, 149, 205, 232, 205, 232, 30, 155, 238, 56, 30, 213, 146, 245, 205, 230, 66, 242, 30, 223, 146, 245, 205, 230, 66, 242, 30, 158, 30, 214, 146, 245, 205, 230, 66, 242, 30, 215, 146, 245, 205, 230, 66, 242, 30, 250, 164, 185, 204, 208, 97, 204, 228, 97, 232, 149, 97, 204, 230, 205, 232, 97, 310, 149, 97, 204, 239, 205, 236, 97, 309, 149, 97, 204, 236, 97, 234, 149, 97, 204, 310, 97, 309, 149, 149, 30, 251, 164, 185, 204, 209, 97, 204, 232, 97, 228, 149, 97, 204, 310, 97, 231, 205, 232, 149, 97, 204, 309, 97, 239, 205, 236, 149, 97, 204, 234, 97, 236, 149, 97, 204, 309, 97, 310, 149, 149, 30, 252, 164, 185, 204, 211, 97, 204, 228, 97, 232, 149, 97, 204, 231, 205, 232, 97, 310, 149, 97, 204, 239, 205, 236, 97, 309, 149, 97, 204, 236, 97, 234, 149, 97, 204, 310, 97, 309, 149, 149, 30, 253, 164, 50, 204, 250, 97, 254, 164, 204, 309, 97, 310, 149, 149, 30, 255, 164, 50, 204, 251, 97, 254, 164, 204, 309, 97, 310, 149, 149, 30, 256, 164, 50, 204, 252, 97, 254, 164, 204, 309, 97, 310, 149, 149, 30, 257, 164, 185, 204, 219, 97, 204, 228, 97, 236, 149, 97, 204, 236, 205, 231, 97, 310, 149, 97, 204, 239, 205, 236, 97, 309, 149, 97, 204, 236, 97, 236, 149, 97, 204, 310, 97, 309, 149, 149, 30, 258, 164, 50, 204, 257, 97, 254, 164, 204, 309, 97, 310, 149, 149, 73, 244, 204, 253, 73, 81, 149, 30, 259, 164, 67, 204, 309, 97, 236, 149, 30, 260, 164, 259, 193, 56, 97, 169, 26, 127, 259, 193, 169, 97, 56, 26, 30, 261, 164, 185, 204, 212, 97, 204, 228, 97, 149, 97, 204, 231, 97, 149, 97, 204, 239, 205, 236, 97, 149, 97, 204, 236, 97, 149, 97, 204, 309, 97, 149, 149, 30, 262, 164, 50, 204, 261, 97, 254, 164, 204, 309, 97, 149, 149, 30, 263, 164, 204, 256, 205, 262, 193, 56, 97, 169, 26, 149, 73, 244, 204, 256, 73, 81, 149, 30, 264, 164, 15, 204, 258, 73, 244, 204, 263, 73, 81, 149, 97, 263, 149, 73, 244, 204, 263, 73, 81, 149, 30, 265, 164, 172, 204, 260, 97, 15, 204, 253, 97, 64, 204, 256, 149, 149, 97, 309, 149, 73, 244, 204, 253, 73, 81, 149, 30, 266, 164, 15, 204, 265, 97, 258, 149, 73, 244, 204, 253, 73, 81, 149, 30, 267, 164, 172, 204, 259, 193, 56, 97, 169, 26, 110, 259, 193, 169, 97, 56, 26, 97, 15, 204, 263, 97, 255, 149, 97, 309, 149, 73, 244, 204, 256, 73, 81, 149, 30, 268, 164, 172, 204, 260, 97, 15, 204, 253, 97, 255, 149, 4, 15, 204, 266, 97, 267, 149, 97, 309, 149, 30, 253, 164, 253, 4, 15, 204, 266, 97, 263, 149, 30, 269, 164, 185, 204, 217, 97, 204, 228, 97, 232, 149, 97, 204, 232, 205, 230, 97, 310, 149, 97, 204, 239, 205, 236, 97, 309, 149, 97, 204, 236, 97, 232, 149, 97, 204, 310, 97, 309, 149, 149, 30, 10, 204, 269, 97, 253, 73, 244, 204, 269, 73, 81, 73, 102, 149, 97, 254, 164, 204, 309, 97, 310, 149, 149, 30, 155, 242, 190, 229, 68, 309, 56, 30, 270, 164, 15, 204, 64, 204, 256, 149, 97, 264, 149, 30, 271, 164, 185, 204, 216, 97, 204, 232, 97, 232, 149, 97, 204, 232, 97, 310, 149, 97, 204, 309, 97, 309, 149, 97, 204, 234, 97, 234, 149, 97, 204, 310, 97, 309, 149, 149, 30, 10, 204, 271, 97, 270, 73, 244, 204, 271, 73, 81, 73, 102, 149, 97, 254, 164, 204, 309, 97, 310, 149, 149, 30, 272, 164, 15, 204, 258, 97, 267, 149, 73, 244, 204, 256, 73, 81, 149, 30, 273, 164, 185, 204, 218, 97, 204, 232, 97, 228, 149, 97, 204, 310, 97, 232, 205, 231, 149, 97, 204, 309, 97, 239, 205, 236, 149, 97, 204, 234, 97, 236, 149, 97, 204, 309, 97, 310, 149, 149, 30, 10, 204, 273, 97, 204, 255, 4, 15, 204, 64, 204, 256, 149, 97, 272, 149, 149, 73, 244, 204, 273, 73, 81, 73, 102, 149, 97, 254, 164, 204, 309, 97, 310, 149, 149, 30, 158, 30, 155, 238, 56, 30, 274, 164, 185, 204, 213, 97, 204, 228, 97, 149, 97, 204, 230, 97, 149, 97, 204, 239, 205, 236, 97, 149, 97, 204, 236, 97, 149, 97, 204, 309, 97, 149, 149, 30, 275, 164, 50, 204, 274, 97, 254, 164, 204, 309, 97, 149, 149, 30, 268, 164, 268, 66, 204, 275, 193, 56, 97, 169, 26, 4, 275, 193, 169, 97, 56, 26, 149, 30, 268, 164, 172, 204, 204, 239, 205, 236, 66, 67, 204, 309, 97, 236, 149, 1, 228, 149, 193, 56, 97, 169, 26, 97, 268, 97, 276, 204, 313, 149, 149, 30, 158, 30, 277, 164, 185, 204, 214, 97, 204, 228, 97, 149, 97, 204, 230, 97, 149, 97, 204, 239, 205, 236, 97, 149, 97, 204, 236, 97, 149, 97, 204, 309, 97, 149, 149, 30, 278, 164, 50, 204, 277, 97, 254, 164, 204, 309, 97, 149, 149, 30, 279, 164, 185, 204, 215, 97, 204, 228, 97, 149, 97, 204, 230, 97, 149, 97, 204, 239, 205, 236, 97, 149, 97, 204, 236, 97, 149, 97, 204, 309, 97, 149, 149, 30, 280, 164, 50, 204, 279, 97, 254, 164, 204, 309, 97, 149, 149, 30, 281, 164, 178, 204, 172, 204, 259, 193, 56, 97, 169, 26, 127, 259, 193, 169, 97, 56, 26, 97, 268, 205, 249, 4, 278, 193, 56, 97, 169, 26, 97, 276, 204, 313, 149, 149, 149, 30, 282, 164, 185, 204, 222, 97, 204, 228, 97, 233, 149, 97, 204, 230, 205, 233, 97, 310, 149, 97, 204, 239, 205, 236, 97, 309, 149, 97, 204, 236, 97, 235, 149, 97, 204, 310, 97, 309, 149, 149, 30, 283, 164, 50, 204, 282, 97, 254, 164, 204, 309, 97, 310, 149, 149, 30, 284, 164, 15, 204, 64, 204, 281, 73, 244, 204, 283, 73, 81, 149, 149, 97, 283, 149, 30, 285, 164, 185, 204, 221, 97, 204, 228, 97, 233, 149, 97, 204, 230, 205, 233, 97, 310, 149, 97, 204, 239, 205, 236, 97, 309, 149, 97, 204, 236, 97, 235, 149, 97, 204, 310, 97, 309, 149, 149, 30, 10, 204, 285, 97, 284, 73, 244, 204, 285, 73, 81, 73, 102, 149, 97, 254, 164, 204, 309, 97, 310, 149, 149, 30, 286, 164, 185, 204, 210, 97, 204, 233, 97, 228, 149, 97, 204, 310, 97, 231, 205, 233, 149, 97, 204, 309, 97, 239, 205, 236, 149, 97, 204, 235, 97, 236, 149, 97, 204, 309, 97, 310, 149, 149, 30, 287, 164, 50, 204, 286, 97, 254, 164, 204, 309, 97, 310, 149, 149, 30, 288, 164, 15, 204, 283, 97, 287, 149, 30, 289, 164, 204, 288, 4, 280, 193, 56, 97, 169, 26, 149, 205, 281, 205, 224, 30, 290, 164, 187, 204, 289, 97, 291, 164, 310, 149, 4, 187, 204, 289, 97, 291, 164, 309, 149, 30, 289, 164, 289, 73, 244, 204, 287, 73, 81, 149, 30, 155, 238, 56, 30, 292, 164, 185, 204, 223, 97, 204, 228, 97, 149, 97, 204, 230, 97, 149, 97, 204, 239, 205, 236, 97, 149, 97, 204, 236, 97, 149, 97, 204, 309, 97, 149, 149, 30, 10, 204, 292, 97, 290, 73, 244, 204, 292, 73, 81, 73, 102, 149, 97, 254, 164, 204, 309, 97, 149, 149, 30, 158, 30, 293, 164, 185, 204, 220, 97, 204, 228, 97, 236, 149, 97, 204, 236, 205, 230, 97, 310, 149, 97, 204, 239, 205, 236, 97, 309, 149, 97, 204, 236, 97, 236, 149, 97, 204, 310, 97, 309, 149, 149, 30, 10, 204, 293, 97, 289, 73, 244, 204, 293, 73, 81, 73, 102, 149, 97, 254, 164, 204, 309, 97, 310, 149, 149, 30, 3, 30]}, {"code": "def intra_chunk_preprocess_fwd_kernel(\n    q,\n    k,\n    v,\n    w,\n    beta,\n    g_cumsum,\n    o,\n    A,\n    L,\n    M,\n    h,\n    q_new,\n    k_new,\n    scale,\n    indices,\n    offsets,\n    chunk_offsets,\n    T,\n    H: tl.constexpr,\n    G: tl.constexpr,\n    HQ: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    BT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    USE_G: tl.constexpr,\n):\n    i_t, i_nh = tl.program_id(0), tl.program_id(1)\n    i_n, i_hq = i_nh // HQ, i_nh % HQ\n    i_h = i_hq // G\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(\n            indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(\n            tl.int32\n        )\n        T = eos - bos\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n\n    sm_scale = scale * 1.44269504\n\n    A += (bos * H + i_h) * BT\n    q += (bos * HQ + i_hq) * K\n    q_new += (bos * HQ + i_hq) * K\n    k += (bos * H + i_h) * K\n    k_new += (bos * H + i_h) * K\n    w += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V\n    o += (bos * HQ + i_hq) * V\n    beta += bos * H + i_h\n    h += ((boh + i_t) * H + i_h) * K * K\n    if USE_G:\n        g_cumsum += bos * HQ + i_hq\n    L += bos * HQ + i_hq\n    M += bos * HQ + i_hq\n\n    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1))\n    p_w = tl.make_block_ptr(w, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_kt = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_w = tl.load(p_w, boundary_check=(0, 1))\n    p_T = tl.make_block_ptr(A, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_T = tl.load(p_T, boundary_check=(0, 1)).to(b_q.dtype)\n\n    o_i = tl.arange(0, BT)\n    m_t = o_i[:, None] >= o_i[None, :]\n    p_beta = tl.make_block_ptr(beta, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_w_beta = (b_w * b_beta[:, None]).to(b_w.dtype)\n\n    b_qw = tl.where(m_t, tl.dot(b_q, tl.trans(b_w)), 0).to(b_q.dtype)\n    b_qwT = tl.dot(b_qw, b_T).to(b_q.dtype)\n    b_wbk = tl.where(o_i[:, None] > o_i[None, :], tl.dot(b_w_beta, b_kt), 0).to(\n        b_w.dtype\n    )\n    b_A = tl.where(m_t, tl.dot(b_q, b_kt) - tl.dot(b_qwT, b_wbk), 0)\n\n    b_q = b_q - tl.dot(b_qwT, b_w_beta)\n    p_q_new = tl.make_block_ptr(\n        q_new, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, K), (1, 0)\n    )\n    tl.store(p_q_new, b_q.to(p_q_new.dtype.element_ty), boundary_check=(0, 1))\n\n    if i_hq % G == 0:\n        b_Twb = tl.dot(b_T.to(b_w_beta.dtype), b_w_beta).to(b_w_beta.dtype)\n        b_h = tl.dot(tl.trans(b_w), b_Twb)\n        p_h = tl.make_block_ptr(h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_T_wbk = tl.dot(b_T, b_wbk).to(b_w.dtype)\n        p_k_new = tl.make_block_ptr(\n            k_new, (K, T), (1, K * H), (0, i_t * BT), (BK, BT), (0, 1)\n        )\n        tl.store(\n            p_k_new,\n            (b_kt - tl.dot(tl.trans(b_w), b_T_wbk)).to(p_k_new.dtype.element_ty),\n            boundary_check=(0, 1),\n        )\n\n    if USE_G:\n        p_g_cumsum = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n        b_g_cumsum = tl.load(p_g_cumsum, boundary_check=(0,))\n        b_A = b_A + (b_g_cumsum[:, None] - b_g_cumsum[None, :])\n        b_A = tl.where((i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float(\"-inf\"))\n\n    b_qkT_softmax = tl.where(\n        o_i[:, None] >= o_i[None, :], b_A * sm_scale, float(\"-inf\")\n    )\n    m_i = tl.max(b_qkT_softmax, 1)\n    b_qkT_softmax = tl.math.exp2(b_qkT_softmax - m_i[:, None])\n    l_i = tl.sum(b_qkT_softmax, 1)\n    b_o = tl.dot(b_qkT_softmax.to(b_v.dtype), b_v)\n    p_o = tl.make_block_ptr(o, (T, V), (V * HQ, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    p_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    p_m = tl.make_block_ptr(M, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_m, m_i.to(p_m.dtype.element_ty), boundary_check=(0,))\n    tl.store(p_l, l_i.to(p_l.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 309, 205, 209, 98, 210, 98, 211, 98, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 98, 227, 57, 6, 98, 228, 57, 6, 98, 229, 57, 6, 98, 230, 57, 6, 98, 231, 57, 6, 98, 232, 57, 6, 98, 233, 57, 6, 98, 234, 57, 6, 98, 235, 57, 6, 98, 236, 57, 6, 150, 57, 30, -1, 237, 98, 238, 165, 205, 146, 205, 310, 150, 98, 146, 205, 311, 150, 150, 30, 239, 98, 240, 165, 205, 238, 45, 229, 98, 238, 191, 229, 150, 30, 241, 165, 240, 45, 228, 30, 156, 235, 57, 30, 239, 98, 237, 165, 205, 52, 205, 223, 67, 237, 206, 312, 150, 74, 242, 205, 56, 150, 98, 52, 205, 223, 67, 237, 206, 312, 67, 311, 150, 74, 242, 205, 56, 150, 150, 30, 243, 98, 244, 165, 205, 52, 205, 224, 67, 239, 150, 74, 242, 205, 56, 150, 98, 52, 205, 224, 67, 239, 67, 311, 150, 74, 242, 205, 56, 150, 150, 30, 226, 165, 244, 4, 243, 30, 245, 165, 52, 205, 225, 67, 239, 150, 74, 242, 205, 56, 150, 30, 159, 30, 29, 57, 30, 243, 98, 244, 165, 205, 239, 206, 226, 98, 239, 206, 226, 67, 226, 150, 30, 246, 165, 59, 205, 226, 98, 234, 150, 30, 245, 165, 239, 206, 246, 30, 51, 30, 247, 165, 222, 206, 313, 30, 216, 147, 205, 243, 206, 227, 67, 241, 150, 206, 234, 30, 209, 147, 205, 243, 206, 229, 67, 240, 150, 206, 230, 30, 220, 147, 205, 243, 206, 229, 67, 240, 150, 206, 230, 30, 210, 147, 205, 243, 206, 227, 67, 241, 150, 206, 230, 30, 221, 147, 205, 243, 206, 227, 67, 241, 150, 206, 230, 30, 212, 147, 205, 243, 206, 227, 67, 241, 150, 206, 230, 30, 211, 147, 205, 243, 206, 227, 67, 241, 150, 206, 231, 30, 215, 147, 205, 243, 206, 229, 67, 240, 150, 206, 231, 30, 213, 147, 243, 206, 227, 67, 241, 30, 219, 147, 205, 205, 245, 67, 237, 150, 206, 227, 67, 241, 150, 206, 230, 206, 230, 30, 156, 236, 57, 30, 214, 147, 243, 206, 229, 67, 240, 30, 159, 30, 217, 147, 243, 206, 229, 67, 240, 30, 218, 147, 243, 206, 229, 67, 240, 30, 248, 165, 186, 205, 209, 98, 205, 226, 98, 230, 150, 98, 205, 229, 206, 230, 98, 311, 150, 98, 205, 237, 206, 234, 98, 310, 150, 98, 205, 234, 98, 232, 150, 98, 205, 311, 98, 310, 150, 150, 30, 249, 165, 186, 205, 210, 98, 205, 230, 98, 226, 150, 98, 205, 311, 98, 227, 206, 230, 150, 98, 205, 310, 98, 237, 206, 234, 150, 98, 205, 232, 98, 234, 150, 98, 205, 310, 98, 311, 150, 150, 30, 250, 165, 186, 205, 212, 98, 205, 226, 98, 230, 150, 98, 205, 227, 206, 230, 98, 311, 150, 98, 205, 237, 206, 234, 98, 310, 150, 98, 205, 234, 98, 232, 150, 98, 205, 311, 98, 310, 150, 150, 30, 251, 165, 186, 205, 211, 98, 205, 226, 98, 231, 150, 98, 205, 227, 206, 231, 98, 311, 150, 98, 205, 237, 206, 234, 98, 310, 150, 98, 205, 234, 98, 233, 150, 98, 205, 311, 98, 310, 150, 150, 30, 252, 165, 52, 205, 248, 98, 253, 165, 205, 310, 98, 311, 150, 150, 30, 254, 165, 52, 205, 249, 98, 253, 165, 205, 310, 98, 311, 150, 150, 30, 255, 165, 52, 205, 251, 98, 253, 165, 205, 310, 98, 311, 150, 150, 30, 256, 165, 52, 205, 250, 98, 253, 165, 205, 310, 98, 311, 150, 150, 30, 257, 165, 186, 205, 216, 98, 205, 226, 98, 234, 150, 98, 205, 234, 206, 227, 98, 311, 150, 98, 205, 237, 206, 234, 98, 310, 150, 98, 205, 234, 98, 234, 150, 98, 205, 311, 98, 310, 150, 150, 30, 258, 165, 52, 205, 257, 98, 253, 165, 205, 310, 98, 311, 150, 150, 74, 242, 205, 252, 74, 82, 150, 30, 259, 165, 68, 205, 310, 98, 234, 150, 30, 260, 165, 259, 194, 57, 98, 170, 26, 128, 259, 194, 170, 98, 57, 26, 30, 261, 165, 186, 205, 213, 98, 205, 226, 98, 150, 98, 205, 227, 98, 150, 98, 205, 237, 206, 234, 98, 150, 98, 205, 234, 98, 150, 98, 205, 310, 98, 150, 150, 30, 262, 165, 52, 205, 261, 98, 253, 165, 205, 310, 98, 150, 150, 30, 263, 165, 205, 256, 206, 262, 194, 57, 98, 170, 26, 150, 74, 242, 205, 256, 74, 82, 150, 30, 264, 165, 173, 205, 260, 98, 15, 205, 252, 98, 65, 205, 256, 150, 150, 98, 310, 150, 74, 242, 205, 252, 74, 82, 150, 30, 265, 165, 15, 205, 264, 98, 258, 150, 74, 242, 205, 252, 74, 82, 150, 30, 266, 165, 173, 205, 259, 194, 57, 98, 170, 26, 111, 259, 194, 170, 98, 57, 26, 98, 15, 205, 263, 98, 254, 150, 98, 310, 150, 74, 242, 205, 256, 74, 82, 150, 30, 267, 165, 173, 205, 260, 98, 15, 205, 252, 98, 254, 150, 4, 15, 205, 265, 98, 266, 150, 98, 310, 150, 30, 252, 165, 252, 4, 15, 205, 265, 98, 263, 150, 30, 268, 165, 186, 205, 220, 98, 205, 226, 98, 230, 150, 98, 205, 230, 206, 229, 98, 311, 150, 98, 205, 237, 206, 234, 98, 310, 150, 98, 205, 234, 98, 230, 150, 98, 205, 311, 98, 310, 150, 150, 30, 10, 205, 268, 98, 252, 74, 242, 205, 268, 74, 82, 74, 103, 150, 98, 253, 165, 205, 310, 98, 311, 150, 150, 30, 156, 240, 191, 228, 69, 310, 57, 30, 269, 165, 15, 205, 258, 74, 242, 205, 263, 74, 82, 150, 98, 263, 150, 74, 242, 205, 263, 74, 82, 150, 30, 270, 165, 15, 205, 65, 205, 256, 150, 98, 269, 150, 30, 271, 165, 186, 205, 219, 98, 205, 230, 98, 230, 150, 98, 205, 230, 98, 311, 150, 98, 205, 310, 98, 310, 150, 98, 205, 232, 98, 232, 150, 98, 205, 311, 98, 310, 150, 150, 30, 10, 205, 271, 98, 270, 74, 242, 205, 271, 74, 82, 74, 103, 150, 98, 253, 165, 205, 310, 98, 311, 150, 150, 30, 272, 165, 15, 205, 258, 98, 266, 150, 74, 242, 205, 256, 74, 82, 150, 30, 273, 165, 186, 205, 221, 98, 205, 230, 98, 226, 150, 98, 205, 311, 98, 230, 206, 227, 150, 98, 205, 310, 98, 237, 206, 234, 150, 98, 205, 232, 98, 234, 150, 98, 205, 310, 98, 311, 150, 150, 30, 10, 205, 273, 98, 205, 254, 4, 15, 205, 65, 205, 256, 150, 98, 272, 150, 150, 74, 242, 205, 273, 74, 82, 74, 103, 150, 98, 253, 165, 205, 310, 98, 311, 150, 150, 30, 159, 30, 156, 236, 57, 30, 274, 165, 186, 205, 214, 98, 205, 226, 98, 150, 98, 205, 229, 98, 150, 98, 205, 237, 206, 234, 98, 150, 98, 205, 234, 98, 150, 98, 205, 310, 98, 150, 150, 30, 275, 165, 52, 205, 274, 98, 253, 165, 205, 310, 98, 150, 150, 30, 267, 165, 267, 67, 205, 275, 194, 57, 98, 170, 26, 4, 275, 194, 170, 98, 57, 26, 150, 30, 267, 165, 173, 205, 205, 237, 206, 234, 67, 68, 205, 310, 98, 234, 150, 1, 226, 150, 194, 57, 98, 170, 26, 98, 267, 98, 276, 205, 314, 150, 150, 30, 159, 30, 277, 165, 173, 205, 259, 194, 57, 98, 170, 26, 128, 259, 194, 170, 98, 57, 26, 98, 267, 206, 247, 98, 276, 205, 314, 150, 150, 30, 278, 165, 12, 205, 277, 98, 311, 150, 30, 277, 165, 136, 205, 277, 4, 278, 194, 57, 98, 170, 26, 150, 30, 279, 165, 188, 205, 277, 98, 311, 150, 30, 280, 165, 15, 205, 277, 74, 242, 205, 255, 74, 82, 150, 98, 255, 150, 30, 281, 165, 186, 205, 215, 98, 205, 226, 98, 231, 150, 98, 205, 231, 206, 229, 98, 311, 150, 98, 205, 237, 206, 234, 98, 310, 150, 98, 205, 234, 98, 233, 150, 98, 205, 311, 98, 310, 150, 150, 30, 10, 205, 281, 98, 280, 74, 242, 205, 281, 74, 82, 74, 103, 150, 98, 253, 165, 205, 310, 98, 311, 150, 150, 30, 282, 165, 186, 205, 217, 98, 205, 226, 98, 150, 98, 205, 229, 98, 150, 98, 205, 237, 206, 234, 98, 150, 98, 205, 234, 98, 150, 98, 205, 310, 98, 150, 150, 30, 283, 165, 186, 205, 218, 98, 205, 226, 98, 150, 98, 205, 229, 98, 150, 98, 205, 237, 206, 234, 98, 150, 98, 205, 234, 98, 150, 98, 205, 310, 98, 150, 150, 30, 10, 205, 283, 98, 278, 74, 242, 205, 283, 74, 82, 74, 103, 150, 98, 253, 165, 205, 310, 98, 150, 150, 30, 10, 205, 282, 98, 279, 74, 242, 205, 282, 74, 82, 74, 103, 150, 98, 253, 165, 205, 310, 98, 150, 150, 30, 3, 30]}, {"code": "def parallel_path_bwd_dkv_kernel(\n    q,\n    k,\n    v,\n    g_cumsum,\n    hc_whole,\n    scale,\n    L,\n    D,\n    dk,\n    dv,\n    do,\n    dg_cumsum,\n    cu_seqlens,\n    indices,\n    split_offsets,\n    T,\n    G: tl.constexpr,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    S: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    USE_GATE: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // G\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(\n            indices + i_t * 2 + 1\n        ).to(tl.int32)\n        boh_large = tl.load(split_offsets + i_n).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        i_n = i_b\n        bos, eos = i_n * T, i_n * T + T\n        boh_large = i_n * tl.cdiv(T, S)\n\n    q += (bos * HQ + i_hq) * K\n    do += (bos * HQ + i_hq) * V\n    dk += (bos * HQ + i_hq) * K\n    dv += (bos * HQ + i_hq) * K\n    L += bos * HQ + i_hq\n    D += bos * HQ + i_hq\n\n    k += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V\n    hc_whole += (boh_large * H + i_h) * K * K\n\n    if USE_GATE:\n        g_cumsum += bos * HQ + i_hq\n        dg_cumsum += bos * HQ + i_hq\n\n    stride_h = H * K * K\n    sm_scale = scale * 1.44269504\n\n    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    b_k_origin = tl.load(p_k, boundary_check=(0, 1))\n    p_v = tl.make_block_ptr(v, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n\n    if USE_GATE:\n        b_g_cumsum_k = tl.zeros(\n            [\n                BT,\n            ],\n            dtype=tl.float32,\n        )\n        p_g_cumsum_k = tl.make_block_ptr(\n            g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,)\n        )\n        b_g_cumsum_k += tl.load(p_g_cumsum_k, boundary_check=(0,))\n        b_dg_cumsum_k = tl.zeros(\n            [\n                BT,\n            ],\n            dtype=tl.float32,\n        )\n    else:\n        b_g_cumsum_k = None\n        b_dg_cumsum_k = None\n\n    b_dk = tl.zeros([BT, K], dtype=tl.float32)\n    b_dv = tl.zeros([BT, K], dtype=tl.float32)\n    idx_i = (i_t * BT // S).to(tl.int32)\n\n    last_chunk_start = tl.floor(T / S).to(tl.int32) * S\n\n    if i_t * BT < last_chunk_start:\n\n        if T % S != 0:\n            idx_j = last_chunk_start // S\n            b_k_accum = tl.zeros([BT, BK], dtype=tl.float32)\n            b_k_accum += b_k_origin\n            for i in range(idx_i + 1, idx_j):\n                p_h = tl.make_block_ptr(\n                    hc_whole + i * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)\n                )\n                b_h = tl.load(p_h, boundary_check=(0, 1))\n                b_k_accum = b_k_accum - tl.dot(b_k_accum.to(b_h.dtype), tl.trans(b_h))\n            b_k = b_k_accum.to(b_k_origin.dtype)\n\n            for offset in range(\n                tl.ceil(T / BS).to(tl.int32) * BS - BS, last_chunk_start - BS, -BS\n            ):\n                p_delta = tl.make_block_ptr(D, (T,), (HQ,), (offset,), (BS,), (0,))\n                p_l = tl.make_block_ptr(L, (T,), (HQ,), (offset,), (BS,), (0,))\n                b_delta = tl.load(p_delta, boundary_check=(0,))\n                b_l = tl.load(p_l, boundary_check=(0,))\n                p_q = tl.make_block_ptr(\n                    q, (T, K), (HQ * K, 1), (offset, 0), (BS, BK), (1, 0)\n                )\n                b_q = tl.load(p_q, boundary_check=(0, 1))\n                b_A = tl.dot(b_q, tl.trans(b_k))\n                if USE_GATE:\n                    p_g_cumsum_q = tl.make_block_ptr(\n                        g_cumsum, (T,), (HQ,), (offset,), (BS,), (0,)\n                    )\n                    b_g_cumsum_q = tl.load(p_g_cumsum_q, boundary_check=(0,))\n                    b_A = b_A + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]\n                    b_A = tl.where(\n                        (offset + tl.arange(0, BS) < T)[:, None], b_A, float(\"-inf\")\n                    )\n                b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])\n                p_do = tl.make_block_ptr(\n                    do, (T, V), (HQ * V, 1), (offset, 0), (BS, BV), (1, 0)\n                )\n                b_do = tl.load(p_do, boundary_check=(0, 1))\n                b_dv += tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)\n                b_dp = tl.dot(b_do, tl.trans(b_v))\n                b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale\n                if USE_GATE:\n                    b_dg_cumsum_k -= tl.sum(b_dA, axis=0)\n                b_dA = b_dA.to(b_v.dtype)\n                b_dk += tl.dot(tl.trans(b_dA), b_q)\n\n        for offset_outer in range(last_chunk_start, i_t * BT + S, -S):\n            idx_j = (offset_outer // S) - 1\n            b_k_accum = tl.zeros([BT, BK], dtype=tl.float32)\n            b_k_accum += b_k_origin\n            for i in range(idx_i + 1, idx_j):\n                p_h = tl.make_block_ptr(\n                    hc_whole + i * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)\n                )\n                b_h = tl.load(p_h, boundary_check=(0, 1))\n                b_k_accum = b_k_accum - tl.dot(b_k_accum.to(b_h.dtype), tl.trans(b_h))\n            b_k = b_k_accum.to(b_k_origin.dtype)\n\n            p_h = tl.make_block_ptr(\n                hc_whole + (idx_j) * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)\n            )\n            b_h = tl.load(p_h, boundary_check=(0, 1))\n            b_dk = b_dk - tl.dot(b_dk.to(b_h.dtype), b_h)\n\n            for offset in range(offset_outer - BS, offset_outer - S - BS, -BS):\n                p_delta = tl.make_block_ptr(D, (T,), (HQ,), (offset,), (BS,), (0,))\n                p_l = tl.make_block_ptr(L, (T,), (HQ,), (offset,), (BS,), (0,))\n                b_delta = tl.load(p_delta, boundary_check=(0,))\n                b_l = tl.load(p_l, boundary_check=(0,))\n                p_q = tl.make_block_ptr(\n                    q, (T, K), (HQ * K, 1), (offset, 0), (BS, BK), (1, 0)\n                )\n                b_q = tl.load(p_q, boundary_check=(0, 1))\n                b_A = tl.dot(b_q, tl.trans(b_k))\n                if USE_GATE:\n                    p_g_cumsum_q = tl.make_block_ptr(\n                        g_cumsum, (T,), (HQ,), (offset,), (BS,), (0,)\n                    )\n                    b_g_cumsum_q = tl.load(p_g_cumsum_q, boundary_check=(0,))\n                    b_A = b_A + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]\n                    b_A = tl.where(\n                        (offset + tl.arange(0, BS) < T)[:, None], b_A, float(\"-inf\")\n                    )\n                b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])\n                p_do = tl.make_block_ptr(\n                    do, (T, V), (HQ * V, 1), (offset, 0), (BS, BV), (1, 0)\n                )\n                b_do = tl.load(p_do, boundary_check=(0, 1))\n                b_dv += tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)\n                b_dp = tl.dot(b_do, tl.trans(b_v))\n\n                b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale\n                if USE_GATE:\n                    b_dg_cumsum_k -= tl.sum(b_dA, axis=0)\n                b_dA = b_dA.to(b_v.dtype)\n                b_dk += tl.dot(tl.trans(b_dA), b_q)\n\n    p_dk = tl.make_block_ptr(dk, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    tl.store(p_dk, b_dk.to(dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.atomic_add(\n        dv + (i_t * BT + tl.arange(0, BT))[:, None] * HQ * K + tl.arange(0, K)[None, :],\n        b_dv,\n        sem=\"relaxed\",\n    )\n    if USE_GATE:\n        tl.atomic_add(\n            dg_cumsum + (i_t * BT + tl.arange(0, BT)) * HQ, b_dg_cumsum_k, sem=\"relaxed\"\n        )", "encoded": [28, 312, 208, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 98, 227, 98, 228, 57, 6, 98, 229, 57, 6, 98, 230, 57, 6, 98, 231, 57, 6, 98, 232, 57, 6, 98, 233, 57, 6, 98, 234, 57, 6, 98, 235, 57, 6, 98, 236, 57, 6, 98, 237, 57, 6, 98, 238, 57, 6, 98, 239, 57, 6, 151, 57, 31, -1, 240, 98, 241, 166, 208, 147, 208, 313, 151, 98, 147, 208, 314, 151, 151, 31, 242, 98, 243, 166, 208, 241, 46, 229, 98, 241, 193, 229, 151, 31, 244, 166, 243, 46, 228, 31, 157, 238, 57, 31, 245, 98, 240, 166, 208, 51, 208, 225, 67, 240, 209, 315, 151, 74, 246, 208, 211, 151, 98, 51, 208, 225, 67, 240, 209, 315, 67, 314, 151, 74, 246, 208, 211, 151, 151, 31, 247, 166, 51, 208, 226, 67, 245, 151, 74, 246, 208, 211, 151, 31, 248, 98, 249, 166, 208, 51, 208, 224, 67, 245, 151, 74, 246, 208, 211, 151, 98, 51, 208, 224, 67, 245, 67, 314, 151, 74, 246, 208, 211, 151, 151, 31, 227, 166, 249, 4, 248, 31, 160, 31, 29, 57, 31, 245, 166, 242, 31, 248, 98, 249, 166, 208, 245, 209, 227, 98, 245, 209, 227, 67, 227, 151, 31, 247, 166, 245, 209, 59, 208, 227, 98, 237, 151, 31, 52, 31, 212, 148, 208, 248, 209, 229, 67, 243, 151, 209, 231, 31, 222, 148, 208, 248, 209, 229, 67, 243, 151, 209, 232, 31, 220, 148, 208, 248, 209, 229, 67, 243, 151, 209, 231, 31, 221, 148, 208, 248, 209, 229, 67, 243, 151, 209, 231, 31, 218, 148, 248, 209, 229, 67, 243, 31, 219, 148, 248, 209, 229, 67, 243, 31, 213, 148, 208, 248, 209, 230, 67, 244, 151, 209, 231, 31, 214, 148, 208, 248, 209, 230, 67, 244, 151, 209, 232, 31, 216, 148, 208, 247, 209, 230, 67, 244, 151, 209, 231, 209, 231, 31, 157, 239, 57, 31, 215, 148, 248, 209, 229, 67, 243, 31, 223, 148, 248, 209, 229, 67, 243, 31, 160, 31, 250, 166, 230, 209, 231, 209, 231, 31, 251, 166, 217, 209, 316, 31, 252, 166, 188, 208, 213, 98, 208, 227, 98, 231, 151, 98, 208, 230, 209, 231, 98, 314, 151, 98, 208, 240, 209, 233, 98, 313, 151, 98, 208, 233, 98, 235, 151, 98, 208, 314, 98, 313, 151, 151, 31, 253, 166, 51, 208, 252, 98, 254, 166, 208, 313, 98, 314, 151, 151, 31, 255, 166, 188, 208, 214, 98, 208, 227, 98, 231, 151, 98, 208, 230, 209, 231, 98, 314, 151, 98, 208, 240, 209, 233, 98, 313, 151, 98, 208, 233, 98, 235, 151, 98, 208, 314, 98, 313, 151, 151, 31, 256, 166, 51, 208, 255, 98, 254, 166, 208, 313, 98, 314, 151, 151, 31, 157, 239, 57, 31, 257, 166, 152, 208, 196, 233, 26, 98, 82, 166, 129, 151, 31, 258, 166, 188, 208, 215, 98, 208, 227, 98, 151, 98, 208, 229, 98, 151, 98, 208, 240, 209, 233, 98, 151, 98, 208, 233, 98, 151, 98, 208, 313, 98, 151, 151, 31, 257, 148, 51, 208, 258, 98, 254, 166, 208, 313, 98, 151, 151, 31, 259, 166, 152, 208, 196, 233, 26, 98, 82, 166, 129, 151, 31, 160, 31, 29, 57, 31, 257, 166, 172, 31, 259, 166, 172, 31, 52, 31, 260, 166, 152, 208, 196, 233, 98, 231, 26, 98, 82, 166, 129, 151, 31, 261, 166, 152, 208, 196, 233, 98, 231, 26, 98, 82, 166, 129, 151, 31, 262, 166, 208, 240, 209, 233, 46, 237, 151, 74, 246, 208, 211, 151, 31, 263, 166, 207, 208, 227, 41, 237, 151, 74, 246, 208, 211, 151, 209, 237, 31, 157, 240, 209, 233, 1, 263, 57, 31, 157, 227, 193, 237, 161, 313, 57, 31, 264, 166, 263, 46, 237, 31, 265, 166, 152, 208, 196, 233, 98, 235, 26, 98, 82, 166, 129, 151, 31, 265, 148, 253, 31, 120, 266, 138, 5, 208, 262, 67, 314, 98, 264, 151, 57, 31, 267, 166, 188, 208, 216, 67, 266, 209, 250, 98, 208, 231, 98, 231, 151, 98, 208, 231, 98, 314, 151, 98, 208, 313, 98, 313, 151, 98, 208, 235, 98, 235, 151, 98, 208, 314, 98, 313, 151, 151, 31, 268, 166, 51, 208, 267, 98, 254, 166, 208, 313, 98, 314, 151, 151, 31, 265, 166, 265, 4, 15, 208, 265, 74, 246, 208, 268, 74, 82, 151, 98, 65, 208, 268, 151, 151, 31, 70, 31, 269, 166, 265, 74, 246, 208, 253, 74, 82, 151, 31, 120, 270, 138, 5, 208, 30, 208, 227, 41, 234, 151, 74, 246, 208, 211, 151, 209, 234, 4, 234, 98, 263, 4, 234, 98, 4, 234, 151, 57, 31, 271, 166, 188, 208, 219, 98, 208, 227, 98, 151, 98, 208, 229, 98, 151, 98, 208, 270, 98, 151, 98, 208, 234, 98, 151, 98, 208, 313, 98, 151, 151, 31, 272, 166, 188, 208, 218, 98, 208, 227, 98, 151, 98, 208, 229, 98, 151, 98, 208, 270, 98, 151, 98, 208, 234, 98, 151, 98, 208, 313, 98, 151, 151, 31, 273, 166, 51, 208, 271, 98, 254, 166, 208, 313, 98, 151, 151, 31, 274, 166, 51, 208, 272, 98, 254, 166, 208, 313, 98, 151, 151, 31, 275, 166, 188, 208, 212, 98, 208, 227, 98, 231, 151, 98, 208, 229, 209, 231, 98, 314, 151, 98, 208, 270, 98, 313, 151, 98, 208, 234, 98, 235, 151, 98, 208, 314, 98, 313, 151, 151, 31, 276, 166, 51, 208, 275, 98, 254, 166, 208, 313, 98, 314, 151, 151, 31, 277, 166, 15, 208, 276, 98, 65, 208, 269, 151, 151, 31, 157, 239, 57, 31, 278, 166, 188, 208, 215, 98, 208, 227, 98, 151, 98, 208, 229, 98, 151, 98, 208, 270, 98, 151, 98, 208, 234, 98, 151, 98, 208, 313, 98, 151, 151, 31, 279, 166, 51, 208, 278, 98, 254, 166, 208, 313, 98, 151, 151, 31, 277, 166, 277, 67, 279, 196, 57, 98, 172, 26, 4, 257, 196, 172, 98, 57, 26, 31, 277, 166, 175, 208, 208, 270, 67, 68, 208, 313, 98, 234, 151, 1, 227, 151, 196, 57, 98, 172, 26, 98, 277, 98, 280, 208, 317, 151, 151, 31, 160, 31, 281, 166, 136, 208, 277, 209, 251, 4, 274, 196, 57, 98, 172, 26, 151, 31, 282, 166, 188, 208, 222, 98, 208, 227, 98, 232, 151, 98, 208, 229, 209, 232, 98, 314, 151, 98, 208, 270, 98, 313, 151, 98, 208, 234, 98, 236, 151, 98, 208, 314, 98, 313, 151, 151, 31, 283, 166, 51, 208, 282, 98, 254, 166, 208, 313, 98, 314, 151, 151, 31, 261, 148, 15, 208, 65, 208, 281, 74, 246, 208, 283, 74, 82, 151, 151, 98, 283, 151, 31, 284, 166, 15, 208, 283, 98, 65, 208, 256, 151, 151, 31, 285, 166, 208, 284, 4, 273, 196, 57, 98, 172, 26, 151, 209, 281, 209, 217, 31, 157, 239, 57, 31, 259, 2, 190, 208, 285, 98, 286, 166, 313, 151, 31, 160, 31, 285, 166, 285, 74, 246, 208, 256, 74, 82, 151, 31, 260, 148, 15, 208, 65, 208, 285, 151, 98, 276, 151, 31, 70, 31, 160, 31, 120, 287, 138, 5, 208, 263, 98, 240, 209, 233, 67, 237, 98, 4, 237, 151, 57, 31, 264, 166, 287, 46, 237, 4, 314, 31, 265, 166, 152, 208, 196, 233, 98, 235, 26, 98, 82, 166, 129, 151, 31, 265, 148, 253, 31, 120, 266, 138, 5, 208, 262, 67, 314, 98, 264, 151, 57, 31, 267, 166, 188, 208, 216, 67, 266, 209, 250, 98, 208, 231, 98, 231, 151, 98, 208, 231, 98, 314, 151, 98, 208, 313, 98, 313, 151, 98, 208, 235, 98, 235, 151, 98, 208, 314, 98, 313, 151, 151, 31, 268, 166, 51, 208, 267, 98, 254, 166, 208, 313, 98, 314, 151, 151, 31, 265, 166, 265, 4, 15, 208, 265, 74, 246, 208, 268, 74, 82, 151, 98, 65, 208, 268, 151, 151, 31, 70, 31, 269, 166, 265, 74, 246, 208, 253, 74, 82, 151, 31, 267, 166, 188, 208, 216, 67, 264, 209, 250, 98, 208, 231, 98, 231, 151, 98, 208, 231, 98, 314, 151, 98, 208, 313, 98, 313, 151, 98, 208, 235, 98, 235, 151, 98, 208, 314, 98, 313, 151, 151, 31, 268, 166, 51, 208, 267, 98, 254, 166, 208, 313, 98, 314, 151, 151, 31, 260, 166, 260, 4, 15, 208, 260, 74, 246, 208, 268, 74, 82, 151, 98, 268, 151, 31, 120, 270, 138, 5, 208, 287, 4, 234, 98, 287, 4, 237, 4, 234, 98, 4, 234, 151, 57, 31, 271, 166, 188, 208, 219, 98, 208, 227, 98, 151, 98, 208, 229, 98, 151, 98, 208, 270, 98, 151, 98, 208, 234, 98, 151, 98, 208, 313, 98, 151, 151, 31, 272, 166, 188, 208, 218, 98, 208, 227, 98, 151, 98, 208, 229, 98, 151, 98, 208, 270, 98, 151, 98, 208, 234, 98, 151, 98, 208, 313, 98, 151, 151, 31, 273, 166, 51, 208, 271, 98, 254, 166, 208, 313, 98, 151, 151, 31, 274, 166, 51, 208, 272, 98, 254, 166, 208, 313, 98, 151, 151, 31, 275, 166, 188, 208, 212, 98, 208, 227, 98, 231, 151, 98, 208, 229, 209, 231, 98, 314, 151, 98, 208, 270, 98, 313, 151, 98, 208, 234, 98, 235, 151, 98, 208, 314, 98, 313, 151, 151, 31, 276, 166, 51, 208, 275, 98, 254, 166, 208, 313, 98, 314, 151, 151, 31, 277, 166, 15, 208, 276, 98, 65, 208, 269, 151, 151, 31, 157, 239, 57, 31, 278, 166, 188, 208, 215, 98, 208, 227, 98, 151, 98, 208, 229, 98, 151, 98, 208, 270, 98, 151, 98, 208, 234, 98, 151, 98, 208, 313, 98, 151, 151, 31, 279, 166, 51, 208, 278, 98, 254, 166, 208, 313, 98, 151, 151, 31, 277, 166, 277, 67, 279, 196, 57, 98, 172, 26, 4, 257, 196, 172, 98, 57, 26, 31, 277, 166, 175, 208, 208, 270, 67, 68, 208, 313, 98, 234, 151, 1, 227, 151, 196, 57, 98, 172, 26, 98, 277, 98, 280, 208, 317, 151, 151, 31, 160, 31, 281, 166, 136, 208, 277, 209, 251, 4, 274, 196, 57, 98, 172, 26, 151, 31, 282, 166, 188, 208, 222, 98, 208, 227, 98, 232, 151, 98, 208, 229, 209, 232, 98, 314, 151, 98, 208, 270, 98, 313, 151, 98, 208, 234, 98, 236, 151, 98, 208, 314, 98, 313, 151, 151, 31, 283, 166, 51, 208, 282, 98, 254, 166, 208, 313, 98, 314, 151, 151, 31, 261, 148, 15, 208, 65, 208, 281, 74, 246, 208, 283, 74, 82, 151, 151, 98, 283, 151, 31, 284, 166, 15, 208, 283, 98, 65, 208, 256, 151, 151, 31, 285, 166, 208, 284, 4, 273, 196, 57, 98, 172, 26, 151, 209, 281, 209, 217, 31, 157, 239, 57, 31, 259, 2, 190, 208, 285, 98, 286, 166, 313, 151, 31, 160, 31, 285, 166, 285, 74, 246, 208, 256, 74, 82, 151, 31, 260, 148, 15, 208, 65, 208, 285, 151, 98, 276, 151, 31, 70, 31, 70, 31, 160, 31, 288, 166, 188, 208, 220, 98, 208, 227, 98, 231, 151, 98, 208, 229, 209, 231, 98, 314, 151, 98, 208, 240, 209, 233, 98, 313, 151, 98, 208, 233, 98, 235, 151, 98, 208, 314, 98, 313, 151, 151, 31, 10, 208, 288, 98, 260, 74, 246, 208, 220, 74, 82, 74, 103, 151, 98, 254, 166, 208, 313, 98, 314, 151, 151, 31, 168, 208, 221, 67, 208, 240, 209, 233, 67, 68, 208, 313, 98, 233, 151, 151, 196, 57, 98, 172, 26, 209, 229, 209, 231, 67, 68, 208, 313, 98, 231, 151, 196, 172, 98, 57, 26, 98, 261, 98, 289, 166, 318, 151, 31, 157, 239, 57, 31, 168, 208, 223, 67, 208, 240, 209, 233, 67, 68, 208, 313, 98, 233, 151, 151, 209, 229, 98, 259, 98, 289, 166, 318, 151, 31, 160, 31, 3, 31]}, {"code": "def parallel_path_bwd_dq_kernel(\n    q,\n    k,\n    v,\n    g_cumsum,\n    hc_whole,\n    scale,\n    L,\n    D,\n    dq,\n    do,\n    dhc_whole,\n    dg_cumsum,\n    cu_seqlens,\n    indices,\n    split_offsets,\n    T,\n    G: tl.constexpr,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    S: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    USE_GATE: tl.constexpr,\n):\n    i_t, i_nh = tl.program_id(0), tl.program_id(1)\n    i_n, i_hq = i_nh // HQ, i_nh % HQ\n    i_h = i_hq // G\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(\n            indices + i_t * 2 + 1\n        ).to(tl.int32)\n        boh_large = tl.load(split_offsets + i_n).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        boh_large = i_n * tl.cdiv(T, S)\n\n    q += (bos * HQ + i_hq) * K\n    dq += (bos * HQ + i_hq) * K\n    k += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V\n    do += (bos * HQ + i_hq) * V\n    hc_whole += (boh_large * H + i_h) * K * K\n    dhc_whole += (boh_large * HQ + i_hq) * K * K\n    L += bos * HQ + i_hq\n    D += bos * HQ + i_hq\n    if USE_GATE:\n        g_cumsum += bos * HQ + i_hq\n        dg_cumsum += bos * HQ + i_hq\n\n    stride_h = H * K * K\n    stride_hq = HQ * K * K\n    sm_scale = scale * 1.44269504\n\n    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    b_q_origin = tl.load(p_q, boundary_check=(0, 1))\n    p_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n\n    p_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    p_d = tl.make_block_ptr(D, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_l = tl.load(p_l, boundary_check=(0,))\n    b_delta = tl.load(p_d, boundary_check=(0,))\n\n    if USE_GATE:\n        b_g_cumsum_q = tl.zeros(\n            [\n                BT,\n            ],\n            dtype=tl.float32,\n        )\n        p_g_cumsum_q = tl.make_block_ptr(\n            g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,)\n        )\n        b_g_cumsum_q += tl.load(p_g_cumsum_q, boundary_check=(0,))\n        b_dg_cumsum_q = tl.zeros(\n            [\n                BT,\n            ],\n            dtype=tl.float32,\n        )\n    else:\n        b_g_cumsum_q = None\n        b_dg_cumsum_q = None\n\n    idx_i = i_t * BT // S\n    curr_end = (tl.floor(i_t * BT / S).to(tl.int32) * S).to(tl.int32)\n    b_dq = tl.zeros([BT, K], dtype=tl.float32)\n\n    for offset_outer in range(0, curr_end, S):\n        idx_j = offset_outer // S\n        b_q_accum = tl.zeros([BT, BK], dtype=tl.float32)\n        b_q_accum += b_q_origin\n        for i in range(idx_i - 1, idx_j, -1):\n            p_h = tl.make_block_ptr(\n                hc_whole + i * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)\n            )\n            b_h = tl.load(p_h, boundary_check=(0, 1))\n            b_q_accum = b_q_accum - tl.dot(b_q_accum.to(b_h.dtype), b_h)\n        b_q = b_q_accum.to(b_q_origin.dtype)\n        b_dh = -tl.dot(tl.trans(b_q), b_dq.to(b_q.dtype))\n\n        tl.atomic_add(\n            dhc_whole\n            + idx_j * stride_hq\n            + tl.arange(0, K)[:, None] * K\n            + tl.arange(0, K)[None, :],\n            b_dh,\n            sem=\"relaxed\",\n        )\n        p_h = tl.make_block_ptr(\n            hc_whole + idx_j * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)\n        )\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dq = b_dq - tl.dot(b_dq.to(b_h.dtype), tl.trans(b_h))\n\n        for offset in range(offset_outer, min(offset_outer + S, i_t * BT), BS):\n            p_k = tl.make_block_ptr(\n                k, (T, K), (H * K, 1), (offset, 0), (BS, BK), (1, 0)\n            )\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_A = tl.dot(b_q, tl.trans(b_k))\n            if USE_GATE:\n                p_g_cumsum_k = tl.make_block_ptr(\n                    g_cumsum, (T,), (HQ,), (offset,), (BS,), (0,)\n                )\n                b_g_cumsum_k = tl.load(p_g_cumsum_k, boundary_check=(0,))\n                b_A = b_A + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]\n                b_A = tl.where(\n                    (i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float(\"-inf\")\n                )\n            b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])\n            p_v = tl.make_block_ptr(\n                v, (V, T), (1, V * H), (0, offset), (BK, BS), (0, 1)\n            )\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n            b_dp = tl.dot(b_do, b_v)\n            b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale\n            b_dq += tl.dot(b_dA.to(b_k.dtype), b_k)\n            if USE_GATE:\n                b_dg_cumsum_q += tl.sum(b_dA, axis=1)\n\n    p_dq = tl.make_block_ptr(dq, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(dq.dtype.element_ty), boundary_check=(0, 1))\n    if USE_GATE:\n        tl.atomic_add(\n            dg_cumsum + (i_t * BT + tl.arange(0, BT)) * HQ, b_dg_cumsum_q, sem=\"relaxed\"\n        )", "encoded": [28, 312, 208, 212, 99, 213, 99, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 99, 224, 99, 225, 99, 226, 99, 227, 99, 228, 58, 6, 99, 229, 58, 6, 99, 230, 58, 6, 99, 231, 58, 6, 99, 232, 58, 6, 99, 233, 58, 6, 99, 234, 58, 6, 99, 235, 58, 6, 99, 236, 58, 6, 99, 237, 58, 6, 99, 238, 58, 6, 99, 239, 58, 6, 151, 58, 31, -1, 240, 99, 241, 166, 208, 147, 208, 313, 151, 99, 147, 208, 314, 151, 151, 31, 242, 99, 243, 166, 208, 241, 46, 229, 99, 241, 193, 229, 151, 31, 244, 166, 243, 46, 228, 31, 157, 238, 58, 31, 242, 99, 240, 166, 208, 53, 208, 225, 68, 240, 209, 315, 151, 75, 245, 208, 57, 151, 99, 53, 208, 225, 68, 240, 209, 315, 68, 314, 151, 75, 245, 208, 57, 151, 151, 31, 246, 166, 53, 208, 226, 68, 242, 151, 75, 245, 208, 57, 151, 31, 247, 99, 248, 166, 208, 53, 208, 224, 68, 242, 151, 75, 245, 208, 57, 151, 99, 53, 208, 224, 68, 242, 68, 314, 151, 75, 245, 208, 57, 151, 151, 31, 227, 166, 248, 4, 247, 31, 160, 31, 29, 58, 31, 247, 99, 248, 166, 208, 242, 209, 227, 99, 242, 209, 227, 68, 227, 151, 31, 246, 166, 242, 209, 60, 208, 227, 99, 237, 151, 31, 52, 31, 212, 148, 208, 247, 209, 229, 68, 243, 151, 209, 231, 31, 220, 148, 208, 247, 209, 229, 68, 243, 151, 209, 231, 31, 213, 148, 208, 247, 209, 230, 68, 244, 151, 209, 231, 31, 214, 148, 208, 247, 209, 230, 68, 244, 151, 209, 232, 31, 221, 148, 208, 247, 209, 229, 68, 243, 151, 209, 232, 31, 216, 148, 208, 246, 209, 230, 68, 244, 151, 209, 231, 209, 231, 31, 222, 148, 208, 246, 209, 229, 68, 243, 151, 209, 231, 209, 231, 31, 218, 148, 247, 209, 229, 68, 243, 31, 219, 148, 247, 209, 229, 68, 243, 31, 157, 239, 58, 31, 215, 148, 247, 209, 229, 68, 243, 31, 223, 148, 247, 209, 229, 68, 243, 31, 160, 31, 249, 166, 230, 209, 231, 209, 231, 31, 250, 166, 229, 209, 231, 209, 231, 31, 251, 166, 217, 209, 316, 31, 252, 166, 188, 208, 212, 99, 208, 227, 99, 231, 151, 99, 208, 229, 209, 231, 99, 314, 151, 99, 208, 240, 209, 233, 99, 313, 151, 99, 208, 233, 99, 235, 151, 99, 208, 314, 99, 313, 151, 151, 31, 253, 166, 53, 208, 252, 99, 254, 166, 208, 313, 99, 314, 151, 151, 31, 255, 166, 188, 208, 221, 99, 208, 227, 99, 232, 151, 99, 208, 229, 209, 232, 99, 314, 151, 99, 208, 240, 209, 233, 99, 313, 151, 99, 208, 233, 99, 236, 151, 99, 208, 314, 99, 313, 151, 151, 31, 256, 166, 53, 208, 255, 99, 254, 166, 208, 313, 99, 314, 151, 151, 31, 257, 166, 188, 208, 218, 99, 208, 227, 99, 151, 99, 208, 229, 99, 151, 99, 208, 240, 209, 233, 99, 151, 99, 208, 233, 99, 151, 99, 208, 313, 99, 151, 151, 31, 258, 166, 188, 208, 219, 99, 208, 227, 99, 151, 99, 208, 229, 99, 151, 99, 208, 240, 209, 233, 99, 151, 99, 208, 233, 99, 151, 99, 208, 313, 99, 151, 151, 31, 259, 166, 53, 208, 257, 99, 254, 166, 208, 313, 99, 151, 151, 31, 260, 166, 53, 208, 258, 99, 254, 166, 208, 313, 99, 151, 151, 31, 157, 239, 58, 31, 261, 166, 152, 208, 196, 233, 26, 99, 83, 166, 130, 151, 31, 262, 166, 188, 208, 215, 99, 208, 227, 99, 151, 99, 208, 229, 99, 151, 99, 208, 240, 209, 233, 99, 151, 99, 208, 233, 99, 151, 99, 208, 313, 99, 151, 151, 31, 261, 148, 53, 208, 262, 99, 254, 166, 208, 313, 99, 151, 151, 31, 263, 166, 152, 208, 196, 233, 26, 99, 83, 166, 130, 151, 31, 160, 31, 29, 58, 31, 261, 166, 172, 31, 263, 166, 172, 31, 52, 31, 264, 166, 240, 209, 233, 46, 237, 31, 265, 166, 208, 207, 208, 240, 209, 233, 41, 237, 151, 75, 245, 208, 57, 151, 209, 237, 151, 75, 245, 208, 57, 151, 31, 266, 166, 152, 208, 196, 233, 99, 231, 26, 99, 83, 166, 130, 151, 31, 121, 267, 138, 5, 208, 313, 99, 265, 99, 237, 151, 58, 31, 268, 166, 267, 46, 237, 31, 269, 166, 152, 208, 196, 233, 99, 235, 26, 99, 83, 166, 130, 151, 31, 269, 148, 253, 31, 121, 270, 138, 5, 208, 264, 4, 314, 99, 268, 99, 4, 314, 151, 58, 31, 271, 166, 188, 208, 216, 68, 270, 209, 249, 99, 208, 231, 99, 231, 151, 99, 208, 231, 99, 314, 151, 99, 208, 313, 99, 313, 151, 99, 208, 235, 99, 235, 151, 99, 208, 314, 99, 313, 151, 151, 31, 272, 166, 53, 208, 271, 99, 254, 166, 208, 313, 99, 314, 151, 151, 31, 269, 166, 269, 4, 15, 208, 269, 75, 245, 208, 272, 75, 83, 151, 99, 272, 151, 31, 71, 31, 273, 166, 269, 75, 245, 208, 253, 75, 83, 151, 31, 274, 166, 4, 15, 208, 66, 208, 273, 151, 99, 266, 75, 245, 208, 273, 75, 83, 151, 151, 31, 168, 208, 222, 68, 268, 209, 250, 68, 69, 208, 313, 99, 231, 151, 196, 58, 99, 172, 26, 209, 231, 68, 69, 208, 313, 99, 231, 151, 196, 172, 99, 58, 26, 99, 274, 99, 275, 166, 317, 151, 31, 271, 166, 188, 208, 216, 68, 268, 209, 249, 99, 208, 231, 99, 231, 151, 99, 208, 231, 99, 314, 151, 99, 208, 313, 99, 313, 151, 99, 208, 235, 99, 235, 151, 99, 208, 314, 99, 313, 151, 151, 31, 272, 166, 53, 208, 271, 99, 254, 166, 208, 313, 99, 314, 151, 151, 31, 266, 166, 266, 4, 15, 208, 266, 75, 245, 208, 272, 75, 83, 151, 99, 66, 208, 272, 151, 151, 31, 121, 276, 138, 5, 208, 267, 99, 38, 208, 267, 68, 237, 99, 240, 209, 233, 151, 99, 234, 151, 58, 31, 277, 166, 188, 208, 213, 99, 208, 227, 99, 231, 151, 99, 208, 230, 209, 231, 99, 314, 151, 99, 208, 276, 99, 313, 151, 99, 208, 234, 99, 235, 151, 99, 208, 314, 99, 313, 151, 151, 31, 278, 166, 53, 208, 277, 99, 254, 166, 208, 313, 99, 314, 151, 151, 31, 279, 166, 15, 208, 273, 99, 66, 208, 278, 151, 151, 31, 157, 239, 58, 31, 280, 166, 188, 208, 215, 99, 208, 227, 99, 151, 99, 208, 229, 99, 151, 99, 208, 276, 99, 151, 99, 208, 234, 99, 151, 99, 208, 313, 99, 151, 151, 31, 281, 166, 53, 208, 280, 99, 254, 166, 208, 313, 99, 151, 151, 31, 279, 166, 279, 68, 261, 196, 58, 99, 172, 26, 4, 281, 196, 172, 99, 58, 26, 31, 279, 166, 175, 208, 208, 240, 209, 233, 68, 69, 208, 313, 99, 233, 151, 1, 227, 151, 196, 58, 99, 172, 26, 99, 279, 99, 282, 208, 318, 151, 151, 31, 160, 31, 283, 166, 137, 208, 279, 209, 251, 4, 259, 196, 58, 99, 172, 26, 151, 31, 284, 166, 188, 208, 214, 99, 208, 232, 99, 227, 151, 99, 208, 314, 99, 232, 209, 230, 151, 99, 208, 313, 99, 276, 151, 99, 208, 235, 99, 234, 151, 99, 208, 313, 99, 314, 151, 151, 31, 285, 166, 53, 208, 284, 99, 254, 166, 208, 313, 99, 314, 151, 151, 31, 286, 166, 15, 208, 256, 99, 285, 151, 31, 287, 166, 208, 286, 4, 260, 196, 58, 99, 172, 26, 151, 209, 283, 209, 217, 31, 266, 148, 15, 208, 287, 75, 245, 208, 278, 75, 83, 151, 99, 278, 151, 31, 157, 239, 58, 31, 263, 148, 190, 208, 287, 99, 288, 166, 314, 151, 31, 160, 31, 71, 31, 71, 31, 289, 166, 188, 208, 220, 99, 208, 227, 99, 231, 151, 99, 208, 231, 209, 229, 99, 314, 151, 99, 208, 240, 209, 233, 99, 313, 151, 99, 208, 233, 99, 235, 151, 99, 208, 314, 99, 313, 151, 151, 31, 10, 208, 289, 99, 266, 75, 245, 208, 220, 75, 83, 75, 104, 151, 99, 254, 166, 208, 313, 99, 314, 151, 151, 31, 157, 239, 58, 31, 168, 208, 223, 68, 208, 240, 209, 233, 68, 69, 208, 313, 99, 233, 151, 151, 209, 229, 99, 263, 99, 275, 166, 317, 151, 31, 160, 31, 3, 31]}, {"code": "def parallel_path_bwd_intra_chunk_kernel(\n    q,\n    k,\n    v,\n    g_cumsum,\n    h,\n    L,\n    D,\n    dq,\n    dq_new,\n    dk,\n    dv,\n    dh,\n    do,\n    dg_cumsum,\n    offsets,\n    indices,\n    chunk_offsets,\n    T,\n    scale,\n    G: tl.constexpr,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    BT: tl.constexpr,\n    S: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    USE_GATE: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // G\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(\n            indices + i_t * 2 + 1\n        ).to(tl.int32)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(\n            tl.int32\n        )\n        T = eos - bos\n    else:\n        i_n = i_b\n        bos, eos = i_n * T, i_n * T + T\n        boh = i_n * tl.cdiv(T, BT)\n\n    k += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V\n    h += (boh * H + i_h) * K * K\n\n    q += (bos * HQ + i_hq) * K\n    dq += (bos * HQ + i_hq) * K\n    dq_new += (bos * HQ + i_hq) * K\n    dk += (bos * HQ + i_hq) * K\n    dv += (bos * HQ + i_hq) * V\n    do += (bos * HQ + i_hq) * V\n    dh += (boh * HQ + i_hq) * K * K\n    L += bos * HQ + i_hq\n    D += bos * HQ + i_hq\n    if USE_GATE:\n        g_cumsum += bos * HQ + i_hq\n        dg_cumsum += bos * HQ + i_hq\n\n    sm_scale = scale * 1.44269504\n\n    p_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n\n    p_delta = tl.make_block_ptr(D, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_delta = tl.load(p_delta, boundary_check=(0,))\n    p_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_l = tl.load(p_l, boundary_check=(0,))\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    p_dq = tl.make_block_ptr(dq, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    b_dq += tl.load(p_dq, boundary_check=(0, 1))\n    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n\n    if USE_GATE:\n        p_gq_cumsum = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n        b_gq_cumsum = tl.load(p_gq_cumsum, boundary_check=(0,))\n        b_dgq = tl.zeros(\n            [\n                BT,\n            ],\n            dtype=tl.float32,\n        )\n    else:\n        b_dgq = None\n\n    curr_start = (tl.floor(i_t * BT / S).to(tl.int32) * S).to(tl.int32)\n\n    for offset in range(curr_start, i_t * BT, BT):\n        p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (offset, 0), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_q_tmp = tl.zeros([BT, BK], dtype=tl.float32)\n        b_q_tmp += b_q\n\n        for i_t_small in range(i_t * BT - BT, offset, -BT):\n            p_h = tl.make_block_ptr(\n                h + tl.cdiv(i_t_small, BT) * H * K * K,\n                (K, K),\n                (K, 1),\n                (0, 0),\n                (BK, BK),\n                (1, 0),\n            )\n            b_h = tl.load(p_h, boundary_check=(0, 1))\n            b_q_tmp -= tl.dot(b_q_tmp.to(b_h.dtype), b_h)\n\n        b_q2 = b_q_tmp.to(b_k.dtype)\n        b_dh = -tl.dot(tl.trans(b_q2), b_dq.to(b_q2.dtype))\n        tl.atomic_add(\n            dh\n            + tl.cdiv(offset, BT) * HQ * K * K\n            + tl.arange(0, K)[:, None] * K\n            + tl.arange(0, K)[None, :],\n            b_dh,\n            sem=\"relaxed\",\n        )\n\n        b_A = tl.dot(b_q2, tl.trans(b_k))\n        if USE_GATE:\n            p_gk_cumsum = tl.make_block_ptr(\n                g_cumsum, (T,), (HQ,), (offset,), (BT,), (0,)\n            )\n            b_gk_cumsum = tl.load(p_gk_cumsum, boundary_check=(0,))\n            b_A = b_A + b_gq_cumsum[:, None] - b_gk_cumsum[None, :]\n            b_A = tl.where(\n                (i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float(\"-inf\")\n            )\n\n        b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])\n        b_dv = tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)\n\n        tl.atomic_add(\n            dv\n            + ((offset + tl.arange(0, BT)) * HQ * V)[:, None]\n            + tl.arange(0, BV)[None, :],\n            b_dv.to(dv.dtype.element_ty),\n            sem=\"relaxed\",\n        )\n\n        p_v = tl.make_block_ptr(v, (T, V), (V * H, 1), (offset, 0), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_dp = tl.dot(b_do, tl.trans(b_v))\n        b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale\n        if USE_GATE:\n            b_dgk = -tl.sum(b_dA, axis=0)\n            tl.atomic_add(\n                dg_cumsum + (offset + tl.arange(0, BT)) * HQ, b_dgk, sem=\"relaxed\"\n            )\n            b_dgq += tl.sum(b_dA, axis=1)\n\n        b_dA = b_dA.to(b_k.dtype)\n        p_h = tl.make_block_ptr(\n            h + tl.cdiv(offset, BT) * H * K * K,\n            (K, K),\n            (1, K),\n            (0, 0),\n            (BK, BK),\n            (0, 1),\n        )\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dk = tl.dot(tl.trans(b_dA), b_q2)\n        tl.atomic_add(\n            dk\n            + (offset + tl.arange(0, BT))[:, None] * HQ * K\n            + tl.arange(0, BK)[None, :],\n            b_dk,\n            sem=\"relaxed\",\n        )\n        b_dq -= tl.dot(b_dq.to(b_h.dtype), b_h)\n        b_dq += tl.dot(b_dA, b_k)\n\n    p_dq_new = tl.make_block_ptr(\n        dq_new, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    tl.store(p_dq_new, b_dq.to(dq_new.dtype.element_ty), boundary_check=(0, 1))\n    if USE_GATE:\n        tl.atomic_add(\n            dg_cumsum + (i_t * BT + tl.arange(0, BT)) * HQ, b_dgq, sem=\"relaxed\"\n        )", "encoded": [28, 312, 208, 212, 98, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 98, 227, 98, 228, 98, 229, 98, 230, 98, 231, 57, 6, 98, 232, 57, 6, 98, 233, 57, 6, 98, 234, 57, 6, 98, 235, 57, 6, 98, 236, 57, 6, 98, 237, 57, 6, 98, 238, 57, 6, 98, 239, 57, 6, 98, 240, 57, 6, 98, 241, 57, 6, 151, 57, 31, -1, 242, 98, 243, 166, 208, 147, 208, 313, 151, 98, 147, 208, 314, 151, 151, 31, 244, 98, 245, 166, 208, 243, 46, 232, 98, 243, 193, 232, 151, 31, 246, 166, 245, 46, 231, 31, 157, 240, 57, 31, 247, 98, 242, 166, 208, 51, 208, 227, 67, 242, 209, 315, 151, 74, 248, 208, 211, 151, 98, 51, 208, 227, 67, 242, 209, 315, 67, 314, 151, 74, 248, 208, 211, 151, 151, 31, 249, 166, 51, 208, 228, 67, 247, 151, 74, 248, 208, 211, 151, 31, 250, 98, 251, 166, 208, 51, 208, 226, 67, 247, 151, 74, 248, 208, 211, 151, 98, 51, 208, 226, 67, 247, 67, 314, 151, 74, 248, 208, 211, 151, 151, 31, 229, 166, 251, 4, 250, 31, 160, 31, 29, 57, 31, 247, 166, 244, 31, 250, 98, 251, 166, 208, 247, 209, 229, 98, 247, 209, 229, 67, 229, 151, 31, 249, 166, 247, 209, 59, 208, 229, 98, 238, 151, 31, 52, 31, 213, 148, 208, 250, 209, 233, 67, 246, 151, 209, 234, 31, 214, 148, 208, 250, 209, 233, 67, 246, 151, 209, 235, 31, 216, 148, 208, 249, 209, 233, 67, 246, 151, 209, 234, 209, 234, 31, 212, 148, 208, 250, 209, 232, 67, 245, 151, 209, 234, 31, 219, 148, 208, 250, 209, 232, 67, 245, 151, 209, 234, 31, 220, 148, 208, 250, 209, 232, 67, 245, 151, 209, 234, 31, 221, 148, 208, 250, 209, 232, 67, 245, 151, 209, 234, 31, 222, 148, 208, 250, 209, 232, 67, 245, 151, 209, 235, 31, 224, 148, 208, 250, 209, 232, 67, 245, 151, 209, 235, 31, 223, 148, 208, 249, 209, 232, 67, 245, 151, 209, 234, 209, 234, 31, 217, 148, 250, 209, 232, 67, 245, 31, 218, 148, 250, 209, 232, 67, 245, 31, 157, 241, 57, 31, 215, 148, 250, 209, 232, 67, 245, 31, 225, 148, 250, 209, 232, 67, 245, 31, 160, 31, 252, 166, 230, 209, 316, 31, 253, 166, 188, 208, 224, 98, 208, 229, 98, 235, 151, 98, 208, 232, 209, 235, 98, 314, 151, 98, 208, 242, 209, 238, 98, 313, 151, 98, 208, 238, 98, 237, 151, 98, 208, 314, 98, 313, 151, 151, 31, 254, 166, 51, 208, 253, 98, 255, 166, 208, 313, 98, 314, 151, 151, 31, 256, 166, 188, 208, 218, 98, 208, 229, 98, 151, 98, 208, 232, 98, 151, 98, 208, 242, 209, 238, 98, 151, 98, 208, 238, 98, 151, 98, 208, 313, 98, 151, 151, 31, 257, 166, 51, 208, 256, 98, 255, 166, 208, 313, 98, 151, 151, 31, 258, 166, 188, 208, 217, 98, 208, 229, 98, 151, 98, 208, 232, 98, 151, 98, 208, 242, 209, 238, 98, 151, 98, 208, 238, 98, 151, 98, 208, 313, 98, 151, 151, 31, 259, 166, 51, 208, 258, 98, 255, 166, 208, 313, 98, 151, 151, 31, 260, 166, 152, 208, 196, 238, 98, 236, 26, 98, 82, 166, 129, 151, 31, 261, 166, 188, 208, 219, 98, 208, 229, 98, 234, 151, 98, 208, 232, 209, 234, 98, 314, 151, 98, 208, 242, 209, 238, 98, 313, 151, 98, 208, 238, 98, 236, 151, 98, 208, 314, 98, 313, 151, 151, 31, 260, 148, 51, 208, 261, 98, 255, 166, 208, 313, 98, 314, 151, 151, 31, 262, 166, 188, 208, 212, 98, 208, 229, 98, 234, 151, 98, 208, 232, 209, 234, 98, 314, 151, 98, 208, 242, 209, 238, 98, 313, 151, 98, 208, 238, 98, 236, 151, 98, 208, 314, 98, 313, 151, 151, 31, 263, 166, 51, 208, 262, 98, 255, 166, 208, 313, 98, 314, 151, 151, 31, 157, 241, 57, 31, 264, 166, 188, 208, 215, 98, 208, 229, 98, 151, 98, 208, 232, 98, 151, 98, 208, 242, 209, 238, 98, 151, 98, 208, 238, 98, 151, 98, 208, 313, 98, 151, 151, 31, 265, 166, 51, 208, 264, 98, 255, 166, 208, 313, 98, 151, 151, 31, 266, 166, 152, 208, 196, 238, 26, 98, 82, 166, 129, 151, 31, 160, 31, 29, 57, 31, 266, 166, 172, 31, 52, 31, 267, 166, 208, 207, 208, 242, 209, 238, 41, 239, 151, 74, 248, 208, 211, 151, 209, 239, 151, 74, 248, 208, 211, 151, 31, 120, 268, 138, 5, 208, 267, 98, 242, 209, 238, 98, 238, 151, 57, 31, 269, 166, 188, 208, 213, 98, 208, 229, 98, 234, 151, 98, 208, 233, 209, 234, 98, 314, 151, 98, 208, 268, 98, 313, 151, 98, 208, 238, 98, 236, 151, 98, 208, 314, 98, 313, 151, 151, 31, 270, 166, 51, 208, 269, 98, 255, 166, 208, 313, 98, 314, 151, 151, 31, 271, 166, 152, 208, 196, 238, 98, 236, 26, 98, 82, 166, 129, 151, 31, 271, 148, 263, 31, 120, 272, 138, 5, 208, 242, 209, 238, 4, 238, 98, 268, 98, 4, 238, 151, 57, 31, 273, 166, 188, 208, 216, 67, 59, 208, 272, 98, 238, 151, 209, 233, 209, 234, 209, 234, 98, 208, 234, 98, 234, 151, 98, 208, 234, 98, 314, 151, 98, 208, 313, 98, 313, 151, 98, 208, 236, 98, 236, 151, 98, 208, 314, 98, 313, 151, 151, 31, 274, 166, 51, 208, 273, 98, 255, 166, 208, 313, 98, 314, 151, 151, 31, 271, 2, 15, 208, 271, 74, 248, 208, 274, 74, 82, 151, 98, 274, 151, 31, 70, 31, 275, 166, 271, 74, 248, 208, 270, 74, 82, 151, 31, 276, 166, 4, 15, 208, 65, 208, 275, 151, 98, 260, 74, 248, 208, 275, 74, 82, 151, 151, 31, 168, 208, 223, 67, 59, 208, 268, 98, 238, 151, 209, 232, 209, 234, 209, 234, 67, 68, 208, 313, 98, 234, 151, 196, 57, 98, 172, 26, 209, 234, 67, 68, 208, 313, 98, 234, 151, 196, 172, 98, 57, 26, 98, 276, 98, 277, 166, 317, 151, 31, 278, 166, 15, 208, 275, 98, 65, 208, 270, 151, 151, 31, 157, 241, 57, 31, 279, 166, 188, 208, 215, 98, 208, 229, 98, 151, 98, 208, 232, 98, 151, 98, 208, 268, 98, 151, 98, 208, 238, 98, 151, 98, 208, 313, 98, 151, 151, 31, 280, 166, 51, 208, 279, 98, 255, 166, 208, 313, 98, 151, 151, 31, 278, 166, 278, 67, 265, 196, 57, 98, 172, 26, 4, 280, 196, 172, 98, 57, 26, 31, 278, 166, 175, 208, 208, 242, 209, 238, 67, 68, 208, 313, 98, 238, 151, 1, 229, 151, 196, 57, 98, 172, 26, 98, 278, 98, 281, 208, 318, 151, 151, 31, 160, 31, 282, 166, 136, 208, 278, 209, 252, 4, 259, 196, 57, 98, 172, 26, 151, 31, 283, 166, 15, 208, 65, 208, 282, 74, 248, 208, 254, 74, 82, 151, 151, 98, 254, 151, 31, 168, 208, 222, 67, 208, 208, 268, 67, 68, 208, 313, 98, 238, 151, 151, 209, 232, 209, 235, 151, 196, 57, 98, 172, 26, 67, 68, 208, 313, 98, 237, 151, 196, 172, 98, 57, 26, 98, 283, 74, 248, 208, 222, 74, 82, 74, 103, 151, 98, 277, 166, 317, 151, 31, 284, 166, 188, 208, 214, 98, 208, 229, 98, 235, 151, 98, 208, 235, 209, 233, 98, 314, 151, 98, 208, 268, 98, 313, 151, 98, 208, 238, 98, 237, 151, 98, 208, 314, 98, 313, 151, 151, 31, 285, 166, 51, 208, 284, 98, 255, 166, 208, 313, 98, 314, 151, 151, 31, 286, 166, 15, 208, 254, 98, 65, 208, 285, 151, 151, 31, 287, 166, 208, 286, 4, 257, 196, 57, 98, 172, 26, 151, 209, 282, 209, 230, 31, 157, 241, 57, 31, 288, 166, 4, 190, 208, 287, 98, 289, 166, 313, 151, 31, 168, 208, 225, 67, 208, 268, 67, 68, 208, 313, 98, 238, 151, 151, 209, 232, 98, 288, 98, 277, 166, 317, 151, 31, 266, 148, 190, 208, 287, 98, 289, 166, 314, 151, 31, 160, 31, 287, 166, 287, 74, 248, 208, 270, 74, 82, 151, 31, 273, 166, 188, 208, 216, 67, 59, 208, 268, 98, 238, 151, 209, 233, 209, 234, 209, 234, 98, 208, 234, 98, 234, 151, 98, 208, 314, 98, 234, 151, 98, 208, 313, 98, 313, 151, 98, 208, 236, 98, 236, 151, 98, 208, 313, 98, 314, 151, 151, 31, 274, 166, 51, 208, 273, 98, 255, 166, 208, 313, 98, 314, 151, 151, 31, 290, 166, 15, 208, 65, 208, 287, 151, 98, 275, 151, 31, 168, 208, 221, 67, 208, 268, 67, 68, 208, 313, 98, 238, 151, 151, 196, 57, 98, 172, 26, 209, 232, 209, 234, 67, 68, 208, 313, 98, 236, 151, 196, 172, 98, 57, 26, 98, 290, 98, 277, 166, 317, 151, 31, 260, 2, 15, 208, 260, 74, 248, 208, 274, 74, 82, 151, 98, 274, 151, 31, 260, 148, 15, 208, 287, 98, 270, 151, 31, 70, 31, 291, 166, 188, 208, 220, 98, 208, 229, 98, 234, 151, 98, 208, 232, 209, 234, 98, 314, 151, 98, 208, 242, 209, 238, 98, 313, 151, 98, 208, 238, 98, 236, 151, 98, 208, 314, 98, 313, 151, 151, 31, 10, 208, 291, 98, 260, 74, 248, 208, 220, 74, 82, 74, 103, 151, 98, 255, 166, 208, 313, 98, 314, 151, 151, 31, 157, 241, 57, 31, 168, 208, 225, 67, 208, 242, 209, 238, 67, 68, 208, 313, 98, 238, 151, 151, 209, 232, 98, 266, 98, 277, 166, 317, 151, 31, 160, 31, 3, 31]}, {"code": "def parallel_path_fwd_kernel(\n    q,\n    k,\n    v,\n    o,\n    o_new,\n    g_cumsum,\n    h,\n    scale,\n    L,\n    L_new,\n    M,\n    offsets,\n    indices,\n    chunk_offsets,\n    T,\n    G: tl.constexpr,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    USE_GATE: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_hq = i_bh // HQ, i_bh % HQ\n    i_h = i_hq // G\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(\n            indices + i_t * 2 + 1\n        ).to(tl.int32)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(\n            tl.int32\n        )\n        T = eos - bos\n    else:\n        i_n = i_b\n        bos, eos = i_n * T, i_n * T + T\n        boh = i_n * tl.cdiv(T, BS)\n    sm_scale = scale * 1.44269504\n    p_q = tl.make_block_ptr(\n        q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    b_q = tl.zeros([BT, BK], dtype=tl.float32)\n    b_q += tl.load(p_q, boundary_check=(0, 1))\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    p_o = tl.make_block_ptr(\n        o + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n    )\n    b_o += tl.load(p_o, boundary_check=(0, 1))\n\n    p_L = tl.make_block_ptr(L + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    p_M = tl.make_block_ptr(M + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_l = tl.load(p_L, boundary_check=(0,))\n    b_m = tl.load(p_M, boundary_check=(0,))\n\n    if USE_GATE:\n        p_g_cumsum_q = tl.make_block_ptr(\n            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)\n        )\n        b_g_cumsum_q = tl.load(p_g_cumsum_q, boundary_check=(0,))\n    else:\n        b_g_cumsum_q = None\n\n    for offset in range((i_t + 1) * BT - 2 * BS, i_t * BT - BS, -BS):\n        i_tk = tl.cdiv(offset, BS)\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K, (K, T), (1, K * H), (0, offset), (BK, BS), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V, (T, V), (V * H, 1), (offset, 0), (BS, BV), (1, 0)\n        )\n        p_h = tl.make_block_ptr(\n            h + ((boh + i_tk) * H + i_h) * K * K,\n            (K, K),\n            (K, 1),\n            (0, 0),\n            (BK, BK),\n            (1, 0),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n\n        m_s = i_t * BT + tl.arange(0, BT) >= (offset + BS)\n        b_s = tl.dot(b_q.to(b_k.dtype), b_k)\n        b_q_minus = tl.dot(b_q.to(b_h.dtype), b_h)\n        b_q = tl.where(m_s[:, None], b_q - b_q_minus, b_q)\n        if USE_GATE:\n            p_g_cumsum_k = tl.make_block_ptr(\n                g_cumsum + (bos * HQ + i_hq), (T,), (HQ,), (offset,), (BS,), (0,)\n            )\n            b_g_cumsum_k = tl.load(p_g_cumsum_k, boundary_check=(0,))\n            b_s = b_s + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]\n        b_s = tl.where(m_s[:, None], b_s * sm_scale, float(\"-inf\"))\n        b_m_new = tl.maximum(b_m, tl.max(b_s, 1))\n        alpha = tl.math.exp2((b_m - b_m_new))\n        b_s = tl.math.exp2(b_s - b_m_new[:, None])\n        b_s = tl.where(m_s[:, None], b_s, 0)\n        b_o *= alpha[:, None]\n        b_l = b_l * alpha + tl.sum(b_s, 1)\n        b_m = b_m_new\n        b_o += tl.dot(b_s.to(b_v.dtype), b_v)\n\n    tl.debug_barrier()\n\n    for offset in range(i_t * BT - BS, -BS, -BS):\n        i_tk = tl.cdiv(offset, BS)\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K, (K, T), (1, K * H), (0, offset), (BK, BS), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V, (T, V), (V * H, 1), (offset, 0), (BS, BV), (1, 0)\n        )\n        p_h = tl.make_block_ptr(\n            h + ((boh + i_tk) * H + i_h) * K * K,\n            (K, K),\n            (K, 1),\n            (0, 0),\n            (BK, BK),\n            (1, 0),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q.to(b_k.dtype), b_k)\n        b_q -= tl.dot(b_q.to(b_h.dtype), b_h)\n        if USE_GATE:\n            p_g_cumsum_k = tl.make_block_ptr(\n                g_cumsum + (bos * HQ + i_hq), (T,), (HQ,), (offset,), (BS,), (0,)\n            )\n            b_g_cumsum_k = tl.load(p_g_cumsum_k, boundary_check=(0,))\n            b_s = b_s + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]\n        b_s = b_s * sm_scale\n        b_m_new = tl.maximum(b_m, tl.max(b_s, 1))\n        alpha = tl.math.exp2((b_m - b_m_new))\n        b_s = tl.math.exp2(b_s - b_m_new[:, None])\n        b_o *= alpha[:, None]\n        b_l = b_l * alpha + tl.sum(b_s, 1)\n        b_m = b_m_new\n        b_o += tl.dot(b_s.to(b_v.dtype), b_v)\n\n    b_o = b_o / b_l[:, None]\n    p_o_new = tl.make_block_ptr(\n        o_new + (bos * HQ + i_hq) * V,\n        (T, V),\n        (HQ * V, 1),\n        (i_t * BT, 0),\n        (BT, BV),\n        (1, 0),\n    )\n    tl.store(p_o_new, b_o.to(p_o_new.dtype.element_ty), boundary_check=(0, 1))\n    b_l = tl.math.log2(b_l) + b_m\n    p_L_new = tl.make_block_ptr(\n        L_new + (bos * HQ + i_hq), (T,), (HQ,), (i_t * BT,), (BT,), (0,)\n    )\n    tl.store(p_L_new, b_l.to(p_L_new.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 313, 209, 213, 99, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 99, 224, 99, 225, 99, 226, 99, 227, 99, 228, 58, 6, 99, 229, 58, 6, 99, 230, 58, 6, 99, 231, 58, 6, 99, 232, 58, 6, 99, 233, 58, 6, 99, 234, 58, 6, 99, 235, 58, 6, 99, 236, 58, 6, 99, 237, 58, 6, 99, 238, 58, 6, 152, 58, 31, -1, 239, 99, 240, 167, 209, 148, 209, 314, 152, 99, 148, 209, 315, 152, 152, 31, 241, 99, 242, 167, 209, 240, 46, 229, 99, 240, 194, 229, 152, 31, 243, 167, 242, 46, 228, 31, 158, 237, 58, 31, 244, 99, 239, 167, 209, 53, 209, 225, 68, 239, 210, 316, 152, 75, 245, 209, 57, 152, 99, 53, 209, 225, 68, 239, 210, 316, 68, 315, 152, 75, 245, 209, 57, 152, 152, 31, 246, 167, 53, 209, 226, 68, 244, 152, 75, 245, 209, 57, 152, 31, 247, 99, 248, 167, 209, 53, 209, 224, 68, 244, 152, 75, 245, 209, 57, 152, 99, 53, 209, 224, 68, 244, 68, 315, 152, 75, 245, 209, 57, 152, 152, 31, 227, 167, 248, 4, 247, 31, 161, 31, 29, 58, 31, 244, 167, 241, 31, 247, 99, 248, 167, 209, 244, 210, 227, 99, 244, 210, 227, 68, 227, 152, 31, 246, 167, 244, 210, 60, 209, 227, 99, 234, 152, 31, 52, 31, 249, 167, 220, 210, 317, 31, 250, 167, 189, 209, 213, 68, 209, 247, 210, 229, 68, 242, 152, 210, 231, 99, 209, 227, 99, 231, 152, 99, 209, 229, 210, 231, 99, 315, 152, 99, 209, 239, 210, 233, 99, 314, 152, 99, 209, 233, 99, 235, 152, 99, 209, 315, 99, 314, 152, 152, 31, 251, 167, 153, 209, 197, 233, 99, 235, 26, 99, 83, 167, 131, 152, 31, 251, 149, 53, 209, 250, 99, 252, 167, 209, 314, 99, 315, 152, 152, 31, 253, 167, 153, 209, 197, 233, 99, 236, 26, 99, 83, 167, 131, 152, 31, 254, 167, 189, 209, 216, 68, 209, 247, 210, 229, 68, 242, 152, 210, 232, 99, 209, 227, 99, 232, 152, 99, 209, 229, 210, 232, 99, 315, 152, 99, 209, 239, 210, 233, 99, 314, 152, 99, 209, 233, 99, 236, 152, 99, 209, 315, 99, 314, 152, 152, 31, 253, 149, 53, 209, 254, 99, 252, 167, 209, 314, 99, 315, 152, 152, 31, 255, 167, 189, 209, 221, 68, 247, 210, 229, 68, 242, 99, 209, 227, 99, 152, 99, 209, 229, 99, 152, 99, 209, 239, 210, 233, 99, 152, 99, 209, 233, 99, 152, 99, 209, 314, 99, 152, 152, 31, 256, 167, 189, 209, 223, 68, 247, 210, 229, 68, 242, 99, 209, 227, 99, 152, 99, 209, 229, 99, 152, 99, 209, 239, 210, 233, 99, 152, 99, 209, 233, 99, 152, 99, 209, 314, 99, 152, 152, 31, 257, 167, 53, 209, 255, 99, 252, 167, 209, 314, 99, 152, 152, 31, 258, 167, 53, 209, 256, 99, 252, 167, 209, 314, 99, 152, 152, 31, 158, 238, 58, 31, 259, 167, 189, 209, 218, 68, 247, 210, 229, 68, 242, 99, 209, 227, 99, 152, 99, 209, 229, 99, 152, 99, 209, 239, 210, 233, 99, 152, 99, 209, 233, 99, 152, 99, 209, 314, 99, 152, 152, 31, 260, 167, 53, 209, 259, 99, 252, 167, 209, 314, 99, 152, 152, 31, 161, 31, 29, 58, 31, 260, 167, 173, 31, 52, 31, 122, 261, 139, 5, 209, 209, 239, 68, 315, 152, 210, 233, 4, 316, 210, 234, 99, 239, 210, 233, 4, 234, 99, 4, 234, 152, 58, 31, 262, 167, 60, 209, 261, 99, 234, 152, 31, 263, 167, 189, 209, 214, 68, 209, 247, 210, 230, 68, 243, 152, 210, 231, 99, 209, 231, 99, 227, 152, 99, 209, 315, 99, 231, 210, 230, 152, 99, 209, 314, 99, 261, 152, 99, 209, 235, 99, 234, 152, 99, 209, 314, 99, 315, 152, 152, 31, 264, 167, 189, 209, 215, 68, 209, 247, 210, 230, 68, 243, 152, 210, 232, 99, 209, 227, 99, 232, 152, 99, 209, 232, 210, 230, 99, 315, 152, 99, 209, 261, 99, 314, 152, 99, 209, 234, 99, 236, 152, 99, 209, 315, 99, 314, 152, 152, 31, 265, 167, 189, 209, 219, 68, 209, 209, 246, 68, 262, 152, 210, 230, 68, 243, 152, 210, 231, 210, 231, 99, 209, 231, 99, 231, 152, 99, 209, 231, 99, 315, 152, 99, 209, 314, 99, 314, 152, 99, 209, 235, 99, 235, 152, 99, 209, 315, 99, 314, 152, 152, 31, 266, 167, 53, 209, 263, 99, 252, 167, 209, 314, 99, 315, 152, 152, 31, 267, 167, 53, 209, 264, 99, 252, 167, 209, 314, 99, 315, 152, 152, 31, 268, 167, 53, 209, 265, 99, 252, 167, 209, 314, 99, 315, 152, 152, 31, 269, 167, 239, 210, 233, 68, 69, 209, 314, 99, 233, 152, 130, 261, 68, 234, 31, 270, 167, 15, 209, 251, 75, 245, 209, 266, 75, 83, 152, 99, 266, 152, 31, 271, 167, 15, 209, 251, 75, 245, 209, 268, 75, 83, 152, 99, 268, 152, 31, 251, 167, 176, 209, 269, 197, 58, 99, 173, 26, 99, 251, 4, 271, 99, 251, 152, 31, 158, 238, 58, 31, 272, 167, 189, 209, 218, 68, 209, 247, 210, 229, 68, 242, 152, 99, 209, 227, 99, 152, 99, 209, 229, 99, 152, 99, 209, 261, 99, 152, 99, 209, 234, 99, 152, 99, 209, 314, 99, 152, 152, 31, 273, 167, 53, 209, 272, 99, 252, 167, 209, 314, 99, 152, 152, 31, 270, 167, 270, 68, 260, 197, 58, 99, 173, 26, 4, 273, 197, 173, 99, 58, 26, 31, 161, 31, 270, 167, 176, 209, 269, 197, 58, 99, 173, 26, 99, 270, 210, 249, 99, 274, 209, 318, 152, 152, 31, 275, 167, 166, 209, 258, 99, 12, 209, 270, 99, 315, 152, 152, 31, 276, 167, 138, 209, 258, 4, 275, 152, 31, 270, 167, 138, 209, 270, 4, 275, 197, 58, 99, 173, 26, 152, 31, 270, 167, 176, 209, 269, 197, 58, 99, 173, 26, 99, 270, 99, 314, 152, 31, 253, 23, 276, 197, 58, 99, 173, 26, 31, 257, 167, 257, 210, 276, 68, 191, 209, 270, 99, 315, 152, 31, 258, 167, 275, 31, 253, 149, 15, 209, 270, 75, 245, 209, 267, 75, 83, 152, 99, 267, 152, 31, 71, 31, 48, 209, 152, 31, 122, 261, 139, 5, 209, 239, 210, 233, 4, 234, 99, 4, 234, 99, 4, 234, 152, 58, 31, 262, 167, 60, 209, 261, 99, 234, 152, 31, 263, 167, 189, 209, 214, 68, 209, 247, 210, 230, 68, 243, 152, 210, 231, 99, 209, 231, 99, 227, 152, 99, 209, 315, 99, 231, 210, 230, 152, 99, 209, 314, 99, 261, 152, 99, 209, 235, 99, 234, 152, 99, 209, 314, 99, 315, 152, 152, 31, 264, 167, 189, 209, 215, 68, 209, 247, 210, 230, 68, 243, 152, 210, 232, 99, 209, 227, 99, 232, 152, 99, 209, 232, 210, 230, 99, 315, 152, 99, 209, 261, 99, 314, 152, 99, 209, 234, 99, 236, 152, 99, 209, 315, 99, 314, 152, 152, 31, 265, 167, 189, 209, 219, 68, 209, 209, 246, 68, 262, 152, 210, 230, 68, 243, 152, 210, 231, 210, 231, 99, 209, 231, 99, 231, 152, 99, 209, 231, 99, 315, 152, 99, 209, 314, 99, 314, 152, 99, 209, 235, 99, 235, 152, 99, 209, 315, 99, 314, 152, 152, 31, 266, 167, 53, 209, 263, 99, 252, 167, 209, 314, 99, 315, 152, 152, 31, 267, 167, 53, 209, 264, 99, 252, 167, 209, 314, 99, 315, 152, 152, 31, 268, 167, 53, 209, 265, 99, 252, 167, 209, 314, 99, 315, 152, 152, 31, 270, 167, 15, 209, 251, 75, 245, 209, 266, 75, 83, 152, 99, 266, 152, 31, 251, 2, 15, 209, 251, 75, 245, 209, 268, 75, 83, 152, 99, 268, 152, 31, 158, 238, 58, 31, 272, 167, 189, 209, 218, 68, 209, 247, 210, 229, 68, 242, 152, 99, 209, 227, 99, 152, 99, 209, 229, 99, 152, 99, 209, 261, 99, 152, 99, 209, 234, 99, 152, 99, 209, 314, 99, 152, 152, 31, 273, 167, 53, 209, 272, 99, 252, 167, 209, 314, 99, 152, 152, 31, 270, 167, 270, 68, 260, 197, 58, 99, 173, 26, 4, 273, 197, 173, 99, 58, 26, 31, 161, 31, 270, 167, 270, 210, 249, 31, 275, 167, 166, 209, 258, 99, 12, 209, 270, 99, 315, 152, 152, 31, 276, 167, 138, 209, 258, 4, 275, 152, 31, 270, 167, 138, 209, 270, 4, 275, 197, 58, 99, 173, 26, 152, 31, 253, 23, 276, 197, 58, 99, 173, 26, 31, 257, 167, 257, 210, 276, 68, 191, 209, 270, 99, 315, 152, 31, 258, 167, 275, 31, 253, 149, 15, 209, 270, 75, 245, 209, 267, 75, 83, 152, 99, 267, 152, 31, 71, 31, 253, 167, 253, 41, 257, 197, 58, 99, 173, 26, 31, 277, 167, 189, 209, 217, 68, 209, 247, 210, 229, 68, 242, 152, 210, 232, 99, 209, 227, 99, 232, 152, 99, 209, 229, 210, 232, 99, 315, 152, 99, 209, 239, 210, 233, 99, 314, 152, 99, 209, 233, 99, 236, 152, 99, 209, 315, 99, 314, 152, 152, 31, 10, 209, 277, 99, 253, 75, 245, 209, 277, 75, 83, 75, 105, 152, 99, 252, 167, 209, 314, 99, 315, 152, 152, 31, 257, 167, 102, 209, 257, 152, 68, 258, 31, 278, 167, 189, 209, 222, 68, 209, 247, 210, 229, 68, 242, 152, 99, 209, 227, 99, 152, 99, 209, 229, 99, 152, 99, 209, 239, 210, 233, 99, 152, 99, 209, 233, 99, 152, 99, 209, 314, 99, 152, 152, 31, 10, 209, 278, 99, 257, 75, 245, 209, 278, 75, 83, 75, 105, 152, 99, 252, 167, 209, 314, 99, 152, 152, 31, 3, 31]}, {"code": "def parallel_path_fwd_kernel_prepare_k_cache(\n    k,\n    k_new,\n    h,\n    offsets,\n    indices,\n    chunk_offsets,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(\n            indices + i_t * 2 + 1\n        ).to(tl.int32)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(\n            tl.int32\n        )\n        T = eos - bos\n    else:\n        i_n = i_b\n        bos, eos = i_n * T, i_n * T + T\n        NT = triton.cdiv(T, BT)\n        boh = i_n * NT\n\n    k += (bos * H + i_h) * K\n    k_new += (bos * H + i_h) * K\n    h += (boh * H + i_h) * K * K\n\n    stride_h = H * K * K\n    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    b_k = tl.zeros([BT, BK], dtype=tl.float32)\n    b_k += tl.load(p_k, boundary_check=(0, 1))\n    for k_block_idx in range(i_t + 1, tl.cdiv(T, BT)):\n        p_h = tl.make_block_ptr(\n            h + k_block_idx * stride_h, (K, K), (1, K), (0, 0), (BK, BK), (0, 1)\n        )\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_k_minus = tl.dot(b_k.to(b_h.dtype), b_h)\n        b_k = b_k - b_k_minus\n    p_k_new = tl.make_block_ptr(\n        k_new, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n    )\n    tl.store(p_k_new, b_k.to(p_k_new.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 313, 209, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 152, 57, 31, -1, 225, 98, 226, 167, 209, 148, 209, 314, 152, 98, 148, 209, 315, 152, 152, 31, 227, 98, 228, 167, 209, 226, 46, 220, 98, 226, 194, 220, 152, 31, 158, 224, 57, 31, 229, 98, 225, 167, 209, 51, 209, 217, 67, 225, 210, 316, 152, 74, 230, 209, 212, 152, 98, 51, 209, 217, 67, 225, 210, 316, 67, 315, 152, 74, 230, 209, 212, 152, 152, 31, 231, 167, 51, 209, 218, 67, 229, 152, 74, 230, 209, 212, 152, 31, 232, 98, 233, 167, 209, 51, 209, 216, 67, 229, 152, 74, 230, 209, 212, 152, 98, 51, 209, 216, 67, 229, 67, 315, 152, 74, 230, 209, 212, 152, 152, 31, 219, 167, 233, 4, 232, 31, 161, 31, 29, 57, 31, 229, 167, 227, 31, 232, 98, 233, 167, 209, 229, 210, 219, 98, 229, 210, 219, 67, 219, 152, 31, 234, 167, 20, 74, 235, 209, 219, 98, 222, 152, 31, 231, 167, 229, 210, 234, 31, 52, 31, 213, 149, 209, 232, 210, 220, 67, 228, 152, 210, 221, 31, 214, 149, 209, 232, 210, 220, 67, 228, 152, 210, 221, 31, 215, 149, 209, 231, 210, 220, 67, 228, 152, 210, 221, 210, 221, 31, 236, 167, 220, 210, 221, 210, 221, 31, 237, 167, 189, 209, 213, 98, 209, 219, 98, 221, 152, 98, 209, 220, 210, 221, 98, 315, 152, 98, 209, 225, 210, 222, 98, 314, 152, 98, 209, 222, 98, 223, 152, 98, 209, 315, 98, 314, 152, 152, 31, 238, 167, 153, 209, 197, 222, 98, 223, 26, 98, 82, 167, 130, 152, 31, 238, 149, 51, 209, 237, 98, 239, 167, 209, 314, 98, 315, 152, 152, 31, 121, 240, 139, 5, 209, 225, 67, 315, 98, 59, 209, 219, 98, 222, 152, 152, 57, 31, 241, 167, 189, 209, 215, 67, 240, 210, 236, 98, 209, 221, 98, 221, 152, 98, 209, 315, 98, 221, 152, 98, 209, 314, 98, 314, 152, 98, 209, 223, 98, 223, 152, 98, 209, 314, 98, 315, 152, 152, 31, 242, 167, 51, 209, 241, 98, 239, 167, 209, 314, 98, 315, 152, 152, 31, 243, 167, 15, 209, 238, 74, 230, 209, 242, 74, 82, 152, 98, 242, 152, 31, 238, 167, 238, 4, 243, 31, 70, 31, 244, 167, 189, 209, 214, 98, 209, 219, 98, 221, 152, 98, 209, 220, 210, 221, 98, 315, 152, 98, 209, 225, 210, 222, 98, 314, 152, 98, 209, 222, 98, 223, 152, 98, 209, 315, 98, 314, 152, 152, 31, 10, 209, 244, 98, 238, 74, 230, 209, 244, 74, 82, 74, 104, 152, 98, 239, 167, 209, 314, 98, 315, 152, 152, 31, 3, 31]}, {"code": "def parallel_rebased_fwd_kernel(\n    q,\n    k,\n    v,\n    o,\n    z,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BTL: tl.constexpr,\n    BTS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // (NV)\n    i_v = i_kv % (NV)\n\n    p_q = tl.make_block_ptr(\n        q + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0)\n    )\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BTS), (0, 1)\n    )\n    p_v = tl.make_block_ptr(\n        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BTS, BV), (1, 0)\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n\n    for _ in range(0, i_c * BTL, BTS):\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, (b_k), allow_tf32=False)\n        b_s = b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n\n        b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_c * BTL), (BK, BTS), (0, 1)\n    )\n    p_v = tl.make_block_ptr(\n        v + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTS, BV), (1, 0)\n    )\n\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n\n        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n\n    p_o = tl.make_block_ptr(\n        o + (i_bh + B * H * i_k) * T * V,\n        (T, V),\n        (V, 1),\n        (i_c * BTL, i_v * BV),\n        (BTL, BV),\n        (1, 0),\n    )\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(\n        p_z, b_z.to(p_z.dtype.element_ty), mask=((i_c * BTL + tl.arange(0, BTL)) < T)\n    )", "encoded": [28, 313, 209, 213, 99, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 58, 6, 99, 221, 58, 6, 99, 222, 58, 6, 99, 223, 58, 6, 99, 224, 58, 6, 99, 225, 58, 6, 99, 226, 58, 6, 99, 227, 58, 6, 152, 58, 31, -1, 228, 99, 229, 99, 230, 167, 209, 148, 209, 314, 152, 99, 148, 209, 315, 152, 99, 148, 209, 316, 152, 152, 31, 231, 167, 60, 209, 223, 99, 227, 152, 31, 232, 167, 228, 46, 231, 31, 233, 167, 228, 194, 231, 31, 234, 167, 189, 209, 213, 68, 230, 210, 219, 210, 222, 99, 209, 219, 99, 222, 152, 99, 209, 222, 99, 315, 152, 99, 209, 229, 210, 224, 99, 232, 210, 226, 152, 99, 209, 224, 99, 226, 152, 99, 209, 315, 99, 314, 152, 152, 31, 235, 167, 189, 209, 214, 68, 230, 210, 219, 210, 222, 99, 209, 222, 99, 219, 152, 99, 209, 315, 99, 222, 152, 99, 209, 232, 210, 226, 99, 314, 152, 99, 209, 226, 99, 225, 152, 99, 209, 314, 99, 315, 152, 152, 31, 236, 167, 189, 209, 215, 68, 230, 210, 219, 210, 223, 99, 209, 219, 99, 223, 152, 99, 209, 223, 99, 315, 152, 99, 209, 314, 99, 233, 210, 227, 152, 99, 209, 225, 99, 227, 152, 99, 209, 315, 99, 314, 152, 152, 31, 237, 167, 53, 209, 234, 99, 238, 167, 209, 314, 99, 315, 152, 152, 31, 237, 167, 209, 237, 210, 218, 152, 75, 239, 209, 237, 75, 83, 152, 31, 240, 167, 153, 209, 197, 224, 99, 227, 26, 99, 83, 167, 131, 152, 31, 241, 167, 153, 209, 197, 224, 26, 99, 83, 167, 131, 152, 31, 122, 242, 139, 5, 209, 314, 99, 229, 210, 224, 99, 225, 152, 58, 31, 243, 167, 53, 209, 235, 99, 238, 167, 209, 314, 99, 315, 152, 152, 31, 244, 167, 53, 209, 236, 99, 238, 167, 209, 314, 99, 315, 152, 152, 31, 245, 167, 15, 209, 237, 99, 243, 99, 246, 167, 55, 152, 31, 245, 167, 245, 210, 245, 31, 241, 149, 191, 209, 245, 99, 247, 167, 315, 152, 31, 240, 167, 240, 68, 15, 209, 245, 75, 239, 209, 244, 75, 83, 152, 99, 244, 99, 246, 167, 55, 152, 31, 235, 167, 129, 209, 235, 99, 209, 314, 99, 225, 152, 152, 31, 236, 167, 129, 209, 236, 99, 209, 225, 99, 314, 152, 152, 31, 71, 31, 48, 209, 152, 31, 248, 167, 69, 209, 314, 99, 224, 152, 31, 249, 167, 69, 209, 314, 99, 225, 152, 31, 235, 167, 189, 209, 214, 68, 230, 210, 219, 210, 222, 99, 209, 222, 99, 219, 152, 99, 209, 315, 99, 222, 152, 99, 209, 232, 210, 226, 99, 229, 210, 224, 152, 99, 209, 226, 99, 225, 152, 99, 209, 314, 99, 315, 152, 152, 31, 236, 167, 189, 209, 215, 68, 230, 210, 219, 210, 223, 99, 209, 219, 99, 223, 152, 99, 209, 223, 99, 315, 152, 99, 209, 229, 210, 224, 99, 233, 210, 227, 152, 99, 209, 225, 99, 227, 152, 99, 209, 315, 99, 314, 152, 152, 31, 122, 242, 139, 5, 209, 229, 210, 224, 99, 209, 229, 68, 315, 152, 210, 224, 99, 225, 152, 58, 31, 243, 167, 53, 209, 235, 99, 238, 167, 209, 314, 99, 315, 152, 152, 31, 244, 167, 53, 209, 236, 99, 238, 167, 209, 314, 99, 315, 152, 152, 31, 250, 167, 248, 197, 58, 99, 173, 26, 130, 249, 197, 173, 99, 58, 26, 31, 245, 167, 15, 209, 237, 99, 243, 99, 246, 167, 55, 152, 31, 245, 167, 245, 210, 245, 31, 245, 167, 176, 209, 250, 99, 245, 99, 314, 152, 31, 241, 149, 191, 209, 245, 99, 247, 167, 315, 152, 31, 240, 149, 15, 209, 245, 75, 239, 209, 237, 75, 83, 152, 99, 244, 99, 246, 167, 55, 152, 31, 235, 167, 129, 209, 235, 99, 209, 314, 99, 225, 152, 152, 31, 236, 167, 129, 209, 236, 99, 209, 225, 99, 314, 152, 152, 31, 249, 149, 225, 31, 71, 31, 251, 167, 189, 209, 216, 68, 209, 230, 68, 220, 210, 221, 210, 232, 152, 210, 219, 210, 223, 99, 209, 219, 99, 223, 152, 99, 209, 223, 99, 315, 152, 99, 209, 229, 210, 224, 99, 233, 210, 227, 152, 99, 209, 224, 99, 227, 152, 99, 209, 315, 99, 314, 152, 152, 31, 252, 167, 217, 68, 209, 230, 68, 220, 210, 221, 210, 232, 152, 210, 219, 68, 229, 210, 224, 68, 69, 209, 314, 99, 224, 152, 31, 10, 209, 251, 99, 240, 75, 239, 209, 251, 75, 83, 75, 105, 152, 99, 238, 167, 209, 314, 99, 315, 152, 152, 31, 10, 209, 252, 99, 241, 75, 239, 209, 252, 75, 83, 75, 105, 152, 99, 253, 167, 229, 210, 224, 68, 69, 209, 314, 99, 224, 152, 1, 219, 152, 31, 3, 31]}, {"code": "def _parallel_rebased_bwd_dq(\n    i_bh,\n    i_c,\n    i_k,\n    i_v,\n    i_h,\n    q,\n    k,\n    v,\n    do,\n    dz,\n    dq,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BTL: tl.constexpr,\n    BTS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n    p_do = tl.make_block_ptr(\n        do + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0)\n    )\n    p_q = tl.make_block_ptr(\n        q + (i_bh) * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0)\n    )\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BTS, BK), (1, 0)\n    )\n    p_v = tl.make_block_ptr(\n        v + i_bh * T * V, (V, T), (1, V), (i_v * BV, 0), (BV, BTS), (0, 1)\n    )\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=(i_c * BTL + tl.arange(0, BTL)) < T)\n\n    for _ in range(0, i_c * BTL, BTS):\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n\n        b_dq += tl.dot((2 * b_ds * b_s).to(b_v.dtype), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTS, BK), (1, 0)\n    )\n    p_v = tl.make_block_ptr(\n        v + i_bh * T * V, (V, T), (1, V), (i_v * BV, i_c * BTL), (BV, BTS), (0, 1)\n    )\n\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n\n        b_dq += tl.dot((2 * b_ds * b_s).to(b_k.dtype), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(\n        dq + (i_bh + B * H * i_v) * T * K,\n        (T, K),\n        (K, 1),\n        (i_c * BTL, i_k * BK),\n        (BTL, BK),\n        (1, 0),\n    )\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    return", "encoded": [28, 313, 209, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 98, 229, 57, 6, 98, 230, 57, 6, 98, 231, 57, 6, 98, 232, 57, 6, 98, 233, 57, 6, 152, 57, 31, -1, 234, 167, 189, 209, 221, 67, 213, 210, 225, 210, 229, 98, 209, 225, 98, 229, 152, 98, 209, 229, 98, 314, 152, 98, 209, 214, 210, 230, 98, 216, 210, 233, 152, 98, 209, 230, 98, 233, 152, 98, 209, 314, 98, 315, 152, 152, 31, 235, 167, 189, 209, 218, 67, 213, 210, 225, 210, 228, 98, 209, 225, 98, 228, 152, 98, 209, 228, 98, 314, 152, 98, 209, 214, 210, 230, 98, 215, 210, 232, 152, 98, 209, 230, 98, 232, 152, 98, 209, 314, 98, 315, 152, 152, 31, 236, 167, 51, 209, 235, 98, 237, 167, 209, 315, 98, 314, 152, 152, 31, 238, 167, 51, 209, 234, 98, 237, 167, 209, 315, 98, 314, 152, 152, 74, 239, 209, 236, 74, 82, 152, 31, 236, 167, 209, 236, 210, 224, 152, 74, 239, 209, 236, 74, 82, 152, 31, 240, 167, 153, 209, 197, 230, 98, 232, 26, 98, 82, 167, 130, 152, 31, 241, 167, 189, 209, 219, 67, 213, 210, 225, 210, 228, 98, 209, 225, 98, 228, 152, 98, 209, 228, 98, 314, 152, 98, 209, 315, 98, 215, 210, 232, 152, 98, 209, 231, 98, 232, 152, 98, 209, 314, 98, 315, 152, 152, 31, 242, 167, 189, 209, 220, 67, 213, 210, 225, 210, 229, 98, 209, 229, 98, 225, 152, 98, 209, 314, 98, 229, 152, 98, 209, 216, 210, 233, 98, 315, 152, 98, 209, 233, 98, 231, 152, 98, 209, 315, 98, 314, 152, 152, 31, 243, 167, 222, 67, 213, 210, 225, 67, 214, 210, 230, 67, 68, 209, 315, 98, 230, 152, 31, 244, 167, 51, 209, 243, 98, 245, 167, 214, 210, 230, 67, 68, 209, 315, 98, 230, 152, 1, 225, 152, 31, 121, 246, 139, 5, 209, 315, 98, 214, 210, 230, 98, 231, 152, 57, 31, 247, 167, 51, 209, 241, 98, 237, 167, 209, 315, 98, 314, 152, 152, 31, 248, 167, 51, 209, 242, 98, 237, 167, 209, 315, 98, 314, 152, 152, 31, 249, 167, 15, 209, 238, 98, 248, 98, 250, 167, 55, 152, 31, 158, 216, 69, 315, 57, 31, 249, 149, 244, 197, 57, 98, 173, 26, 31, 161, 31, 29, 57, 31, 249, 167, 249, 31, 52, 31, 251, 167, 15, 209, 236, 98, 65, 209, 247, 152, 98, 250, 167, 55, 152, 31, 240, 149, 15, 209, 209, 316, 210, 249, 210, 251, 152, 74, 239, 209, 248, 74, 82, 152, 98, 247, 98, 250, 167, 55, 152, 31, 241, 167, 128, 209, 241, 98, 209, 231, 98, 315, 152, 152, 31, 242, 167, 128, 209, 242, 98, 209, 315, 98, 231, 152, 152, 31, 70, 31, 240, 23, 224, 31, 252, 167, 68, 209, 315, 98, 230, 152, 31, 253, 167, 68, 209, 315, 98, 231, 152, 31, 241, 167, 189, 209, 219, 67, 213, 210, 225, 210, 228, 98, 209, 225, 98, 228, 152, 98, 209, 228, 98, 314, 152, 98, 209, 214, 210, 230, 98, 215, 210, 232, 152, 98, 209, 231, 98, 232, 152, 98, 209, 314, 98, 315, 152, 152, 31, 242, 167, 189, 209, 220, 67, 213, 210, 225, 210, 229, 98, 209, 229, 98, 225, 152, 98, 209, 314, 98, 229, 152, 98, 209, 216, 210, 233, 98, 214, 210, 230, 152, 98, 209, 233, 98, 231, 152, 98, 209, 315, 98, 314, 152, 152, 31, 121, 246, 139, 5, 209, 214, 210, 230, 98, 209, 214, 67, 314, 152, 210, 230, 98, 231, 152, 57, 31, 247, 167, 51, 209, 241, 98, 237, 167, 209, 315, 98, 314, 152, 152, 31, 248, 167, 51, 209, 242, 98, 237, 167, 209, 315, 98, 314, 152, 152, 31, 254, 167, 252, 197, 57, 98, 173, 26, 129, 253, 197, 173, 98, 57, 26, 31, 249, 167, 15, 209, 238, 98, 248, 98, 250, 167, 55, 152, 31, 158, 216, 69, 315, 57, 31, 249, 149, 244, 197, 57, 98, 173, 26, 31, 161, 31, 29, 57, 31, 249, 167, 249, 31, 52, 31, 249, 167, 176, 209, 254, 98, 249, 98, 315, 152, 210, 224, 31, 251, 167, 15, 209, 236, 98, 65, 209, 247, 152, 98, 250, 167, 55, 152, 31, 251, 167, 176, 209, 254, 98, 251, 98, 315, 152, 31, 240, 149, 15, 209, 209, 316, 210, 249, 210, 251, 152, 74, 239, 209, 247, 74, 82, 152, 98, 247, 98, 250, 167, 55, 152, 31, 241, 167, 128, 209, 241, 98, 209, 231, 98, 315, 152, 152, 31, 242, 167, 128, 209, 242, 98, 209, 315, 98, 231, 152, 152, 31, 253, 149, 231, 31, 70, 31, 255, 167, 189, 209, 223, 67, 209, 213, 67, 226, 210, 227, 210, 216, 152, 210, 225, 210, 228, 98, 209, 225, 98, 228, 152, 98, 209, 228, 98, 314, 152, 98, 209, 214, 210, 230, 98, 215, 210, 232, 152, 98, 209, 230, 98, 232, 152, 98, 209, 314, 98, 315, 152, 152, 31, 10, 209, 255, 98, 240, 74, 239, 209, 255, 74, 82, 74, 104, 152, 98, 237, 167, 209, 315, 98, 314, 152, 152, 31, 198, 31, 3, 31]}, {"code": "def _parallel_rebased_bwd_dkv(\n    i_bh,\n    i_c,\n    i_k,\n    i_v,\n    i_h,\n    q,\n    k,\n    v,\n    do,\n    dz,\n    dk,\n    dv,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BTL: tl.constexpr,\n    BTS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0)\n    )\n    p_v = tl.make_block_ptr(\n        v + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0)\n    )\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v, boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros(\n        [BTL, BV], dtype=tl.float32\n    )\n\n    for i in range((tl.cdiv(T, BTS) * BTS) - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(\n            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, i), (BK, BTS), (0, 1)\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * T * V, (V, T), (1, V), (i_v * BV, i), (BV, BTS), (0, 1)\n        )\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n\n        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n        b_dz = tl.load(p_dz, mask=(i + tl.arange(0, BTS)) < T)\n\n        b_s = tl.dot(b_k.to(b_q.dtype), b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype), tl.trans(b_q), allow_tf32=False)\n\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(\n            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, i), (BK, BTS), (0, 1)\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * T * V, (V, T), (1, V), (i_v * BV, i), (BV, BTS), (0, 1)\n        )\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n        b_dz = tl.load(p_dz, mask=(i + tl.arange(0, BTS)) < T)\n\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n\n        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype), tl.trans(b_q), allow_tf32=False)\n        o_q += BTS\n\n    p_dk = tl.make_block_ptr(\n        dk + (i_bh + B * H * i_v) * T * K,\n        (T, K),\n        (K, 1),\n        (i_c * BTL, i_k * BK),\n        (BTL, BK),\n        (1, 0),\n    )\n    p_dv = tl.make_block_ptr(\n        dv + (i_bh + B * H * i_k) * T * V,\n        (T, V),\n        (V, 1),\n        (i_c * BTL, i_v * BV),\n        (BTL, BV),\n        (1, 0),\n    )\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    return", "encoded": [28, 313, 209, 213, 99, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 99, 224, 99, 225, 99, 226, 99, 227, 58, 6, 99, 228, 58, 6, 99, 229, 58, 6, 99, 230, 58, 6, 99, 231, 58, 6, 99, 232, 58, 6, 99, 233, 58, 6, 99, 234, 58, 6, 152, 58, 31, -1, 235, 167, 189, 209, 219, 68, 213, 210, 226, 210, 229, 99, 209, 226, 99, 229, 152, 99, 209, 229, 99, 314, 152, 99, 209, 214, 210, 231, 99, 215, 210, 233, 152, 99, 209, 231, 99, 233, 152, 99, 209, 314, 99, 315, 152, 152, 31, 236, 167, 189, 209, 220, 68, 213, 210, 226, 210, 230, 99, 209, 226, 99, 230, 152, 99, 209, 230, 99, 314, 152, 99, 209, 214, 210, 231, 99, 216, 210, 234, 152, 99, 209, 231, 99, 234, 152, 99, 209, 314, 99, 315, 152, 152, 31, 237, 99, 238, 167, 209, 53, 209, 235, 99, 239, 167, 209, 315, 99, 314, 152, 152, 99, 53, 209, 236, 99, 239, 167, 209, 315, 99, 314, 152, 152, 152, 31, 240, 99, 241, 167, 209, 153, 209, 197, 231, 99, 233, 26, 99, 83, 167, 131, 152, 99, 153, 209, 197, 231, 99, 234, 26, 99, 83, 167, 131, 152, 152, 31, 122, 242, 139, 5, 209, 60, 209, 226, 99, 232, 152, 210, 232, 4, 232, 99, 209, 214, 68, 314, 152, 210, 231, 4, 232, 99, 4, 232, 152, 58, 31, 243, 167, 189, 209, 218, 68, 213, 210, 226, 210, 229, 99, 209, 229, 99, 226, 152, 99, 209, 314, 99, 229, 152, 99, 209, 215, 210, 233, 99, 242, 152, 99, 209, 233, 99, 232, 152, 99, 209, 315, 99, 314, 152, 152, 31, 244, 167, 189, 209, 221, 68, 213, 210, 226, 210, 230, 99, 209, 230, 99, 226, 152, 99, 209, 314, 99, 230, 152, 99, 209, 216, 210, 234, 99, 242, 152, 99, 209, 234, 99, 232, 152, 99, 209, 315, 99, 314, 152, 152, 31, 245, 167, 222, 68, 213, 210, 226, 68, 242, 68, 69, 209, 315, 99, 232, 152, 31, 246, 167, 53, 209, 243, 99, 239, 167, 209, 315, 99, 314, 152, 152, 31, 247, 167, 53, 209, 244, 99, 239, 167, 209, 315, 99, 314, 152, 152, 75, 248, 209, 246, 75, 83, 152, 31, 249, 167, 53, 209, 245, 99, 250, 167, 242, 68, 69, 209, 315, 99, 232, 152, 1, 226, 152, 31, 251, 167, 15, 209, 237, 75, 248, 209, 246, 75, 83, 152, 99, 246, 99, 252, 167, 55, 152, 210, 225, 31, 253, 167, 251, 210, 251, 31, 241, 149, 15, 209, 253, 75, 248, 209, 246, 75, 83, 152, 99, 66, 209, 247, 152, 99, 252, 167, 55, 152, 31, 254, 167, 15, 209, 238, 99, 247, 99, 252, 167, 55, 152, 210, 225, 31, 158, 216, 70, 315, 58, 31, 254, 149, 249, 197, 173, 99, 58, 26, 210, 225, 31, 161, 31, 29, 58, 31, 254, 167, 254, 31, 52, 31, 240, 149, 15, 209, 209, 316, 210, 254, 210, 251, 152, 75, 248, 209, 246, 75, 83, 152, 99, 66, 209, 246, 152, 99, 252, 167, 55, 152, 31, 71, 31, 48, 209, 152, 31, 255, 99, 256, 167, 209, 69, 209, 315, 99, 232, 152, 99, 69, 209, 315, 99, 231, 152, 152, 31, 122, 242, 139, 5, 209, 214, 210, 231, 99, 209, 214, 68, 314, 152, 210, 231, 99, 232, 152, 58, 31, 243, 167, 189, 209, 218, 68, 213, 210, 226, 210, 229, 99, 209, 229, 99, 226, 152, 99, 209, 314, 99, 229, 152, 99, 209, 215, 210, 233, 99, 242, 152, 99, 209, 233, 99, 232, 152, 99, 209, 315, 99, 314, 152, 152, 31, 244, 167, 189, 209, 221, 68, 213, 210, 226, 210, 230, 99, 209, 230, 99, 226, 152, 99, 209, 314, 99, 230, 152, 99, 209, 216, 210, 234, 99, 242, 152, 99, 209, 234, 99, 232, 152, 99, 209, 315, 99, 314, 152, 152, 31, 245, 167, 222, 68, 213, 210, 226, 68, 242, 68, 69, 209, 315, 99, 232, 152, 31, 246, 167, 53, 209, 243, 99, 239, 167, 209, 315, 99, 314, 152, 152, 31, 247, 167, 53, 209, 244, 99, 239, 167, 209, 315, 99, 314, 152, 152, 75, 248, 209, 246, 75, 83, 152, 31, 249, 167, 53, 209, 245, 99, 250, 167, 242, 68, 69, 209, 315, 99, 232, 152, 1, 226, 152, 31, 257, 167, 256, 197, 58, 99, 173, 26, 188, 255, 197, 173, 99, 58, 26, 31, 251, 167, 15, 209, 237, 99, 246, 99, 252, 167, 55, 152, 210, 225, 31, 253, 167, 251, 210, 251, 31, 251, 167, 176, 209, 257, 99, 251, 99, 315, 152, 31, 253, 167, 176, 209, 257, 99, 253, 99, 315, 152, 31, 254, 167, 15, 209, 238, 99, 247, 99, 252, 167, 55, 152, 31, 158, 216, 70, 315, 58, 31, 254, 149, 249, 197, 173, 99, 58, 26, 31, 161, 31, 29, 58, 31, 254, 167, 254, 31, 52, 31, 254, 167, 176, 209, 257, 99, 254, 99, 315, 152, 210, 225, 31, 241, 149, 15, 209, 253, 75, 248, 209, 246, 75, 83, 152, 99, 66, 209, 247, 152, 99, 252, 167, 55, 152, 31, 240, 149, 15, 209, 209, 316, 210, 254, 210, 251, 152, 75, 248, 209, 246, 75, 83, 152, 99, 66, 209, 246, 152, 99, 252, 167, 55, 152, 31, 255, 149, 232, 31, 71, 31, 258, 167, 189, 209, 223, 68, 209, 213, 68, 227, 210, 228, 210, 216, 152, 210, 226, 210, 229, 99, 209, 226, 99, 229, 152, 99, 209, 229, 99, 314, 152, 99, 209, 214, 210, 231, 99, 215, 210, 233, 152, 99, 209, 231, 99, 233, 152, 99, 209, 314, 99, 315, 152, 152, 31, 259, 167, 189, 209, 224, 68, 209, 213, 68, 227, 210, 228, 210, 215, 152, 210, 226, 210, 230, 99, 209, 226, 99, 230, 152, 99, 209, 230, 99, 314, 152, 99, 209, 214, 210, 231, 99, 216, 210, 234, 152, 99, 209, 231, 99, 234, 152, 99, 209, 314, 99, 315, 152, 152, 31, 10, 209, 258, 99, 240, 75, 248, 209, 258, 75, 83, 75, 105, 152, 99, 239, 167, 209, 315, 99, 314, 152, 152, 31, 10, 209, 259, 99, 241, 75, 248, 209, 259, 75, 83, 75, 105, 152, 99, 239, 167, 209, 315, 99, 314, 152, 152, 31, 198, 31, 3, 31]}, {"code": "def fused_chunk_retention_fwd_kernel(\n    q,\n    k,\n    v,\n    o,\n    h0,\n    ht,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    CHECK: tl.constexpr,\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n\n    o_i = tl.arange(0, BT)\n\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n\n    d_b, d_o, d_h = (\n        tl.math.exp2(BT * b_b),\n        tl.math.exp2((o_i + 1) * b_b),\n        tl.math.exp2((BT - o_i - 1) * b_b),\n    )\n\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    p_q = tl.make_block_ptr(\n        q + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_k = tl.make_block_ptr(\n        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BT), (0, 1)\n    )\n    p_v = tl.make_block_ptr(\n        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_o = tl.make_block_ptr(\n        o + (i_k * B * H + i_bh).to(tl.int64) * T * V,\n        (T, V),\n        (V, 1),\n        (0, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(\n            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n\n    NT = tl.cdiv(T, BT)\n    for i in range(0, NT):\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n\n        b_o = tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False) * d_o[:, None]\n            b_h = d_b * b_h + tl.dot(\n                b_k, (b_v * d_h[:, None]).to(b_k.dtype), allow_tf32=False\n            )\n        else:\n            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False) * d_o[:, None]\n            if i == NT - 1 and (T % BT) != 0:\n                d_b = tl.math.exp2((T % BT) * b_b)\n                d_h = tl.math.exp2(((T % BT) - o_i - 1) * b_b)\n            b_h = d_b * b_h + tl.dot(\n                b_k, (b_v * d_h[:, None]).to(b_k.dtype), allow_tf32=False\n            )\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(\n            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 313, 209, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 98, 229, 57, 6, 98, 230, 57, 6, 152, 57, 31, -1, 231, 98, 232, 98, 233, 167, 209, 148, 209, 314, 152, 98, 148, 209, 315, 152, 98, 148, 209, 316, 152, 152, 31, 234, 167, 233, 194, 222, 31, 235, 167, 68, 209, 314, 98, 225, 152, 31, 236, 167, 101, 209, 315, 4, 137, 209, 4, 317, 4, 234, 210, 315, 152, 152, 31, 237, 98, 238, 98, 239, 167, 209, 137, 209, 225, 210, 236, 152, 98, 137, 209, 209, 235, 67, 315, 152, 210, 236, 152, 98, 137, 209, 209, 225, 4, 235, 4, 315, 152, 210, 236, 152, 152, 31, 240, 167, 235, 197, 57, 98, 173, 26, 129, 235, 197, 173, 98, 57, 26, 31, 241, 167, 176, 209, 240, 98, 137, 209, 209, 235, 197, 57, 98, 173, 26, 4, 235, 197, 173, 98, 57, 26, 152, 210, 236, 152, 98, 314, 152, 31, 242, 167, 153, 209, 197, 226, 98, 227, 26, 98, 82, 167, 130, 152, 31, 243, 167, 189, 209, 213, 67, 233, 210, 220, 210, 223, 98, 209, 220, 98, 223, 152, 98, 209, 223, 98, 315, 152, 98, 209, 314, 98, 232, 210, 226, 152, 98, 209, 225, 98, 226, 152, 98, 209, 315, 98, 314, 152, 152, 31, 244, 167, 189, 209, 214, 67, 233, 210, 220, 210, 223, 98, 209, 223, 98, 220, 152, 98, 209, 315, 98, 223, 152, 98, 209, 232, 210, 226, 98, 314, 152, 98, 209, 226, 98, 225, 152, 98, 209, 314, 98, 315, 152, 152, 31, 245, 167, 189, 209, 215, 67, 233, 210, 220, 210, 224, 98, 209, 220, 98, 224, 152, 98, 209, 224, 98, 315, 152, 98, 209, 314, 98, 231, 210, 227, 152, 98, 209, 225, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 246, 167, 189, 209, 216, 67, 209, 232, 210, 221, 210, 222, 67, 233, 152, 74, 247, 209, 157, 152, 210, 220, 210, 224, 98, 209, 220, 98, 224, 152, 98, 209, 224, 98, 315, 152, 98, 209, 314, 98, 231, 210, 227, 152, 98, 209, 225, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 158, 228, 57, 31, 248, 167, 189, 209, 217, 67, 233, 210, 223, 210, 224, 98, 209, 223, 98, 224, 152, 98, 209, 224, 98, 315, 152, 98, 209, 232, 210, 226, 98, 231, 210, 227, 152, 98, 209, 226, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 242, 167, 51, 209, 248, 98, 249, 167, 209, 314, 98, 315, 152, 152, 74, 247, 209, 130, 152, 31, 161, 31, 250, 167, 59, 209, 220, 98, 225, 152, 31, 121, 251, 139, 5, 209, 314, 98, 250, 152, 57, 31, 252, 167, 51, 209, 243, 98, 249, 167, 209, 314, 98, 315, 152, 152, 31, 252, 167, 209, 252, 210, 219, 152, 74, 247, 209, 252, 74, 82, 152, 31, 253, 167, 51, 209, 244, 98, 249, 167, 209, 314, 98, 315, 152, 152, 31, 254, 167, 51, 209, 245, 98, 249, 167, 209, 314, 98, 315, 152, 152, 31, 255, 167, 15, 209, 252, 98, 253, 98, 256, 167, 55, 152, 210, 241, 31, 257, 167, 15, 209, 255, 74, 247, 209, 252, 74, 82, 152, 98, 254, 98, 256, 167, 55, 152, 31, 158, 230, 92, 251, 69, 314, 57, 31, 257, 149, 15, 209, 252, 98, 242, 74, 247, 209, 252, 74, 82, 152, 98, 256, 167, 55, 152, 210, 238, 197, 57, 98, 173, 26, 31, 242, 167, 237, 210, 242, 67, 15, 209, 253, 98, 209, 254, 210, 239, 197, 57, 98, 173, 26, 152, 74, 247, 209, 253, 74, 82, 152, 98, 256, 167, 55, 152, 31, 161, 31, 29, 57, 31, 257, 149, 15, 209, 252, 98, 242, 74, 247, 209, 252, 74, 82, 152, 98, 256, 167, 55, 152, 210, 238, 197, 57, 98, 173, 26, 31, 158, 251, 69, 250, 4, 315, 92, 220, 194, 225, 162, 314, 57, 31, 237, 167, 137, 209, 220, 194, 225, 210, 236, 152, 31, 239, 167, 137, 209, 209, 220, 194, 225, 4, 235, 4, 315, 152, 210, 236, 152, 31, 161, 31, 242, 167, 237, 210, 242, 67, 15, 209, 253, 98, 209, 254, 210, 239, 197, 57, 98, 173, 26, 152, 74, 247, 209, 253, 74, 82, 152, 98, 256, 167, 55, 152, 31, 52, 31, 10, 209, 246, 98, 257, 74, 247, 209, 246, 74, 82, 74, 104, 152, 98, 249, 167, 209, 314, 98, 315, 152, 152, 31, 243, 167, 128, 209, 243, 98, 209, 225, 98, 314, 152, 152, 31, 244, 167, 128, 209, 244, 98, 209, 314, 98, 225, 152, 152, 31, 245, 167, 128, 209, 245, 98, 209, 225, 98, 314, 152, 152, 31, 246, 167, 128, 209, 246, 98, 209, 225, 98, 314, 152, 152, 31, 70, 31, 158, 229, 57, 31, 258, 167, 189, 209, 218, 67, 233, 210, 223, 210, 224, 98, 209, 223, 98, 224, 152, 98, 209, 224, 98, 315, 152, 98, 209, 232, 210, 226, 98, 231, 210, 227, 152, 98, 209, 226, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 10, 209, 258, 98, 242, 74, 247, 209, 258, 74, 82, 74, 104, 152, 98, 249, 167, 209, 314, 98, 315, 152, 152, 31, 161, 31, 3, 31]}, {"code": "def fused_chunk_retention_bwd_kernel(\n    q,\n    k,\n    v,\n    do,\n    dq,\n    dk,\n    dv,\n    h0,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    CHECK: tl.constexpr,\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n\n    o_i = tl.arange(0, BT)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b) * scale, tl.math.exp2((BT - o_i - 1) * b_b)\n    d_b = tl.math.exp2(BT * b_b)\n\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0) * scale\n\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(\n            h0 + i_bh * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)\n        )\n        b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(\n            k + i_bh * T * K, (T, K), (K, 1), (i * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_v = tl.make_block_ptr(\n            v + i_bh * T * V, (V, T), (1, V), (i_v * BV, i * BT), (BV, BT), (0, 1)\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * T * V, (T, V), (V, 1), (i * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_dq = tl.make_block_ptr(\n            dq + (i_bh + i_v * B * H).to(tl.int64) * T * K,\n            (T, K),\n            (K, 1),\n            (i * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = (b_do * d_q[:, None]).to(b_do.dtype)\n\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = (b_ds * d_s).to(b_k.dtype)\n\n        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_dd, b_h.to(b_k.dtype), allow_tf32=False)\n            b_h = d_b * b_h + tl.dot(\n                (b_v * d_k[None, :]).to(b_k.dtype), b_k, allow_tf32=False\n            )\n        else:\n            b_dq += tl.dot(b_dd, b_h.to(b_k.dtype), allow_tf32=False)\n            b_h = d_b * b_h + tl.dot(\n                (b_v * d_k[None, :]).to(b_k.dtype), b_k, allow_tf32=False\n            )\n\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n\n    b_h = None\n    tl.debug_barrier()\n    d_s = tl.trans(d_s)\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(\n            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, T - i * BT), (BK, BT), (0, 1)\n        )\n        p_k = tl.make_block_ptr(\n            k + i_bh * T * K, (T, K), (K, 1), (T - i * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_v = tl.make_block_ptr(\n            v + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_dk = tl.make_block_ptr(\n            dk + (i_bh + i_v * B * H).to(tl.int64) * T * K,\n            (T, K),\n            (K, 1),\n            (T - i * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dv = tl.make_block_ptr(\n            dv + (i_bh + i_k * B * H).to(tl.int64) * T * V,\n            (T, V),\n            (V, 1),\n            (T - i * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = (b_do * d_q[:, None]).to(b_do.dtype)\n\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = (b_ds * d_s).to(b_k.dtype)\n\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n\n        b_dv = tl.dot(b_s.to(b_q.dtype), b_do, allow_tf32=False)\n        if CHECK and i == 1:\n            b_dk += (\n                tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)\n                * d_k[:, None]\n            )\n            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False) * d_k[:, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n        else:\n            b_dk += (\n                tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)\n                * d_k[:, None]\n            )\n            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False) * d_k[:, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 313, 209, 213, 99, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 58, 6, 99, 224, 58, 6, 99, 225, 58, 6, 99, 226, 58, 6, 99, 227, 58, 6, 99, 228, 58, 6, 99, 229, 58, 6, 99, 230, 58, 6, 99, 231, 58, 6, 152, 58, 31, -1, 232, 99, 233, 99, 234, 167, 209, 148, 209, 314, 152, 99, 148, 209, 315, 152, 99, 148, 209, 316, 152, 152, 31, 235, 167, 234, 194, 224, 31, 236, 167, 69, 209, 314, 99, 227, 152, 31, 237, 167, 102, 209, 315, 4, 138, 209, 4, 317, 4, 235, 210, 315, 152, 152, 31, 238, 99, 239, 167, 209, 138, 209, 209, 236, 68, 315, 152, 210, 237, 152, 210, 221, 99, 138, 209, 209, 227, 4, 236, 4, 315, 152, 210, 237, 152, 152, 31, 240, 167, 138, 209, 227, 210, 237, 152, 31, 241, 167, 236, 197, 58, 99, 173, 26, 130, 236, 197, 173, 99, 58, 26, 31, 242, 167, 176, 209, 241, 99, 138, 209, 209, 236, 197, 58, 99, 173, 26, 4, 236, 197, 173, 99, 58, 26, 152, 210, 237, 152, 99, 314, 152, 210, 221, 31, 243, 167, 153, 209, 197, 229, 99, 228, 26, 99, 83, 167, 131, 152, 31, 158, 230, 58, 31, 244, 167, 189, 209, 220, 68, 234, 210, 225, 210, 226, 99, 209, 226, 99, 225, 152, 99, 209, 315, 99, 226, 152, 99, 209, 232, 210, 229, 99, 233, 210, 228, 152, 99, 209, 229, 99, 228, 152, 99, 209, 314, 99, 315, 152, 152, 31, 243, 167, 53, 209, 244, 99, 245, 167, 209, 314, 99, 315, 152, 152, 75, 246, 209, 131, 152, 31, 161, 31, 122, 247, 139, 5, 209, 314, 99, 60, 209, 222, 99, 227, 152, 152, 58, 31, 248, 167, 189, 209, 214, 68, 234, 210, 222, 210, 225, 99, 209, 222, 99, 225, 152, 99, 209, 225, 99, 315, 152, 99, 209, 247, 210, 227, 99, 233, 210, 228, 152, 99, 209, 227, 99, 228, 152, 99, 209, 315, 99, 314, 152, 152, 31, 249, 167, 189, 209, 215, 68, 234, 210, 222, 210, 226, 99, 209, 226, 99, 222, 152, 99, 209, 315, 99, 226, 152, 99, 209, 232, 210, 229, 99, 247, 210, 227, 152, 99, 209, 229, 99, 227, 152, 99, 209, 314, 99, 315, 152, 152, 31, 250, 167, 189, 209, 216, 68, 234, 210, 222, 210, 226, 99, 209, 222, 99, 226, 152, 99, 209, 226, 99, 315, 152, 99, 209, 247, 210, 227, 99, 232, 210, 229, 152, 99, 209, 227, 99, 229, 152, 99, 209, 315, 99, 314, 152, 152, 31, 251, 167, 189, 209, 217, 68, 209, 234, 68, 232, 210, 223, 210, 224, 152, 75, 246, 209, 157, 152, 210, 222, 210, 225, 99, 209, 222, 99, 225, 152, 99, 209, 225, 99, 315, 152, 99, 209, 247, 210, 227, 99, 233, 210, 228, 152, 99, 209, 227, 99, 228, 152, 99, 209, 315, 99, 314, 152, 152, 31, 252, 167, 53, 209, 248, 99, 245, 167, 209, 314, 99, 315, 152, 152, 31, 253, 167, 53, 209, 249, 99, 245, 167, 209, 314, 99, 315, 152, 152, 31, 254, 167, 53, 209, 250, 99, 245, 167, 209, 314, 99, 315, 152, 152, 31, 255, 167, 209, 254, 210, 238, 197, 58, 99, 173, 26, 152, 75, 246, 209, 254, 75, 83, 152, 31, 256, 167, 15, 209, 254, 99, 253, 99, 257, 167, 55, 152, 31, 256, 167, 209, 256, 210, 242, 152, 75, 246, 209, 252, 75, 83, 152, 31, 258, 167, 15, 209, 256, 99, 252, 99, 257, 167, 55, 152, 31, 158, 231, 93, 247, 70, 314, 58, 31, 258, 149, 15, 209, 255, 99, 243, 75, 246, 209, 252, 75, 83, 152, 99, 257, 167, 55, 152, 31, 243, 167, 240, 210, 243, 68, 15, 209, 209, 253, 210, 239, 197, 173, 99, 58, 26, 152, 75, 246, 209, 252, 75, 83, 152, 99, 252, 99, 257, 167, 55, 152, 31, 161, 31, 29, 58, 31, 258, 149, 15, 209, 255, 99, 243, 75, 246, 209, 252, 75, 83, 152, 99, 257, 167, 55, 152, 31, 243, 167, 240, 210, 243, 68, 15, 209, 209, 253, 210, 239, 197, 173, 99, 58, 26, 152, 75, 246, 209, 252, 75, 83, 152, 99, 252, 99, 257, 167, 55, 152, 31, 52, 31, 10, 209, 251, 99, 258, 75, 246, 209, 251, 75, 83, 75, 105, 152, 99, 245, 167, 209, 314, 99, 315, 152, 152, 31, 71, 31, 243, 167, 173, 31, 48, 209, 152, 31, 242, 167, 66, 209, 242, 152, 31, 259, 167, 153, 209, 197, 228, 99, 229, 26, 99, 83, 167, 131, 152, 31, 122, 247, 139, 5, 209, 315, 99, 60, 209, 222, 99, 227, 152, 68, 315, 152, 58, 31, 260, 167, 189, 209, 213, 68, 234, 210, 222, 210, 225, 99, 209, 225, 99, 222, 152, 99, 209, 315, 99, 225, 152, 99, 209, 233, 210, 228, 99, 222, 4, 247, 210, 227, 152, 99, 209, 228, 99, 227, 152, 99, 209, 314, 99, 315, 152, 152, 31, 248, 167, 189, 209, 214, 68, 234, 210, 222, 210, 225, 99, 209, 222, 99, 225, 152, 99, 209, 225, 99, 315, 152, 99, 209, 222, 4, 247, 210, 227, 99, 233, 210, 228, 152, 99, 209, 227, 99, 228, 152, 99, 209, 315, 99, 314, 152, 152, 31, 249, 167, 189, 209, 215, 68, 234, 210, 222, 210, 226, 99, 209, 222, 99, 226, 152, 99, 209, 226, 99, 315, 152, 99, 209, 222, 4, 247, 210, 227, 99, 232, 210, 229, 152, 99, 209, 227, 99, 229, 152, 99, 209, 315, 99, 314, 152, 152, 31, 250, 167, 189, 209, 216, 68, 234, 210, 222, 210, 226, 99, 209, 222, 99, 226, 152, 99, 209, 226, 99, 315, 152, 99, 209, 222, 4, 247, 210, 227, 99, 232, 210, 229, 152, 99, 209, 227, 99, 229, 152, 99, 209, 315, 99, 314, 152, 152, 31, 261, 167, 189, 209, 218, 68, 209, 234, 68, 232, 210, 223, 210, 224, 152, 75, 246, 209, 157, 152, 210, 222, 210, 225, 99, 209, 222, 99, 225, 152, 99, 209, 225, 99, 315, 152, 99, 209, 222, 4, 247, 210, 227, 99, 233, 210, 228, 152, 99, 209, 227, 99, 228, 152, 99, 209, 315, 99, 314, 152, 152, 31, 262, 167, 189, 209, 219, 68, 209, 234, 68, 233, 210, 223, 210, 224, 152, 75, 246, 209, 157, 152, 210, 222, 210, 226, 99, 209, 222, 99, 226, 152, 99, 209, 226, 99, 315, 152, 99, 209, 222, 4, 247, 210, 227, 99, 232, 210, 229, 152, 99, 209, 227, 99, 229, 152, 99, 209, 315, 99, 314, 152, 152, 31, 263, 167, 53, 209, 260, 99, 245, 167, 209, 314, 99, 315, 152, 152, 31, 252, 167, 53, 209, 248, 99, 245, 167, 209, 314, 99, 315, 152, 152, 31, 253, 167, 53, 209, 249, 99, 245, 167, 209, 314, 99, 315, 152, 152, 31, 254, 167, 53, 209, 250, 99, 245, 167, 209, 314, 99, 315, 152, 152, 31, 255, 167, 209, 254, 210, 238, 197, 58, 99, 173, 26, 152, 75, 246, 209, 254, 75, 83, 152, 31, 256, 167, 15, 209, 253, 99, 66, 209, 254, 152, 99, 257, 167, 55, 152, 31, 256, 167, 209, 256, 210, 242, 152, 75, 246, 209, 252, 75, 83, 152, 31, 264, 167, 15, 209, 252, 99, 263, 99, 257, 167, 55, 152, 210, 242, 31, 265, 167, 15, 209, 256, 99, 66, 209, 263, 152, 99, 257, 167, 55, 152, 31, 266, 167, 15, 209, 264, 75, 246, 209, 263, 75, 83, 152, 99, 254, 99, 257, 167, 55, 152, 31, 158, 231, 93, 247, 70, 315, 58, 31, 265, 149, 15, 209, 253, 99, 66, 209, 259, 152, 75, 246, 209, 253, 75, 83, 152, 99, 257, 167, 55, 152, 210, 239, 197, 58, 99, 173, 26, 31, 266, 149, 15, 209, 252, 99, 259, 75, 246, 209, 252, 75, 83, 152, 99, 257, 167, 55, 152, 210, 239, 197, 58, 99, 173, 26, 31, 259, 167, 240, 210, 259, 68, 15, 209, 263, 99, 255, 99, 257, 167, 55, 152, 31, 161, 31, 29, 58, 31, 265, 149, 15, 209, 253, 99, 66, 209, 259, 152, 75, 246, 209, 253, 75, 83, 152, 99, 257, 167, 55, 152, 210, 239, 197, 58, 99, 173, 26, 31, 266, 149, 15, 209, 252, 99, 259, 75, 246, 209, 252, 75, 83, 152, 99, 257, 167, 55, 152, 210, 239, 197, 58, 99, 173, 26, 31, 259, 167, 240, 210, 259, 68, 15, 209, 263, 99, 255, 99, 257, 167, 55, 152, 31, 52, 31, 10, 209, 261, 99, 265, 75, 246, 209, 261, 75, 83, 75, 105, 152, 99, 245, 167, 209, 314, 99, 315, 152, 152, 31, 10, 209, 262, 99, 266, 75, 246, 209, 262, 75, 83, 75, 105, 152, 99, 245, 167, 209, 314, 99, 315, 152, 152, 31, 71, 31, 3, 31]}, {"code": "def chunk_rwkv6_fwd_cumsum_kernel(\n    s,\n    oi,\n    oe,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    o_i = tl.arange(0, BT)\n    m_i = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0).to(tl.float32)\n    m_e = tl.where(o_i[:, None] > o_i[None, :], 1.0, 0.0).to(tl.float32)\n\n    p_s = tl.make_block_ptr(\n        s + (bos * H + i_h) * S,\n        (T, S),\n        (H * S, 1),\n        (i_t * BT, i_s * BS),\n        (BT, BS),\n        (1, 0),\n    )\n    p_oi = tl.make_block_ptr(\n        oi + (bos * H + i_h) * S,\n        (T, S),\n        (H * S, 1),\n        (i_t * BT, i_s * BS),\n        (BT, BS),\n        (1, 0),\n    )\n    p_oe = tl.make_block_ptr(\n        oe + (bos * H + i_h) * S,\n        (T, S),\n        (H * S, 1),\n        (i_t * BT, i_s * BS),\n        (BT, BS),\n        (1, 0),\n    )\n\n    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n    b_oi = tl.dot(m_i, b_s)\n    b_oe = tl.dot(m_e, b_s)\n    tl.store(\n        p_oi,\n        b_oi.to(p_oi.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_oe,\n        b_oe.to(p_oe.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )", "encoded": [28, 313, 209, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 152, 57, 31, -1, 224, 98, 225, 98, 226, 167, 209, 148, 209, 314, 152, 98, 148, 209, 315, 152, 98, 148, 209, 316, 152, 152, 31, 227, 98, 228, 167, 209, 226, 46, 219, 98, 226, 194, 219, 152, 31, 158, 223, 57, 31, 229, 98, 225, 167, 209, 51, 209, 217, 67, 225, 210, 316, 152, 74, 230, 209, 212, 152, 98, 51, 209, 217, 67, 225, 210, 316, 67, 315, 152, 74, 230, 209, 212, 152, 152, 31, 231, 98, 232, 167, 209, 51, 209, 216, 67, 229, 152, 74, 230, 209, 212, 152, 98, 51, 209, 216, 67, 229, 67, 315, 152, 74, 230, 209, 212, 152, 152, 31, 218, 167, 232, 4, 231, 31, 161, 31, 29, 57, 31, 231, 98, 232, 167, 209, 227, 210, 218, 98, 227, 210, 218, 67, 218, 152, 31, 52, 31, 233, 167, 68, 209, 314, 98, 221, 152, 31, 234, 167, 176, 209, 233, 197, 57, 98, 173, 26, 129, 233, 197, 173, 98, 57, 26, 98, 315, 98, 314, 152, 74, 230, 209, 130, 152, 31, 235, 167, 176, 209, 233, 197, 57, 98, 173, 26, 112, 233, 197, 173, 98, 57, 26, 98, 315, 98, 314, 152, 74, 230, 209, 130, 152, 31, 236, 167, 189, 209, 213, 67, 209, 231, 210, 219, 67, 228, 152, 210, 220, 98, 209, 218, 98, 220, 152, 98, 209, 219, 210, 220, 98, 315, 152, 98, 209, 225, 210, 221, 98, 224, 210, 222, 152, 98, 209, 221, 98, 222, 152, 98, 209, 315, 98, 314, 152, 152, 31, 237, 167, 189, 209, 214, 67, 209, 231, 210, 219, 67, 228, 152, 210, 220, 98, 209, 218, 98, 220, 152, 98, 209, 219, 210, 220, 98, 315, 152, 98, 209, 225, 210, 221, 98, 224, 210, 222, 152, 98, 209, 221, 98, 222, 152, 98, 209, 315, 98, 314, 152, 152, 31, 238, 167, 189, 209, 215, 67, 209, 231, 210, 219, 67, 228, 152, 210, 220, 98, 209, 218, 98, 220, 152, 98, 209, 219, 210, 220, 98, 315, 152, 98, 209, 225, 210, 221, 98, 224, 210, 222, 152, 98, 209, 221, 98, 222, 152, 98, 209, 315, 98, 314, 152, 152, 31, 239, 167, 51, 209, 236, 98, 240, 167, 209, 314, 98, 315, 152, 152, 74, 230, 209, 130, 152, 31, 241, 167, 15, 209, 234, 98, 239, 152, 31, 242, 167, 15, 209, 235, 98, 239, 152, 31, 10, 209, 237, 98, 241, 74, 230, 209, 237, 74, 82, 74, 104, 98, 145, 167, 317, 152, 98, 240, 167, 209, 314, 98, 315, 152, 152, 31, 10, 209, 238, 98, 242, 74, 230, 209, 238, 74, 82, 74, 104, 98, 145, 167, 317, 152, 98, 240, 167, 209, 314, 98, 315, 152, 152, 31, 3, 31]}, {"code": "def chunk_rwkv6_fwd_A_kernel_intra_sub_inter(\n    q,\n    k,\n    gi,\n    ge,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_i, i_j = i_c // NC, i_c % NC\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    if i_t * BT + i_i * BC >= T:\n        return\n    if i_i <= i_j:\n        return\n\n    m_i = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        m_k = o_k < K\n\n        p_q = tl.make_block_ptr(\n            q + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT + i_i * BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_gq = tl.make_block_ptr(\n            ge + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT + i_i * BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT + i_j * BC),\n            (BK, BC),\n            (0, 1),\n        )\n        p_gk = tl.make_block_ptr(\n            gi + (bos * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT + i_j * BC),\n            (BK, BC),\n            (0, 1),\n        )\n        p_gn = gi + (bos + i_t * BT + i_i * BC - 1) * H * K + i_h * K + o_k\n\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_gq = tl.where(\n            m_i[:, None] & m_k, tl.load(p_gq, boundary_check=(0, 1)), float(\"-inf\")\n        )\n        b_qg = b_q * exp(b_gq - b_gn[None, :]) * scale\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * exp(b_gn[:, None] - b_gk)\n\n        b_A += tl.dot(b_qg, b_kg)\n\n    p_A = tl.make_block_ptr(\n        A + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT + i_i * BC, i_j * BC),\n        (BC, BC),\n        (1, 0),\n    )\n    tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 313, 209, 213, 99, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 58, 6, 99, 223, 58, 6, 99, 224, 58, 6, 99, 225, 58, 6, 99, 226, 58, 6, 99, 227, 58, 6, 99, 228, 58, 6, 152, 58, 31, -1, 229, 99, 230, 99, 231, 167, 209, 148, 209, 314, 152, 99, 148, 209, 315, 152, 99, 148, 209, 316, 152, 152, 31, 232, 99, 233, 167, 209, 231, 46, 222, 99, 231, 194, 222, 152, 31, 234, 99, 235, 167, 209, 230, 46, 227, 99, 230, 194, 227, 152, 31, 158, 228, 58, 31, 236, 99, 229, 167, 209, 53, 209, 219, 68, 229, 210, 316, 152, 75, 237, 209, 57, 152, 99, 53, 209, 219, 68, 229, 210, 316, 68, 315, 152, 75, 237, 209, 57, 152, 152, 31, 238, 99, 239, 167, 209, 53, 209, 218, 68, 236, 152, 75, 237, 209, 57, 152, 99, 53, 209, 218, 68, 236, 68, 315, 152, 75, 237, 209, 57, 152, 152, 31, 221, 167, 239, 4, 238, 31, 161, 31, 29, 58, 31, 238, 99, 239, 167, 209, 232, 210, 221, 99, 232, 210, 221, 68, 221, 152, 31, 52, 31, 158, 229, 210, 224, 68, 234, 210, 225, 130, 221, 58, 31, 198, 31, 161, 31, 158, 234, 188, 235, 58, 31, 198, 31, 161, 31, 240, 167, 229, 210, 224, 68, 234, 210, 225, 68, 69, 209, 314, 99, 225, 152, 1, 221, 31, 241, 167, 153, 209, 197, 225, 99, 225, 26, 99, 83, 167, 131, 152, 31, 122, 242, 139, 5, 209, 60, 209, 223, 99, 226, 152, 152, 58, 31, 243, 167, 242, 210, 226, 68, 69, 209, 314, 99, 226, 152, 31, 244, 167, 243, 1, 223, 31, 245, 167, 189, 209, 213, 68, 209, 238, 210, 222, 68, 233, 152, 210, 223, 99, 209, 221, 99, 223, 152, 99, 209, 222, 210, 223, 99, 315, 152, 99, 209, 229, 210, 224, 68, 234, 210, 225, 99, 242, 210, 226, 152, 99, 209, 225, 99, 226, 152, 99, 209, 315, 99, 314, 152, 152, 31, 246, 167, 189, 209, 216, 68, 209, 238, 210, 222, 68, 233, 152, 210, 223, 99, 209, 221, 99, 223, 152, 99, 209, 222, 210, 223, 99, 315, 152, 99, 209, 229, 210, 224, 68, 234, 210, 225, 99, 242, 210, 226, 152, 99, 209, 225, 99, 226, 152, 99, 209, 315, 99, 314, 152, 152, 31, 247, 167, 189, 209, 214, 68, 209, 238, 210, 222, 68, 233, 152, 210, 223, 99, 209, 223, 99, 221, 152, 99, 209, 315, 99, 222, 210, 223, 152, 99, 209, 242, 210, 226, 99, 229, 210, 224, 68, 235, 210, 225, 152, 99, 209, 226, 99, 225, 152, 99, 209, 314, 99, 315, 152, 152, 31, 248, 167, 189, 209, 215, 68, 209, 238, 210, 222, 68, 233, 152, 210, 223, 99, 209, 223, 99, 221, 152, 99, 209, 315, 99, 222, 210, 223, 152, 99, 209, 242, 210, 226, 99, 229, 210, 224, 68, 235, 210, 225, 152, 99, 209, 226, 99, 225, 152, 99, 209, 314, 99, 315, 152, 152, 31, 249, 167, 215, 68, 209, 238, 68, 229, 210, 224, 68, 234, 210, 225, 4, 315, 152, 210, 222, 210, 223, 68, 233, 210, 223, 68, 243, 31, 250, 167, 53, 209, 249, 99, 251, 167, 244, 99, 252, 167, 314, 152, 31, 253, 167, 53, 209, 245, 99, 254, 167, 209, 314, 99, 315, 152, 152, 31, 255, 167, 176, 209, 240, 197, 58, 99, 173, 26, 150, 244, 99, 53, 209, 246, 99, 254, 167, 209, 314, 99, 315, 152, 152, 99, 256, 209, 317, 152, 152, 31, 257, 167, 253, 210, 178, 209, 255, 4, 250, 197, 173, 99, 58, 26, 152, 210, 220, 31, 258, 167, 53, 209, 247, 99, 254, 167, 209, 314, 99, 315, 152, 152, 31, 259, 167, 53, 209, 248, 99, 254, 167, 209, 314, 99, 315, 152, 152, 31, 260, 167, 258, 210, 178, 209, 250, 197, 58, 99, 173, 26, 4, 259, 152, 31, 241, 149, 15, 209, 257, 99, 260, 152, 31, 71, 31, 261, 167, 189, 209, 217, 68, 209, 238, 210, 222, 68, 233, 152, 210, 224, 99, 209, 221, 99, 224, 152, 99, 209, 222, 210, 224, 99, 315, 152, 99, 209, 229, 210, 224, 68, 234, 210, 225, 99, 235, 210, 225, 152, 99, 209, 225, 99, 225, 152, 99, 209, 315, 99, 314, 152, 152, 31, 10, 209, 261, 99, 241, 75, 237, 209, 217, 75, 83, 75, 105, 152, 99, 254, 167, 209, 314, 99, 315, 152, 152, 31, 3, 31]}, {"code": "def chunk_rwkv6_fwd_A_kernel_intra_sub_intra(\n    q,\n    k,\n    gi,\n    ge,\n    u,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_j = i_i\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    if i_t * BT + i_i * BC >= T:\n        return\n\n    o_i = tl.arange(0, BC)\n    o_k = tl.arange(0, BK)\n    m_k = o_k < K\n    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n    o_A = (bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT + i_h * BT + i_j * BC\n    p_q = tl.make_block_ptr(\n        q + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, 0),\n        (BC, BK),\n        (1, 0),\n    )\n    p_g = tl.make_block_ptr(\n        ge + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, 0),\n        (BC, BK),\n        (1, 0),\n    )\n    p_qj = q + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\n    p_kj = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\n    p_gk = gi + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n\n    p_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (0,), (BK,), (0,))\n    b_u = tl.load(p_u, boundary_check=(0,))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)\n        b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)\n        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n        b_A = tl.sum(b_q * b_kj[None, :] * exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i > j, b_A * scale, 0.0)\n        b_A = tl.where(o_i != j, b_A, tl.sum(b_qj * b_kj * b_u * scale))\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_qj += H * K\n        p_kj += H * K\n        p_gk += H * K", "encoded": [28, 313, 209, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 152, 57, 31, -1, 229, 98, 230, 98, 231, 167, 209, 148, 209, 314, 152, 98, 148, 209, 315, 152, 98, 148, 209, 316, 152, 152, 31, 232, 98, 233, 167, 209, 231, 46, 223, 98, 231, 194, 223, 152, 31, 234, 167, 230, 31, 158, 228, 57, 31, 235, 98, 229, 167, 209, 51, 209, 220, 67, 229, 210, 316, 152, 74, 236, 209, 212, 152, 98, 51, 209, 220, 67, 229, 210, 316, 67, 315, 152, 74, 236, 209, 212, 152, 152, 31, 237, 98, 238, 167, 209, 51, 209, 219, 67, 235, 152, 74, 236, 209, 212, 152, 98, 51, 209, 219, 67, 235, 67, 315, 152, 74, 236, 209, 212, 152, 152, 31, 222, 167, 238, 4, 237, 31, 161, 31, 29, 57, 31, 237, 98, 238, 167, 209, 232, 210, 222, 98, 232, 210, 222, 67, 222, 152, 31, 52, 31, 158, 229, 210, 225, 67, 230, 210, 226, 129, 222, 57, 31, 198, 31, 161, 31, 239, 167, 68, 209, 314, 98, 226, 152, 31, 240, 167, 68, 209, 314, 98, 227, 152, 31, 241, 167, 240, 1, 224, 31, 242, 167, 229, 210, 225, 67, 230, 210, 226, 67, 68, 209, 314, 98, 226, 152, 1, 222, 31, 243, 167, 209, 237, 67, 229, 210, 225, 67, 230, 210, 226, 67, 68, 209, 314, 98, 226, 152, 152, 210, 223, 210, 225, 67, 233, 210, 225, 67, 234, 210, 226, 31, 244, 167, 189, 209, 213, 67, 209, 237, 210, 223, 67, 233, 152, 210, 224, 98, 209, 222, 98, 224, 152, 98, 209, 223, 210, 224, 98, 315, 152, 98, 209, 229, 210, 225, 67, 230, 210, 226, 98, 314, 152, 98, 209, 226, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 245, 167, 189, 209, 216, 67, 209, 237, 210, 223, 67, 233, 152, 210, 224, 98, 209, 222, 98, 224, 152, 98, 209, 223, 210, 224, 98, 315, 152, 98, 209, 229, 210, 225, 67, 230, 210, 226, 98, 314, 152, 98, 209, 226, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 246, 167, 213, 67, 209, 237, 67, 229, 210, 225, 67, 234, 210, 226, 152, 210, 223, 210, 224, 67, 233, 210, 224, 67, 240, 31, 247, 167, 214, 67, 209, 237, 67, 229, 210, 225, 67, 234, 210, 226, 152, 210, 223, 210, 224, 67, 233, 210, 224, 67, 240, 31, 248, 167, 215, 67, 209, 237, 67, 229, 210, 225, 67, 234, 210, 226, 152, 210, 223, 210, 224, 67, 233, 210, 224, 67, 240, 31, 249, 167, 51, 209, 244, 98, 250, 167, 209, 314, 98, 315, 152, 152, 31, 251, 167, 51, 209, 245, 98, 250, 167, 209, 314, 98, 315, 152, 152, 31, 252, 167, 189, 209, 217, 67, 233, 210, 224, 98, 209, 224, 98, 152, 98, 209, 315, 98, 152, 98, 209, 314, 98, 152, 98, 209, 227, 98, 152, 98, 209, 314, 98, 152, 152, 31, 253, 167, 51, 209, 252, 98, 250, 167, 209, 314, 98, 152, 152, 31, 121, 254, 139, 5, 209, 314, 98, 38, 209, 226, 98, 222, 4, 229, 210, 225, 4, 230, 210, 226, 152, 152, 57, 31, 255, 167, 51, 209, 246, 98, 256, 167, 241, 98, 257, 167, 314, 152, 74, 236, 209, 130, 152, 31, 258, 167, 51, 209, 247, 98, 256, 167, 241, 98, 257, 167, 314, 152, 74, 236, 209, 130, 152, 31, 259, 167, 51, 209, 248, 98, 256, 167, 241, 98, 257, 167, 314, 152, 74, 236, 209, 130, 152, 31, 260, 167, 191, 209, 249, 210, 258, 197, 173, 98, 57, 26, 210, 178, 209, 251, 4, 259, 197, 173, 98, 57, 26, 152, 98, 315, 152, 31, 260, 167, 176, 209, 239, 112, 254, 98, 260, 210, 221, 98, 314, 152, 31, 260, 167, 176, 209, 239, 162, 254, 98, 260, 98, 191, 209, 255, 210, 258, 210, 253, 210, 221, 152, 152, 31, 10, 209, 218, 67, 243, 67, 254, 98, 260, 98, 256, 167, 242, 152, 31, 246, 149, 223, 210, 224, 31, 247, 149, 223, 210, 224, 31, 248, 149, 223, 210, 224, 31, 70, 31, 3, 31]}, {"code": "def chunk_rwkv6_fwd_A_kernel_intra_sub_intra_split(\n    q,\n    k,\n    gi,\n    ge,\n    u,\n    A,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    B: tl.constexpr,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_t, i_i = i_tc // NC, i_tc % NC\n    i_j = i_i\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        all = T\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        all = B * T\n\n    if i_t * BT + i_i * BC >= T:\n        return\n\n    o_i = tl.arange(0, BC)\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n\n    o_A = (i_k * all + bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BC + i_h * BC\n    p_q = tl.make_block_ptr(\n        q + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    p_g = tl.make_block_ptr(\n        ge + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    p_qj = q + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\n    p_kj = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\n    p_gk = gi + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n\n    p_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (i_k * BK), (BK,), (0,))\n    b_u = tl.load(p_u, boundary_check=(0,))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)\n        b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)\n        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n        b_A = tl.sum(b_q * b_kj[None, :] * exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i > j, b_A * scale, 0.0)\n        b_A = tl.where(o_i != j, b_A, tl.sum(b_qj * b_kj * b_u * scale))\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_qj += H * K\n        p_kj += H * K\n        p_gk += H * K", "encoded": [28, 313, 209, 213, 99, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 58, 6, 99, 223, 99, 224, 58, 6, 99, 225, 58, 6, 99, 226, 58, 6, 99, 227, 58, 6, 99, 228, 58, 6, 99, 229, 58, 6, 99, 230, 58, 6, 152, 58, 31, -1, 231, 99, 232, 99, 233, 167, 209, 148, 209, 314, 152, 99, 148, 209, 315, 152, 99, 148, 209, 316, 152, 152, 31, 234, 99, 235, 167, 209, 233, 46, 224, 99, 233, 194, 224, 152, 31, 236, 99, 237, 167, 209, 232, 46, 229, 99, 232, 194, 229, 152, 31, 238, 167, 237, 31, 158, 230, 58, 31, 239, 99, 236, 167, 209, 53, 209, 220, 68, 236, 210, 316, 152, 75, 240, 209, 57, 152, 99, 53, 209, 220, 68, 236, 210, 316, 68, 315, 152, 75, 240, 209, 57, 152, 152, 31, 241, 99, 242, 167, 209, 53, 209, 219, 68, 239, 152, 75, 240, 209, 57, 152, 99, 53, 209, 219, 68, 239, 68, 315, 152, 75, 240, 209, 57, 152, 152, 31, 144, 167, 223, 31, 223, 167, 242, 4, 241, 31, 161, 31, 29, 58, 31, 241, 99, 242, 167, 209, 234, 210, 223, 99, 234, 210, 223, 68, 223, 152, 31, 144, 167, 222, 210, 223, 31, 52, 31, 158, 236, 210, 226, 68, 237, 210, 227, 130, 223, 58, 31, 198, 31, 161, 31, 243, 167, 69, 209, 314, 99, 227, 152, 31, 244, 167, 231, 210, 228, 68, 69, 209, 314, 99, 228, 152, 31, 245, 167, 244, 1, 225, 31, 246, 167, 236, 210, 226, 68, 237, 210, 227, 68, 69, 209, 314, 99, 227, 152, 1, 223, 31, 247, 167, 209, 231, 210, 144, 68, 241, 68, 236, 210, 226, 68, 237, 210, 227, 68, 69, 209, 314, 99, 227, 152, 152, 210, 224, 210, 227, 68, 235, 210, 227, 31, 248, 167, 189, 209, 213, 68, 209, 241, 210, 224, 68, 235, 152, 210, 225, 99, 209, 223, 99, 225, 152, 99, 209, 224, 210, 225, 99, 315, 152, 99, 209, 236, 210, 226, 68, 237, 210, 227, 99, 231, 210, 228, 152, 99, 209, 227, 99, 228, 152, 99, 209, 315, 99, 314, 152, 152, 31, 249, 167, 189, 209, 216, 68, 209, 241, 210, 224, 68, 235, 152, 210, 225, 99, 209, 223, 99, 225, 152, 99, 209, 224, 210, 225, 99, 315, 152, 99, 209, 236, 210, 226, 68, 237, 210, 227, 99, 231, 210, 228, 152, 99, 209, 227, 99, 228, 152, 99, 209, 315, 99, 314, 152, 152, 31, 250, 167, 213, 68, 209, 241, 68, 236, 210, 226, 68, 238, 210, 227, 152, 210, 224, 210, 225, 68, 235, 210, 225, 68, 244, 31, 251, 167, 214, 68, 209, 241, 68, 236, 210, 226, 68, 238, 210, 227, 152, 210, 224, 210, 225, 68, 235, 210, 225, 68, 244, 31, 252, 167, 215, 68, 209, 241, 68, 236, 210, 226, 68, 238, 210, 227, 152, 210, 224, 210, 225, 68, 235, 210, 225, 68, 244, 31, 253, 167, 53, 209, 248, 99, 254, 167, 209, 314, 99, 315, 152, 152, 31, 255, 167, 53, 209, 249, 99, 254, 167, 209, 314, 99, 315, 152, 152, 31, 256, 167, 189, 209, 217, 68, 235, 210, 225, 99, 209, 225, 99, 152, 99, 209, 315, 99, 152, 99, 231, 210, 228, 99, 209, 228, 99, 152, 99, 209, 314, 99, 152, 152, 31, 257, 167, 53, 209, 256, 99, 254, 167, 209, 314, 99, 152, 152, 31, 122, 258, 139, 5, 209, 314, 99, 38, 209, 227, 99, 223, 4, 236, 210, 226, 4, 237, 210, 227, 152, 152, 58, 31, 259, 167, 53, 209, 250, 99, 260, 167, 245, 99, 261, 167, 314, 152, 75, 240, 209, 131, 152, 31, 262, 167, 53, 209, 251, 99, 260, 167, 245, 99, 261, 167, 314, 152, 75, 240, 209, 131, 152, 31, 263, 167, 53, 209, 252, 99, 260, 167, 245, 99, 261, 167, 314, 152, 75, 240, 209, 131, 152, 31, 264, 167, 191, 209, 253, 210, 262, 197, 173, 99, 58, 26, 210, 178, 209, 255, 4, 263, 197, 173, 99, 58, 26, 152, 99, 315, 152, 31, 264, 167, 176, 209, 243, 113, 258, 99, 264, 210, 221, 99, 314, 152, 31, 264, 167, 176, 209, 243, 162, 258, 99, 264, 99, 191, 209, 259, 210, 262, 210, 257, 210, 221, 152, 152, 31, 10, 209, 218, 68, 247, 68, 258, 99, 264, 99, 260, 167, 246, 152, 31, 250, 149, 224, 210, 225, 31, 251, 149, 224, 210, 225, 31, 252, 149, 224, 210, 225, 31, 71, 31, 3, 31]}, {"code": "def chunk_rwkv6_fwd_A_kernel_intra_sub_intra_merge(\n    A,\n    A2,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    NK: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        all = T\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        all = B * T\n\n    if i_t * BT + i_c * BC >= T:\n        return\n\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(0, NK):\n        p_A = tl.make_block_ptr(\n            A + (i_k * all + bos) * H * BC + i_h * BC,\n            (T, BC),\n            (H * BC, 1),\n            (i_t * BT + i_c * BC, 0),\n            (BC, BC),\n            (1, 0),\n        )\n        b_A += tl.load(p_A, boundary_check=(0, 1))\n    p_A2 = tl.make_block_ptr(\n        A2 + (bos * H + i_h) * BT,\n        (T, BT),\n        (H * BT, 1),\n        (i_t * BT + i_c * BC, i_c * BC),\n        (BC, BC),\n        (1, 0),\n    )\n    tl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 313, 209, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 152, 57, 31, -1, 224, 98, 225, 98, 226, 167, 209, 148, 209, 314, 152, 98, 148, 209, 315, 152, 98, 148, 209, 316, 152, 152, 31, 227, 98, 228, 167, 209, 226, 46, 219, 98, 226, 194, 219, 152, 31, 158, 223, 57, 31, 229, 98, 224, 167, 209, 51, 209, 216, 67, 224, 210, 316, 152, 74, 230, 209, 212, 152, 98, 51, 209, 216, 67, 224, 210, 316, 67, 315, 152, 74, 230, 209, 212, 152, 152, 31, 231, 98, 232, 167, 209, 51, 209, 215, 67, 229, 152, 74, 230, 209, 212, 152, 98, 51, 209, 215, 67, 229, 67, 315, 152, 74, 230, 209, 212, 152, 152, 31, 144, 167, 217, 31, 217, 167, 232, 4, 231, 31, 161, 31, 29, 57, 31, 231, 98, 232, 167, 209, 227, 210, 217, 98, 227, 210, 217, 67, 217, 152, 31, 144, 167, 218, 210, 217, 31, 52, 31, 158, 224, 210, 220, 67, 225, 210, 221, 129, 217, 57, 31, 198, 31, 161, 31, 233, 167, 153, 209, 197, 221, 98, 221, 26, 98, 82, 167, 130, 152, 31, 121, 234, 139, 5, 209, 314, 98, 222, 152, 57, 31, 235, 167, 189, 209, 213, 67, 209, 234, 210, 144, 67, 231, 152, 210, 219, 210, 221, 67, 228, 210, 221, 98, 209, 217, 98, 221, 152, 98, 209, 219, 210, 221, 98, 315, 152, 98, 209, 224, 210, 220, 67, 225, 210, 221, 98, 314, 152, 98, 209, 221, 98, 221, 152, 98, 209, 315, 98, 314, 152, 152, 31, 233, 149, 51, 209, 235, 98, 236, 167, 209, 314, 98, 315, 152, 152, 31, 70, 31, 237, 167, 189, 209, 214, 67, 209, 231, 210, 219, 67, 228, 152, 210, 220, 98, 209, 217, 98, 220, 152, 98, 209, 219, 210, 220, 98, 315, 152, 98, 209, 224, 210, 220, 67, 225, 210, 221, 98, 225, 210, 221, 152, 98, 209, 221, 98, 221, 152, 98, 209, 315, 98, 314, 152, 152, 31, 10, 209, 237, 98, 233, 74, 230, 209, 214, 74, 82, 74, 104, 152, 98, 236, 167, 209, 314, 98, 315, 152, 152, 31, 3, 31]}, {"code": "def chunk_rwkv6_bwd_kernel_dh(\n    q,\n    gi,\n    ge,\n    do,\n    dh,\n    dht,\n    dh0,\n    cu_seqlens,\n    chunk_offsets,\n    scale,\n    T,\n    HQ: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NG: tl.constexpr,\n    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_hq = i_nh // HQ, i_nh % HQ\n    i_h = i_hq // NG\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(\n            dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT - 1, -1, -1):\n        p_dh = tl.make_block_ptr(\n            dh + ((boh + i_t) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        last_idx = min(i_t * BT + BT, T) - 1\n\n        p_q = tl.make_block_ptr(\n            q + (bos * HQ + i_hq) * K,\n            (K, T),\n            (1, HQ * K),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_do = tl.make_block_ptr(\n            do + (bos * HQ + i_hq) * V,\n            (T, V),\n            (HQ * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        p_gk = tl.make_block_ptr(\n            ge + (bos * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_gk_last = (\n            gi + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        )\n\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_q = (b_q * exp(b_gk) * scale).to(b_q.dtype)\n        b_gk_last = tl.load(\n            p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0\n        )\n        b_dh *= exp(b_gk_last)[:, None]\n        b_dh += tl.dot(b_q, b_do)\n\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = tl.make_block_ptr(\n            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 313, 209, 213, 99, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 99, 224, 58, 6, 99, 225, 58, 6, 99, 226, 58, 6, 99, 227, 58, 6, 99, 228, 58, 6, 99, 229, 58, 6, 99, 230, 58, 6, 99, 231, 58, 6, 99, 232, 58, 6, 99, 233, 58, 6, 99, 234, 58, 6, 152, 58, 31, -1, 235, 99, 236, 99, 237, 167, 209, 148, 209, 314, 152, 99, 148, 209, 315, 152, 99, 148, 209, 316, 152, 152, 31, 238, 99, 239, 167, 209, 237, 46, 224, 99, 237, 194, 224, 152, 31, 240, 167, 239, 46, 231, 31, 158, 234, 58, 31, 241, 99, 242, 167, 209, 53, 209, 220, 68, 238, 152, 75, 243, 209, 57, 152, 99, 53, 209, 220, 68, 238, 68, 315, 152, 75, 243, 209, 57, 152, 152, 31, 223, 167, 242, 4, 241, 31, 244, 167, 60, 209, 223, 99, 228, 152, 31, 245, 167, 53, 209, 221, 68, 238, 152, 75, 243, 209, 57, 152, 31, 161, 31, 29, 58, 31, 241, 99, 242, 167, 209, 238, 210, 223, 99, 238, 210, 223, 68, 223, 152, 31, 244, 167, 60, 209, 223, 99, 228, 152, 31, 245, 167, 238, 210, 244, 31, 52, 31, 246, 167, 153, 209, 197, 229, 99, 230, 26, 99, 83, 167, 131, 152, 31, 158, 233, 58, 31, 247, 167, 189, 209, 218, 68, 237, 210, 226, 210, 227, 99, 209, 226, 99, 227, 152, 99, 209, 227, 99, 315, 152, 99, 209, 235, 210, 229, 99, 236, 210, 230, 152, 99, 209, 229, 99, 230, 152, 99, 209, 315, 99, 314, 152, 152, 31, 246, 149, 53, 209, 247, 99, 248, 167, 209, 314, 99, 315, 152, 152, 75, 243, 209, 131, 152, 31, 161, 31, 122, 249, 139, 5, 209, 244, 4, 315, 99, 4, 315, 99, 4, 315, 152, 58, 31, 250, 167, 189, 209, 217, 68, 209, 209, 245, 68, 249, 152, 210, 225, 68, 240, 152, 210, 226, 210, 227, 99, 209, 226, 99, 227, 152, 99, 209, 227, 99, 315, 152, 99, 209, 235, 210, 229, 99, 236, 210, 230, 152, 99, 209, 229, 99, 230, 152, 99, 209, 315, 99, 314, 152, 152, 31, 10, 209, 250, 99, 246, 75, 243, 209, 250, 75, 83, 75, 105, 152, 99, 248, 167, 209, 314, 99, 315, 152, 152, 31, 251, 167, 38, 209, 249, 210, 228, 68, 228, 99, 223, 152, 4, 315, 31, 252, 167, 189, 209, 213, 68, 209, 241, 210, 224, 68, 239, 152, 210, 226, 99, 209, 226, 99, 223, 152, 99, 209, 315, 99, 224, 210, 226, 152, 99, 209, 235, 210, 229, 99, 249, 210, 228, 152, 99, 209, 229, 99, 228, 152, 99, 209, 314, 99, 315, 152, 152, 31, 253, 167, 189, 209, 216, 68, 209, 241, 210, 224, 68, 239, 152, 210, 227, 99, 209, 223, 99, 227, 152, 99, 209, 224, 210, 227, 99, 315, 152, 99, 209, 249, 210, 228, 99, 236, 210, 230, 152, 99, 209, 228, 99, 230, 152, 99, 209, 315, 99, 314, 152, 152, 31, 254, 167, 53, 209, 252, 99, 248, 167, 209, 314, 99, 315, 152, 152, 31, 255, 167, 53, 209, 253, 99, 248, 167, 209, 314, 99, 315, 152, 152, 31, 256, 167, 189, 209, 215, 68, 209, 241, 210, 225, 68, 240, 152, 210, 226, 99, 209, 226, 99, 223, 152, 99, 209, 315, 99, 225, 210, 226, 152, 99, 209, 235, 210, 229, 99, 249, 210, 228, 152, 99, 209, 229, 99, 228, 152, 99, 209, 314, 99, 315, 152, 152, 31, 257, 167, 214, 68, 209, 241, 68, 251, 152, 210, 225, 210, 226, 68, 240, 210, 226, 68, 235, 210, 229, 68, 69, 209, 314, 99, 229, 152, 31, 258, 167, 53, 209, 256, 99, 248, 167, 209, 314, 99, 315, 152, 152, 31, 254, 167, 209, 254, 210, 178, 209, 258, 152, 210, 222, 152, 75, 243, 209, 254, 75, 83, 152, 31, 259, 167, 53, 209, 257, 99, 260, 167, 235, 210, 229, 68, 69, 209, 314, 99, 229, 152, 1, 226, 99, 261, 167, 314, 152, 31, 246, 23, 178, 209, 259, 152, 197, 58, 99, 173, 26, 31, 246, 149, 15, 209, 254, 99, 255, 152, 31, 71, 31, 158, 232, 58, 31, 262, 167, 189, 209, 219, 68, 237, 210, 226, 210, 227, 99, 209, 226, 99, 227, 152, 99, 209, 227, 99, 315, 152, 99, 209, 235, 210, 229, 99, 236, 210, 230, 152, 99, 209, 229, 99, 230, 152, 99, 209, 315, 99, 314, 152, 152, 31, 10, 209, 262, 99, 246, 75, 243, 209, 262, 75, 83, 75, 105, 152, 99, 248, 167, 209, 314, 99, 315, 152, 152, 31, 161, 31, 3, 31]}, {"code": "def chunk_rwkv6_bwd_kernel_intra(\n    q,\n    k,\n    gi,\n    ge,\n    dA,\n    dq,\n    dk,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_t, i_i = i_c // NC, i_c % NC\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n    T = eos - bos\n    if i_t * BT + i_i * BC >= T:\n        return\n\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n\n    p_ge = tl.make_block_ptr(\n        ge + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n\n    b_ge = tl.load(p_ge, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    if i_i > 0:\n        p_gn = gi + (bos + i_t * BT + i_i * BC - 1) * H * K + i_h * K + o_k\n\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        for i_j in range(0, i_i):\n            p_k = tl.make_block_ptr(\n                k + (bos * H + i_h) * K,\n                (T, K),\n                (H * K, 1),\n                (i_t * BT + i_j * BC, i_k * BK),\n                (BC, BK),\n                (1, 0),\n            )\n            p_gk = tl.make_block_ptr(\n                gi + (bos * H + i_h) * K,\n                (T, K),\n                (H * K, 1),\n                (i_t * BT + i_j * BC, i_k * BK),\n                (BC, BK),\n                (1, 0),\n            )\n            p_dA = tl.make_block_ptr(\n                dA + (bos * H + i_h) * BT,\n                (T, BT),\n                (H * BT, 1),\n                (i_t * BT + i_i * BC, i_j * BC),\n                (BC, BC),\n                (1, 0),\n            )\n\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_kg = b_k * exp(b_gn[None, :] - b_gk)\n\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n\n            b_dq += tl.dot(b_dA, b_kg)\n        b_dq *= exp(b_ge - b_gn[None, :])\n\n    o_i = tl.arange(0, BC)\n    m_dA = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n    o_dA = (\n        bos * H * BT\n        + (i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT\n        + i_h * BT\n        + i_i * BC\n    )\n    p_kj = k + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\n    p_gkj = gi + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\n    p_dq = tl.make_block_ptr(\n        dq + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n\n        b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)\n        b_gkj = tl.load(p_gkj, mask=m_k, other=0).to(tl.float32)\n\n        m_i = o_i[:, None] > j\n\n        b_dq += tl.where(\n            m_i, b_dA[:, None] * b_kj[None, :] * exp(b_ge - b_gkj[None, :]), 0.0\n        )\n        p_kj += H * K\n        p_gkj += H * K\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n\n    tl.debug_barrier()\n    p_k = tl.make_block_ptr(\n        k + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    p_gk = tl.make_block_ptr(\n        gi + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n\n    NC = min(NC, tl.cdiv(T - i_t * BT, BC))\n    if i_i < NC - 1:\n        p_gn = gi + (bos + min(i_t * BT + i_i * BC + BC, T) - 1) * H * K + i_h * K + o_k\n\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        for i_j in range(i_i + 1, NC):\n            m_j = (i_t * BT + i_j * BC + tl.arange(0, BC)) < T\n            p_q = tl.make_block_ptr(\n                q + (bos * H + i_h) * K,\n                (T, K),\n                (H * K, 1),\n                (i_t * BT + i_j * BC, i_k * BK),\n                (BC, BK),\n                (1, 0),\n            )\n            p_gq = tl.make_block_ptr(\n                ge + (bos * H + i_h) * K,\n                (T, K),\n                (H * K, 1),\n                (i_t * BT + i_j * BC, i_k * BK),\n                (BC, BK),\n                (1, 0),\n            )\n            p_dA = tl.make_block_ptr(\n                dA + (bos * H + i_h) * BT,\n                (BT, T),\n                (1, H * BT),\n                (i_i * BC, i_t * BT + i_j * BC),\n                (BC, BC),\n                (0, 1),\n            )\n\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_gq = tl.where(\n                m_j[:, None] & m_k, tl.load(p_gq, boundary_check=(0, 1)), float(\"-inf\")\n            )\n            b_qg = b_q * exp(b_gq - b_gn[None, :])\n\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n\n            b_dk += tl.dot(b_dA, b_qg)\n        b_dk *= exp(b_gn[None, :] - b_gk)\n    o_dA = (\n        bos * H * BT\n        + (i_t * BT + i_i * BC) * H * BT\n        + i_h * BT\n        + i_i * BC\n        + tl.arange(0, BC)\n    )\n    p_qj = q + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\n    p_gqj = ge + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\n    p_dk = tl.make_block_ptr(\n        dk + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n\n        b_dA = tl.load(dA + o_dA + j * H * BT)\n\n        b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)\n        b_gqj = tl.load(p_gqj, mask=m_k, other=0).to(tl.float32)\n\n        m_i = o_i[:, None] < j\n        b_dk += tl.where(\n            m_i, b_dA[:, None] * b_qj[None, :] * exp(b_gqj[None, :] - b_gk), 0.0\n        )\n        p_qj += H * K\n        p_gqj += H * K\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 313, 209, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 98, 229, 57, 6, 152, 57, 31, -1, 230, 98, 231, 98, 232, 167, 209, 148, 209, 314, 152, 98, 148, 209, 315, 152, 98, 148, 209, 316, 152, 152, 31, 233, 98, 234, 167, 209, 232, 46, 223, 98, 232, 194, 223, 152, 31, 235, 98, 236, 167, 209, 231, 46, 228, 98, 231, 194, 228, 152, 31, 158, 229, 57, 31, 237, 98, 235, 167, 209, 51, 209, 221, 67, 235, 210, 316, 152, 74, 238, 209, 212, 152, 98, 51, 209, 221, 67, 235, 210, 316, 67, 315, 152, 74, 238, 209, 212, 152, 152, 31, 239, 98, 240, 167, 209, 51, 209, 220, 67, 237, 152, 74, 238, 209, 212, 152, 98, 51, 209, 220, 67, 237, 67, 315, 152, 74, 238, 209, 212, 152, 152, 31, 161, 31, 29, 57, 31, 239, 98, 240, 167, 209, 233, 210, 222, 98, 233, 210, 222, 67, 222, 152, 31, 52, 31, 222, 167, 240, 4, 239, 31, 158, 235, 210, 225, 67, 236, 210, 226, 129, 222, 57, 31, 198, 31, 161, 31, 241, 167, 230, 210, 227, 67, 68, 209, 314, 98, 227, 152, 31, 242, 167, 241, 1, 224, 31, 243, 167, 189, 209, 216, 67, 209, 239, 210, 223, 67, 234, 152, 210, 224, 98, 209, 222, 98, 224, 152, 98, 209, 223, 210, 224, 98, 315, 152, 98, 209, 235, 210, 225, 67, 236, 210, 226, 98, 230, 210, 227, 152, 98, 209, 226, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 244, 167, 51, 209, 243, 98, 245, 167, 209, 314, 98, 315, 152, 152, 31, 246, 167, 153, 209, 197, 226, 98, 227, 26, 98, 82, 167, 130, 152, 31, 158, 236, 112, 314, 57, 31, 247, 167, 215, 67, 209, 239, 67, 235, 210, 225, 67, 236, 210, 226, 4, 315, 152, 210, 223, 210, 224, 67, 234, 210, 224, 67, 241, 31, 248, 167, 51, 209, 247, 98, 249, 167, 242, 98, 250, 167, 314, 152, 31, 121, 251, 139, 5, 209, 314, 98, 236, 152, 57, 31, 252, 167, 189, 209, 214, 67, 209, 239, 210, 223, 67, 234, 152, 210, 224, 98, 209, 222, 98, 224, 152, 98, 209, 223, 210, 224, 98, 315, 152, 98, 209, 235, 210, 225, 67, 251, 210, 226, 98, 230, 210, 227, 152, 98, 209, 226, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 253, 167, 189, 209, 215, 67, 209, 239, 210, 223, 67, 234, 152, 210, 224, 98, 209, 222, 98, 224, 152, 98, 209, 223, 210, 224, 98, 315, 152, 98, 209, 235, 210, 225, 67, 251, 210, 226, 98, 230, 210, 227, 152, 98, 209, 226, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 254, 167, 189, 209, 217, 67, 209, 239, 210, 223, 67, 234, 152, 210, 225, 98, 209, 222, 98, 225, 152, 98, 209, 223, 210, 225, 98, 315, 152, 98, 209, 235, 210, 225, 67, 236, 210, 226, 98, 251, 210, 226, 152, 98, 209, 226, 98, 226, 152, 98, 209, 315, 98, 314, 152, 152, 31, 255, 167, 51, 209, 252, 98, 245, 167, 209, 314, 98, 315, 152, 152, 31, 256, 167, 51, 209, 253, 98, 245, 167, 209, 314, 98, 315, 152, 152, 31, 257, 167, 255, 210, 178, 209, 248, 197, 173, 98, 57, 26, 4, 256, 152, 31, 258, 167, 51, 209, 254, 98, 245, 167, 209, 314, 98, 315, 152, 152, 31, 246, 149, 15, 209, 258, 98, 257, 152, 31, 70, 31, 246, 23, 178, 209, 244, 4, 248, 197, 173, 98, 57, 26, 152, 31, 161, 31, 259, 167, 68, 209, 314, 98, 226, 152, 31, 260, 167, 235, 210, 225, 67, 236, 210, 226, 67, 68, 209, 314, 98, 226, 152, 1, 222, 31, 261, 167, 239, 210, 223, 210, 225, 67, 209, 235, 210, 225, 67, 236, 210, 226, 67, 68, 209, 314, 98, 226, 152, 152, 210, 223, 210, 225, 67, 234, 210, 225, 67, 236, 210, 226, 31, 262, 167, 214, 67, 209, 239, 67, 235, 210, 225, 67, 236, 210, 226, 152, 210, 223, 210, 224, 67, 234, 210, 224, 67, 241, 31, 263, 167, 215, 67, 209, 239, 67, 235, 210, 225, 67, 236, 210, 226, 152, 210, 223, 210, 224, 67, 234, 210, 224, 67, 241, 31, 264, 167, 189, 209, 218, 67, 209, 239, 210, 223, 67, 234, 152, 210, 224, 98, 209, 222, 98, 224, 152, 98, 209, 223, 210, 224, 98, 315, 152, 98, 209, 235, 210, 225, 67, 236, 210, 226, 98, 230, 210, 227, 152, 98, 209, 226, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 121, 265, 139, 5, 209, 314, 98, 38, 209, 226, 98, 222, 4, 235, 210, 225, 4, 236, 210, 226, 152, 152, 57, 31, 258, 167, 51, 209, 217, 67, 261, 67, 265, 98, 249, 167, 260, 98, 250, 167, 314, 152, 31, 266, 167, 51, 209, 262, 98, 249, 167, 242, 98, 250, 167, 314, 152, 74, 238, 209, 130, 152, 31, 267, 167, 51, 209, 263, 98, 249, 167, 242, 98, 250, 167, 314, 152, 74, 238, 209, 130, 152, 31, 268, 167, 259, 197, 57, 98, 173, 26, 112, 265, 31, 246, 149, 176, 209, 268, 98, 258, 197, 57, 98, 173, 26, 210, 266, 197, 173, 98, 57, 26, 210, 178, 209, 244, 4, 267, 197, 173, 98, 57, 26, 152, 98, 314, 152, 31, 262, 149, 223, 210, 224, 31, 263, 149, 223, 210, 224, 31, 70, 31, 10, 209, 264, 98, 246, 74, 238, 209, 264, 74, 82, 74, 104, 152, 98, 245, 167, 209, 314, 98, 315, 152, 152, 31, 48, 209, 152, 31, 252, 167, 189, 209, 214, 67, 209, 239, 210, 223, 67, 234, 152, 210, 224, 98, 209, 222, 98, 224, 152, 98, 209, 223, 210, 224, 98, 315, 152, 98, 209, 235, 210, 225, 67, 236, 210, 226, 98, 230, 210, 227, 152, 98, 209, 226, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 253, 167, 189, 209, 215, 67, 209, 239, 210, 223, 67, 234, 152, 210, 224, 98, 209, 222, 98, 224, 152, 98, 209, 223, 210, 224, 98, 315, 152, 98, 209, 235, 210, 225, 67, 236, 210, 226, 98, 230, 210, 227, 152, 98, 209, 226, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 255, 167, 51, 209, 252, 98, 245, 167, 209, 314, 98, 315, 152, 152, 31, 256, 167, 51, 209, 253, 98, 245, 167, 209, 314, 98, 315, 152, 152, 31, 269, 167, 153, 209, 197, 226, 98, 227, 26, 98, 82, 167, 130, 152, 31, 228, 167, 38, 209, 228, 98, 59, 209, 222, 4, 235, 210, 225, 98, 226, 152, 152, 31, 158, 236, 1, 228, 4, 315, 57, 31, 247, 167, 215, 67, 209, 239, 67, 38, 209, 235, 210, 225, 67, 236, 210, 226, 67, 226, 98, 222, 152, 4, 315, 152, 210, 223, 210, 224, 67, 234, 210, 224, 67, 241, 31, 248, 167, 51, 209, 247, 98, 249, 167, 242, 98, 250, 167, 314, 152, 31, 121, 251, 139, 5, 209, 236, 67, 315, 98, 228, 152, 57, 31, 270, 167, 235, 210, 225, 67, 251, 210, 226, 67, 68, 209, 314, 98, 226, 152, 1, 222, 31, 271, 167, 189, 209, 213, 67, 209, 239, 210, 223, 67, 234, 152, 210, 224, 98, 209, 222, 98, 224, 152, 98, 209, 223, 210, 224, 98, 315, 152, 98, 209, 235, 210, 225, 67, 251, 210, 226, 98, 230, 210, 227, 152, 98, 209, 226, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 272, 167, 189, 209, 216, 67, 209, 239, 210, 223, 67, 234, 152, 210, 224, 98, 209, 222, 98, 224, 152, 98, 209, 223, 210, 224, 98, 315, 152, 98, 209, 235, 210, 225, 67, 251, 210, 226, 98, 230, 210, 227, 152, 98, 209, 226, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 254, 167, 189, 209, 217, 67, 209, 239, 210, 223, 67, 234, 152, 210, 225, 98, 209, 225, 98, 222, 152, 98, 209, 315, 98, 223, 210, 225, 152, 98, 209, 236, 210, 226, 98, 235, 210, 225, 67, 251, 210, 226, 152, 98, 209, 226, 98, 226, 152, 98, 209, 314, 98, 315, 152, 152, 31, 273, 167, 51, 209, 271, 98, 245, 167, 209, 314, 98, 315, 152, 152, 31, 274, 167, 176, 209, 270, 197, 57, 98, 173, 26, 150, 242, 98, 51, 209, 272, 98, 245, 167, 209, 314, 98, 315, 152, 152, 98, 275, 209, 317, 152, 152, 31, 276, 167, 273, 210, 178, 209, 274, 4, 248, 197, 173, 98, 57, 26, 152, 31, 258, 167, 51, 209, 254, 98, 245, 167, 209, 314, 98, 315, 152, 152, 31, 269, 149, 15, 209, 258, 98, 276, 152, 31, 70, 31, 269, 23, 178, 209, 248, 197, 173, 98, 57, 26, 4, 256, 152, 31, 161, 31, 261, 167, 239, 210, 223, 210, 225, 67, 209, 235, 210, 225, 67, 236, 210, 226, 152, 210, 223, 210, 225, 67, 234, 210, 225, 67, 236, 210, 226, 67, 68, 209, 314, 98, 226, 152, 31, 277, 167, 213, 67, 209, 239, 67, 235, 210, 225, 67, 236, 210, 226, 152, 210, 223, 210, 224, 67, 234, 210, 224, 67, 241, 31, 278, 167, 216, 67, 209, 239, 67, 235, 210, 225, 67, 236, 210, 226, 152, 210, 223, 210, 224, 67, 234, 210, 224, 67, 241, 31, 279, 167, 189, 209, 219, 67, 209, 239, 210, 223, 67, 234, 152, 210, 224, 98, 209, 222, 98, 224, 152, 98, 209, 223, 210, 224, 98, 315, 152, 98, 209, 235, 210, 225, 67, 236, 210, 226, 98, 230, 210, 227, 152, 98, 209, 226, 98, 227, 152, 98, 209, 315, 98, 314, 152, 152, 31, 121, 265, 139, 5, 209, 314, 98, 38, 209, 226, 98, 222, 4, 235, 210, 225, 4, 236, 210, 226, 152, 152, 57, 31, 258, 167, 51, 209, 217, 67, 261, 67, 265, 210, 223, 210, 225, 152, 31, 280, 167, 51, 209, 277, 98, 249, 167, 242, 98, 250, 167, 314, 152, 74, 238, 209, 130, 152, 31, 281, 167, 51, 209, 278, 98, 249, 167, 242, 98, 250, 167, 314, 152, 74, 238, 209, 130, 152, 31, 268, 167, 259, 197, 57, 98, 173, 26, 1, 265, 31, 269, 149, 176, 209, 268, 98, 258, 197, 57, 98, 173, 26, 210, 280, 197, 173, 98, 57, 26, 210, 178, 209, 281, 197, 173, 98, 57, 26, 4, 256, 152, 98, 314, 152, 31, 277, 149, 223, 210, 224, 31, 278, 149, 223, 210, 224, 31, 70, 31, 10, 209, 279, 98, 269, 74, 238, 209, 279, 74, 82, 74, 104, 152, 98, 245, 167, 209, 314, 98, 315, 152, 152, 31, 3, 31]}, {"code": "def chunk_rwkv6_bwd_kernel_inter(\n    q,\n    k,\n    v,\n    h,\n    gi,\n    ge,\n    u,\n    do,\n    dh,\n    dA,\n    dq,\n    dk,\n    dq2,\n    dk2,\n    dg,\n    du,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n\n    p_gk = tl.make_block_ptr(\n        ge + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_gi = tl.make_block_ptr(\n        gi + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_gn = gi + (bos + min(T, i_t * BT + BT) - 1) * H * K + i_h * K + o_k\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dgk = tl.zeros(\n        [\n            BK,\n        ],\n        dtype=tl.float32,\n    )\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_do = tl.make_block_ptr(\n            do + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_h = tl.make_block_ptr(\n            h + (i_tg * H + i_h) * K * V,\n            (V, K),\n            (1, V),\n            (i_v * BV, i_k * BK),\n            (BV, BK),\n            (0, 1),\n        )\n        p_dh = tl.make_block_ptr(\n            dh + (i_tg * H + i_h) * K * V,\n            (V, K),\n            (1, V),\n            (i_v * BV, i_k * BK),\n            (BV, BK),\n            (0, 1),\n        )\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        b_dgk += tl.sum(b_h * b_dh, axis=0)\n\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\n    b_dgk *= exp(b_gn)\n    b_dq *= scale\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_gi = tl.load(p_gi, boundary_check=(0, 1))\n    b_dq = b_dq * exp(b_gk)\n    b_dk = b_dk * exp(b_gn[None, :] - b_gi)\n\n    o_i = tl.arange(0, BT)\n    p_q = tl.make_block_ptr(\n        q + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_k = tl.make_block_ptr(\n        k + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_dq = tl.make_block_ptr(\n        dq + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_dk = tl.make_block_ptr(\n        dk + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_dA_dig = dA + ((bos + i_t * BT + o_i) * H + i_h) * BT + o_i\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dgk += tl.sum(b_dk * b_k, axis=0)\n\n    b_dq += tl.load(p_dq, boundary_check=(0, 1))\n    b_dk += tl.load(p_dk, boundary_check=(0, 1))\n    b_dg = b_q * b_dq - b_k * b_dk\n    b_dg = (\n        b_dg\n        - tl.cumsum(b_dg, axis=0)\n        + tl.sum(b_dg, axis=0)[None, :]\n        + b_dgk[None, :]\n        - b_q * b_dq\n    )\n\n    b_dA_dig = tl.load(p_dA_dig, mask=(i_t * BT + o_i) < T, other=0)\n\n    p_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (i_k * BK,), (BK,), (0,))\n    b_u = tl.load(p_u, boundary_check=(0,))\n\n    b_dq += b_dA_dig[:, None] * b_u[None, :] * b_k\n    b_dk += b_dA_dig[:, None] * b_u[None, :] * b_q\n    b_du = tl.sum(b_dA_dig[:, None] * b_q * b_k, axis=0)\n    p_du = tl.make_block_ptr(\n        du + (i_tg * H + i_h) * K, (K,), (1,), (i_k * BK,), (BK,), (0,)\n    )\n    tl.store(p_du, b_du, boundary_check=(0,))\n\n    p_dq = tl.make_block_ptr(\n        dq2 + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_dk = tl.make_block_ptr(\n        dk2 + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_dg = tl.make_block_ptr(\n        dg + (bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 313, 209, 213, 99, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 99, 224, 99, 225, 99, 226, 99, 227, 99, 228, 99, 229, 99, 230, 99, 231, 99, 232, 99, 233, 58, 6, 99, 234, 58, 6, 99, 235, 58, 6, 99, 236, 58, 6, 99, 237, 58, 6, 99, 238, 58, 6, 99, 239, 58, 6, 152, 58, 31, -1, 240, 99, 241, 99, 242, 167, 209, 148, 209, 314, 152, 99, 148, 209, 315, 152, 99, 148, 209, 316, 152, 152, 31, 243, 99, 244, 167, 209, 242, 46, 233, 99, 242, 194, 233, 152, 31, 158, 239, 58, 31, 245, 167, 241, 31, 246, 99, 241, 167, 209, 53, 209, 230, 68, 241, 210, 316, 152, 75, 247, 209, 57, 152, 99, 53, 209, 230, 68, 241, 210, 316, 68, 315, 152, 75, 247, 209, 57, 152, 152, 31, 248, 99, 249, 167, 209, 53, 209, 229, 68, 246, 152, 75, 247, 209, 57, 152, 99, 53, 209, 229, 68, 246, 68, 315, 152, 75, 247, 209, 57, 152, 152, 31, 232, 167, 249, 4, 248, 31, 250, 167, 60, 209, 232, 99, 236, 152, 31, 161, 31, 29, 58, 31, 250, 167, 60, 209, 232, 99, 236, 152, 31, 245, 167, 243, 210, 250, 68, 241, 31, 248, 99, 249, 167, 209, 243, 210, 232, 99, 243, 210, 232, 68, 232, 152, 31, 52, 31, 251, 167, 240, 210, 237, 68, 69, 209, 314, 99, 237, 152, 31, 252, 167, 251, 1, 234, 31, 253, 167, 189, 209, 218, 68, 209, 248, 210, 233, 68, 244, 152, 210, 234, 99, 209, 232, 99, 234, 152, 99, 209, 233, 210, 234, 99, 315, 152, 99, 209, 241, 210, 236, 99, 240, 210, 237, 152, 99, 209, 236, 99, 237, 152, 99, 209, 315, 99, 314, 152, 152, 31, 254, 167, 189, 209, 217, 68, 209, 248, 210, 233, 68, 244, 152, 210, 234, 99, 209, 232, 99, 234, 152, 99, 209, 233, 210, 234, 99, 315, 152, 99, 209, 241, 210, 236, 99, 240, 210, 237, 152, 99, 209, 236, 99, 237, 152, 99, 209, 315, 99, 314, 152, 152, 31, 255, 167, 217, 68, 209, 248, 68, 38, 209, 232, 99, 241, 210, 236, 68, 236, 152, 4, 315, 152, 210, 233, 210, 234, 68, 244, 210, 234, 68, 251, 31, 256, 167, 53, 209, 255, 99, 257, 167, 252, 99, 258, 167, 314, 152, 31, 259, 167, 153, 209, 197, 236, 99, 237, 26, 99, 83, 167, 131, 152, 31, 260, 167, 153, 209, 197, 236, 99, 237, 26, 99, 83, 167, 131, 152, 31, 261, 167, 153, 209, 197, 237, 26, 99, 83, 167, 131, 152, 31, 122, 262, 139, 5, 209, 60, 209, 235, 99, 238, 152, 152, 58, 31, 263, 167, 189, 209, 215, 68, 209, 248, 210, 233, 68, 244, 152, 210, 235, 99, 209, 232, 99, 235, 152, 99, 209, 233, 210, 235, 99, 315, 152, 99, 209, 241, 210, 236, 99, 262, 210, 238, 152, 99, 209, 236, 99, 238, 152, 99, 209, 315, 99, 314, 152, 152, 31, 264, 167, 189, 209, 220, 68, 209, 248, 210, 233, 68, 244, 152, 210, 235, 99, 209, 232, 99, 235, 152, 99, 209, 233, 210, 235, 99, 315, 152, 99, 209, 241, 210, 236, 99, 262, 210, 238, 152, 99, 209, 236, 99, 238, 152, 99, 209, 315, 99, 314, 152, 152, 31, 265, 167, 189, 209, 216, 68, 209, 245, 210, 233, 68, 244, 152, 210, 234, 210, 235, 99, 209, 235, 99, 234, 152, 99, 209, 315, 99, 235, 152, 99, 209, 262, 210, 238, 99, 240, 210, 237, 152, 99, 209, 238, 99, 237, 152, 99, 209, 314, 99, 315, 152, 152, 31, 266, 167, 189, 209, 221, 68, 209, 245, 210, 233, 68, 244, 152, 210, 234, 210, 235, 99, 209, 235, 99, 234, 152, 99, 209, 315, 99, 235, 152, 99, 209, 262, 210, 238, 99, 240, 210, 237, 152, 99, 209, 238, 99, 237, 152, 99, 209, 314, 99, 315, 152, 152, 31, 267, 167, 53, 209, 263, 99, 268, 167, 209, 314, 99, 315, 152, 152, 31, 269, 167, 53, 209, 264, 99, 268, 167, 209, 314, 99, 315, 152, 152, 31, 270, 167, 53, 209, 265, 99, 268, 167, 209, 314, 99, 315, 152, 152, 31, 271, 167, 53, 209, 266, 99, 268, 167, 209, 314, 99, 315, 152, 152, 31, 261, 149, 191, 209, 270, 210, 271, 99, 272, 167, 314, 152, 31, 259, 149, 15, 209, 269, 99, 270, 75, 247, 209, 269, 75, 83, 152, 152, 31, 260, 149, 15, 209, 267, 99, 271, 75, 247, 209, 267, 75, 83, 152, 152, 31, 71, 31, 261, 23, 178, 209, 256, 152, 31, 259, 23, 231, 31, 273, 167, 53, 209, 253, 99, 268, 167, 209, 314, 99, 315, 152, 152, 31, 274, 167, 53, 209, 254, 99, 268, 167, 209, 314, 99, 315, 152, 152, 31, 259, 167, 259, 210, 178, 209, 273, 152, 31, 260, 167, 260, 210, 178, 209, 256, 197, 173, 99, 58, 26, 4, 274, 152, 31, 275, 167, 69, 209, 314, 99, 236, 152, 31, 276, 167, 189, 209, 213, 68, 209, 248, 210, 233, 68, 244, 152, 210, 234, 99, 209, 232, 99, 234, 152, 99, 209, 233, 210, 234, 99, 315, 152, 99, 209, 241, 210, 236, 99, 240, 210, 237, 152, 99, 209, 236, 99, 237, 152, 99, 209, 315, 99, 314, 152, 152, 31, 277, 167, 189, 209, 214, 68, 209, 248, 210, 233, 68, 244, 152, 210, 234, 99, 209, 232, 99, 234, 152, 99, 209, 233, 210, 234, 99, 315, 152, 99, 209, 241, 210, 236, 99, 240, 210, 237, 152, 99, 209, 236, 99, 237, 152, 99, 209, 315, 99, 314, 152, 152, 31, 278, 167, 189, 209, 223, 68, 209, 248, 210, 233, 68, 244, 152, 210, 234, 99, 209, 232, 99, 234, 152, 99, 209, 233, 210, 234, 99, 315, 152, 99, 209, 241, 210, 236, 99, 240, 210, 237, 152, 99, 209, 236, 99, 237, 152, 99, 209, 315, 99, 314, 152, 152, 31, 279, 167, 189, 209, 224, 68, 209, 248, 210, 233, 68, 244, 152, 210, 234, 99, 209, 232, 99, 234, 152, 99, 209, 233, 210, 234, 99, 315, 152, 99, 209, 241, 210, 236, 99, 240, 210, 237, 152, 99, 209, 236, 99, 237, 152, 99, 209, 315, 99, 314, 152, 152, 31, 280, 167, 222, 68, 209, 209, 248, 68, 241, 210, 236, 68, 275, 152, 210, 233, 68, 244, 152, 210, 236, 68, 275, 31, 281, 167, 53, 209, 276, 99, 268, 167, 209, 314, 99, 315, 152, 152, 31, 282, 167, 53, 209, 277, 99, 268, 167, 209, 314, 99, 315, 152, 152, 31, 261, 149, 191, 209, 260, 210, 282, 99, 272, 167, 314, 152, 31, 259, 149, 53, 209, 278, 99, 268, 167, 209, 314, 99, 315, 152, 152, 31, 260, 149, 53, 209, 279, 99, 268, 167, 209, 314, 99, 315, 152, 152, 31, 283, 167, 281, 210, 259, 4, 282, 210, 260, 31, 283, 167, 283, 4, 77, 209, 283, 99, 272, 167, 314, 152, 68, 191, 209, 283, 99, 272, 167, 314, 152, 197, 173, 99, 58, 26, 68, 261, 197, 173, 99, 58, 26, 4, 281, 210, 259, 31, 284, 167, 53, 209, 280, 99, 257, 167, 241, 210, 236, 68, 275, 1, 232, 99, 258, 167, 314, 152, 31, 285, 167, 189, 209, 219, 68, 244, 210, 234, 99, 209, 234, 99, 152, 99, 209, 315, 99, 152, 99, 209, 240, 210, 237, 99, 152, 99, 209, 237, 99, 152, 99, 209, 314, 99, 152, 152, 31, 286, 167, 53, 209, 285, 99, 268, 167, 209, 314, 99, 152, 152, 31, 259, 149, 284, 197, 58, 99, 173, 26, 210, 286, 197, 173, 99, 58, 26, 210, 282, 31, 260, 149, 284, 197, 58, 99, 173, 26, 210, 286, 197, 173, 99, 58, 26, 210, 281, 31, 287, 167, 191, 209, 284, 197, 58, 99, 173, 26, 210, 281, 210, 282, 99, 272, 167, 314, 152, 31, 288, 167, 189, 209, 228, 68, 209, 245, 210, 233, 68, 244, 152, 210, 234, 99, 209, 234, 99, 152, 99, 209, 315, 99, 152, 99, 209, 240, 210, 237, 99, 152, 99, 209, 237, 99, 152, 99, 209, 314, 99, 152, 152, 31, 10, 209, 288, 99, 287, 99, 268, 167, 209, 314, 99, 152, 152, 31, 278, 167, 189, 209, 225, 68, 209, 248, 210, 233, 68, 244, 152, 210, 234, 99, 209, 232, 99, 234, 152, 99, 209, 233, 210, 234, 99, 315, 152, 99, 209, 241, 210, 236, 99, 240, 210, 237, 152, 99, 209, 236, 99, 237, 152, 99, 209, 315, 99, 314, 152, 152, 31, 279, 167, 189, 209, 226, 68, 209, 248, 210, 233, 68, 244, 152, 210, 234, 99, 209, 232, 99, 234, 152, 99, 209, 233, 210, 234, 99, 315, 152, 99, 209, 241, 210, 236, 99, 240, 210, 237, 152, 99, 209, 236, 99, 237, 152, 99, 209, 315, 99, 314, 152, 152, 31, 289, 167, 189, 209, 227, 68, 209, 248, 210, 233, 68, 244, 152, 210, 234, 99, 209, 232, 99, 234, 152, 99, 209, 233, 210, 234, 99, 315, 152, 99, 209, 241, 210, 236, 99, 240, 210, 237, 152, 99, 209, 236, 99, 237, 152, 99, 209, 315, 99, 314, 152, 152, 31, 10, 209, 278, 99, 259, 75, 247, 209, 278, 75, 83, 75, 105, 152, 99, 268, 167, 209, 314, 99, 315, 152, 152, 31, 10, 209, 279, 99, 260, 75, 247, 209, 279, 75, 83, 75, 105, 152, 99, 268, 167, 209, 314, 99, 315, 152, 152, 31, 10, 209, 289, 99, 283, 75, 247, 209, 289, 75, 83, 75, 105, 152, 99, 268, 167, 209, 314, 99, 315, 152, 152, 31, 3, 31]}, {"code": "def fused_recurrent_rwkv6_fwd_kernel(\n    q,\n    k,\n    v,\n    w,\n    u,\n    o,\n    h0,\n    ht,\n    cu_seqlens,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    REVERSE: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_k, i_nh = (\n        tl.program_id(0).to(tl.int64),\n        tl.program_id(1).to(tl.int64),\n        tl.program_id(2).to(tl.int64),\n    )\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        all = T\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        all = B * T\n\n    o_k = i_k * BK + tl.arange(0, BK)\n    o_v = i_v * BV + tl.arange(0, BV)\n    p_q = q + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_k = k + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_v = v + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v\n    p_w = w + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_o = o + ((i_k * all + bos) + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v\n    p_u = u + i_h * K + o_k\n\n    mask_k = o_k < K\n    mask_v = o_v < V\n    mask_h = mask_k[:, None] & mask_v[None, :]\n\n    b_u = tl.load(p_u, mask=mask_k, other=0).to(tl.float32)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)\n        b_kv = b_k[:, None] * b_v[None, :]\n        b_o = tl.sum((b_h + b_kv * b_u[:, None]) * b_q[:, None], 0)\n        b_h = b_h * exp(b_w)[:, None] + b_kv\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n        p_q += (-1 if REVERSE else 1) * H * K\n        p_k += (-1 if REVERSE else 1) * H * K\n        p_v += (-1 if REVERSE else 1) * H * V\n        p_w += (-1 if REVERSE else 1) * H * K\n        p_o += (-1 if REVERSE else 1) * H * V\n\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_nh * K * V + o_k[:, None] * V + o_v[None, :]\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)", "encoded": [28, 313, 209, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 98, 229, 57, 6, 98, 230, 57, 6, 98, 231, 57, 6, 98, 232, 57, 6, 98, 233, 57, 6, 152, 57, 31, -1, 234, 98, 235, 98, 236, 167, 209, 148, 209, 314, 152, 74, 237, 209, 157, 152, 98, 148, 209, 315, 152, 74, 237, 209, 157, 152, 98, 148, 209, 316, 152, 74, 237, 209, 157, 152, 152, 31, 238, 98, 239, 167, 209, 236, 46, 225, 98, 236, 194, 225, 152, 31, 158, 233, 57, 31, 240, 98, 241, 167, 209, 51, 209, 221, 67, 238, 152, 74, 237, 209, 157, 152, 98, 51, 209, 221, 67, 238, 67, 315, 152, 74, 237, 209, 157, 152, 152, 31, 144, 167, 223, 31, 223, 167, 241, 4, 240, 31, 161, 31, 29, 57, 31, 240, 98, 241, 167, 209, 238, 210, 223, 98, 238, 210, 223, 67, 223, 152, 31, 144, 167, 224, 210, 223, 31, 52, 31, 242, 167, 235, 210, 228, 67, 68, 209, 314, 98, 228, 152, 31, 243, 167, 234, 210, 229, 67, 68, 209, 314, 98, 229, 152, 31, 244, 167, 213, 67, 209, 240, 67, 209, 223, 4, 315, 158, 230, 29, 314, 152, 152, 210, 225, 210, 226, 67, 239, 210, 226, 67, 242, 31, 245, 167, 214, 67, 209, 240, 67, 209, 223, 4, 315, 158, 230, 29, 314, 152, 152, 210, 225, 210, 226, 67, 239, 210, 226, 67, 242, 31, 246, 167, 215, 67, 209, 240, 67, 209, 223, 4, 315, 158, 230, 29, 314, 152, 152, 210, 225, 210, 227, 67, 239, 210, 227, 67, 243, 31, 247, 167, 216, 67, 209, 240, 67, 209, 223, 4, 315, 158, 230, 29, 314, 152, 152, 210, 225, 210, 226, 67, 239, 210, 226, 67, 242, 31, 248, 167, 218, 67, 209, 235, 210, 144, 67, 240, 67, 209, 223, 4, 315, 158, 230, 29, 314, 152, 152, 210, 225, 210, 227, 67, 239, 210, 227, 67, 243, 31, 249, 167, 217, 67, 239, 210, 226, 67, 242, 31, 250, 167, 242, 1, 226, 31, 251, 167, 243, 1, 227, 31, 252, 167, 250, 197, 57, 98, 173, 26, 150, 251, 197, 173, 98, 57, 26, 31, 253, 167, 51, 209, 249, 98, 254, 167, 250, 98, 255, 167, 314, 152, 74, 237, 209, 130, 152, 31, 256, 167, 153, 209, 197, 228, 98, 229, 26, 98, 82, 167, 130, 152, 31, 158, 231, 57, 31, 257, 167, 219, 67, 236, 210, 226, 210, 227, 67, 242, 197, 57, 98, 173, 26, 210, 227, 67, 243, 197, 173, 98, 57, 26, 31, 256, 149, 51, 209, 257, 98, 254, 167, 252, 98, 255, 167, 314, 152, 74, 237, 209, 130, 152, 31, 161, 31, 121, 258, 139, 5, 209, 314, 98, 223, 152, 57, 31, 259, 167, 51, 209, 244, 98, 254, 167, 250, 98, 255, 167, 314, 152, 74, 237, 209, 130, 152, 210, 222, 31, 260, 167, 51, 209, 245, 98, 254, 167, 250, 98, 255, 167, 314, 152, 74, 237, 209, 130, 152, 31, 261, 167, 51, 209, 246, 98, 254, 167, 251, 98, 255, 167, 314, 152, 74, 237, 209, 130, 152, 31, 262, 167, 51, 209, 247, 98, 254, 167, 250, 98, 255, 167, 314, 152, 74, 237, 209, 130, 152, 31, 263, 167, 260, 197, 57, 98, 173, 26, 210, 261, 197, 173, 98, 57, 26, 31, 264, 167, 191, 209, 209, 256, 67, 263, 210, 253, 197, 57, 98, 173, 26, 152, 210, 259, 197, 57, 98, 173, 26, 98, 314, 152, 31, 256, 167, 256, 210, 178, 209, 262, 152, 197, 57, 98, 173, 26, 67, 263, 31, 10, 209, 248, 98, 264, 74, 237, 209, 248, 74, 82, 74, 104, 152, 98, 254, 167, 251, 152, 31, 244, 149, 209, 4, 315, 158, 230, 29, 315, 152, 210, 225, 210, 226, 31, 245, 149, 209, 4, 315, 158, 230, 29, 315, 152, 210, 225, 210, 226, 31, 246, 149, 209, 4, 315, 158, 230, 29, 315, 152, 210, 225, 210, 227, 31, 247, 149, 209, 4, 315, 158, 230, 29, 315, 152, 210, 225, 210, 226, 31, 248, 149, 209, 4, 315, 158, 230, 29, 315, 152, 210, 225, 210, 227, 31, 70, 31, 158, 232, 57, 31, 265, 167, 220, 67, 236, 210, 226, 210, 227, 67, 242, 197, 57, 98, 173, 26, 210, 227, 67, 243, 197, 173, 98, 57, 26, 31, 10, 209, 265, 98, 256, 74, 237, 209, 265, 74, 82, 74, 104, 152, 98, 254, 167, 252, 152, 31, 161, 31, 3, 31]}, {"code": "def fused_recurrent_rwkv6_bwd_kernel_dq(\n    k,\n    v,\n    w,\n    u,\n    do,\n    dq,\n    dq1,\n    h0,\n    cu_seqlens,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    REVERSE: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_k, i_nh = (\n        tl.program_id(0).to(tl.int64),\n        tl.program_id(1).to(tl.int64),\n        tl.program_id(2).to(tl.int64),\n    )\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        all = T\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        all = B * T\n\n    o_k = i_k * BK + tl.arange(0, BK)\n    o_v = i_v * BV + tl.arange(0, BV)\n    p_k = k + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_v = v + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v\n    p_w = w + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_do = do + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v\n    p_dq = (\n        dq + ((i_v * all + bos) + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    )\n    p_dq1 = (\n        dq1 + ((i_v * all + bos) + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    )\n    p_u = u + i_h * K + o_k\n\n    mask_k = o_k < K\n    mask_v = o_v < V\n    mask_h = mask_k[:, None] & mask_v[None, :]\n\n    b_u = tl.load(p_u, mask=mask_k, other=0).to(tl.float32)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        b_kv = b_k[:, None] * b_v[None, :]\n\n        b_hq = b_h * b_do[None, :]\n        b_dq = tl.sum(b_hq + b_kv * b_u[:, None] * b_do[None, :], 1) * scale\n        b_dq1 = tl.sum(b_hq, 1)\n        b_h = b_h * exp(b_w)[:, None]\n        b_h += b_kv\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), mask=mask_k)\n        tl.store(p_dq1, b_dq1.to(p_dq1.dtype.element_ty), mask=mask_k)\n\n        p_k += (-1 if REVERSE else 1) * H * K\n        p_v += (-1 if REVERSE else 1) * H * V\n        p_w += (-1 if REVERSE else 1) * H * K\n        p_do += (-1 if REVERSE else 1) * H * V\n        p_dq += (-1 if REVERSE else 1) * H * K\n        p_dq1 += (-1 if REVERSE else 1) * H * K", "encoded": [28, 313, 209, 213, 99, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 99, 224, 58, 6, 99, 225, 58, 6, 99, 226, 58, 6, 99, 227, 58, 6, 99, 228, 58, 6, 99, 229, 58, 6, 99, 230, 58, 6, 99, 231, 58, 6, 99, 232, 58, 6, 152, 58, 31, -1, 233, 99, 234, 99, 235, 167, 209, 148, 209, 314, 152, 75, 236, 209, 157, 152, 99, 148, 209, 315, 152, 75, 236, 209, 157, 152, 99, 148, 209, 316, 152, 75, 236, 209, 157, 152, 152, 31, 237, 99, 238, 167, 209, 235, 46, 225, 99, 235, 194, 225, 152, 31, 158, 232, 58, 31, 239, 99, 240, 167, 209, 53, 209, 221, 68, 237, 152, 75, 236, 209, 157, 152, 99, 53, 209, 221, 68, 237, 68, 315, 152, 75, 236, 209, 157, 152, 152, 31, 144, 167, 223, 31, 223, 167, 240, 4, 239, 31, 161, 31, 29, 58, 31, 239, 99, 240, 167, 209, 237, 210, 223, 99, 237, 210, 223, 68, 223, 152, 31, 144, 167, 224, 210, 223, 31, 52, 31, 241, 167, 234, 210, 228, 68, 69, 209, 314, 99, 228, 152, 31, 242, 167, 233, 210, 229, 68, 69, 209, 314, 99, 229, 152, 31, 243, 167, 213, 68, 209, 239, 68, 209, 223, 4, 315, 158, 230, 29, 314, 152, 152, 210, 225, 210, 226, 68, 238, 210, 226, 68, 241, 31, 244, 167, 214, 68, 209, 239, 68, 209, 223, 4, 315, 158, 230, 29, 314, 152, 152, 210, 225, 210, 227, 68, 238, 210, 227, 68, 242, 31, 245, 167, 215, 68, 209, 239, 68, 209, 223, 4, 315, 158, 230, 29, 314, 152, 152, 210, 225, 210, 226, 68, 238, 210, 226, 68, 241, 31, 246, 167, 217, 68, 209, 239, 68, 209, 223, 4, 315, 158, 230, 29, 314, 152, 152, 210, 225, 210, 227, 68, 238, 210, 227, 68, 242, 31, 247, 167, 218, 68, 209, 233, 210, 144, 68, 239, 68, 209, 223, 4, 315, 158, 230, 29, 314, 152, 152, 210, 225, 210, 226, 68, 238, 210, 226, 68, 241, 31, 248, 167, 219, 68, 209, 233, 210, 144, 68, 239, 68, 209, 223, 4, 315, 158, 230, 29, 314, 152, 152, 210, 225, 210, 226, 68, 238, 210, 226, 68, 241, 31, 249, 167, 216, 68, 238, 210, 226, 68, 241, 31, 250, 167, 241, 1, 226, 31, 251, 167, 242, 1, 227, 31, 252, 167, 250, 197, 58, 99, 173, 26, 150, 251, 197, 173, 99, 58, 26, 31, 253, 167, 53, 209, 249, 99, 254, 167, 250, 99, 255, 167, 314, 152, 75, 236, 209, 131, 152, 31, 256, 167, 153, 209, 197, 228, 99, 229, 26, 99, 83, 167, 131, 152, 31, 158, 231, 58, 31, 257, 167, 220, 68, 235, 210, 226, 210, 227, 68, 241, 197, 58, 99, 173, 26, 210, 227, 68, 242, 197, 173, 99, 58, 26, 31, 256, 149, 53, 209, 257, 99, 254, 167, 252, 99, 255, 167, 314, 152, 75, 236, 209, 131, 152, 31, 161, 31, 122, 258, 139, 5, 209, 314, 99, 223, 152, 58, 31, 259, 167, 53, 209, 243, 99, 254, 167, 250, 99, 255, 167, 314, 152, 75, 236, 209, 131, 152, 31, 260, 167, 53, 209, 244, 99, 254, 167, 251, 99, 255, 167, 314, 152, 75, 236, 209, 131, 152, 31, 261, 167, 53, 209, 245, 99, 254, 167, 250, 99, 255, 167, 314, 152, 75, 236, 209, 131, 152, 31, 262, 167, 53, 209, 246, 99, 254, 167, 251, 99, 255, 167, 314, 152, 75, 236, 209, 131, 152, 31, 263, 167, 259, 197, 58, 99, 173, 26, 210, 260, 197, 173, 99, 58, 26, 31, 264, 167, 256, 210, 262, 197, 173, 99, 58, 26, 31, 265, 167, 191, 209, 264, 68, 263, 210, 253, 197, 58, 99, 173, 26, 210, 262, 197, 173, 99, 58, 26, 99, 315, 152, 210, 222, 31, 266, 167, 191, 209, 264, 99, 315, 152, 31, 256, 167, 256, 210, 178, 209, 261, 152, 197, 58, 99, 173, 26, 31, 256, 149, 263, 31, 10, 209, 247, 99, 265, 75, 236, 209, 247, 75, 83, 75, 105, 152, 99, 254, 167, 250, 152, 31, 10, 209, 248, 99, 266, 75, 236, 209, 248, 75, 83, 75, 105, 152, 99, 254, 167, 250, 152, 31, 243, 149, 209, 4, 315, 158, 230, 29, 315, 152, 210, 225, 210, 226, 31, 244, 149, 209, 4, 315, 158, 230, 29, 315, 152, 210, 225, 210, 227, 31, 245, 149, 209, 4, 315, 158, 230, 29, 315, 152, 210, 225, 210, 226, 31, 246, 149, 209, 4, 315, 158, 230, 29, 315, 152, 210, 225, 210, 227, 31, 247, 149, 209, 4, 315, 158, 230, 29, 315, 152, 210, 225, 210, 226, 31, 248, 149, 209, 4, 315, 158, 230, 29, 315, 152, 210, 225, 210, 226, 31, 71, 31, 3, 31]}, {"code": "def fused_recurrent_rwkv6_bwd_kernel_dkv(\n    q,\n    k,\n    v,\n    w,\n    u,\n    do,\n    dk,\n    dk1,\n    dv,\n    dh0,\n    cu_seqlens,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    REVERSE: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_k, i_nh = (\n        tl.program_id(0).to(tl.int64),\n        tl.program_id(1).to(tl.int64),\n        tl.program_id(2).to(tl.int64),\n    )\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        all = T\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        all = B * T\n\n    o_k = i_k * BK + tl.arange(0, BK)\n    o_v = i_v * BV + tl.arange(0, BV)\n    p_q = q + (bos + ((T - 1) if not REVERSE else 0)) * H * K + i_h * K + o_k\n    p_k = k + (bos + ((T - 1) if not REVERSE else 0)) * H * K + i_h * K + o_k\n    p_v = v + (bos + ((T - 1) if not REVERSE else 0)) * H * V + i_h * V + o_v\n    p_w = w + (bos + ((T - 1) if not REVERSE else 0)) * H * K + i_h * K + o_k\n    p_do = do + (bos + ((T - 1) if not REVERSE else 0)) * H * V + i_h * V + o_v\n    p_dk = (\n        dk\n        + ((i_v * all + bos) + ((T - 1) if not REVERSE else 0)) * H * K\n        + i_h * K\n        + o_k\n    )\n    p_dk1 = (\n        dk1\n        + ((i_v * all + bos) + ((T - 1) if not REVERSE else 0)) * H * K\n        + i_h * K\n        + o_k\n    )\n    p_dv = (\n        dv\n        + ((i_k * all + bos) + ((T - 1) if not REVERSE else 0)) * H * V\n        + i_h * V\n        + o_v\n    )\n    p_u = u + i_h * K + o_k\n\n    mask_k = o_k < K\n    mask_v = o_v < V\n    mask_h = mask_k[:, None] & mask_v[None, :]\n\n    b_u = tl.load(p_u, mask=mask_k, other=0).to(tl.float32)\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T - 1, -1, -1):\n        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n        b_dkv = b_q[:, None] * b_do[None, :]\n        b_dk = tl.sum(b_dh * b_v[None, :], 1)\n        tl.store(p_dk1, b_dk.to(p_dk1.dtype.element_ty), mask=mask_k)\n        b_dk += tl.sum(b_dkv * b_u[:, None] * b_v[None, :], 1)\n        b_dv = tl.sum((b_dh + (b_dkv * b_u[:, None])) * b_k[:, None], 0)\n\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)\n        b_dh *= exp(b_w)[:, None]\n        b_dh += b_dkv\n\n        p_q += (-1 if not REVERSE else 1) * H * K\n        p_k += (-1 if not REVERSE else 1) * H * K\n        p_v += (-1 if not REVERSE else 1) * H * V\n        p_w += (-1 if not REVERSE else 1) * H * K\n        p_do += (-1 if not REVERSE else 1) * H * V\n        p_dk += (-1 if not REVERSE else 1) * H * K\n        p_dk1 += (-1 if not REVERSE else 1) * H * K\n        p_dv += (-1 if not REVERSE else 1) * H * V\n\n    if USE_INITIAL_STATE:\n        p_dh0 = dh0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_h)", "encoded": [28, 313, 209, 213, 98, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 98, 229, 57, 6, 98, 230, 57, 6, 98, 231, 57, 6, 98, 232, 57, 6, 98, 233, 57, 6, 98, 234, 57, 6, 152, 57, 31, -1, 235, 98, 236, 98, 237, 167, 209, 148, 209, 314, 152, 74, 238, 209, 157, 152, 98, 148, 209, 315, 152, 74, 238, 209, 157, 152, 98, 148, 209, 316, 152, 74, 238, 209, 157, 152, 152, 31, 239, 98, 240, 167, 209, 237, 46, 227, 98, 237, 194, 227, 152, 31, 158, 234, 57, 31, 241, 98, 242, 167, 209, 51, 209, 223, 67, 239, 152, 74, 238, 209, 157, 152, 98, 51, 209, 223, 67, 239, 67, 315, 152, 74, 238, 209, 157, 152, 152, 31, 144, 167, 225, 31, 225, 167, 242, 4, 241, 31, 161, 31, 29, 57, 31, 241, 98, 242, 167, 209, 239, 210, 225, 98, 239, 210, 225, 67, 225, 152, 31, 144, 167, 226, 210, 225, 31, 52, 31, 243, 167, 236, 210, 230, 67, 68, 209, 314, 98, 230, 152, 31, 244, 167, 235, 210, 231, 67, 68, 209, 314, 98, 231, 152, 31, 245, 167, 213, 67, 209, 241, 67, 209, 225, 4, 315, 158, 58, 232, 29, 314, 152, 152, 210, 227, 210, 228, 67, 240, 210, 228, 67, 243, 31, 246, 167, 214, 67, 209, 241, 67, 209, 225, 4, 315, 158, 58, 232, 29, 314, 152, 152, 210, 227, 210, 228, 67, 240, 210, 228, 67, 243, 31, 247, 167, 215, 67, 209, 241, 67, 209, 225, 4, 315, 158, 58, 232, 29, 314, 152, 152, 210, 227, 210, 229, 67, 240, 210, 229, 67, 244, 31, 248, 167, 216, 67, 209, 241, 67, 209, 225, 4, 315, 158, 58, 232, 29, 314, 152, 152, 210, 227, 210, 228, 67, 240, 210, 228, 67, 243, 31, 249, 167, 218, 67, 209, 241, 67, 209, 225, 4, 315, 158, 58, 232, 29, 314, 152, 152, 210, 227, 210, 229, 67, 240, 210, 229, 67, 244, 31, 250, 167, 219, 67, 209, 235, 210, 144, 67, 241, 67, 209, 225, 4, 315, 158, 58, 232, 29, 314, 152, 152, 210, 227, 210, 228, 67, 240, 210, 228, 67, 243, 31, 251, 167, 220, 67, 209, 235, 210, 144, 67, 241, 67, 209, 225, 4, 315, 158, 58, 232, 29, 314, 152, 152, 210, 227, 210, 228, 67, 240, 210, 228, 67, 243, 31, 252, 167, 221, 67, 209, 236, 210, 144, 67, 241, 67, 209, 225, 4, 315, 158, 58, 232, 29, 314, 152, 152, 210, 227, 210, 229, 67, 240, 210, 229, 67, 244, 31, 253, 167, 217, 67, 240, 210, 228, 67, 243, 31, 254, 167, 243, 1, 228, 31, 255, 167, 244, 1, 229, 31, 256, 167, 254, 197, 57, 98, 173, 26, 150, 255, 197, 173, 98, 57, 26, 31, 257, 167, 51, 209, 253, 98, 258, 167, 254, 98, 259, 167, 314, 152, 74, 238, 209, 130, 152, 31, 260, 167, 153, 209, 197, 230, 98, 231, 26, 98, 82, 167, 130, 152, 31, 121, 261, 139, 5, 209, 225, 4, 315, 98, 4, 315, 98, 4, 315, 152, 57, 31, 262, 167, 51, 209, 245, 98, 258, 167, 254, 98, 259, 167, 314, 152, 74, 238, 209, 130, 152, 210, 224, 31, 263, 167, 51, 209, 246, 98, 258, 167, 254, 98, 259, 167, 314, 152, 74, 238, 209, 130, 152, 31, 264, 167, 51, 209, 247, 98, 258, 167, 255, 98, 259, 167, 314, 152, 74, 238, 209, 130, 152, 31, 265, 167, 51, 209, 248, 98, 258, 167, 254, 98, 259, 167, 314, 152, 74, 238, 209, 130, 152, 31, 266, 167, 51, 209, 249, 98, 258, 167, 255, 98, 259, 167, 314, 152, 74, 238, 209, 130, 152, 31, 267, 167, 262, 197, 57, 98, 173, 26, 210, 266, 197, 173, 98, 57, 26, 31, 268, 167, 191, 209, 260, 210, 264, 197, 173, 98, 57, 26, 98, 315, 152, 31, 10, 209, 251, 98, 268, 74, 238, 209, 251, 74, 82, 74, 104, 152, 98, 258, 167, 254, 152, 31, 268, 149, 191, 209, 267, 210, 257, 197, 57, 98, 173, 26, 210, 264, 197, 173, 98, 57, 26, 98, 315, 152, 31, 269, 167, 191, 209, 209, 260, 67, 267, 210, 257, 197, 57, 98, 173, 26, 152, 210, 263, 197, 57, 98, 173, 26, 98, 314, 152, 31, 10, 209, 250, 98, 268, 74, 238, 209, 250, 74, 82, 74, 104, 152, 98, 258, 167, 254, 152, 31, 10, 209, 252, 98, 269, 74, 238, 209, 252, 74, 82, 74, 104, 152, 98, 258, 167, 255, 152, 31, 260, 23, 178, 209, 265, 152, 197, 57, 98, 173, 26, 31, 260, 149, 267, 31, 245, 149, 209, 4, 315, 158, 58, 232, 29, 315, 152, 210, 227, 210, 228, 31, 246, 149, 209, 4, 315, 158, 58, 232, 29, 315, 152, 210, 227, 210, 228, 31, 247, 149, 209, 4, 315, 158, 58, 232, 29, 315, 152, 210, 227, 210, 229, 31, 248, 149, 209, 4, 315, 158, 58, 232, 29, 315, 152, 210, 227, 210, 228, 31, 249, 149, 209, 4, 315, 158, 58, 232, 29, 315, 152, 210, 227, 210, 229, 31, 250, 149, 209, 4, 315, 158, 58, 232, 29, 315, 152, 210, 227, 210, 228, 31, 251, 149, 209, 4, 315, 158, 58, 232, 29, 315, 152, 210, 227, 210, 228, 31, 252, 149, 209, 4, 315, 158, 58, 232, 29, 315, 152, 210, 227, 210, 229, 31, 70, 31, 158, 233, 57, 31, 270, 167, 222, 67, 237, 210, 228, 210, 229, 67, 243, 197, 57, 98, 173, 26, 210, 229, 67, 244, 197, 173, 98, 57, 26, 31, 10, 209, 270, 98, 260, 74, 238, 209, 270, 74, 82, 74, 104, 152, 98, 258, 167, 256, 152, 31, 161, 31, 3, 31]}, {"code": "def fused_recurrent_rwkv6_bwd_kernel_dw(\n    q,\n    k,\n    dq,\n    dk,\n    dw,\n    cu_seqlens,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    REVERSE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_nh = tl.program_id(0), tl.program_id(1)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n\n    o_i = tl.arange(0, BT)\n    m_i = (\n        tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n        if not REVERSE\n        else tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\n    )\n\n    b_z = tl.zeros([BK], dtype=tl.float32)\n\n    i_t = 0 if not REVERSE else NT - 1\n    for _ in range(NT):\n        p_q = tl.make_block_ptr(\n            q + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT + 1, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (T - 1, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dq = tl.make_block_ptr(\n            dq + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT + 1, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dk = tl.make_block_ptr(\n            dk + (bos * H + i_h) * K,\n            (T - 1, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_dw = tl.make_block_ptr(\n            dw + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1)).to(tl.float32)\n        b_dq = tl.load(p_dq, boundary_check=(0, 1)).to(tl.float32)\n        b_k = tl.load(p_k, boundary_check=(0, 1)).to(tl.float32)\n        b_dk = tl.load(p_dk, boundary_check=(0, 1)).to(tl.float32)\n        b_dw = (b_q * b_dq * scale) - b_k * b_dk\n        b_c = b_z[None, :] + tl.dot(m_i, b_dw, allow_tf32=False)\n        tl.store(p_dw, b_c.to(p_dw.dtype.element_ty), boundary_check=(0, 1))\n        if i_t >= 0:\n            b_z += tl.sum(b_dw, 0)\n\n        i_t += 1 if not REVERSE else -1", "encoded": [28, 313, 209, 213, 99, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 58, 6, 99, 222, 58, 6, 99, 223, 58, 6, 99, 224, 58, 6, 99, 225, 58, 6, 99, 226, 58, 6, 152, 58, 31, -1, 227, 99, 228, 167, 209, 148, 209, 314, 152, 99, 148, 209, 315, 152, 152, 31, 229, 99, 230, 167, 209, 228, 46, 221, 99, 228, 194, 221, 152, 31, 158, 226, 58, 31, 231, 99, 232, 167, 209, 53, 209, 218, 68, 229, 152, 75, 233, 209, 57, 152, 99, 53, 209, 218, 68, 229, 68, 315, 152, 75, 233, 209, 57, 152, 152, 31, 161, 31, 29, 58, 31, 231, 99, 232, 167, 209, 229, 210, 220, 99, 229, 210, 220, 68, 220, 152, 31, 52, 31, 220, 167, 232, 4, 231, 31, 234, 167, 60, 209, 220, 99, 223, 152, 31, 235, 167, 69, 209, 314, 99, 223, 152, 31, 236, 167, 176, 209, 235, 197, 58, 99, 173, 26, 130, 235, 197, 173, 99, 58, 26, 99, 315, 99, 314, 152, 158, 59, 225, 29, 176, 209, 235, 197, 58, 99, 173, 26, 188, 235, 197, 173, 99, 58, 26, 99, 315, 99, 314, 152, 31, 237, 167, 153, 209, 197, 224, 26, 99, 83, 167, 131, 152, 31, 238, 167, 314, 158, 59, 225, 29, 234, 4, 315, 31, 122, 239, 139, 5, 209, 234, 152, 58, 31, 240, 167, 189, 209, 213, 68, 209, 231, 210, 221, 68, 230, 152, 210, 222, 99, 209, 220, 99, 222, 152, 99, 209, 221, 210, 222, 99, 315, 152, 99, 209, 238, 210, 223, 68, 315, 99, 227, 210, 224, 152, 99, 209, 223, 99, 224, 152, 99, 209, 315, 99, 314, 152, 152, 31, 241, 167, 189, 209, 214, 68, 209, 231, 210, 221, 68, 230, 152, 210, 222, 99, 209, 220, 4, 315, 99, 222, 152, 99, 209, 221, 210, 222, 99, 315, 152, 99, 209, 238, 210, 223, 99, 227, 210, 224, 152, 99, 209, 223, 99, 224, 152, 99, 209, 315, 99, 314, 152, 152, 31, 242, 167, 189, 209, 215, 68, 209, 231, 210, 221, 68, 230, 152, 210, 222, 99, 209, 220, 99, 222, 152, 99, 209, 221, 210, 222, 99, 315, 152, 99, 209, 238, 210, 223, 68, 315, 99, 227, 210, 224, 152, 99, 209, 223, 99, 224, 152, 99, 209, 315, 99, 314, 152, 152, 31, 243, 167, 189, 209, 216, 68, 209, 231, 210, 221, 68, 230, 152, 210, 222, 99, 209, 220, 4, 315, 99, 222, 152, 99, 209, 221, 210, 222, 99, 315, 152, 99, 209, 238, 210, 223, 99, 227, 210, 224, 152, 99, 209, 223, 99, 224, 152, 99, 209, 315, 99, 314, 152, 152, 31, 244, 167, 189, 209, 217, 68, 209, 231, 210, 221, 68, 230, 152, 210, 222, 99, 209, 220, 99, 222, 152, 99, 209, 221, 210, 222, 99, 315, 152, 99, 209, 238, 210, 223, 99, 227, 210, 224, 152, 99, 209, 223, 99, 224, 152, 99, 209, 315, 99, 314, 152, 152, 31, 245, 167, 53, 209, 240, 99, 246, 167, 209, 314, 99, 315, 152, 152, 75, 233, 209, 131, 152, 31, 247, 167, 53, 209, 242, 99, 246, 167, 209, 314, 99, 315, 152, 152, 75, 233, 209, 131, 152, 31, 248, 167, 53, 209, 241, 99, 246, 167, 209, 314, 99, 315, 152, 152, 75, 233, 209, 131, 152, 31, 249, 167, 53, 209, 243, 99, 246, 167, 209, 314, 99, 315, 152, 152, 75, 233, 209, 131, 152, 31, 250, 167, 245, 210, 247, 210, 219, 4, 248, 210, 249, 31, 251, 167, 237, 197, 173, 99, 58, 26, 68, 15, 209, 236, 99, 250, 99, 252, 167, 55, 152, 31, 10, 209, 244, 99, 251, 75, 233, 209, 244, 75, 83, 75, 105, 152, 99, 246, 167, 209, 314, 99, 315, 152, 152, 31, 158, 238, 130, 314, 58, 31, 237, 149, 191, 209, 250, 99, 314, 152, 31, 161, 31, 238, 149, 315, 158, 59, 225, 29, 4, 315, 31, 71, 31, 3, 31]}, {"code": "def rwkv_seq_mix_kernel(\n    x_ptr,\n    x_prev_ptr,\n    mix_k_ptr,\n    output_ptr,\n    batch_size: tl.constexpr,\n    token_length,\n    hidden_dim: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    block_start = tl.program_id(0) * BLOCK_SIZE\n    block_idx = block_start + tl.arange(0, BLOCK_SIZE)[:]\n\n    total_seq_dim = token_length * hidden_dim\n    batch_idx = block_idx // total_seq_dim\n    seq_and_feat = block_idx % total_seq_dim\n    seq_idx = seq_and_feat // hidden_dim\n    feat_idx = seq_and_feat % hidden_dim\n\n    is_valid = (batch_idx < batch_size) & (seq_idx < token_length)\n\n    x_idx = batch_idx * total_seq_dim + seq_idx * hidden_dim + feat_idx\n\n    curr_x = tl.load(x_ptr + x_idx, mask=is_valid, other=0.0).to(tl.float32)\n    k_value = tl.load(mix_k_ptr + feat_idx).to(tl.float32)\n\n    is_first = seq_idx < 1\n    prev_state_idx = batch_idx * hidden_dim + feat_idx\n    prev_state = tl.load(\n        x_prev_ptr + prev_state_idx, mask=(is_first & is_valid), other=0.0\n    ).to(tl.float32)\n\n    prev_x_idx = x_idx - hidden_dim\n    prev_x = tl.load(x_ptr + prev_x_idx, mask=(~is_first & is_valid), other=0.0).to(\n        tl.float32\n    )\n\n    prev_value = tl.where(is_first, prev_state, prev_x)\n    state_diff = prev_value - curr_x\n    mixed = state_diff * k_value\n    result = tl.cast(\n        curr_x + mixed, dtype=output_ptr.dtype.element_ty, fp_downcast_rounding=\"rtne\"\n    )\n    tl.store(output_ptr + x_idx, result, mask=is_valid)", "encoded": [28, 313, 209, 213, 98, 214, 98, 215, 98, 216, 98, 217, 57, 6, 98, 218, 98, 219, 57, 6, 98, 220, 57, 6, 152, 57, 31, -1, 221, 167, 148, 209, 314, 152, 210, 220, 31, 222, 167, 221, 67, 68, 209, 314, 98, 220, 152, 197, 57, 26, 31, 223, 167, 218, 210, 219, 31, 224, 167, 222, 46, 223, 31, 225, 167, 222, 194, 223, 31, 226, 167, 225, 46, 219, 31, 227, 167, 225, 194, 219, 31, 228, 167, 209, 224, 1, 217, 152, 150, 209, 226, 1, 218, 152, 31, 229, 167, 224, 210, 223, 67, 226, 210, 219, 67, 227, 31, 230, 167, 51, 209, 213, 67, 229, 98, 231, 167, 228, 98, 232, 167, 314, 152, 74, 233, 209, 130, 152, 31, 234, 167, 51, 209, 215, 67, 227, 152, 74, 233, 209, 130, 152, 31, 235, 167, 226, 1, 315, 31, 236, 167, 224, 210, 219, 67, 227, 31, 237, 167, 51, 209, 214, 67, 236, 98, 231, 167, 235, 150, 228, 98, 232, 167, 314, 152, 74, 233, 209, 130, 152, 31, 238, 167, 229, 4, 219, 31, 239, 167, 51, 209, 213, 67, 238, 98, 231, 167, 62, 235, 150, 228, 98, 232, 167, 314, 152, 74, 233, 209, 130, 152, 31, 240, 167, 176, 209, 235, 98, 237, 98, 239, 152, 31, 241, 167, 240, 4, 230, 31, 242, 167, 241, 210, 234, 31, 243, 167, 192, 209, 230, 67, 242, 98, 82, 167, 216, 74, 82, 74, 104, 98, 145, 167, 316, 152, 31, 10, 209, 216, 67, 229, 98, 243, 98, 231, 167, 228, 152, 31, 3, 31]}, {"code": "def rwkv_mix_bwd_kenel(\n    dk1_ptr0,\n    xk_ptr,\n    dx_ptr,\n    dx_prev_ptr,\n    batch_size,\n    token_length,\n    hidden_dim: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n\n    batch_idx = offsets // (token_length * hidden_dim)\n    seq_feat = offsets % (token_length * hidden_dim)\n    seq_idx = seq_feat // hidden_dim\n    feat_idx = seq_feat % hidden_dim\n\n    is_valid = offsets < (batch_size * token_length * hidden_dim)\n\n    dk1 = tl.load(dk1_ptr0 + offsets, mask=is_valid)\n    xk = tl.load(xk_ptr + feat_idx, mask=is_valid)\n    prod = dk1 * xk\n\n    mask_next = seq_idx < (token_length - 1)\n    next_offset = offsets + hidden_dim\n    dk1_next = tl.load(dk1_ptr0 + next_offset, mask=mask_next & is_valid, other=0.0)\n    prod_next = dk1_next * xk\n    dx_val = dk1 - prod + tl.where(mask_next, prod_next, 0.0)\n    dx_val = tl.cast(dx_val, dtype=dx_ptr.dtype.element_ty, fp_downcast_rounding=\"rtne\")\n    tl.store(dx_ptr + offsets, dx_val, mask=is_valid)\n\n    dx_prev_offset = batch_idx * hidden_dim + feat_idx\n    is_first_step = seq_idx == 0\n\n    tl.store(\n        dx_prev_ptr + dx_prev_offset,\n        tl.cast(prod, dtype=dx_prev_ptr.dtype.element_ty),\n        mask=is_first_step,\n    )", "encoded": [28, 313, 209, 213, 99, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 58, 6, 99, 220, 58, 6, 152, 58, 31, -1, 221, 167, 148, 209, 314, 152, 31, 222, 167, 221, 210, 220, 68, 69, 209, 314, 99, 220, 152, 31, 223, 167, 222, 46, 209, 218, 210, 219, 152, 31, 224, 167, 222, 194, 209, 218, 210, 219, 152, 31, 225, 167, 224, 46, 219, 31, 226, 167, 224, 194, 219, 31, 227, 167, 222, 1, 217, 210, 218, 210, 219, 31, 228, 167, 53, 209, 213, 68, 222, 99, 229, 167, 227, 152, 31, 230, 167, 53, 209, 214, 68, 226, 99, 229, 167, 227, 152, 31, 231, 167, 228, 210, 230, 31, 232, 167, 225, 1, 218, 4, 315, 31, 233, 167, 222, 68, 219, 31, 234, 167, 53, 209, 213, 68, 233, 99, 229, 167, 232, 150, 227, 99, 235, 167, 314, 152, 31, 236, 167, 234, 210, 230, 31, 237, 167, 228, 4, 231, 68, 176, 209, 232, 99, 236, 99, 314, 152, 31, 237, 167, 192, 209, 237, 99, 83, 167, 215, 75, 83, 75, 105, 99, 145, 167, 316, 152, 31, 10, 209, 215, 68, 222, 99, 237, 99, 229, 167, 227, 152, 31, 238, 167, 223, 210, 219, 68, 226, 31, 239, 167, 225, 70, 314, 31, 10, 209, 216, 68, 238, 99, 192, 209, 231, 99, 83, 167, 216, 75, 83, 75, 105, 152, 99, 229, 167, 239, 152, 31, 3, 31]}, {"code": "def fused_addcmul_fwd_kernel(\n    hidden,\n    delta,\n    ixr,\n    ixw,\n    ixk,\n    ixv,\n    ixa,\n    ixg,\n    oxr,\n    oxw,\n    oxk,\n    oxv,\n    oxa,\n    oxg,\n    use_xg: tl.constexpr,\n    T,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n):\n    i_b, i_t = tl.program_id(0), tl.program_id(1)\n    xoffset = i_b * T * D + i_t * D\n    indices = tl.arange(0, BD)\n    xindex = xoffset + indices\n    xmask = indices < D\n    b_hiddn = tl.load(hidden + xindex, xmask)\n    b_x = tl.load(delta + xindex, xmask)\n    b_ixr = tl.load(ixr + indices, xmask)\n    b_ixw = tl.load(ixw + indices, xmask)\n    b_ixk = tl.load(ixk + indices, xmask)\n    b_ixv = tl.load(ixv + indices, xmask)\n    b_ixa = tl.load(ixa + indices, xmask)\n    b_oxr = tl.fma(b_x, b_ixr, b_hiddn)\n    b_oxw = tl.fma(b_x, b_ixw, b_hiddn)\n    b_oxk = tl.fma(b_x, b_ixk, b_hiddn)\n    b_oxv = tl.fma(b_x, b_ixv, b_hiddn)\n    b_oxa = tl.fma(b_x, b_ixa, b_hiddn)\n\n    tl.store(oxr + xindex, b_oxr.to(oxr.dtype.element_ty), xmask)\n    tl.store(oxw + xindex, b_oxw.to(oxw.dtype.element_ty), xmask)\n    tl.store(oxk + xindex, b_oxk.to(oxk.dtype.element_ty), xmask)\n    tl.store(oxv + xindex, b_oxv.to(oxv.dtype.element_ty), xmask)\n    tl.store(oxa + xindex, b_oxa.to(oxa.dtype.element_ty), xmask)\n\n    if use_xg:\n        b_ixg = tl.load(ixg + indices)\n        b_oxg = tl.fma(b_x, b_ixg, b_hiddn)\n        tl.store(oxg + xindex, b_oxg.to(oxg.dtype.element_ty), xmask)", "encoded": [28, 314, 210, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 98, 227, 98, 228, 57, 6, 98, 229, 98, 230, 57, 6, 98, 231, 57, 6, 152, 57, 31, -1, 232, 98, 233, 167, 210, 148, 210, 315, 152, 98, 148, 210, 316, 152, 152, 31, 234, 167, 232, 211, 229, 211, 230, 67, 233, 211, 230, 31, 235, 167, 68, 210, 315, 98, 231, 152, 31, 236, 167, 234, 67, 235, 31, 237, 167, 235, 1, 230, 31, 238, 167, 51, 210, 214, 67, 236, 98, 237, 152, 31, 239, 167, 51, 210, 215, 67, 236, 98, 237, 152, 31, 240, 167, 51, 210, 216, 67, 235, 98, 237, 152, 31, 241, 167, 51, 210, 217, 67, 235, 98, 237, 152, 31, 242, 167, 51, 210, 218, 67, 235, 98, 237, 152, 31, 243, 167, 51, 210, 219, 67, 235, 98, 237, 152, 31, 244, 167, 51, 210, 220, 67, 235, 98, 237, 152, 31, 245, 167, 197, 210, 239, 98, 240, 98, 238, 152, 31, 246, 167, 197, 210, 239, 98, 241, 98, 238, 152, 31, 247, 167, 197, 210, 239, 98, 242, 98, 238, 152, 31, 248, 167, 197, 210, 239, 98, 243, 98, 238, 152, 31, 249, 167, 197, 210, 239, 98, 244, 98, 238, 152, 31, 10, 210, 222, 67, 236, 98, 245, 74, 250, 210, 222, 74, 82, 74, 104, 152, 98, 237, 152, 31, 10, 210, 223, 67, 236, 98, 246, 74, 250, 210, 223, 74, 82, 74, 104, 152, 98, 237, 152, 31, 10, 210, 224, 67, 236, 98, 247, 74, 250, 210, 224, 74, 82, 74, 104, 152, 98, 237, 152, 31, 10, 210, 225, 67, 236, 98, 248, 74, 250, 210, 225, 74, 82, 74, 104, 152, 98, 237, 152, 31, 10, 210, 226, 67, 236, 98, 249, 74, 250, 210, 226, 74, 82, 74, 104, 152, 98, 237, 152, 31, 158, 228, 57, 31, 251, 167, 51, 210, 221, 67, 235, 152, 31, 252, 167, 197, 210, 239, 98, 251, 98, 238, 152, 31, 10, 210, 227, 67, 236, 98, 252, 74, 250, 210, 227, 74, 82, 74, 104, 152, 98, 237, 152, 31, 161, 31, 3, 31]}, {"code": "def addcmul_bwd_kernel1(\n    ixr,\n    ixw,\n    ixk,\n    ixv,\n    ixa,\n    ixg,\n    dxr,\n    dxw,\n    dxk,\n    dxv,\n    dxa,\n    dxg,\n    ghidden,\n    gx,\n    use_xg: tl.constexpr,\n    T,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    DTYPE: tl.constexpr,\n):\n    i_b, i_t = tl.program_id(0), tl.program_id(1)\n    xoffset = i_b * T * D + i_t * D\n    indices = tl.arange(0, BD)\n    xindex = xoffset + indices\n    xmask = indices < D\n    b_dxr = tl.load(dxr + xindex, xmask).to(DTYPE)\n    b_dxw = tl.load(dxw + xindex, xmask).to(DTYPE)\n    b_dxk = tl.load(dxk + xindex, xmask).to(DTYPE)\n    b_dxv = tl.load(dxv + xindex, xmask).to(DTYPE)\n    b_dxa = tl.load(dxa + xindex, xmask).to(DTYPE)\n    b_ixr = tl.load(ixr + indices, xmask).to(DTYPE)\n    b_ixw = tl.load(ixw + indices, xmask).to(DTYPE)\n    b_iwk = tl.load(ixk + indices, xmask).to(DTYPE)\n    b_ixv = tl.load(ixv + indices, xmask).to(DTYPE)\n    b_ixa = tl.load(ixa + indices, xmask).to(DTYPE)\n\n    if use_xg:\n        b_dxg = tl.load(dxg + xindex, xmask).to(DTYPE)\n        b_ixg = tl.load(ixg + indices, xmask).to(DTYPE)\n        g_hidden = b_dxr + b_dxw + b_dxk + b_dxv + b_dxa + b_dxg\n        g_x = (\n            b_dxr * b_ixr\n            + b_dxw * b_ixw\n            + b_dxk * b_iwk\n            + b_dxv * b_ixv\n            + b_dxa * b_ixa\n            + b_dxg * b_ixg\n        )\n    else:\n        g_hidden = b_dxr + b_dxw + b_dxk + b_dxv + b_dxa\n        g_x = (\n            b_dxr * b_ixr\n            + b_dxw * b_ixw\n            + b_dxk * b_iwk\n            + b_dxv * b_ixv\n            + b_dxa * b_ixa\n        )\n\n    tl.store(ghidden + xindex, g_hidden.to(ghidden.dtype.element_ty), xmask)\n    tl.store(gx + xindex, g_x.to(gx.dtype.element_ty), xmask)", "encoded": [28, 314, 210, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 99, 224, 99, 225, 99, 226, 99, 227, 99, 228, 58, 6, 99, 229, 99, 230, 58, 6, 99, 231, 58, 6, 99, 232, 58, 6, 152, 58, 31, -1, 233, 99, 234, 167, 210, 148, 210, 315, 152, 99, 148, 210, 316, 152, 152, 31, 235, 167, 233, 211, 229, 211, 230, 68, 234, 211, 230, 31, 236, 167, 69, 210, 315, 99, 231, 152, 31, 237, 167, 235, 68, 236, 31, 238, 167, 236, 1, 230, 31, 239, 167, 53, 210, 220, 68, 237, 99, 238, 152, 75, 240, 210, 232, 152, 31, 241, 167, 53, 210, 221, 68, 237, 99, 238, 152, 75, 240, 210, 232, 152, 31, 242, 167, 53, 210, 222, 68, 237, 99, 238, 152, 75, 240, 210, 232, 152, 31, 243, 167, 53, 210, 223, 68, 237, 99, 238, 152, 75, 240, 210, 232, 152, 31, 244, 167, 53, 210, 224, 68, 237, 99, 238, 152, 75, 240, 210, 232, 152, 31, 245, 167, 53, 210, 214, 68, 236, 99, 238, 152, 75, 240, 210, 232, 152, 31, 246, 167, 53, 210, 215, 68, 236, 99, 238, 152, 75, 240, 210, 232, 152, 31, 247, 167, 53, 210, 216, 68, 236, 99, 238, 152, 75, 240, 210, 232, 152, 31, 248, 167, 53, 210, 217, 68, 236, 99, 238, 152, 75, 240, 210, 232, 152, 31, 249, 167, 53, 210, 218, 68, 236, 99, 238, 152, 75, 240, 210, 232, 152, 31, 158, 228, 58, 31, 250, 167, 53, 210, 225, 68, 237, 99, 238, 152, 75, 240, 210, 232, 152, 31, 251, 167, 53, 210, 219, 68, 236, 99, 238, 152, 75, 240, 210, 232, 152, 31, 252, 167, 239, 68, 241, 68, 242, 68, 243, 68, 244, 68, 250, 31, 253, 167, 239, 211, 245, 68, 241, 211, 246, 68, 242, 211, 247, 68, 243, 211, 248, 68, 244, 211, 249, 68, 250, 211, 251, 31, 161, 31, 29, 58, 31, 252, 167, 239, 68, 241, 68, 242, 68, 243, 68, 244, 31, 253, 167, 239, 211, 245, 68, 241, 211, 246, 68, 242, 211, 247, 68, 243, 211, 248, 68, 244, 211, 249, 31, 52, 31, 10, 210, 226, 68, 237, 99, 252, 75, 240, 210, 226, 75, 83, 75, 105, 152, 99, 238, 152, 31, 10, 210, 227, 68, 237, 99, 253, 75, 240, 210, 227, 75, 83, 75, 105, 152, 99, 238, 152, 31, 3, 31]}, {"code": "def k_update_fwd_kernel(\n    k,\n    a,\n    ka,\n    out,\n    xnumel,\n    hidden_dim: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    xoffset = tl.program_id(0) * BLOCK_SIZE\n    xindex = xoffset + tl.arange(0, BLOCK_SIZE)[:]\n\n    xmask = xindex < xnumel\n    x0 = xindex % hidden_dim\n\n    b_k = tl.load(k + xindex, xmask, other=0.0).to(tl.float32)\n    b_a = tl.load(a + xindex, xmask, other=0.0).to(tl.float32)\n    b_ka = tl.load(ka + x0, eviction_policy=\"evict_last\").to(tl.float32)\n\n    output = b_k * (1 + (b_a - 1) * b_ka)\n\n    tl.store(out + xindex, output.to(out.dtype.element_ty), xmask)", "encoded": [28, 314, 210, 214, 98, 215, 98, 216, 98, 14, 98, 217, 98, 218, 57, 6, 98, 219, 57, 6, 152, 57, 31, -1, 220, 167, 148, 210, 315, 152, 211, 219, 31, 221, 167, 220, 67, 68, 210, 315, 98, 219, 152, 198, 57, 26, 31, 222, 167, 221, 1, 217, 31, 223, 167, 221, 194, 218, 31, 224, 167, 51, 210, 214, 67, 221, 98, 222, 98, 225, 167, 315, 152, 74, 226, 210, 130, 152, 31, 227, 167, 51, 210, 215, 67, 221, 98, 222, 98, 225, 167, 315, 152, 74, 226, 210, 130, 152, 31, 228, 167, 51, 210, 216, 67, 223, 98, 229, 167, 316, 152, 74, 226, 210, 130, 152, 31, 230, 167, 224, 211, 210, 317, 67, 210, 227, 4, 317, 152, 211, 228, 152, 31, 10, 210, 14, 67, 221, 98, 230, 74, 226, 210, 14, 74, 82, 74, 104, 152, 98, 222, 152, 31, 3, 31]}, {"code": "def k_update_bwd_kernel(\n    grad_output,\n    k,\n    a,\n    ka,\n    dk,\n    da,\n    dka,\n    xnumel,\n    hidden_dim: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    xoffset = tl.program_id(0) * BLOCK_SIZE\n    xindex = xoffset + tl.arange(0, BLOCK_SIZE)[:]\n\n    xmask = xindex < xnumel\n    x0 = xindex % hidden_dim\n\n    b_grad_output = tl.load(grad_output + xindex, xmask, other=0.0)\n    b_k = tl.load(k + xindex, xmask, other=0.0).to(tl.float32)\n    b_a = tl.load(a + xindex, xmask, other=0.0).to(tl.float32)\n    b_ka = tl.load(ka + x0, eviction_policy=\"evict_last\").to(tl.float32)\n\n    b_dk = b_grad_output * (1 + (b_a - 1) * b_ka)\n    b_da = b_grad_output * b_k * b_ka\n    b_dka = b_grad_output * b_k * (b_a - 1)\n\n    tl.store(dk + xindex, b_dk.to(dk.dtype.element_ty), xmask)\n    tl.store(da + xindex, b_da.to(da.dtype.element_ty), xmask)\n    tl.store(dka + xindex, b_dka.to(dka.dtype.element_ty), xmask)", "encoded": [28, 314, 210, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 58, 6, 99, 223, 58, 6, 152, 58, 31, -1, 224, 167, 148, 210, 315, 152, 211, 223, 31, 225, 167, 224, 68, 69, 210, 315, 99, 223, 152, 198, 58, 26, 31, 226, 167, 225, 1, 221, 31, 227, 167, 225, 194, 222, 31, 228, 167, 53, 210, 214, 68, 225, 99, 226, 99, 229, 167, 315, 152, 31, 230, 167, 53, 210, 215, 68, 225, 99, 226, 99, 229, 167, 315, 152, 75, 231, 210, 131, 152, 31, 232, 167, 53, 210, 216, 68, 225, 99, 226, 99, 229, 167, 315, 152, 75, 231, 210, 131, 152, 31, 233, 167, 53, 210, 217, 68, 227, 99, 234, 167, 316, 152, 75, 231, 210, 131, 152, 31, 235, 167, 228, 211, 210, 317, 68, 210, 232, 4, 317, 152, 211, 233, 152, 31, 236, 167, 228, 211, 230, 211, 233, 31, 237, 167, 228, 211, 230, 211, 210, 232, 4, 317, 152, 31, 10, 210, 218, 68, 225, 99, 235, 75, 231, 210, 218, 75, 83, 75, 105, 152, 99, 226, 152, 31, 10, 210, 219, 68, 225, 99, 236, 75, 231, 210, 219, 75, 83, 75, 105, 152, 99, 226, 152, 31, 10, 210, 220, 68, 225, 99, 237, 75, 231, 210, 220, 75, 83, 75, 105, 152, 99, 226, 152, 31, 3, 31]}, {"code": "def fused_recurrent_rwkv7_fwd_kernel(\n    r,\n    w,\n    k,\n    v,\n    kk,\n    a,\n    o,\n    h0,\n    ht,\n    cu_seqlens,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    REVERSE: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    IS_DECODE: tl.constexpr,\n):\n    i_v, i_nh = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64)\n    i_n, i_h = i_nh // H, i_nh % H\n\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int64)\n        T = eos - bos\n    else:\n        bos, eos = i_n * T, i_n * T + T\n\n    o_k = tl.arange(0, BK)\n    o_v = i_v * BV + tl.arange(0, BV)\n    p_r = r + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_w = w + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_k = k + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_v = v + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v\n    p_a = a + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n    p_kk = kk + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k\n\n    p_o = o + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v\n\n    mask_k = o_k < K\n    mask_v = o_v < V\n    mask_h = mask_k[None, :] & mask_v[:, None]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_nh * K * V + o_k[None, :] * V + o_v[:, None]\n        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\n\n    if IS_DECODE:\n        b_r = tl.load(p_r, mask=mask_k, other=0).to(tl.float32) * scale\n        b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)\n        b_kk = tl.load(p_kk, mask=mask_k, other=0).to(tl.float32)\n        b_act_a = -b_kk\n        b_b = b_kk * b_a\n\n        tmp = tl.sum(b_h * b_act_a[None, :], axis=1)\n        b_h = exp(b_w)[None, :] * b_h + (\n            tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None]\n        )\n        b_o = tl.sum(b_h * b_r[None, :], axis=1)\n\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n    else:\n        for _ in range(0, T):\n            b_r = tl.load(p_r, mask=mask_k, other=0).to(tl.float32) * scale\n            b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)\n            b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n            b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n            b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)\n            b_kk = tl.load(p_kk, mask=mask_k, other=0).to(tl.float32)\n            b_act_a = -b_kk\n            b_b = b_kk * b_a\n\n            tmp = tl.sum(b_h * b_act_a[None, :], axis=1)\n            b_h = exp(b_w)[None, :] * b_h + (\n                tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None]\n            )\n            b_o = tl.sum(b_h * b_r[None, :], axis=1)\n\n            tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n            p_r += (-1 if REVERSE else 1) * H * K\n            p_w += (-1 if REVERSE else 1) * H * K\n            p_k += (-1 if REVERSE else 1) * H * K\n            p_v += (-1 if REVERSE else 1) * H * V\n            p_a += (-1 if REVERSE else 1) * H * K\n            p_kk += (-1 if REVERSE else 1) * H * K\n            p_o += (-1 if REVERSE else 1) * H * V\n\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_nh * K * V + o_k[None, :] * V + o_v[:, None]\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)", "encoded": [28, 314, 210, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 57, 6, 98, 227, 57, 6, 98, 228, 57, 6, 98, 229, 57, 6, 98, 230, 57, 6, 98, 231, 57, 6, 98, 232, 57, 6, 98, 233, 57, 6, 98, 234, 57, 6, 98, 235, 57, 6, 98, 236, 57, 6, 152, 57, 31, -1, 237, 98, 238, 167, 210, 148, 210, 315, 152, 74, 239, 210, 157, 152, 98, 148, 210, 316, 152, 74, 239, 210, 157, 152, 152, 31, 240, 98, 241, 167, 210, 238, 46, 227, 98, 238, 194, 227, 152, 31, 158, 235, 57, 31, 242, 98, 243, 167, 210, 51, 210, 223, 67, 240, 152, 74, 239, 210, 157, 152, 98, 51, 210, 223, 67, 240, 67, 316, 152, 74, 239, 210, 157, 152, 152, 31, 225, 167, 243, 4, 242, 31, 161, 31, 29, 57, 31, 242, 98, 243, 167, 210, 240, 211, 225, 98, 240, 211, 225, 67, 225, 152, 31, 52, 31, 244, 167, 68, 210, 315, 98, 230, 152, 31, 245, 167, 237, 211, 231, 67, 68, 210, 315, 98, 231, 152, 31, 246, 167, 214, 67, 210, 242, 67, 210, 225, 4, 316, 158, 232, 29, 315, 152, 152, 211, 227, 211, 228, 67, 241, 211, 228, 67, 244, 31, 247, 167, 215, 67, 210, 242, 67, 210, 225, 4, 316, 158, 232, 29, 315, 152, 152, 211, 227, 211, 228, 67, 241, 211, 228, 67, 244, 31, 248, 167, 216, 67, 210, 242, 67, 210, 225, 4, 316, 158, 232, 29, 315, 152, 152, 211, 227, 211, 228, 67, 241, 211, 228, 67, 244, 31, 249, 167, 217, 67, 210, 242, 67, 210, 225, 4, 316, 158, 232, 29, 315, 152, 152, 211, 227, 211, 229, 67, 241, 211, 229, 67, 245, 31, 250, 167, 219, 67, 210, 242, 67, 210, 225, 4, 316, 158, 232, 29, 315, 152, 152, 211, 227, 211, 228, 67, 241, 211, 228, 67, 244, 31, 251, 167, 218, 67, 210, 242, 67, 210, 225, 4, 316, 158, 232, 29, 315, 152, 152, 211, 227, 211, 228, 67, 241, 211, 228, 67, 244, 31, 252, 167, 220, 67, 210, 242, 67, 210, 225, 4, 316, 158, 232, 29, 315, 152, 152, 211, 227, 211, 229, 67, 241, 211, 229, 67, 245, 31, 253, 167, 244, 1, 228, 31, 254, 167, 245, 1, 229, 31, 255, 167, 253, 198, 173, 98, 57, 26, 150, 254, 198, 57, 98, 173, 26, 31, 256, 167, 153, 210, 198, 231, 98, 230, 26, 98, 82, 167, 130, 152, 31, 158, 233, 57, 31, 257, 167, 221, 67, 238, 211, 228, 211, 229, 67, 244, 198, 173, 98, 57, 26, 211, 229, 67, 245, 198, 57, 98, 173, 26, 31, 256, 149, 51, 210, 257, 98, 258, 167, 255, 98, 259, 167, 315, 152, 74, 239, 210, 130, 152, 31, 161, 31, 158, 236, 57, 31, 260, 167, 51, 210, 246, 98, 258, 167, 253, 98, 259, 167, 315, 152, 74, 239, 210, 130, 152, 211, 224, 31, 261, 167, 51, 210, 247, 98, 258, 167, 253, 98, 259, 167, 315, 152, 74, 239, 210, 130, 152, 31, 262, 167, 51, 210, 248, 98, 258, 167, 253, 98, 259, 167, 315, 152, 74, 239, 210, 130, 152, 31, 263, 167, 51, 210, 249, 98, 258, 167, 254, 98, 259, 167, 315, 152, 74, 239, 210, 130, 152, 31, 264, 167, 51, 210, 250, 98, 258, 167, 253, 98, 259, 167, 315, 152, 74, 239, 210, 130, 152, 31, 265, 167, 51, 210, 251, 98, 258, 167, 253, 98, 259, 167, 315, 152, 74, 239, 210, 130, 152, 31, 266, 167, 4, 265, 31, 267, 167, 265, 211, 264, 31, 268, 167, 191, 210, 256, 211, 266, 198, 173, 98, 57, 26, 98, 269, 167, 316, 152, 31, 256, 167, 178, 210, 261, 152, 198, 173, 98, 57, 26, 211, 256, 67, 210, 268, 198, 57, 98, 173, 26, 211, 267, 198, 173, 98, 57, 26, 67, 262, 198, 173, 98, 57, 26, 211, 263, 198, 57, 98, 173, 26, 152, 31, 270, 167, 191, 210, 256, 211, 260, 198, 173, 98, 57, 26, 98, 269, 167, 316, 152, 31, 10, 210, 252, 98, 270, 74, 239, 210, 252, 74, 82, 74, 104, 152, 98, 258, 167, 254, 152, 31, 161, 31, 29, 57, 31, 121, 271, 139, 5, 210, 315, 98, 225, 152, 57, 31, 260, 167, 51, 210, 246, 98, 258, 167, 253, 98, 259, 167, 315, 152, 74, 239, 210, 130, 152, 211, 224, 31, 261, 167, 51, 210, 247, 98, 258, 167, 253, 98, 259, 167, 315, 152, 74, 239, 210, 130, 152, 31, 262, 167, 51, 210, 248, 98, 258, 167, 253, 98, 259, 167, 315, 152, 74, 239, 210, 130, 152, 31, 263, 167, 51, 210, 249, 98, 258, 167, 254, 98, 259, 167, 315, 152, 74, 239, 210, 130, 152, 31, 264, 167, 51, 210, 250, 98, 258, 167, 253, 98, 259, 167, 315, 152, 74, 239, 210, 130, 152, 31, 265, 167, 51, 210, 251, 98, 258, 167, 253, 98, 259, 167, 315, 152, 74, 239, 210, 130, 152, 31, 266, 167, 4, 265, 31, 267, 167, 265, 211, 264, 31, 268, 167, 191, 210, 256, 211, 266, 198, 173, 98, 57, 26, 98, 269, 167, 316, 152, 31, 256, 167, 178, 210, 261, 152, 198, 173, 98, 57, 26, 211, 256, 67, 210, 268, 198, 57, 98, 173, 26, 211, 267, 198, 173, 98, 57, 26, 67, 262, 198, 173, 98, 57, 26, 211, 263, 198, 57, 98, 173, 26, 152, 31, 270, 167, 191, 210, 256, 211, 260, 198, 173, 98, 57, 26, 98, 269, 167, 316, 152, 31, 10, 210, 252, 98, 270, 74, 239, 210, 252, 74, 82, 74, 104, 152, 98, 258, 167, 254, 152, 31, 246, 149, 210, 4, 316, 158, 232, 29, 316, 152, 211, 227, 211, 228, 31, 247, 149, 210, 4, 316, 158, 232, 29, 316, 152, 211, 227, 211, 228, 31, 248, 149, 210, 4, 316, 158, 232, 29, 316, 152, 211, 227, 211, 228, 31, 249, 149, 210, 4, 316, 158, 232, 29, 316, 152, 211, 227, 211, 229, 31, 250, 149, 210, 4, 316, 158, 232, 29, 316, 152, 211, 227, 211, 228, 31, 251, 149, 210, 4, 316, 158, 232, 29, 316, 152, 211, 227, 211, 228, 31, 252, 149, 210, 4, 316, 158, 232, 29, 316, 152, 211, 227, 211, 229, 31, 70, 31, 52, 31, 158, 234, 57, 31, 272, 167, 222, 67, 238, 211, 228, 211, 229, 67, 244, 198, 173, 98, 57, 26, 211, 229, 67, 245, 198, 57, 98, 173, 26, 31, 10, 210, 272, 98, 256, 74, 239, 210, 272, 74, 82, 74, 104, 152, 98, 258, 167, 255, 152, 31, 161, 31, 3, 31]}, {"code": "def parallel_simple_gla_fwd_kernel(\n    q,\n    k,\n    v,\n    g,\n    o,\n    attn,\n    scale,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NV: tl.constexpr,\n    OUTPUT_ATTENTIONS: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    USE_G: tl.constexpr,\n):\n    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_k, i_v = i_kv // NV, i_kv % NV\n    i_b, i_h = i_bh // H, i_bh % H\n    o += i_k * B * T * H * V\n\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    q += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V\n    o += (bos * H + i_h) * V\n    if USE_G:\n        g += bos * H + i_h\n    if OUTPUT_ATTENTIONS:\n        attn += (bos * H + i_h * T) * T + i_k * B * H * T * T\n    stride_qk = H * K\n    stride_vo = H * V\n    stride_g = H\n\n    p_q = tl.make_block_ptr(\n        q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n\n    o_q = i_t * BT + tl.arange(0, BT)\n\n    o_k = i_t * BT + tl.arange(0, BS)\n\n    if USE_G:\n        p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))\n\n        b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)\n\n    else:\n        b_gq = None\n\n    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n        p_k = tl.make_block_ptr(\n            k, (K, T), (1, stride_qk), (i_k * BK, i_s), (BK, BS), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0)\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k)\n        if USE_G:\n            p_gk = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))\n            b_gk = tl.load(p_gk, boundary_check=(0,))\n            b_s *= safe_exp(b_gq[:, None] - b_gk[None, :])\n            b_s = tl.where(m_s, b_s, 0)\n        else:\n            b_s = tl.where(m_s, b_s, 0)\n\n        if i_s >= 0:\n            b_o += tl.dot(b_s.to(b_q.dtype), b_v)\n        if OUTPUT_ATTENTIONS:\n            p_a = tl.make_block_ptr(\n                attn, (T, T), (T, 1), (i_t * BT, i_s), (BT, BS), (1, 0)\n            )\n            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n        o_k += BS\n\n    for i_s in range(i_t * BT - BS, -BS, -BS):\n        p_k = tl.make_block_ptr(\n            k, (K, T), (1, stride_qk), (i_k * BK, i_s), (BK, BS), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0)\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k)\n        if USE_G:\n            p_g = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))\n            b_g = tl.load(p_g, boundary_check=(0,))\n            b_gn = tl.load(g + (min(i_s + BS, T) - 1) * stride_g)\n            b_gp = tl.load(g + (i_s - 1) * stride_g) if i_s % BT > 0 else 0.0\n\n            b_s *= safe_exp(b_gq[:, None] + (b_gn - b_g)[None, :])\n            b_gq += b_gn - b_gp\n        if OUTPUT_ATTENTIONS:\n            p_a = tl.make_block_ptr(\n                attn, (T, T), (T, 1), (i_t * BT, i_s), (BT, BS), (1, 0)\n            )\n            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n        if i_s >= 0:\n            b_o += tl.dot(b_s.to(b_v.dtype), b_v)\n    p_o = tl.make_block_ptr(\n        o, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 314, 210, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 99, 224, 58, 6, 99, 225, 58, 6, 99, 226, 58, 6, 99, 227, 58, 6, 99, 228, 58, 6, 99, 229, 58, 6, 99, 230, 58, 6, 99, 231, 58, 6, 99, 232, 58, 6, 99, 233, 58, 6, 99, 234, 58, 6, 99, 235, 58, 6, 152, 58, 31, -1, 236, 99, 237, 99, 238, 167, 210, 148, 210, 315, 152, 99, 148, 210, 316, 152, 99, 148, 210, 317, 152, 152, 31, 239, 99, 240, 167, 210, 236, 46, 232, 99, 236, 194, 232, 152, 31, 241, 99, 242, 167, 210, 238, 46, 225, 99, 238, 194, 225, 152, 31, 218, 149, 239, 211, 224, 211, 223, 211, 225, 211, 227, 31, 158, 234, 58, 31, 243, 99, 237, 167, 210, 53, 210, 222, 68, 237, 211, 317, 152, 75, 244, 210, 57, 152, 99, 53, 210, 222, 68, 237, 211, 317, 68, 316, 152, 75, 244, 210, 57, 152, 152, 31, 245, 99, 246, 167, 210, 53, 210, 221, 68, 243, 152, 75, 244, 210, 57, 152, 99, 53, 210, 221, 68, 243, 68, 316, 152, 75, 244, 210, 57, 152, 152, 31, 223, 167, 246, 4, 245, 31, 161, 31, 29, 58, 31, 245, 99, 246, 167, 210, 241, 211, 223, 99, 241, 211, 223, 68, 223, 152, 31, 52, 31, 214, 149, 210, 245, 211, 225, 68, 242, 152, 211, 226, 31, 215, 149, 210, 245, 211, 225, 68, 242, 152, 211, 226, 31, 216, 149, 210, 245, 211, 225, 68, 242, 152, 211, 227, 31, 218, 149, 210, 245, 211, 225, 68, 242, 152, 211, 227, 31, 158, 235, 58, 31, 217, 149, 245, 211, 225, 68, 242, 31, 161, 31, 158, 233, 58, 31, 219, 149, 210, 245, 211, 225, 68, 242, 211, 223, 152, 211, 223, 68, 239, 211, 224, 211, 225, 211, 223, 211, 223, 31, 161, 31, 247, 167, 225, 211, 226, 31, 248, 167, 225, 211, 227, 31, 249, 167, 225, 31, 250, 167, 189, 210, 214, 99, 210, 223, 99, 226, 152, 99, 210, 247, 99, 316, 152, 99, 210, 237, 211, 228, 99, 239, 211, 230, 152, 99, 210, 228, 99, 230, 152, 99, 210, 316, 99, 315, 152, 152, 31, 251, 167, 53, 210, 250, 99, 252, 167, 210, 315, 99, 316, 152, 152, 31, 251, 167, 210, 251, 211, 220, 152, 75, 244, 210, 251, 75, 83, 152, 31, 253, 167, 153, 210, 198, 228, 99, 231, 26, 99, 83, 167, 131, 152, 31, 254, 167, 237, 211, 228, 68, 69, 210, 315, 99, 228, 152, 31, 255, 167, 237, 211, 228, 68, 69, 210, 315, 99, 229, 152, 31, 158, 235, 58, 31, 256, 167, 189, 210, 217, 99, 210, 223, 99, 152, 99, 210, 249, 99, 152, 99, 210, 237, 211, 228, 99, 152, 99, 210, 228, 99, 152, 99, 210, 315, 99, 152, 152, 31, 257, 167, 53, 210, 256, 99, 252, 167, 210, 315, 99, 152, 152, 75, 244, 210, 131, 152, 31, 161, 31, 29, 58, 31, 257, 167, 173, 31, 52, 31, 122, 258, 139, 5, 210, 237, 211, 228, 99, 38, 210, 210, 237, 68, 316, 152, 211, 228, 99, 223, 152, 99, 229, 152, 58, 31, 259, 167, 189, 210, 215, 99, 210, 226, 99, 223, 152, 99, 210, 316, 99, 247, 152, 99, 210, 239, 211, 230, 99, 258, 152, 99, 210, 230, 99, 229, 152, 99, 210, 315, 99, 316, 152, 152, 31, 260, 167, 189, 210, 216, 99, 210, 223, 99, 227, 152, 99, 210, 248, 99, 316, 152, 99, 210, 258, 99, 240, 211, 231, 152, 99, 210, 229, 99, 231, 152, 99, 210, 316, 99, 315, 152, 152, 31, 261, 167, 53, 210, 259, 99, 252, 167, 210, 315, 99, 316, 152, 152, 31, 262, 167, 53, 210, 260, 99, 252, 167, 210, 315, 99, 316, 152, 152, 31, 263, 167, 254, 198, 58, 99, 173, 26, 130, 255, 198, 173, 99, 58, 26, 31, 264, 167, 15, 210, 251, 99, 261, 152, 31, 158, 235, 58, 31, 265, 167, 189, 210, 217, 99, 210, 223, 99, 152, 99, 210, 249, 99, 152, 99, 210, 258, 99, 152, 99, 210, 229, 99, 152, 99, 210, 315, 99, 152, 152, 31, 266, 167, 53, 210, 265, 99, 252, 167, 210, 315, 99, 152, 152, 31, 264, 23, 267, 210, 257, 198, 58, 99, 173, 26, 4, 266, 198, 173, 99, 58, 26, 152, 31, 264, 167, 176, 210, 263, 99, 264, 99, 315, 152, 31, 161, 31, 29, 58, 31, 264, 167, 176, 210, 263, 99, 264, 99, 315, 152, 31, 52, 31, 158, 258, 130, 315, 58, 31, 253, 149, 15, 210, 264, 75, 244, 210, 251, 75, 83, 152, 99, 262, 152, 31, 161, 31, 158, 233, 58, 31, 268, 167, 189, 210, 219, 99, 210, 223, 99, 223, 152, 99, 210, 223, 99, 316, 152, 99, 210, 237, 211, 228, 99, 258, 152, 99, 210, 228, 99, 229, 152, 99, 210, 316, 99, 315, 152, 152, 31, 10, 210, 268, 99, 264, 75, 244, 210, 268, 75, 83, 75, 105, 152, 99, 252, 167, 210, 315, 99, 316, 152, 152, 31, 161, 31, 255, 149, 229, 31, 71, 31, 122, 258, 139, 5, 210, 237, 211, 228, 4, 229, 99, 4, 229, 99, 4, 229, 152, 58, 31, 259, 167, 189, 210, 215, 99, 210, 226, 99, 223, 152, 99, 210, 316, 99, 247, 152, 99, 210, 239, 211, 230, 99, 258, 152, 99, 210, 230, 99, 229, 152, 99, 210, 315, 99, 316, 152, 152, 31, 260, 167, 189, 210, 216, 99, 210, 223, 99, 227, 152, 99, 210, 248, 99, 316, 152, 99, 210, 258, 99, 240, 211, 231, 152, 99, 210, 229, 99, 231, 152, 99, 210, 316, 99, 315, 152, 152, 31, 261, 167, 53, 210, 259, 99, 252, 167, 210, 315, 99, 316, 152, 152, 31, 262, 167, 53, 210, 260, 99, 252, 167, 210, 315, 99, 316, 152, 152, 31, 264, 167, 15, 210, 251, 99, 261, 152, 31, 158, 235, 58, 31, 269, 167, 189, 210, 217, 99, 210, 223, 99, 152, 99, 210, 249, 99, 152, 99, 210, 258, 99, 152, 99, 210, 229, 99, 152, 99, 210, 315, 99, 152, 152, 31, 270, 167, 53, 210, 269, 99, 252, 167, 210, 315, 99, 152, 152, 31, 271, 167, 53, 210, 217, 68, 210, 38, 210, 258, 68, 229, 99, 223, 152, 4, 316, 152, 211, 249, 152, 31, 272, 167, 53, 210, 217, 68, 210, 258, 4, 316, 152, 211, 249, 152, 158, 258, 194, 228, 113, 315, 29, 315, 31, 264, 23, 267, 210, 257, 198, 58, 99, 173, 26, 68, 210, 271, 4, 270, 152, 198, 173, 99, 58, 26, 152, 31, 257, 149, 271, 4, 272, 31, 161, 31, 158, 233, 58, 31, 268, 167, 189, 210, 219, 99, 210, 223, 99, 223, 152, 99, 210, 223, 99, 316, 152, 99, 210, 237, 211, 228, 99, 258, 152, 99, 210, 228, 99, 229, 152, 99, 210, 316, 99, 315, 152, 152, 31, 10, 210, 268, 99, 264, 75, 244, 210, 268, 75, 83, 75, 105, 152, 99, 252, 167, 210, 315, 99, 316, 152, 152, 31, 161, 31, 158, 258, 130, 315, 58, 31, 253, 149, 15, 210, 264, 75, 244, 210, 262, 75, 83, 152, 99, 262, 152, 31, 161, 31, 71, 31, 273, 167, 189, 210, 218, 99, 210, 223, 99, 227, 152, 99, 210, 248, 99, 316, 152, 99, 210, 237, 211, 228, 99, 240, 211, 231, 152, 99, 210, 228, 99, 231, 152, 99, 210, 316, 99, 315, 152, 152, 31, 10, 210, 273, 99, 253, 75, 244, 210, 273, 75, 83, 75, 105, 152, 99, 252, 167, 210, 315, 99, 316, 152, 152, 31, 3, 31]}, {"code": "def parallel_simple_gla_bwd_kernel_dq(\n    i_t,\n    i_k,\n    i_v,\n    q,\n    k,\n    v,\n    g,\n    do,\n    dq,\n    dg,\n    stride_qk,\n    stride_vo,\n    stride_g,\n    scale,\n    T,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n):\n    p_do = tl.make_block_ptr(\n        do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n\n    for i_s in range(0, i_t * BT, BS):\n        p_k = tl.make_block_ptr(\n            k, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0)\n        )\n        p_v = tl.make_block_ptr(\n            v, (V, T), (1, stride_vo), (i_v * BV, i_s), (BV, BS), (0, 1)\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_ds = tl.dot(b_do, b_v)\n        if USE_G:\n            p_g = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))\n            b_g = tl.load(p_g, boundary_check=(0,))\n            b_gn = tl.load(g + (min(i_s + BS, T) - 1) * stride_g)\n            b_gp = tl.load(g + (i_s - 1) * stride_g) if i_s % BT > 0 else 0.0\n            b_ds *= safe_exp(b_gn - b_g)[None, :]\n            if i_s > 0:\n                b_dq *= safe_exp(b_gn - b_gp)\n\n        b_dq += tl.dot(b_ds.to(b_v.dtype), b_k)\n\n    if USE_G:\n        p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))\n\n        b_gq = tl.load(p_gq, boundary_check=(0,))\n\n        b_dq *= safe_exp(b_gq)[:, None]\n\n    o_q = i_t * BT + tl.arange(0, BT)\n\n    o_k = i_t * BT + tl.arange(0, BS)\n\n    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n        p_k = tl.make_block_ptr(\n            k, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0)\n        )\n        p_v = tl.make_block_ptr(\n            v, (V, T), (1, stride_vo), (i_v * BV, i_s), (BV, BS), (0, 1)\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_ds = tl.dot(b_do, b_v)\n        if USE_G:\n            p_gk = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))\n            b_gk = tl.load(p_gk, boundary_check=(0,))\n            b_ds *= safe_exp(b_gq[:, None] - b_gk[None, :])\n        b_ds = tl.where(o_q[:, None] >= o_k[None, :], b_ds, 0)\n\n        b_dq += tl.dot(b_ds.to(b_k.dtype), b_k)\n        o_k += BS\n\n    b_dq *= scale\n    p_dq = tl.make_block_ptr(\n        dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    if USE_G:\n        p_q = tl.make_block_ptr(\n            q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_dg = tl.sum(b_dq * b_q, 1)\n        p_dg = tl.make_block_ptr(dg, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 314, 210, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 98, 227, 98, 228, 98, 229, 57, 6, 98, 230, 57, 6, 98, 231, 57, 6, 98, 232, 57, 6, 98, 233, 57, 6, 98, 234, 57, 6, 98, 235, 57, 6, 152, 57, 31, -1, 236, 167, 189, 210, 221, 98, 210, 228, 98, 230, 152, 98, 210, 225, 98, 315, 152, 98, 210, 214, 211, 231, 98, 216, 211, 234, 152, 98, 210, 231, 98, 234, 152, 98, 210, 315, 98, 316, 152, 152, 31, 237, 167, 51, 210, 236, 98, 238, 167, 210, 316, 98, 315, 152, 152, 31, 239, 167, 153, 210, 198, 231, 98, 233, 26, 98, 82, 167, 130, 152, 31, 121, 240, 139, 5, 210, 316, 98, 214, 211, 231, 98, 232, 152, 57, 31, 241, 167, 189, 210, 218, 98, 210, 228, 98, 229, 152, 98, 210, 224, 98, 315, 152, 98, 210, 240, 98, 215, 211, 233, 152, 98, 210, 232, 98, 233, 152, 98, 210, 315, 98, 316, 152, 152, 31, 242, 167, 189, 210, 219, 98, 210, 230, 98, 228, 152, 98, 210, 315, 98, 225, 152, 98, 210, 216, 211, 234, 98, 240, 152, 98, 210, 234, 98, 232, 152, 98, 210, 316, 98, 315, 152, 152, 31, 243, 167, 51, 210, 241, 98, 238, 167, 210, 316, 98, 315, 152, 152, 31, 244, 167, 51, 210, 242, 98, 238, 167, 210, 316, 98, 315, 152, 152, 31, 245, 167, 15, 210, 237, 98, 244, 152, 31, 158, 235, 57, 31, 246, 167, 189, 210, 220, 98, 210, 228, 98, 152, 98, 210, 226, 98, 152, 98, 210, 240, 98, 152, 98, 210, 232, 98, 152, 98, 210, 316, 98, 152, 152, 31, 247, 167, 51, 210, 246, 98, 238, 167, 210, 316, 98, 152, 152, 31, 248, 167, 51, 210, 220, 67, 210, 38, 210, 240, 67, 232, 98, 228, 152, 4, 315, 152, 211, 226, 152, 31, 249, 167, 51, 210, 220, 67, 210, 240, 4, 315, 152, 211, 226, 152, 158, 240, 194, 231, 112, 316, 29, 316, 31, 245, 23, 250, 210, 248, 4, 247, 152, 198, 173, 98, 57, 26, 31, 158, 240, 112, 316, 57, 31, 239, 23, 250, 210, 248, 4, 249, 152, 31, 161, 31, 161, 31, 239, 149, 15, 210, 245, 74, 251, 210, 244, 74, 82, 152, 98, 243, 152, 31, 70, 31, 158, 235, 57, 31, 252, 167, 189, 210, 220, 98, 210, 228, 98, 152, 98, 210, 226, 98, 152, 98, 210, 214, 211, 231, 98, 152, 98, 210, 231, 98, 152, 98, 210, 316, 98, 152, 152, 31, 253, 167, 51, 210, 252, 98, 238, 167, 210, 316, 98, 152, 152, 31, 239, 23, 250, 210, 253, 152, 198, 57, 98, 173, 26, 31, 161, 31, 254, 167, 214, 211, 231, 67, 68, 210, 316, 98, 231, 152, 31, 255, 167, 214, 211, 231, 67, 68, 210, 316, 98, 232, 152, 31, 121, 240, 139, 5, 210, 214, 211, 231, 98, 38, 210, 210, 214, 67, 315, 152, 211, 231, 98, 228, 152, 98, 232, 152, 57, 31, 241, 167, 189, 210, 218, 98, 210, 228, 98, 229, 152, 98, 210, 224, 98, 315, 152, 98, 210, 240, 98, 215, 211, 233, 152, 98, 210, 232, 98, 233, 152, 98, 210, 315, 98, 316, 152, 152, 31, 242, 167, 189, 210, 219, 98, 210, 230, 98, 228, 152, 98, 210, 315, 98, 225, 152, 98, 210, 216, 211, 234, 98, 240, 152, 98, 210, 234, 98, 232, 152, 98, 210, 316, 98, 315, 152, 152, 31, 243, 167, 51, 210, 241, 98, 238, 167, 210, 316, 98, 315, 152, 152, 31, 244, 167, 51, 210, 242, 98, 238, 167, 210, 316, 98, 315, 152, 152, 31, 245, 167, 15, 210, 237, 98, 244, 152, 31, 158, 235, 57, 31, 256, 167, 189, 210, 220, 98, 210, 228, 98, 152, 98, 210, 226, 98, 152, 98, 210, 240, 98, 152, 98, 210, 232, 98, 152, 98, 210, 316, 98, 152, 152, 31, 257, 167, 51, 210, 256, 98, 238, 167, 210, 316, 98, 152, 152, 31, 245, 23, 250, 210, 253, 198, 57, 98, 173, 26, 4, 257, 198, 173, 98, 57, 26, 152, 31, 161, 31, 245, 167, 176, 210, 254, 198, 57, 98, 173, 26, 129, 255, 198, 173, 98, 57, 26, 98, 245, 98, 316, 152, 31, 239, 149, 15, 210, 245, 74, 251, 210, 243, 74, 82, 152, 98, 243, 152, 31, 255, 149, 232, 31, 70, 31, 239, 23, 227, 31, 258, 167, 189, 210, 222, 98, 210, 228, 98, 229, 152, 98, 210, 224, 98, 315, 152, 98, 210, 214, 211, 231, 98, 215, 211, 233, 152, 98, 210, 231, 98, 233, 152, 98, 210, 315, 98, 316, 152, 152, 31, 10, 210, 258, 98, 239, 74, 251, 210, 258, 74, 82, 74, 104, 152, 98, 238, 167, 210, 316, 98, 315, 152, 152, 31, 158, 235, 57, 31, 259, 167, 189, 210, 217, 98, 210, 228, 98, 229, 152, 98, 210, 224, 98, 315, 152, 98, 210, 214, 211, 231, 98, 215, 211, 233, 152, 98, 210, 231, 98, 233, 152, 98, 210, 315, 98, 316, 152, 152, 31, 260, 167, 51, 210, 259, 98, 238, 167, 210, 316, 98, 315, 152, 152, 31, 261, 167, 191, 210, 239, 211, 260, 98, 315, 152, 31, 262, 167, 189, 210, 223, 98, 210, 228, 98, 152, 98, 210, 226, 98, 152, 98, 210, 214, 211, 231, 98, 152, 98, 210, 231, 98, 152, 98, 210, 316, 98, 152, 152, 31, 10, 210, 262, 98, 261, 74, 251, 210, 262, 74, 82, 74, 104, 152, 98, 238, 167, 210, 316, 98, 152, 152, 31, 161, 31, 3, 31]}, {"code": "def parallel_simple_gla_bwd_kernel_dkv(\n    i_t,\n    i_k,\n    i_v,\n    q,\n    k,\n    v,\n    g,\n    do,\n    dk,\n    dv,\n    dg,\n    scale,\n    stride_qk,\n    stride_vo,\n    stride_g,\n    T,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_G: tl.constexpr,\n):\n\n    p_k = tl.make_block_ptr(\n        k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n\n    p_v = tl.make_block_ptr(\n        v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n    if USE_G:\n        p_gk = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))\n        b_gk = tl.load(p_gk, boundary_check=(0,))\n    NTS = tl.cdiv(T, BS)\n\n    for i_s in range(NTS * BS - BS, (i_t + 1) * BT - BS, -BS):\n        p_q = tl.make_block_ptr(\n            q, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0)\n        )\n        p_do = tl.make_block_ptr(\n            do, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0)\n        )\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_v, tl.trans(b_do))\n        b_s = tl.dot(b_k, tl.trans(b_q))\n        if USE_G:\n            p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))\n            b_gq = tl.load(p_gq, boundary_check=(0,))\n            b_gp = tl.load(g + (min(i_s + BS, T) - 1) * stride_g)\n            b_gn = tl.load(g + (i_s - 1) * stride_g) if i_s % BT > 0 else 0.0\n            if i_s >= 0:\n                tmp = safe_exp(b_gp - b_gn)\n                b_dk *= tmp\n                b_dv *= tmp\n                tmp2 = safe_exp(b_gq - b_gn)\n                b_ds *= tmp2[None, :]\n                b_s *= tmp2[None, :]\n\n        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\n\n        b_dv += tl.dot(b_s.to(b_do.dtype), b_do)\n\n    if USE_G:\n        b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * stride_g)\n        if i_t >= 0:\n            tmp2 = safe_exp(b_g_last - b_gk)[:, None]\n            b_dk *= tmp2\n            b_dv *= tmp2\n\n    o_q = i_t * BT + tl.arange(0, BS)\n    o_k = i_t * BT + tl.arange(0, BT)\n    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n        p_q = tl.make_block_ptr(\n            q, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0)\n        )\n        p_do = tl.make_block_ptr(\n            do, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0)\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_ds = tl.dot(b_v, tl.trans(b_do))\n        b_s = tl.dot(b_k, tl.trans(b_q))\n        if USE_G:\n            p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))\n            b_gq = tl.load(p_gq, boundary_check=(0,))\n            if i_s >= 0:\n                tmp = safe_exp(-b_gk[:, None] + b_gq[None, :])\n                b_ds *= tmp\n                b_s *= tmp\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.where(m_s, b_s, 0)\n        b_ds = tl.where(m_s, b_ds, 0)\n\n        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\n        b_dv += tl.dot(b_s.to(b_do.dtype), b_do)\n        o_q += BS\n    b_dk *= scale\n    b_dv *= scale\n    p_dk = tl.make_block_ptr(\n        dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_dv = tl.make_block_ptr(\n        dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    if USE_G:\n        p_dg = tl.make_block_ptr(dg, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))\n        b_dg = tl.load(p_dg, boundary_check=(0,))\n        b_dg -= tl.sum(b_dk * b_k, 1)\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 314, 210, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 99, 224, 99, 225, 99, 226, 99, 227, 99, 228, 99, 229, 99, 230, 58, 6, 99, 231, 58, 6, 99, 232, 58, 6, 99, 233, 58, 6, 99, 234, 58, 6, 99, 235, 58, 6, 99, 236, 58, 6, 152, 58, 31, -1, 237, 167, 189, 210, 218, 99, 210, 229, 99, 230, 152, 99, 210, 226, 99, 315, 152, 99, 210, 214, 211, 232, 99, 215, 211, 234, 152, 99, 210, 232, 99, 234, 152, 99, 210, 315, 99, 316, 152, 152, 31, 238, 167, 53, 210, 237, 99, 239, 167, 210, 316, 99, 315, 152, 152, 31, 240, 167, 153, 210, 198, 232, 99, 234, 26, 99, 83, 167, 131, 152, 31, 241, 167, 189, 210, 219, 99, 210, 229, 99, 231, 152, 99, 210, 227, 99, 315, 152, 99, 210, 214, 211, 232, 99, 216, 211, 235, 152, 99, 210, 232, 99, 235, 152, 99, 210, 315, 99, 316, 152, 152, 31, 242, 167, 53, 210, 241, 99, 239, 167, 210, 316, 99, 315, 152, 152, 31, 243, 167, 153, 210, 198, 232, 99, 235, 26, 99, 83, 167, 131, 152, 31, 158, 236, 58, 31, 244, 167, 189, 210, 220, 99, 210, 229, 99, 152, 99, 210, 228, 99, 152, 99, 210, 214, 211, 232, 99, 152, 99, 210, 232, 99, 152, 99, 210, 316, 99, 152, 152, 31, 245, 167, 53, 210, 244, 99, 239, 167, 210, 316, 99, 152, 152, 31, 161, 31, 246, 167, 60, 210, 229, 99, 233, 152, 31, 122, 247, 139, 5, 210, 246, 211, 233, 4, 233, 99, 210, 214, 68, 315, 152, 211, 232, 4, 233, 99, 4, 233, 152, 58, 31, 248, 167, 189, 210, 217, 99, 210, 229, 99, 230, 152, 99, 210, 226, 99, 315, 152, 99, 210, 247, 99, 215, 211, 234, 152, 99, 210, 233, 99, 234, 152, 99, 210, 315, 99, 316, 152, 152, 31, 249, 167, 189, 210, 221, 99, 210, 229, 99, 231, 152, 99, 210, 227, 99, 315, 152, 99, 210, 247, 99, 216, 211, 235, 152, 99, 210, 233, 99, 235, 152, 99, 210, 315, 99, 316, 152, 152, 31, 250, 167, 53, 210, 248, 99, 239, 167, 210, 316, 99, 315, 152, 152, 31, 251, 167, 53, 210, 249, 99, 239, 167, 210, 316, 99, 315, 152, 152, 31, 252, 167, 15, 210, 242, 99, 66, 210, 251, 152, 152, 31, 253, 167, 15, 210, 238, 99, 66, 210, 250, 152, 152, 31, 158, 236, 58, 31, 254, 167, 189, 210, 220, 99, 210, 229, 99, 152, 99, 210, 228, 99, 152, 99, 210, 247, 99, 152, 99, 210, 233, 99, 152, 99, 210, 316, 99, 152, 152, 31, 255, 167, 53, 210, 254, 99, 239, 167, 210, 316, 99, 152, 152, 31, 256, 167, 53, 210, 220, 68, 210, 38, 210, 247, 68, 233, 99, 229, 152, 4, 315, 152, 211, 228, 152, 31, 257, 167, 53, 210, 220, 68, 210, 247, 4, 315, 152, 211, 228, 152, 158, 247, 194, 232, 113, 316, 29, 316, 31, 158, 247, 130, 316, 58, 31, 258, 167, 259, 210, 256, 4, 257, 152, 31, 240, 23, 258, 31, 243, 23, 258, 31, 260, 167, 259, 210, 255, 4, 257, 152, 31, 252, 23, 260, 198, 173, 99, 58, 26, 31, 253, 23, 260, 198, 173, 99, 58, 26, 31, 161, 31, 161, 31, 240, 149, 15, 210, 252, 75, 261, 210, 250, 75, 83, 152, 99, 250, 152, 31, 243, 149, 15, 210, 253, 75, 261, 210, 251, 75, 83, 152, 99, 251, 152, 31, 71, 31, 158, 236, 58, 31, 262, 167, 53, 210, 220, 68, 210, 38, 210, 214, 211, 232, 68, 232, 99, 229, 152, 4, 315, 152, 211, 228, 152, 31, 158, 214, 130, 316, 58, 31, 260, 167, 259, 210, 262, 4, 245, 152, 198, 58, 99, 173, 26, 31, 240, 23, 260, 31, 243, 23, 260, 31, 161, 31, 161, 31, 263, 167, 214, 211, 232, 68, 69, 210, 316, 99, 233, 152, 31, 264, 167, 214, 211, 232, 68, 69, 210, 316, 99, 232, 152, 31, 122, 247, 139, 5, 210, 214, 211, 232, 99, 38, 210, 210, 214, 68, 315, 152, 211, 232, 99, 229, 152, 99, 233, 152, 58, 31, 248, 167, 189, 210, 217, 99, 210, 229, 99, 230, 152, 99, 210, 226, 99, 315, 152, 99, 210, 247, 99, 215, 211, 234, 152, 99, 210, 233, 99, 234, 152, 99, 210, 315, 99, 316, 152, 152, 31, 249, 167, 189, 210, 221, 99, 210, 229, 99, 231, 152, 99, 210, 227, 99, 315, 152, 99, 210, 247, 99, 216, 211, 235, 152, 99, 210, 233, 99, 235, 152, 99, 210, 315, 99, 316, 152, 152, 31, 250, 167, 53, 210, 248, 99, 239, 167, 210, 316, 99, 315, 152, 152, 31, 251, 167, 53, 210, 249, 99, 239, 167, 210, 316, 99, 315, 152, 152, 31, 252, 167, 15, 210, 242, 99, 66, 210, 251, 152, 152, 31, 253, 167, 15, 210, 238, 99, 66, 210, 250, 152, 152, 31, 158, 236, 58, 31, 254, 167, 189, 210, 220, 99, 210, 229, 99, 152, 99, 210, 228, 99, 152, 99, 210, 247, 99, 152, 99, 210, 233, 99, 152, 99, 210, 316, 99, 152, 152, 31, 255, 167, 53, 210, 254, 99, 239, 167, 210, 316, 99, 152, 152, 31, 158, 247, 130, 316, 58, 31, 258, 167, 259, 210, 4, 245, 198, 58, 99, 173, 26, 68, 255, 198, 173, 99, 58, 26, 152, 31, 252, 23, 258, 31, 253, 23, 258, 31, 161, 31, 161, 31, 265, 167, 264, 198, 58, 99, 173, 26, 188, 263, 198, 173, 99, 58, 26, 31, 253, 167, 176, 210, 265, 99, 253, 99, 316, 152, 31, 252, 167, 176, 210, 265, 99, 252, 99, 316, 152, 31, 240, 149, 15, 210, 252, 75, 261, 210, 250, 75, 83, 152, 99, 250, 152, 31, 243, 149, 15, 210, 253, 75, 261, 210, 251, 75, 83, 152, 99, 251, 152, 31, 263, 149, 233, 31, 71, 31, 240, 23, 225, 31, 243, 23, 225, 31, 266, 167, 189, 210, 222, 99, 210, 229, 99, 230, 152, 99, 210, 226, 99, 315, 152, 99, 210, 214, 211, 232, 99, 215, 211, 234, 152, 99, 210, 232, 99, 234, 152, 99, 210, 315, 99, 316, 152, 152, 31, 267, 167, 189, 210, 223, 99, 210, 229, 99, 231, 152, 99, 210, 227, 99, 315, 152, 99, 210, 214, 211, 232, 99, 216, 211, 235, 152, 99, 210, 232, 99, 235, 152, 99, 210, 315, 99, 316, 152, 152, 31, 10, 210, 266, 99, 240, 75, 261, 210, 266, 75, 83, 75, 105, 152, 99, 239, 167, 210, 316, 99, 315, 152, 152, 31, 10, 210, 267, 99, 243, 75, 261, 210, 267, 75, 83, 75, 105, 152, 99, 239, 167, 210, 316, 99, 315, 152, 152, 31, 158, 236, 58, 31, 268, 167, 189, 210, 224, 99, 210, 229, 99, 152, 99, 210, 228, 99, 152, 99, 210, 214, 211, 232, 99, 152, 99, 210, 232, 99, 152, 99, 210, 316, 99, 152, 152, 31, 269, 167, 53, 210, 268, 99, 239, 167, 210, 316, 99, 152, 152, 31, 269, 2, 191, 210, 240, 211, 238, 99, 315, 152, 31, 10, 210, 268, 99, 269, 75, 261, 210, 268, 75, 83, 75, 105, 152, 99, 239, 167, 210, 316, 99, 152, 152, 31, 161, 31, 3, 31]}, {"code": "def chunk_ttt_linear_fwd_kernel_h(\n    k,\n    v,\n    v_new,\n    eta,\n    w,\n    b,\n    eps,\n    h,\n    hb,\n    h0,\n    hb0,\n    ht,\n    hbt,\n    cu_seqlens,\n    chunk_offsets,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    USE_INITIAL_STATE_B: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    b_hb = tl.zeros([BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(\n            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)\n    if USE_INITIAL_STATE_B:\n        p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\n        b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option=\"zero\").to(tl.float32)\n\n    offs = tl.arange(0, BV)\n    b_w = tl.load(w + i_h * V + offs, mask=offs < V, other=0.0)\n    b_b = tl.load(b + i_h * V + offs, mask=offs < V, other=0.0)\n\n    for i_t in range(NT):\n        p_h = tl.make_block_ptr(\n            h + ((boh + i_t) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        p_hb = tl.make_block_ptr(\n            hb + ((boh + i_t) * H + i_h) * V, (V,), (1,), (i_v * BV,), (BV,), (0,)\n        )\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_hb, b_hb.to(p_hb.dtype.element_ty), boundary_check=(0,))\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_v_new = tl.make_block_ptr(\n            v_new + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_eta_last = (\n            eta + bos * H + i_h + (T - 1) * H\n            if i_t == NT - 1\n            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")\n        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")\n\n        b_kh = (\n            tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32)\n            + b_hb[None, :]\n        )\n        b_kh = tl.where((offs < V)[None, :], b_kh, 0.0)\n        mean = tl.sum(b_kh, axis=1, keep_dims=True) / V\n        xbar = tl.where((offs < V)[None, :], b_kh - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V\n        rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)\n        b_kh_hat = (b_kh - mean) * rstd\n\n        b_v = (\n            b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype)\n            + b_b[None, :].to(b_k.dtype)\n            - b_v.to(b_k.dtype)\n            + tl.trans(b_k)\n        )\n        b_v = tl.where((offs < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)\n        b_v2 = (\n            rstd\n            * (\n                V * b_v\n                - tl.sum(b_v, axis=1, keep_dims=True)\n                - b_kh_hat.to(b_k.dtype)\n                * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)\n            )\n            / V\n        )\n        tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))\n        b_eta_last = tl.load(p_eta_last)\n        b_h = b_h - tl.dot(b_eta_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)\n        b_hb = b_hb - tl.sum(b_eta_last * b_v2.to(b_k.dtype), axis=0)\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(\n            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        p_hbt = tl.make_block_ptr(hbt + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_hbt, b_hb.to(p_hbt.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 314, 210, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 98, 227, 98, 228, 98, 229, 98, 230, 57, 6, 98, 231, 57, 6, 98, 232, 57, 6, 98, 233, 57, 6, 98, 234, 57, 6, 98, 235, 57, 6, 98, 236, 57, 6, 98, 237, 57, 6, 98, 238, 57, 6, 98, 239, 57, 6, 152, 57, 31, -1, 240, 98, 241, 98, 242, 167, 210, 148, 210, 315, 152, 98, 148, 210, 316, 152, 98, 148, 210, 317, 152, 152, 31, 243, 98, 244, 167, 210, 242, 46, 230, 98, 242, 194, 230, 152, 31, 158, 239, 57, 31, 245, 98, 246, 167, 210, 51, 210, 227, 67, 243, 152, 74, 247, 210, 213, 152, 98, 51, 210, 227, 67, 243, 67, 316, 152, 74, 247, 210, 213, 152, 152, 31, 229, 167, 246, 4, 245, 31, 248, 167, 59, 210, 229, 98, 233, 152, 31, 249, 167, 51, 210, 228, 67, 243, 152, 74, 247, 210, 213, 152, 31, 161, 31, 29, 57, 31, 245, 98, 246, 167, 210, 243, 211, 229, 98, 243, 211, 229, 67, 229, 152, 31, 248, 167, 59, 210, 229, 98, 233, 152, 31, 249, 167, 243, 211, 248, 31, 52, 31, 250, 167, 153, 210, 198, 234, 98, 235, 26, 98, 82, 167, 130, 152, 31, 251, 167, 153, 210, 198, 235, 26, 98, 82, 167, 130, 152, 31, 158, 236, 57, 31, 252, 167, 189, 210, 223, 67, 242, 211, 231, 211, 232, 98, 210, 231, 98, 232, 152, 98, 210, 232, 98, 316, 152, 98, 210, 240, 211, 234, 98, 241, 211, 235, 152, 98, 210, 234, 98, 235, 152, 98, 210, 316, 98, 315, 152, 152, 31, 250, 167, 51, 210, 252, 98, 253, 167, 210, 315, 98, 316, 152, 98, 254, 167, 318, 152, 74, 247, 210, 130, 152, 31, 161, 31, 158, 237, 57, 31, 255, 167, 189, 210, 224, 67, 242, 211, 232, 98, 210, 232, 98, 152, 98, 210, 316, 98, 152, 98, 210, 241, 211, 235, 98, 152, 98, 210, 235, 98, 152, 98, 210, 315, 98, 152, 152, 31, 251, 167, 51, 210, 255, 98, 253, 167, 210, 315, 98, 152, 98, 254, 167, 318, 152, 74, 247, 210, 130, 152, 31, 161, 31, 256, 167, 68, 210, 315, 98, 235, 152, 31, 257, 167, 51, 210, 218, 67, 244, 211, 232, 67, 256, 98, 258, 167, 256, 1, 232, 98, 259, 167, 315, 152, 31, 260, 167, 51, 210, 219, 67, 244, 211, 232, 67, 256, 98, 258, 167, 256, 1, 232, 98, 259, 167, 315, 152, 31, 121, 261, 139, 5, 210, 248, 152, 57, 31, 262, 167, 189, 210, 221, 67, 210, 210, 249, 67, 261, 152, 211, 230, 67, 244, 152, 211, 231, 211, 232, 98, 210, 231, 98, 232, 152, 98, 210, 232, 98, 316, 152, 98, 210, 240, 211, 234, 98, 241, 211, 235, 152, 98, 210, 234, 98, 235, 152, 98, 210, 316, 98, 315, 152, 152, 31, 263, 167, 189, 210, 222, 67, 210, 210, 249, 67, 261, 152, 211, 230, 67, 244, 152, 211, 232, 98, 210, 232, 98, 152, 98, 210, 316, 98, 152, 98, 210, 241, 211, 235, 98, 152, 98, 210, 235, 98, 152, 98, 210, 315, 98, 152, 152, 31, 10, 210, 262, 98, 250, 74, 247, 210, 262, 74, 82, 74, 104, 152, 98, 253, 167, 210, 315, 98, 316, 152, 152, 31, 10, 210, 263, 98, 251, 74, 247, 210, 263, 74, 82, 74, 104, 152, 98, 253, 167, 210, 315, 98, 152, 152, 31, 264, 167, 189, 210, 214, 67, 210, 245, 211, 230, 67, 244, 152, 211, 231, 98, 210, 231, 98, 229, 152, 98, 210, 316, 98, 230, 211, 231, 152, 98, 210, 240, 211, 234, 98, 261, 211, 233, 152, 98, 210, 234, 98, 233, 152, 98, 210, 315, 98, 316, 152, 152, 31, 265, 167, 189, 210, 215, 67, 210, 245, 211, 230, 67, 244, 152, 211, 232, 98, 210, 229, 98, 232, 152, 98, 210, 230, 211, 232, 98, 316, 152, 98, 210, 261, 211, 233, 98, 241, 211, 235, 152, 98, 210, 233, 98, 235, 152, 98, 210, 316, 98, 315, 152, 152, 31, 266, 167, 189, 210, 216, 67, 210, 245, 211, 230, 67, 244, 152, 211, 232, 98, 210, 229, 98, 232, 152, 98, 210, 230, 211, 232, 98, 316, 152, 98, 210, 261, 211, 233, 98, 241, 211, 235, 152, 98, 210, 233, 98, 235, 152, 98, 210, 316, 98, 315, 152, 152, 31, 267, 167, 217, 67, 245, 211, 230, 67, 244, 67, 210, 229, 4, 316, 152, 211, 230, 158, 261, 69, 248, 4, 316, 29, 217, 67, 245, 211, 230, 67, 244, 67, 210, 261, 211, 233, 67, 233, 4, 316, 152, 211, 230, 31, 268, 167, 51, 210, 264, 98, 253, 167, 210, 315, 98, 316, 152, 98, 254, 167, 318, 152, 31, 269, 167, 51, 210, 265, 98, 253, 167, 210, 315, 98, 316, 152, 98, 254, 167, 318, 152, 31, 270, 167, 15, 210, 65, 210, 268, 152, 98, 250, 74, 247, 210, 268, 74, 82, 152, 98, 271, 167, 55, 152, 74, 247, 210, 130, 152, 67, 251, 198, 173, 98, 57, 26, 31, 270, 167, 176, 210, 210, 256, 1, 232, 152, 198, 173, 98, 57, 26, 98, 270, 98, 315, 152, 31, 272, 167, 191, 210, 270, 98, 273, 167, 316, 98, 274, 167, 142, 152, 41, 232, 31, 275, 167, 176, 210, 210, 256, 1, 232, 152, 198, 173, 98, 57, 26, 98, 270, 4, 272, 98, 315, 152, 31, 276, 167, 191, 210, 275, 211, 275, 98, 273, 167, 316, 98, 274, 167, 142, 152, 41, 232, 31, 277, 167, 316, 41, 116, 210, 276, 74, 247, 210, 130, 152, 67, 220, 152, 31, 278, 167, 210, 270, 4, 272, 152, 211, 277, 31, 269, 167, 278, 74, 247, 210, 268, 74, 82, 152, 211, 257, 198, 173, 98, 57, 26, 74, 247, 210, 268, 74, 82, 152, 67, 260, 198, 173, 98, 57, 26, 74, 247, 210, 268, 74, 82, 152, 4, 269, 74, 247, 210, 268, 74, 82, 152, 67, 65, 210, 268, 152, 31, 269, 167, 176, 210, 210, 256, 1, 232, 152, 198, 173, 98, 57, 26, 98, 269, 211, 257, 198, 173, 98, 57, 26, 74, 247, 210, 268, 74, 82, 152, 98, 315, 152, 31, 279, 167, 277, 211, 210, 232, 211, 269, 4, 191, 210, 269, 98, 273, 167, 316, 98, 274, 167, 142, 152, 4, 278, 74, 247, 210, 268, 74, 82, 152, 211, 191, 210, 269, 211, 278, 74, 247, 210, 268, 74, 82, 152, 98, 273, 167, 316, 98, 274, 167, 142, 152, 152, 41, 232, 31, 10, 210, 266, 98, 279, 74, 247, 210, 266, 74, 82, 74, 104, 152, 98, 253, 167, 210, 315, 98, 316, 152, 152, 31, 280, 167, 51, 210, 267, 152, 31, 250, 167, 250, 4, 15, 210, 280, 211, 268, 98, 279, 74, 247, 210, 268, 74, 82, 152, 98, 271, 167, 55, 152, 31, 251, 167, 251, 4, 191, 210, 280, 211, 279, 74, 247, 210, 268, 74, 82, 152, 98, 273, 167, 315, 152, 31, 70, 31, 158, 238, 57, 31, 281, 167, 189, 210, 225, 67, 242, 211, 231, 211, 232, 98, 210, 231, 98, 232, 152, 98, 210, 232, 98, 316, 152, 98, 210, 240, 211, 234, 98, 241, 211, 235, 152, 98, 210, 234, 98, 235, 152, 98, 210, 316, 98, 315, 152, 152, 31, 282, 167, 189, 210, 226, 67, 242, 211, 232, 98, 210, 232, 98, 152, 98, 210, 316, 98, 152, 98, 210, 241, 211, 235, 98, 152, 98, 210, 235, 98, 152, 98, 210, 315, 98, 152, 152, 31, 10, 210, 281, 98, 250, 74, 247, 210, 281, 74, 82, 74, 104, 152, 98, 253, 167, 210, 315, 98, 316, 152, 152, 31, 10, 210, 282, 98, 251, 74, 247, 210, 282, 74, 82, 74, 104, 152, 98, 253, 167, 210, 315, 98, 152, 152, 31, 161, 31, 3, 31]}, {"code": "def chunk_ttt_linear_fwd_kernel_o(\n    q,\n    k,\n    v,\n    eta,\n    h,\n    hb,\n    o,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    q += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V\n    eta += bos * H + i_h\n    o += (bos * H + i_h) * V\n    h += (i_tg * H + i_h) * K * V\n    hb += (i_tg * H + i_h) * V\n    stride_qk = H * K\n    stride_vo = H * V\n    stride_eta = H\n\n    p_q = tl.make_block_ptr(q, (T, K), (stride_qk, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k, (K, T), (1, stride_qk), (0, i_t * BT), (BK, BT), (0, 1))\n    p_eta = tl.make_block_ptr(eta, (T,), (stride_eta,), (i_t * BT,), (BT,), (0,))\n    p_h = tl.make_block_ptr(h, (K, V), (V, 1), (0, i_v * BV), (BK, BV), (1, 0))\n    p_hb = tl.make_block_ptr(hb, (V,), (1,), (i_v * BV,), (BV,), (0,))\n\n    b_q = tl.load(p_q, boundary_check=(0, 1), padding_option=\"zero\")\n\n    b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")\n\n    b_eta = tl.load(p_eta, boundary_check=(0,), padding_option=\"zero\")\n\n    b_h = tl.load(p_h, boundary_check=(0, 1), padding_option=\"zero\")\n\n    b_hb = tl.load(p_hb, boundary_check=(0,), padding_option=\"zero\")\n\n    b_o = tl.dot(b_q, b_h, allow_tf32=False)\n\n    b_A = tl.dot(b_q, b_k, allow_tf32=False)\n\n    o_i = tl.arange(0, BT)\n    m_A = o_i[:, None] >= o_i[None, :]\n    b_A = tl.where(m_A, b_A, 0)\n    b_Ae = tl.where(m_A, b_eta[:, None], 0.0)\n\n    p_v = tl.make_block_ptr(\n        v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_o = tl.make_block_ptr(\n        o, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")\n    b_o = (\n        b_o - tl.dot(b_eta[:, None] * b_A.to(b_v.dtype), b_v, allow_tf32=False)\n    ) * scale\n    b_o += b_hb[None, :] - tl.dot(b_Ae.to(b_v.dtype), b_v, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 314, 210, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 99, 224, 99, 225, 58, 6, 99, 226, 58, 6, 99, 227, 58, 6, 99, 228, 58, 6, 99, 229, 58, 6, 99, 230, 58, 6, 99, 231, 58, 6, 152, 58, 31, -1, 232, 99, 233, 99, 234, 167, 210, 148, 210, 315, 152, 99, 148, 210, 316, 152, 99, 148, 210, 317, 152, 152, 31, 235, 99, 236, 167, 210, 234, 46, 225, 99, 234, 194, 225, 152, 31, 158, 231, 58, 31, 237, 167, 233, 31, 238, 99, 233, 167, 210, 53, 210, 222, 68, 233, 211, 317, 152, 75, 239, 210, 57, 152, 99, 53, 210, 222, 68, 233, 211, 317, 68, 316, 152, 75, 239, 210, 57, 152, 152, 31, 240, 99, 241, 167, 210, 53, 210, 221, 68, 238, 152, 75, 239, 210, 57, 152, 99, 53, 210, 221, 68, 238, 68, 316, 152, 75, 239, 210, 57, 152, 152, 31, 224, 167, 241, 4, 240, 31, 242, 167, 60, 210, 224, 99, 228, 152, 31, 161, 31, 29, 58, 31, 242, 167, 60, 210, 224, 99, 228, 152, 31, 237, 167, 235, 211, 242, 68, 233, 31, 240, 99, 241, 167, 210, 235, 211, 224, 99, 235, 211, 224, 68, 224, 152, 31, 52, 31, 214, 149, 210, 240, 211, 225, 68, 236, 152, 211, 226, 31, 215, 149, 210, 240, 211, 225, 68, 236, 152, 211, 226, 31, 216, 149, 210, 240, 211, 225, 68, 236, 152, 211, 227, 31, 217, 149, 240, 211, 225, 68, 236, 31, 220, 149, 210, 240, 211, 225, 68, 236, 152, 211, 227, 31, 218, 149, 210, 237, 211, 225, 68, 236, 152, 211, 226, 211, 227, 31, 219, 149, 210, 237, 211, 225, 68, 236, 152, 211, 227, 31, 243, 167, 225, 211, 226, 31, 244, 167, 225, 211, 227, 31, 245, 167, 225, 31, 246, 167, 189, 210, 214, 99, 210, 224, 99, 226, 152, 99, 210, 243, 99, 316, 152, 99, 210, 233, 211, 228, 99, 315, 152, 99, 210, 228, 99, 229, 152, 99, 210, 316, 99, 315, 152, 152, 31, 247, 167, 189, 210, 215, 99, 210, 226, 99, 224, 152, 99, 210, 316, 99, 243, 152, 99, 210, 315, 99, 233, 211, 228, 152, 99, 210, 229, 99, 228, 152, 99, 210, 315, 99, 316, 152, 152, 31, 248, 167, 189, 210, 217, 99, 210, 224, 99, 152, 99, 210, 245, 99, 152, 99, 210, 233, 211, 228, 99, 152, 99, 210, 228, 99, 152, 99, 210, 315, 99, 152, 152, 31, 249, 167, 189, 210, 218, 99, 210, 226, 99, 227, 152, 99, 210, 227, 99, 316, 152, 99, 210, 315, 99, 232, 211, 230, 152, 99, 210, 229, 99, 230, 152, 99, 210, 316, 99, 315, 152, 152, 31, 250, 167, 189, 210, 219, 99, 210, 227, 99, 152, 99, 210, 316, 99, 152, 99, 210, 232, 211, 230, 99, 152, 99, 210, 230, 99, 152, 99, 210, 315, 99, 152, 152, 31, 251, 167, 53, 210, 246, 99, 252, 167, 210, 315, 99, 316, 152, 99, 253, 167, 318, 152, 31, 254, 167, 53, 210, 247, 99, 252, 167, 210, 315, 99, 316, 152, 99, 253, 167, 318, 152, 31, 255, 167, 53, 210, 248, 99, 252, 167, 210, 315, 99, 152, 99, 253, 167, 318, 152, 31, 256, 167, 53, 210, 249, 99, 252, 167, 210, 315, 99, 316, 152, 99, 253, 167, 318, 152, 31, 257, 167, 53, 210, 250, 99, 252, 167, 210, 315, 99, 152, 99, 253, 167, 318, 152, 31, 258, 167, 15, 210, 251, 99, 256, 99, 259, 167, 55, 152, 31, 260, 167, 15, 210, 251, 99, 254, 99, 259, 167, 55, 152, 31, 261, 167, 69, 210, 315, 99, 228, 152, 31, 262, 167, 261, 198, 58, 99, 173, 26, 130, 261, 198, 173, 99, 58, 26, 31, 260, 167, 176, 210, 262, 99, 260, 99, 315, 152, 31, 263, 167, 176, 210, 262, 99, 255, 198, 58, 99, 173, 26, 99, 315, 152, 31, 264, 167, 189, 210, 216, 99, 210, 224, 99, 227, 152, 99, 210, 244, 99, 316, 152, 99, 210, 233, 211, 228, 99, 232, 211, 230, 152, 99, 210, 228, 99, 230, 152, 99, 210, 316, 99, 315, 152, 152, 31, 265, 167, 189, 210, 220, 99, 210, 224, 99, 227, 152, 99, 210, 244, 99, 316, 152, 99, 210, 233, 211, 228, 99, 232, 211, 230, 152, 99, 210, 228, 99, 230, 152, 99, 210, 316, 99, 315, 152, 152, 31, 266, 167, 53, 210, 264, 99, 252, 167, 210, 315, 99, 316, 152, 99, 253, 167, 318, 152, 31, 258, 167, 210, 258, 4, 15, 210, 255, 198, 58, 99, 173, 26, 211, 260, 75, 239, 210, 266, 75, 83, 152, 99, 266, 99, 259, 167, 55, 152, 152, 211, 223, 31, 258, 149, 257, 198, 173, 99, 58, 26, 4, 15, 210, 263, 75, 239, 210, 266, 75, 83, 152, 99, 266, 99, 259, 167, 55, 152, 31, 10, 210, 265, 99, 258, 75, 239, 210, 265, 75, 83, 75, 105, 152, 99, 252, 167, 210, 315, 99, 316, 152, 152, 31, 3, 31]}, {"code": "def chunk_ttt_linear_bwd_kernel_h(\n    k,\n    v,\n    v_new,\n    eta,\n    w,\n    b,\n    eps,\n    h,\n    h0,\n    hb0,\n    x,\n    y,\n    r,\n    cu_seqlens,\n    chunk_offsets,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    USE_INITIAL_STATE_B: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    b_hb = tl.zeros([BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(\n            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)\n    if USE_INITIAL_STATE_B:\n        p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\n        b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option=\"zero\").to(tl.float32)\n\n    offs = tl.arange(0, BV)\n    b_w = tl.load(w + i_h * V + offs, mask=offs < V, other=0.0)\n    b_b = tl.load(b + i_h * V + offs, mask=offs < V, other=0.0)\n\n    for i_t in range(NT):\n        p_h = tl.make_block_ptr(\n            h + ((boh + i_t) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_v_new = tl.make_block_ptr(\n            v_new + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_x = tl.make_block_ptr(\n            x + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_y = tl.make_block_ptr(\n            y + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_r = tl.make_block_ptr(\n            r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0)\n        )\n        p_eta_last = (\n            eta + bos * H + i_h + (T - 1) * H\n            if i_t == NT - 1\n            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")\n        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")\n\n        b_kh = (\n            tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32)\n            + b_hb[None, :]\n        )\n        b_kh = tl.where((offs < V)[None, :], b_kh, 0.0)\n        mean = tl.sum(b_kh, axis=1, keep_dims=True) / V\n        xbar = tl.where((offs < V)[None, :], b_kh - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V\n        rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)\n        b_kh_hat = (b_kh - mean) * rstd\n\n        b_v = (\n            b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype)\n            + b_b[None, :].to(b_k.dtype)\n            - b_v.to(b_k.dtype)\n            + tl.trans(b_k)\n        )\n        b_v = tl.where((offs < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)\n        b_v2 = (\n            rstd\n            * (\n                V * b_v\n                - tl.sum(b_v, axis=1, keep_dims=True)\n                - b_kh_hat.to(b_k.dtype)\n                * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)\n            )\n            / V\n        )\n        tl.store(p_x, b_kh_hat.to(p_x.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_y, b_v.to(p_y.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_r, rstd.to(p_r.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))\n        b_eta_last = tl.load(p_eta_last)\n        b_h = b_h - tl.dot(b_eta_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)\n        b_hb = b_hb - tl.sum(b_eta_last * b_v2.to(b_k.dtype), axis=0)", "encoded": [28, 314, 210, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 98, 227, 98, 228, 98, 229, 98, 230, 57, 6, 98, 231, 57, 6, 98, 232, 57, 6, 98, 233, 57, 6, 98, 234, 57, 6, 98, 235, 57, 6, 98, 236, 57, 6, 98, 237, 57, 6, 98, 238, 57, 6, 98, 239, 57, 6, 152, 57, 31, -1, 240, 98, 241, 98, 242, 167, 210, 148, 210, 315, 152, 98, 148, 210, 316, 152, 98, 148, 210, 317, 152, 152, 31, 243, 98, 244, 167, 210, 242, 46, 230, 98, 242, 194, 230, 152, 31, 158, 239, 57, 31, 245, 98, 246, 167, 210, 51, 210, 227, 67, 243, 152, 74, 247, 210, 213, 152, 98, 51, 210, 227, 67, 243, 67, 316, 152, 74, 247, 210, 213, 152, 152, 31, 229, 167, 246, 4, 245, 31, 236, 167, 59, 210, 229, 98, 233, 152, 31, 248, 167, 51, 210, 228, 67, 243, 152, 74, 247, 210, 213, 152, 31, 161, 31, 29, 57, 31, 245, 98, 246, 167, 210, 243, 211, 229, 98, 243, 211, 229, 67, 229, 152, 31, 236, 167, 59, 210, 229, 98, 233, 152, 31, 248, 167, 243, 211, 236, 31, 52, 31, 249, 167, 153, 210, 198, 234, 98, 235, 26, 98, 82, 167, 130, 152, 31, 250, 167, 153, 210, 198, 235, 26, 98, 82, 167, 130, 152, 31, 158, 237, 57, 31, 251, 167, 189, 210, 222, 67, 242, 211, 231, 211, 232, 98, 210, 231, 98, 232, 152, 98, 210, 232, 98, 316, 152, 98, 210, 240, 211, 234, 98, 241, 211, 235, 152, 98, 210, 234, 98, 235, 152, 98, 210, 316, 98, 315, 152, 152, 31, 249, 167, 51, 210, 251, 98, 252, 167, 210, 315, 98, 316, 152, 98, 253, 167, 318, 152, 74, 247, 210, 130, 152, 31, 161, 31, 158, 238, 57, 31, 254, 167, 189, 210, 223, 67, 242, 211, 232, 98, 210, 232, 98, 152, 98, 210, 316, 98, 152, 98, 210, 241, 211, 235, 98, 152, 98, 210, 235, 98, 152, 98, 210, 315, 98, 152, 152, 31, 250, 167, 51, 210, 254, 98, 252, 167, 210, 315, 98, 152, 98, 253, 167, 318, 152, 74, 247, 210, 130, 152, 31, 161, 31, 255, 167, 68, 210, 315, 98, 235, 152, 31, 256, 167, 51, 210, 218, 67, 244, 211, 232, 67, 255, 98, 257, 167, 255, 1, 232, 98, 258, 167, 315, 152, 31, 259, 167, 51, 210, 219, 67, 244, 211, 232, 67, 255, 98, 257, 167, 255, 1, 232, 98, 258, 167, 315, 152, 31, 121, 260, 139, 5, 210, 236, 152, 57, 31, 261, 167, 189, 210, 221, 67, 210, 210, 248, 67, 260, 152, 211, 230, 67, 244, 152, 211, 231, 211, 232, 98, 210, 231, 98, 232, 152, 98, 210, 232, 98, 316, 152, 98, 210, 240, 211, 234, 98, 241, 211, 235, 152, 98, 210, 234, 98, 235, 152, 98, 210, 316, 98, 315, 152, 152, 31, 10, 210, 261, 98, 249, 74, 247, 210, 261, 74, 82, 74, 104, 152, 98, 252, 167, 210, 315, 98, 316, 152, 152, 31, 262, 167, 189, 210, 214, 67, 210, 245, 211, 230, 67, 244, 152, 211, 231, 98, 210, 231, 98, 229, 152, 98, 210, 316, 98, 230, 211, 231, 152, 98, 210, 240, 211, 234, 98, 260, 211, 233, 152, 98, 210, 234, 98, 233, 152, 98, 210, 315, 98, 316, 152, 152, 31, 263, 167, 189, 210, 215, 67, 210, 245, 211, 230, 67, 244, 152, 211, 232, 98, 210, 229, 98, 232, 152, 98, 210, 230, 211, 232, 98, 316, 152, 98, 210, 260, 211, 233, 98, 241, 211, 235, 152, 98, 210, 233, 98, 235, 152, 98, 210, 316, 98, 315, 152, 152, 31, 264, 167, 189, 210, 216, 67, 210, 245, 211, 230, 67, 244, 152, 211, 232, 98, 210, 229, 98, 232, 152, 98, 210, 230, 211, 232, 98, 316, 152, 98, 210, 260, 211, 233, 98, 241, 211, 235, 152, 98, 210, 233, 98, 235, 152, 98, 210, 316, 98, 315, 152, 152, 31, 265, 167, 189, 210, 224, 67, 210, 245, 211, 230, 67, 244, 152, 211, 232, 98, 210, 229, 98, 232, 152, 98, 210, 230, 211, 232, 98, 316, 152, 98, 210, 260, 211, 233, 98, 241, 211, 235, 152, 98, 210, 233, 98, 235, 152, 98, 210, 316, 98, 315, 152, 152, 31, 266, 167, 189, 210, 225, 67, 210, 245, 211, 230, 67, 244, 152, 211, 232, 98, 210, 229, 98, 232, 152, 98, 210, 230, 211, 232, 98, 316, 152, 98, 210, 260, 211, 233, 98, 241, 211, 235, 152, 98, 210, 233, 98, 235, 152, 98, 210, 316, 98, 315, 152, 152, 31, 267, 167, 189, 210, 226, 67, 245, 211, 230, 67, 244, 98, 210, 229, 98, 316, 152, 98, 210, 230, 98, 316, 152, 98, 210, 260, 211, 233, 98, 315, 152, 98, 210, 233, 98, 316, 152, 98, 210, 316, 98, 315, 152, 152, 31, 268, 167, 217, 67, 245, 211, 230, 67, 244, 67, 210, 229, 4, 316, 152, 211, 230, 158, 260, 69, 236, 4, 316, 29, 217, 67, 245, 211, 230, 67, 244, 67, 210, 260, 211, 233, 67, 233, 4, 316, 152, 211, 230, 31, 269, 167, 51, 210, 262, 98, 252, 167, 210, 315, 98, 316, 152, 98, 253, 167, 318, 152, 31, 270, 167, 51, 210, 263, 98, 252, 167, 210, 315, 98, 316, 152, 98, 253, 167, 318, 152, 31, 271, 167, 15, 210, 65, 210, 269, 152, 98, 249, 74, 247, 210, 269, 74, 82, 152, 98, 272, 167, 55, 152, 74, 247, 210, 130, 152, 67, 250, 198, 173, 98, 57, 26, 31, 271, 167, 176, 210, 210, 255, 1, 232, 152, 198, 173, 98, 57, 26, 98, 271, 98, 315, 152, 31, 273, 167, 191, 210, 271, 98, 274, 167, 316, 98, 275, 167, 142, 152, 41, 232, 31, 276, 167, 176, 210, 210, 255, 1, 232, 152, 198, 173, 98, 57, 26, 98, 271, 4, 273, 98, 315, 152, 31, 277, 167, 191, 210, 276, 211, 276, 98, 274, 167, 316, 98, 275, 167, 142, 152, 41, 232, 31, 278, 167, 316, 41, 116, 210, 277, 74, 247, 210, 130, 152, 67, 220, 152, 31, 279, 167, 210, 271, 4, 273, 152, 211, 278, 31, 270, 167, 279, 74, 247, 210, 269, 74, 82, 152, 211, 256, 198, 173, 98, 57, 26, 74, 247, 210, 269, 74, 82, 152, 67, 259, 198, 173, 98, 57, 26, 74, 247, 210, 269, 74, 82, 152, 4, 270, 74, 247, 210, 269, 74, 82, 152, 67, 65, 210, 269, 152, 31, 270, 167, 176, 210, 210, 255, 1, 232, 152, 198, 173, 98, 57, 26, 98, 270, 211, 256, 198, 173, 98, 57, 26, 74, 247, 210, 269, 74, 82, 152, 98, 315, 152, 31, 280, 167, 278, 211, 210, 232, 211, 270, 4, 191, 210, 270, 98, 274, 167, 316, 98, 275, 167, 142, 152, 4, 279, 74, 247, 210, 269, 74, 82, 152, 211, 191, 210, 270, 211, 279, 74, 247, 210, 269, 74, 82, 152, 98, 274, 167, 316, 98, 275, 167, 142, 152, 152, 41, 232, 31, 10, 210, 265, 98, 279, 74, 247, 210, 265, 74, 82, 74, 104, 152, 98, 252, 167, 210, 315, 98, 316, 152, 152, 31, 10, 210, 266, 98, 270, 74, 247, 210, 266, 74, 82, 74, 104, 152, 98, 252, 167, 210, 315, 98, 316, 152, 152, 31, 10, 210, 267, 98, 278, 74, 247, 210, 267, 74, 82, 74, 104, 152, 98, 252, 167, 210, 315, 98, 316, 152, 152, 31, 10, 210, 264, 98, 280, 74, 247, 210, 264, 74, 82, 74, 104, 152, 98, 252, 167, 210, 315, 98, 316, 152, 152, 31, 281, 167, 51, 210, 268, 152, 31, 249, 167, 249, 4, 15, 210, 281, 211, 269, 98, 280, 74, 247, 210, 269, 74, 82, 152, 98, 272, 167, 55, 152, 31, 250, 167, 250, 4, 191, 210, 281, 211, 280, 74, 247, 210, 269, 74, 82, 152, 98, 274, 167, 315, 152, 31, 70, 31, 3, 31]}, {"code": "def chunk_ttt_linear_bwd_kernel_dv_local(\n    q,\n    k,\n    eta,\n    do,\n    dv,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    q += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    eta += bos * H + i_h\n    do += (bos * H + i_h) * V\n    dv += (bos * H + i_h) * V\n    stride_qk = H * K\n    stride_vo = H * V\n    stride_eta = H\n\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(\n            k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_q = tl.make_block_ptr(\n            q, (K, T), (1, stride_qk), (i_k * BK, i_t * BT), (BK, BT), (0, 1)\n        )\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_A += tl.dot(b_k, b_q)\n\n    p_eta = tl.make_block_ptr(eta, (T,), (stride_eta,), (i_t * BT,), (BT,), (0,))\n    b_eta = tl.load(p_eta, boundary_check=(0,))\n    mask = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]\n    b_A = -tl.where(mask, b_A * scale * b_eta[None, :], 0).to(do.dtype.element_ty)\n    b_Ae = -tl.where(mask, b_eta[None, :], 0).to(do.dtype.element_ty)\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_do = tl.make_block_ptr(\n            do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_dv = tl.make_block_ptr(\n            dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dv = tl.dot(b_A.to(b_do.dtype), b_do) + tl.dot(b_Ae.to(b_do.dtype), b_do)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 314, 210, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 58, 6, 99, 224, 58, 6, 99, 225, 58, 6, 99, 226, 58, 6, 99, 227, 58, 6, 99, 228, 58, 6, 99, 229, 58, 6, 152, 58, 31, -1, 230, 99, 231, 167, 210, 148, 210, 315, 152, 99, 148, 210, 316, 152, 152, 31, 232, 99, 233, 167, 210, 231, 46, 223, 99, 231, 194, 223, 152, 31, 158, 229, 58, 31, 234, 99, 230, 167, 210, 53, 210, 220, 68, 230, 211, 317, 152, 75, 235, 210, 57, 152, 99, 53, 210, 220, 68, 230, 211, 317, 68, 316, 152, 75, 235, 210, 57, 152, 152, 31, 236, 99, 237, 167, 210, 53, 210, 219, 68, 234, 152, 75, 235, 210, 57, 152, 99, 53, 210, 219, 68, 234, 68, 316, 152, 75, 235, 210, 57, 152, 152, 31, 222, 167, 237, 4, 236, 31, 161, 31, 29, 58, 31, 236, 99, 237, 167, 210, 232, 211, 222, 99, 232, 211, 222, 68, 222, 152, 31, 52, 31, 214, 149, 210, 236, 211, 223, 68, 233, 152, 211, 224, 31, 215, 149, 210, 236, 211, 223, 68, 233, 152, 211, 224, 31, 216, 149, 236, 211, 223, 68, 233, 31, 217, 149, 210, 236, 211, 223, 68, 233, 152, 211, 225, 31, 218, 149, 210, 236, 211, 223, 68, 233, 152, 211, 225, 31, 238, 167, 223, 211, 224, 31, 239, 167, 223, 211, 225, 31, 240, 167, 223, 31, 241, 167, 153, 210, 198, 226, 99, 226, 26, 99, 83, 167, 131, 152, 31, 122, 242, 139, 5, 210, 60, 210, 224, 99, 227, 152, 152, 58, 31, 243, 167, 189, 210, 215, 99, 210, 222, 99, 224, 152, 99, 210, 238, 99, 316, 152, 99, 210, 230, 211, 226, 99, 242, 211, 227, 152, 99, 210, 226, 99, 227, 152, 99, 210, 316, 99, 315, 152, 152, 31, 244, 167, 189, 210, 214, 99, 210, 224, 99, 222, 152, 99, 210, 316, 99, 238, 152, 99, 210, 242, 211, 227, 99, 230, 211, 226, 152, 99, 210, 227, 99, 226, 152, 99, 210, 315, 99, 316, 152, 152, 31, 245, 167, 53, 210, 244, 99, 246, 167, 210, 315, 99, 316, 152, 152, 31, 247, 167, 53, 210, 243, 99, 246, 167, 210, 315, 99, 316, 152, 152, 31, 241, 149, 15, 210, 247, 99, 245, 152, 31, 71, 31, 248, 167, 189, 210, 216, 99, 210, 222, 99, 152, 99, 210, 240, 99, 152, 99, 210, 230, 211, 226, 99, 152, 99, 210, 226, 99, 152, 99, 210, 315, 99, 152, 152, 31, 249, 167, 53, 210, 248, 99, 246, 167, 210, 315, 99, 152, 152, 31, 250, 167, 69, 210, 315, 99, 226, 152, 198, 58, 99, 173, 26, 188, 69, 210, 315, 99, 226, 152, 198, 173, 99, 58, 26, 31, 241, 167, 4, 176, 210, 250, 99, 241, 211, 221, 211, 249, 198, 173, 99, 58, 26, 99, 315, 152, 75, 235, 210, 217, 75, 83, 75, 105, 152, 31, 251, 167, 4, 176, 210, 250, 99, 249, 198, 173, 99, 58, 26, 99, 315, 152, 75, 235, 210, 217, 75, 83, 75, 105, 152, 31, 122, 252, 139, 5, 210, 60, 210, 225, 99, 228, 152, 152, 58, 31, 253, 167, 189, 210, 217, 99, 210, 222, 99, 225, 152, 99, 210, 239, 99, 316, 152, 99, 210, 230, 211, 226, 99, 252, 211, 228, 152, 99, 210, 226, 99, 228, 152, 99, 210, 316, 99, 315, 152, 152, 31, 254, 167, 189, 210, 218, 99, 210, 222, 99, 225, 152, 99, 210, 239, 99, 316, 152, 99, 210, 230, 211, 226, 99, 252, 211, 228, 152, 99, 210, 226, 99, 228, 152, 99, 210, 316, 99, 315, 152, 152, 31, 255, 167, 53, 210, 253, 99, 246, 167, 210, 315, 99, 316, 152, 152, 31, 256, 167, 15, 210, 241, 75, 235, 210, 255, 75, 83, 152, 99, 255, 152, 68, 15, 210, 251, 75, 235, 210, 255, 75, 83, 152, 99, 255, 152, 31, 10, 210, 254, 99, 256, 75, 235, 210, 254, 75, 83, 75, 105, 152, 99, 246, 167, 210, 315, 99, 316, 152, 152, 31, 71, 31, 3, 31]}, {"code": "def chunk_ttt_linear_bwd_kernel_norm(\n    q,\n    k,\n    v,\n    v_new,\n    x,\n    y,\n    r,\n    w,\n    b,\n    eta,\n    h,\n    dht,\n    dhbt,\n    dh0,\n    dhb0,\n    do,\n    dh,\n    dhb,\n    dv,\n    dv_new,\n    dk,\n    dw,\n    db,\n    cu_seqlens,\n    chunk_offsets,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT_B: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    USE_INITIAL_STATE_B: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n        boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n        boh = i_n * NT\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n\n    b_dhb = tl.zeros([BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(\n            dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_dh += tl.load(p_dht, boundary_check=(0, 1), padding_option=\"zero\")\n    if USE_FINAL_STATE_GRADIENT_B:\n        p_dhbt = tl.make_block_ptr(\n            dhbt + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,)\n        )\n        b_dhb += tl.load(p_dhbt, boundary_check=(0,), padding_option=\"zero\")\n\n    offs_v = tl.arange(0, BV)\n    offs_t = tl.arange(0, BT)\n    b_w = tl.load(w + i_h * V + offs_v, mask=offs_v < V, other=0.0)\n    b_b = tl.load(b + i_h * V + offs_v, mask=offs_v < V, other=0.0)\n    b_dw = tl.zeros(\n        [\n            BV,\n        ],\n        dtype=b_w.dtype,\n    )\n    b_db = tl.zeros(\n        [\n            BV,\n        ],\n        dtype=b_b.dtype,\n    )\n    p_dw = tl.make_block_ptr(dw + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\n    p_db = tl.make_block_ptr(db + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\n\n    for i_t in range(NT - 1, -1, -1):\n        p_h = tl.make_block_ptr(\n            h + ((boh + i_t) * H + i_h) * K * V,\n            (V, K),\n            (1, V),\n            (i_v * BV, i_k * BK),\n            (BV, BK),\n            (0, 1),\n        )\n        p_dh = tl.make_block_ptr(\n            dh + ((boh + i_t) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        p_dhb = tl.make_block_ptr(\n            dhb + ((boh + i_t) * H + i_h) * V, (V,), (1,), (i_v * BV,), (BV,), (0,)\n        )\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dhb, b_dhb.to(p_dhb.dtype.element_ty), boundary_check=(0,))\n        p_q = tl.make_block_ptr(\n            q + (bos * H + i_h) * K,\n            (K, T),\n            (1, H * K),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_v_new = tl.make_block_ptr(\n            v_new + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_x = tl.make_block_ptr(\n            x + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_y = tl.make_block_ptr(\n            y + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dv_new = tl.make_block_ptr(\n            dv_new + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dv = tl.make_block_ptr(\n            dv + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dk = tl.make_block_ptr(\n            dk + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_do = tl.make_block_ptr(\n            do + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_r = tl.make_block_ptr(\n            r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0)\n        )\n        p_eta_last = (\n            eta + bos * H + i_h + (T - 1) * H\n            if i_t == NT - 1\n            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H\n        )\n        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")\n        b_dv_new = tl.load(p_dv_new, boundary_check=(0, 1), padding_option=\"zero\").to(\n            b_k.dtype\n        )\n        b_eta_last = tl.load(p_eta_last)\n        b_dv_new -= tl.dot(b_eta_last * b_k, b_dh.to(b_k.dtype))\n        b_dv_new -= b_eta_last * b_dhb.to(b_k.dtype)[None, :]\n\n        b_v_new = tl.load(p_v_new, boundary_check=(0, 1), padding_option=\"zero\")\n        b_x = tl.load(p_x, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)\n        b_y = tl.load(p_y, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)\n        b_rstd = tl.load(p_r, boundary_check=(0, 1), padding_option=\"zero\").to(\n            tl.float32\n        )\n        b_dy = (\n            b_rstd\n            * (\n                b_dv_new * V\n                - tl.sum(b_dv_new, axis=1, keep_dims=True)\n                - b_x * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)\n            )\n            / V\n        )\n        b_dx = (\n            -b_rstd\n            * (\n                b_dv_new * tl.sum(b_x * b_y, axis=1, keep_dims=True)\n                + b_y * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)\n            )\n            / V\n        )\n        b_drstd = tl.sum(\n            b_dv_new.to(b_rstd.dtype) * b_v_new.to(b_rstd.dtype) / b_rstd,\n            axis=1,\n            keep_dims=True,\n        )\n\n        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")\n        b_w = b_w.to(b_k.dtype)\n        b_b = b_b.to(b_k.dtype)\n        b_dv = -b_w * b_dy.to(b_k.dtype)\n        b_dk = b_w * b_dy.to(b_k.dtype)\n        b_dw += tl.sum(\n            2 * b_w * b_x * b_dy.to(b_k.dtype)\n            + (b_b - b_v.to(b_k.dtype) + b_k) * b_dy.to(b_k.dtype),\n            axis=0,\n        ).to(b_dw.dtype)\n        b_db += tl.sum(b_w * b_dy.to(b_k.dtype), axis=0).to(b_db.dtype)\n        b_dx = b_dx.to(b_k.dtype) + b_w * b_w * b_dy.to(b_k.dtype)\n\n        b_q = tl.load(p_q, boundary_check=(0, 1), padding_option=\"zero\")\n        b_h = tl.load(p_h, boundary_check=(0, 1), padding_option=\"zero\")\n        b_do = tl.load(p_do, boundary_check=(0, 1), padding_option=\"zero\")\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_dkh = (\n            b_rstd\n            * (\n                V * b_dx\n                - tl.sum(b_dx, axis=1, keep_dims=True)\n                - b_x * tl.sum(b_x * b_dx, axis=1, keep_dims=True)\n            )\n            / V\n        )\n        b_dkh -= b_rstd * b_rstd * b_drstd * b_x / V\n        b_dkh = tl.where(\n            (offs_v < V)[None, :] * (offs_t < T - i_t * BT)[:, None], b_dkh, 0.0\n        )\n        b_dk += tl.dot(b_dkh, b_h.to(b_dkh.dtype)).to(b_k.dtype)\n        b_dh += tl.dot(b_q, b_do.to(b_q.dtype)) + tl.dot(\n            tl.trans(b_k).to(b_dkh.dtype), b_dkh\n        )\n        b_dhb += tl.sum(b_do + b_dkh, axis=0)\n        b_dh = tl.where((offs_v < V)[None, :], b_dh, 0.0)\n        b_dhb = tl.where((offs_v < V), b_dhb, 0.0)\n\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dw, b_dw.to(p_dw.dtype.element_ty), boundary_check=(0,))\n    tl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0,))\n\n    if USE_INITIAL_STATE:\n        p_dh0 = tl.make_block_ptr(\n            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))\n    if USE_INITIAL_STATE_B:\n        p_dhb0 = tl.make_block_ptr(\n            dhb0 + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,)\n        )\n        tl.store(p_dhb0, b_dhb.to(p_dhb0.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 314, 210, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 98, 227, 98, 228, 98, 229, 98, 230, 98, 231, 98, 232, 98, 233, 98, 234, 98, 235, 98, 236, 98, 237, 98, 238, 98, 239, 98, 240, 98, 241, 57, 6, 98, 242, 57, 6, 98, 243, 57, 6, 98, 244, 57, 6, 98, 245, 57, 6, 98, 246, 57, 6, 98, 247, 57, 6, 98, 248, 57, 6, 98, 249, 57, 6, 98, 250, 57, 6, 98, 251, 57, 6, 152, 57, 31, -1, 252, 98, 253, 98, 254, 167, 210, 148, 210, 315, 152, 98, 148, 210, 316, 152, 98, 148, 210, 317, 152, 152, 31, 255, 98, 256, 167, 210, 254, 46, 241, 98, 254, 194, 241, 152, 31, 158, 251, 57, 31, 257, 98, 258, 167, 210, 51, 210, 237, 67, 255, 152, 74, 259, 210, 213, 152, 98, 51, 210, 237, 67, 255, 67, 316, 152, 74, 259, 210, 213, 152, 152, 31, 240, 167, 258, 4, 257, 31, 260, 167, 59, 210, 240, 98, 244, 152, 31, 261, 167, 51, 210, 238, 67, 255, 152, 74, 259, 210, 213, 152, 31, 161, 31, 29, 57, 31, 257, 98, 258, 167, 210, 255, 211, 240, 98, 255, 211, 240, 67, 240, 152, 31, 260, 167, 59, 210, 240, 98, 244, 152, 31, 261, 167, 255, 211, 260, 31, 52, 31, 262, 167, 153, 210, 198, 245, 98, 246, 26, 98, 82, 167, 130, 152, 31, 263, 167, 153, 210, 198, 246, 26, 98, 82, 167, 130, 152, 31, 158, 247, 57, 31, 264, 167, 189, 210, 225, 67, 254, 211, 242, 211, 243, 98, 210, 242, 98, 243, 152, 98, 210, 243, 98, 316, 152, 98, 210, 252, 211, 245, 98, 253, 211, 246, 152, 98, 210, 245, 98, 246, 152, 98, 210, 316, 98, 315, 152, 152, 31, 262, 149, 51, 210, 264, 98, 265, 167, 210, 315, 98, 316, 152, 98, 266, 167, 318, 152, 31, 161, 31, 158, 248, 57, 31, 267, 167, 189, 210, 226, 67, 254, 211, 243, 98, 210, 243, 98, 152, 98, 210, 316, 98, 152, 98, 210, 253, 211, 246, 98, 152, 98, 210, 246, 98, 152, 98, 210, 315, 98, 152, 152, 31, 263, 149, 51, 210, 267, 98, 265, 167, 210, 315, 98, 152, 98, 266, 167, 318, 152, 31, 161, 31, 268, 167, 68, 210, 315, 98, 246, 152, 31, 269, 167, 68, 210, 315, 98, 244, 152, 31, 270, 167, 51, 210, 221, 67, 256, 211, 243, 67, 268, 98, 271, 167, 268, 1, 243, 98, 272, 167, 315, 152, 31, 273, 167, 51, 210, 222, 67, 256, 211, 243, 67, 268, 98, 271, 167, 268, 1, 243, 98, 272, 167, 315, 152, 31, 274, 167, 153, 210, 198, 246, 26, 98, 82, 167, 270, 74, 82, 152, 31, 275, 167, 153, 210, 198, 246, 26, 98, 82, 167, 273, 74, 82, 152, 31, 276, 167, 189, 210, 235, 67, 254, 211, 243, 98, 210, 243, 98, 152, 98, 210, 316, 98, 152, 98, 210, 253, 211, 246, 98, 152, 98, 210, 246, 98, 152, 98, 210, 315, 98, 152, 152, 31, 277, 167, 189, 210, 236, 67, 254, 211, 243, 98, 210, 243, 98, 152, 98, 210, 316, 98, 152, 98, 210, 253, 211, 246, 98, 152, 98, 210, 246, 98, 152, 98, 210, 315, 98, 152, 152, 31, 121, 278, 139, 5, 210, 260, 4, 316, 98, 4, 316, 98, 4, 316, 152, 57, 31, 279, 167, 189, 210, 224, 67, 210, 210, 261, 67, 278, 152, 211, 241, 67, 256, 152, 211, 242, 211, 243, 98, 210, 243, 98, 242, 152, 98, 210, 316, 98, 243, 152, 98, 210, 253, 211, 246, 98, 252, 211, 245, 152, 98, 210, 246, 98, 245, 152, 98, 210, 315, 98, 316, 152, 152, 31, 280, 167, 189, 210, 230, 67, 210, 210, 261, 67, 278, 152, 211, 241, 67, 256, 152, 211, 242, 211, 243, 98, 210, 242, 98, 243, 152, 98, 210, 243, 98, 316, 152, 98, 210, 252, 211, 245, 98, 253, 211, 246, 152, 98, 210, 245, 98, 246, 152, 98, 210, 316, 98, 315, 152, 152, 31, 281, 167, 189, 210, 231, 67, 210, 210, 261, 67, 278, 152, 211, 241, 67, 256, 152, 211, 243, 98, 210, 243, 98, 152, 98, 210, 316, 98, 152, 98, 210, 253, 211, 246, 98, 152, 98, 210, 246, 98, 152, 98, 210, 315, 98, 152, 152, 31, 10, 210, 280, 98, 262, 74, 259, 210, 280, 74, 82, 74, 104, 152, 98, 265, 167, 210, 315, 98, 316, 152, 152, 31, 10, 210, 281, 98, 263, 74, 259, 210, 281, 74, 82, 74, 104, 152, 98, 265, 167, 210, 315, 98, 152, 152, 31, 282, 167, 189, 210, 214, 67, 210, 257, 211, 241, 67, 256, 152, 211, 242, 98, 210, 242, 98, 240, 152, 98, 210, 316, 98, 241, 211, 242, 152, 98, 210, 252, 211, 245, 98, 278, 211, 244, 152, 98, 210, 245, 98, 244, 152, 98, 210, 315, 98, 316, 152, 152, 31, 283, 167, 189, 210, 215, 67, 210, 257, 211, 241, 67, 256, 152, 211, 242, 98, 210, 240, 98, 242, 152, 98, 210, 241, 211, 242, 98, 316, 152, 98, 210, 278, 211, 244, 98, 252, 211, 245, 152, 98, 210, 244, 98, 245, 152, 98, 210, 316, 98, 315, 152, 152, 31, 284, 167, 189, 210, 216, 67, 210, 257, 211, 241, 67, 256, 152, 211, 243, 98, 210, 240, 98, 243, 152, 98, 210, 241, 211, 243, 98, 316, 152, 98, 210, 278, 211, 244, 98, 253, 211, 246, 152, 98, 210, 244, 98, 246, 152, 98, 210, 316, 98, 315, 152, 152, 31, 285, 167, 189, 210, 217, 67, 210, 257, 211, 241, 67, 256, 152, 211, 243, 98, 210, 240, 98, 243, 152, 98, 210, 241, 211, 243, 98, 316, 152, 98, 210, 278, 211, 244, 98, 253, 211, 246, 152, 98, 210, 244, 98, 246, 152, 98, 210, 316, 98, 315, 152, 152, 31, 286, 167, 189, 210, 218, 67, 210, 257, 211, 241, 67, 256, 152, 211, 243, 98, 210, 240, 98, 243, 152, 98, 210, 241, 211, 243, 98, 316, 152, 98, 210, 278, 211, 244, 98, 253, 211, 246, 152, 98, 210, 244, 98, 246, 152, 98, 210, 316, 98, 315, 152, 152, 31, 287, 167, 189, 210, 219, 67, 210, 257, 211, 241, 67, 256, 152, 211, 243, 98, 210, 240, 98, 243, 152, 98, 210, 241, 211, 243, 98, 316, 152, 98, 210, 278, 211, 244, 98, 253, 211, 246, 152, 98, 210, 244, 98, 246, 152, 98, 210, 316, 98, 315, 152, 152, 31, 288, 167, 189, 210, 233, 67, 210, 257, 211, 241, 67, 256, 152, 211, 243, 98, 210, 240, 98, 243, 152, 98, 210, 241, 211, 243, 98, 316, 152, 98, 210, 278, 211, 244, 98, 253, 211, 246, 152, 98, 210, 244, 98, 246, 152, 98, 210, 316, 98, 315, 152, 152, 31, 289, 167, 189, 210, 232, 67, 210, 257, 211, 241, 67, 256, 152, 211, 243, 98, 210, 240, 98, 243, 152, 98, 210, 241, 211, 243, 98, 316, 152, 98, 210, 278, 211, 244, 98, 253, 211, 246, 152, 98, 210, 244, 98, 246, 152, 98, 210, 316, 98, 315, 152, 152, 31, 290, 167, 189, 210, 234, 67, 210, 257, 211, 241, 67, 256, 152, 211, 242, 98, 210, 240, 98, 242, 152, 98, 210, 241, 211, 242, 98, 316, 152, 98, 210, 278, 211, 244, 98, 252, 211, 245, 152, 98, 210, 244, 98, 245, 152, 98, 210, 316, 98, 315, 152, 152, 31, 291, 167, 189, 210, 229, 67, 210, 257, 211, 241, 67, 256, 152, 211, 243, 98, 210, 240, 98, 243, 152, 98, 210, 241, 211, 243, 98, 316, 152, 98, 210, 278, 211, 244, 98, 253, 211, 246, 152, 98, 210, 244, 98, 246, 152, 98, 210, 316, 98, 315, 152, 152, 31, 292, 167, 189, 210, 220, 67, 257, 211, 241, 67, 256, 98, 210, 240, 98, 316, 152, 98, 210, 241, 98, 316, 152, 98, 210, 278, 211, 244, 98, 315, 152, 98, 210, 244, 98, 316, 152, 98, 210, 316, 98, 315, 152, 152, 31, 293, 167, 223, 67, 257, 211, 241, 67, 256, 67, 210, 240, 4, 316, 152, 211, 241, 158, 278, 69, 260, 4, 316, 29, 223, 67, 257, 211, 241, 67, 256, 67, 210, 278, 211, 244, 67, 244, 4, 316, 152, 211, 241, 31, 294, 167, 51, 210, 283, 98, 265, 167, 210, 315, 98, 316, 152, 98, 266, 167, 318, 152, 31, 295, 167, 51, 210, 288, 98, 265, 167, 210, 315, 98, 316, 152, 98, 266, 167, 318, 152, 74, 259, 210, 294, 74, 82, 152, 31, 296, 167, 51, 210, 293, 152, 31, 295, 2, 15, 210, 296, 211, 294, 98, 262, 74, 259, 210, 294, 74, 82, 152, 152, 31, 295, 2, 296, 211, 263, 74, 259, 210, 294, 74, 82, 152, 198, 173, 98, 57, 26, 31, 297, 167, 51, 210, 285, 98, 265, 167, 210, 315, 98, 316, 152, 98, 266, 167, 318, 152, 31, 298, 167, 51, 210, 286, 98, 265, 167, 210, 315, 98, 316, 152, 98, 266, 167, 318, 152, 74, 259, 210, 294, 74, 82, 152, 31, 299, 167, 51, 210, 287, 98, 265, 167, 210, 315, 98, 316, 152, 98, 266, 167, 318, 152, 74, 259, 210, 294, 74, 82, 152, 31, 300, 167, 51, 210, 292, 98, 265, 167, 210, 315, 98, 316, 152, 98, 266, 167, 318, 152, 74, 259, 210, 130, 152, 31, 301, 167, 300, 211, 210, 295, 211, 243, 4, 191, 210, 295, 98, 302, 167, 316, 98, 303, 167, 142, 152, 4, 298, 211, 191, 210, 295, 211, 298, 98, 302, 167, 316, 98, 303, 167, 142, 152, 152, 41, 243, 31, 304, 167, 4, 300, 211, 210, 295, 211, 191, 210, 298, 211, 299, 98, 302, 167, 316, 98, 303, 167, 142, 152, 67, 299, 211, 191, 210, 295, 211, 298, 98, 302, 167, 316, 98, 303, 167, 142, 152, 152, 41, 243, 31, 305, 167, 191, 210, 295, 74, 259, 210, 300, 74, 82, 152, 211, 297, 74, 259, 210, 300, 74, 82, 152, 41, 300, 98, 302, 167, 316, 98, 303, 167, 142, 152, 31, 306, 167, 51, 210, 284, 98, 265, 167, 210, 315, 98, 316, 152, 98, 266, 167, 318, 152, 31, 270, 167, 270, 74, 259, 210, 294, 74, 82, 152, 31, 273, 167, 273, 74, 259, 210, 294, 74, 82, 152, 31, 307, 167, 4, 270, 211, 301, 74, 259, 210, 294, 74, 82, 152, 31, 308, 167, 270, 211, 301, 74, 259, 210, 294, 74, 82, 152, 31, 274, 149, 191, 210, 317, 211, 270, 211, 298, 211, 301, 74, 259, 210, 294, 74, 82, 152, 67, 210, 273, 4, 306, 74, 259, 210, 294, 74, 82, 152, 67, 294, 152, 211, 301, 74, 259, 210, 294, 74, 82, 152, 98, 302, 167, 315, 152, 74, 259, 210, 274, 74, 82, 152, 31, 275, 149, 191, 210, 270, 211, 301, 74, 259, 210, 294, 74, 82, 152, 98, 302, 167, 315, 152, 74, 259, 210, 275, 74, 82, 152, 31, 304, 167, 304, 74, 259, 210, 294, 74, 82, 152, 67, 270, 211, 270, 211, 301, 74, 259, 210, 294, 74, 82, 152, 31, 309, 167, 51, 210, 282, 98, 265, 167, 210, 315, 98, 316, 152, 98, 266, 167, 318, 152, 31, 310, 167, 51, 210, 279, 98, 265, 167, 210, 315, 98, 316, 152, 98, 266, 167, 318, 152, 31, 311, 167, 51, 210, 291, 98, 265, 167, 210, 315, 98, 316, 152, 98, 266, 167, 318, 152, 31, 309, 167, 210, 309, 211, 239, 152, 74, 259, 210, 309, 74, 82, 152, 31, 312, 167, 300, 211, 210, 243, 211, 304, 4, 191, 210, 304, 98, 302, 167, 316, 98, 303, 167, 142, 152, 4, 298, 211, 191, 210, 298, 211, 304, 98, 302, 167, 316, 98, 303, 167, 142, 152, 152, 41, 243, 31, 312, 2, 300, 211, 300, 211, 305, 211, 298, 41, 243, 31, 312, 167, 176, 210, 210, 268, 1, 243, 152, 198, 173, 98, 57, 26, 211, 210, 269, 1, 240, 4, 278, 211, 244, 152, 198, 57, 98, 173, 26, 98, 312, 98, 315, 152, 31, 308, 149, 15, 210, 312, 98, 310, 74, 259, 210, 312, 74, 82, 152, 152, 74, 259, 210, 294, 74, 82, 152, 31, 262, 149, 15, 210, 309, 98, 311, 74, 259, 210, 309, 74, 82, 152, 152, 67, 15, 210, 65, 210, 294, 152, 74, 259, 210, 312, 74, 82, 152, 98, 312, 152, 31, 263, 149, 191, 210, 311, 67, 312, 98, 302, 167, 315, 152, 31, 262, 167, 176, 210, 210, 268, 1, 243, 152, 198, 173, 98, 57, 26, 98, 262, 98, 315, 152, 31, 263, 167, 176, 210, 268, 1, 243, 98, 263, 98, 315, 152, 31, 10, 210, 289, 98, 307, 74, 259, 210, 289, 74, 82, 74, 104, 152, 98, 265, 167, 210, 315, 98, 316, 152, 152, 31, 10, 210, 290, 98, 308, 74, 259, 210, 290, 74, 82, 74, 104, 152, 98, 265, 167, 210, 315, 98, 316, 152, 152, 31, 70, 31, 10, 210, 276, 98, 274, 74, 259, 210, 276, 74, 82, 74, 104, 152, 98, 265, 167, 210, 315, 98, 152, 152, 31, 10, 210, 277, 98, 275, 74, 259, 210, 277, 74, 82, 74, 104, 152, 98, 265, 167, 210, 315, 98, 152, 152, 31, 158, 249, 57, 31, 313, 167, 189, 210, 227, 67, 254, 211, 242, 211, 243, 98, 210, 242, 98, 243, 152, 98, 210, 243, 98, 316, 152, 98, 210, 252, 211, 245, 98, 253, 211, 246, 152, 98, 210, 245, 98, 246, 152, 98, 210, 316, 98, 315, 152, 152, 31, 10, 210, 313, 98, 262, 74, 259, 210, 313, 74, 82, 74, 104, 152, 98, 265, 167, 210, 315, 98, 316, 152, 152, 31, 161, 31, 158, 250, 57, 31, 319, 167, 189, 210, 228, 67, 254, 211, 243, 98, 210, 243, 98, 152, 98, 210, 316, 98, 152, 98, 210, 253, 211, 246, 98, 152, 98, 210, 246, 98, 152, 98, 210, 315, 98, 152, 152, 31, 10, 210, 319, 98, 263, 74, 259, 210, 319, 74, 82, 74, 104, 152, 98, 265, 167, 210, 315, 98, 152, 152, 31, 161, 31, 3, 31]}, {"code": "def chunk_bwd_kernel_dqke(\n    q,\n    k,\n    v,\n    e,\n    h,\n    do,\n    dh,\n    dhb,\n    dq,\n    dk,\n    de,\n    cu_seqlens,\n    chunk_indices,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    v += (bos * H + i_h) * V\n    do += (bos * H + i_h) * V\n    h += (i_tg * H + i_h) * K * V\n    dh += (i_tg * H + i_h) * K * V\n    dhb += (i_tg * H + i_h) * V\n    q += (bos * H + i_h) * K\n    k += (bos * H + i_h) * K\n    dq += (bos * H + i_h) * K\n    dk += (bos * H + i_h) * K\n    e += bos * H + i_h\n    de += bos * H + i_h\n    stride_qk = H * K\n    stride_vo = H * V\n    stride_e = H\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    b_de = tl.zeros(\n        [\n            BT,\n        ],\n        dtype=tl.float32,\n    )\n\n    p_k = tl.make_block_ptr(\n        k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    p_e_last = (\n        (e + (i_t * BT + BT - 1) * stride_e)\n        if (i_t * BT + BT) <= T\n        else (e + (T - 1) * stride_e)\n    )\n    i_last = (BT - 1) if (i_t * BT + BT) <= T else (T % BT - 1)\n    mask = tl.arange(0, BT) == i_last\n    b_e_last = tl.load(p_e_last)\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_do = tl.make_block_ptr(\n            do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_h = tl.make_block_ptr(\n            h, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)\n        )\n        p_dh = tl.make_block_ptr(\n            dh, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)\n        )\n        p_dhb = tl.make_block_ptr(dhb, (V,), (1,), (i_v * BV,), (BV,), (0,))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        b_dhb = tl.load(p_dhb, boundary_check=(0,))\n\n        b_ds += tl.dot(b_do, tl.trans(b_v))\n\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n\n        b_dk -= b_e_last * tl.dot(b_v, b_dh.to(b_v.dtype))\n        b_de -= mask * tl.sum(tl.trans(b_dh) * tl.dot(tl.trans(b_k), b_v.to(b_k.dtype)))\n        b_de -= mask * tl.sum(b_dhb * tl.sum(b_v, axis=0).to(b_k.dtype))\n\n    o_i = tl.arange(0, BT)\n    p_q = tl.make_block_ptr(\n        q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_e = tl.make_block_ptr(e, (T,), (stride_e,), (i_t * BT,), (BT,), (0,))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_e = tl.load(p_e, boundary_check=(0,))\n\n    p_dq = tl.make_block_ptr(\n        dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_dk = tl.make_block_ptr(\n        dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_de = tl.make_block_ptr(de, (T,), (stride_e,), (i_t * BT,), (BT,), (0,))\n\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds, 0)\n    b_ds = b_ds.to(b_k.dtype)\n    b_dq -= tl.dot(b_ds, b_k) * b_e[:, None]\n    b_dk -= tl.dot(tl.trans(b_ds), b_q * b_e[:, None]) * scale\n    b_de -= tl.sum(scale * tl.dot(b_ds, b_k) * b_q, axis=1)\n    b_de -= tl.sum(b_ds, axis=1)\n    b_dq *= scale\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_de, b_de.to(p_de.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 314, 210, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 99, 224, 99, 225, 99, 226, 99, 227, 99, 228, 99, 229, 58, 6, 99, 230, 58, 6, 99, 231, 58, 6, 99, 232, 58, 6, 99, 233, 58, 6, 99, 234, 58, 6, 99, 235, 58, 6, 99, 236, 58, 6, 152, 58, 31, -1, 237, 99, 238, 99, 239, 167, 210, 148, 210, 315, 152, 99, 148, 210, 316, 152, 99, 148, 210, 317, 152, 152, 31, 240, 99, 241, 167, 210, 239, 46, 230, 99, 239, 194, 230, 152, 31, 158, 236, 58, 31, 242, 167, 238, 31, 243, 99, 238, 167, 210, 53, 210, 226, 68, 238, 211, 317, 152, 75, 244, 210, 57, 152, 99, 53, 210, 226, 68, 238, 211, 317, 68, 316, 152, 75, 244, 210, 57, 152, 152, 31, 245, 99, 246, 167, 210, 53, 210, 225, 68, 243, 152, 75, 244, 210, 57, 152, 99, 53, 210, 225, 68, 243, 68, 316, 152, 75, 244, 210, 57, 152, 152, 31, 228, 167, 246, 4, 245, 31, 247, 167, 60, 210, 228, 99, 233, 152, 31, 161, 31, 29, 58, 31, 247, 167, 60, 210, 228, 99, 233, 152, 31, 242, 167, 240, 211, 247, 68, 238, 31, 245, 99, 246, 167, 210, 240, 211, 228, 99, 240, 211, 228, 68, 228, 152, 31, 52, 31, 216, 149, 210, 245, 211, 230, 68, 241, 152, 211, 232, 31, 219, 149, 210, 245, 211, 230, 68, 241, 152, 211, 232, 31, 218, 149, 210, 242, 211, 230, 68, 241, 152, 211, 231, 211, 232, 31, 220, 149, 210, 242, 211, 230, 68, 241, 152, 211, 231, 211, 232, 31, 221, 149, 210, 242, 211, 230, 68, 241, 152, 211, 232, 31, 214, 149, 210, 245, 211, 230, 68, 241, 152, 211, 231, 31, 215, 149, 210, 245, 211, 230, 68, 241, 152, 211, 231, 31, 222, 149, 210, 245, 211, 230, 68, 241, 152, 211, 231, 31, 223, 149, 210, 245, 211, 230, 68, 241, 152, 211, 231, 31, 217, 149, 245, 211, 230, 68, 241, 31, 224, 149, 245, 211, 230, 68, 241, 31, 248, 167, 230, 211, 231, 31, 249, 167, 230, 211, 232, 31, 250, 167, 230, 31, 251, 167, 153, 210, 198, 233, 99, 234, 26, 99, 83, 167, 131, 152, 31, 252, 167, 153, 210, 198, 233, 99, 234, 26, 99, 83, 167, 131, 152, 31, 253, 167, 153, 210, 198, 233, 99, 233, 26, 99, 83, 167, 131, 152, 31, 254, 167, 153, 210, 198, 233, 26, 99, 83, 167, 131, 152, 31, 255, 167, 189, 210, 215, 99, 210, 228, 99, 231, 152, 99, 210, 248, 99, 316, 152, 99, 210, 238, 211, 233, 99, 237, 211, 234, 152, 99, 210, 233, 99, 234, 152, 99, 210, 316, 99, 315, 152, 152, 31, 256, 167, 53, 210, 255, 99, 257, 167, 210, 315, 99, 316, 152, 152, 31, 258, 167, 217, 68, 210, 238, 211, 233, 68, 233, 4, 316, 152, 211, 250, 158, 238, 211, 233, 68, 233, 188, 228, 29, 217, 68, 210, 228, 4, 316, 152, 211, 250, 31, 259, 167, 233, 4, 316, 158, 238, 211, 233, 68, 233, 188, 228, 29, 228, 194, 233, 4, 316, 31, 260, 167, 69, 210, 315, 99, 233, 152, 70, 259, 31, 261, 167, 53, 210, 258, 152, 31, 122, 262, 139, 5, 210, 60, 210, 232, 99, 235, 152, 152, 58, 31, 263, 167, 189, 210, 216, 99, 210, 228, 99, 232, 152, 99, 210, 249, 99, 316, 152, 99, 210, 238, 211, 233, 99, 262, 211, 235, 152, 99, 210, 233, 99, 235, 152, 99, 210, 316, 99, 315, 152, 152, 31, 264, 167, 189, 210, 219, 99, 210, 228, 99, 232, 152, 99, 210, 249, 99, 316, 152, 99, 210, 238, 211, 233, 99, 262, 211, 235, 152, 99, 210, 233, 99, 235, 152, 99, 210, 316, 99, 315, 152, 152, 31, 265, 167, 189, 210, 218, 99, 210, 232, 99, 231, 152, 99, 210, 316, 99, 232, 152, 99, 210, 262, 211, 235, 99, 237, 211, 234, 152, 99, 210, 235, 99, 234, 152, 99, 210, 315, 99, 316, 152, 152, 31, 266, 167, 189, 210, 220, 99, 210, 232, 99, 231, 152, 99, 210, 316, 99, 232, 152, 99, 210, 262, 211, 235, 99, 237, 211, 234, 152, 99, 210, 235, 99, 234, 152, 99, 210, 315, 99, 316, 152, 152, 31, 267, 167, 189, 210, 221, 99, 210, 232, 99, 152, 99, 210, 316, 99, 152, 99, 210, 262, 211, 235, 99, 152, 99, 210, 235, 99, 152, 99, 210, 315, 99, 152, 152, 31, 268, 167, 53, 210, 263, 99, 257, 167, 210, 315, 99, 316, 152, 152, 31, 269, 167, 53, 210, 264, 99, 257, 167, 210, 315, 99, 316, 152, 152, 31, 270, 167, 53, 210, 265, 99, 257, 167, 210, 315, 99, 316, 152, 152, 31, 271, 167, 53, 210, 266, 99, 257, 167, 210, 315, 99, 316, 152, 152, 31, 272, 167, 53, 210, 267, 99, 257, 167, 210, 315, 99, 152, 152, 31, 253, 149, 15, 210, 269, 99, 66, 210, 268, 152, 152, 31, 251, 149, 15, 210, 269, 99, 270, 75, 244, 210, 269, 75, 83, 152, 152, 31, 252, 2, 261, 211, 15, 210, 268, 99, 271, 75, 244, 210, 268, 75, 83, 152, 152, 31, 254, 2, 260, 211, 191, 210, 66, 210, 271, 152, 211, 15, 210, 66, 210, 256, 152, 99, 268, 75, 244, 210, 256, 75, 83, 152, 152, 152, 31, 254, 2, 260, 211, 191, 210, 272, 211, 191, 210, 268, 99, 273, 167, 315, 152, 75, 244, 210, 256, 75, 83, 152, 152, 31, 71, 31, 274, 167, 69, 210, 315, 99, 233, 152, 31, 275, 167, 189, 210, 214, 99, 210, 228, 99, 231, 152, 99, 210, 248, 99, 316, 152, 99, 210, 238, 211, 233, 99, 237, 211, 234, 152, 99, 210, 233, 99, 234, 152, 99, 210, 316, 99, 315, 152, 152, 31, 276, 167, 189, 210, 217, 99, 210, 228, 99, 152, 99, 210, 250, 99, 152, 99, 210, 238, 211, 233, 99, 152, 99, 210, 233, 99, 152, 99, 210, 315, 99, 152, 152, 31, 277, 167, 53, 210, 275, 99, 257, 167, 210, 315, 99, 316, 152, 152, 31, 278, 167, 53, 210, 276, 99, 257, 167, 210, 315, 99, 152, 152, 31, 279, 167, 189, 210, 222, 99, 210, 228, 99, 231, 152, 99, 210, 248, 99, 316, 152, 99, 210, 238, 211, 233, 99, 237, 211, 234, 152, 99, 210, 233, 99, 234, 152, 99, 210, 316, 99, 315, 152, 152, 31, 280, 167, 189, 210, 223, 99, 210, 228, 99, 231, 152, 99, 210, 248, 99, 316, 152, 99, 210, 238, 211, 233, 99, 237, 211, 234, 152, 99, 210, 233, 99, 234, 152, 99, 210, 316, 99, 315, 152, 152, 31, 281, 167, 189, 210, 224, 99, 210, 228, 99, 152, 99, 210, 250, 99, 152, 99, 210, 238, 211, 233, 99, 152, 99, 210, 233, 99, 152, 99, 210, 315, 99, 152, 152, 31, 253, 167, 176, 210, 274, 198, 58, 99, 173, 26, 130, 274, 198, 173, 99, 58, 26, 99, 253, 99, 315, 152, 31, 253, 167, 253, 75, 244, 210, 256, 75, 83, 152, 31, 251, 2, 15, 210, 253, 99, 256, 152, 211, 278, 198, 58, 99, 173, 26, 31, 252, 2, 15, 210, 66, 210, 253, 152, 99, 277, 211, 278, 198, 58, 99, 173, 26, 152, 211, 227, 31, 254, 2, 191, 210, 227, 211, 15, 210, 253, 99, 256, 152, 211, 277, 99, 273, 167, 316, 152, 31, 254, 2, 191, 210, 253, 99, 273, 167, 316, 152, 31, 251, 23, 227, 31, 10, 210, 279, 99, 251, 75, 244, 210, 279, 75, 83, 75, 105, 152, 99, 257, 167, 210, 315, 99, 316, 152, 152, 31, 10, 210, 280, 99, 252, 75, 244, 210, 280, 75, 83, 75, 105, 152, 99, 257, 167, 210, 315, 99, 316, 152, 152, 31, 10, 210, 281, 99, 254, 75, 244, 210, 281, 75, 83, 75, 105, 152, 99, 257, 167, 210, 315, 99, 152, 152, 31, 3, 31]}, {"code": "def fused_chunk_ttt_linear_fwd_kernel(\n    q,\n    k,\n    v,\n    eta,\n    w,\n    b,\n    o,\n    scale,\n    eps,\n    h0,\n    hb0,\n    ht,\n    hbt,\n    cu_seqlens,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    USE_INITIAL_STATE_B: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_nh = tl.program_id(0)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n        NT = tl.cdiv(T, BT)\n\n    o_i = tl.arange(0, BT)\n    v_i = tl.arange(0, BV)\n    m_A = o_i[:, None] >= o_i[None, :]\n    b_w = tl.load(w + i_h * V + v_i, mask=v_i < V, other=0.0)\n    b_b = tl.load(b + i_h * V + v_i, mask=v_i < V, other=0.0)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    b_hb = tl.zeros([BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(\n            h0 + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)\n        )\n        b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)\n    if USE_INITIAL_STATE_B:\n        p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\n        b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option=\"zero\").to(tl.float32)\n\n    for i_t in range(NT):\n        p_q = tl.make_block_ptr(\n            q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n        )\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n        )\n        p_o = tl.make_block_ptr(\n            o + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n        )\n        p_e = tl.make_block_ptr(\n            eta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)\n        )\n        p_e_last = (\n            eta + bos * H + i_h + (T - 1) * H\n            if i_t == NT - 1\n            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")\n\n        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")\n\n        b_kh = (\n            tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32)\n            + b_hb[None, :]\n        )\n        b_kh = tl.where((v_i < V)[None, :], b_kh, 0.0)\n        mean = tl.sum(b_kh, axis=1, keep_dims=True) / V\n        xbar = tl.where((v_i < V)[None, :], b_kh - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V\n        rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)\n        b_kh_hat = (b_kh - mean) * rstd\n\n        b_v = (\n            b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype)\n            + b_b[None, :].to(b_k.dtype)\n            - b_v.to(b_k.dtype)\n            + tl.trans(b_k)\n        )\n        b_v = tl.where((v_i < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)\n        b_v2 = (\n            rstd\n            * (\n                V * b_v\n                - tl.sum(b_v, axis=1, keep_dims=True)\n                - b_kh_hat.to(b_k.dtype)\n                * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)\n            )\n            / V\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1), padding_option=\"zero\")\n\n        b_e = tl.load(p_e, boundary_check=(0,), padding_option=\"zero\")\n        b_q = (b_q * scale).to(b_k.dtype)\n\n        b_A = tl.dot(b_q, b_k, allow_tf32=False)\n        b_A = tl.where(m_A, b_A, 0)\n        b_Ae = tl.where(m_A, b_e[:, None], 0.0)\n\n        b_o = -tl.dot(b_e[:, None] * b_A.to(b_v2.dtype), b_v2, allow_tf32=False)\n        b_o += b_hb[None, :] - tl.dot(b_Ae.to(b_v2.dtype), b_v2, allow_tf32=False)\n        b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n        b_e_last = tl.load(p_e_last)\n        b_h = b_h - tl.dot(b_e_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)\n        b_hb = b_hb - tl.sum(b_e_last * b_v2.to(b_k.dtype), axis=0)\n        b_h = tl.where((v_i < V)[None, :], b_h, 0.0)\n        b_hb = tl.where((v_i < V), b_hb, 0.0)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(\n            ht + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)\n        )\n        p_hbt = tl.make_block_ptr(hbt + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_hbt, b_hb.to(p_hbt.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 314, 210, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 98, 227, 98, 228, 98, 229, 57, 6, 98, 230, 57, 6, 98, 231, 57, 6, 98, 232, 57, 6, 98, 233, 57, 6, 98, 234, 57, 6, 98, 235, 57, 6, 98, 236, 57, 6, 98, 237, 57, 6, 98, 238, 57, 6, 152, 57, 31, -1, 239, 167, 148, 210, 315, 152, 31, 240, 98, 241, 167, 210, 239, 46, 229, 98, 239, 194, 229, 152, 31, 158, 238, 57, 31, 242, 98, 243, 167, 210, 51, 210, 227, 67, 240, 152, 74, 244, 210, 213, 152, 98, 51, 210, 227, 67, 240, 67, 316, 152, 74, 244, 210, 213, 152, 152, 31, 228, 167, 243, 4, 242, 31, 245, 167, 59, 210, 228, 98, 232, 152, 31, 161, 31, 29, 57, 31, 242, 98, 243, 167, 210, 240, 211, 228, 98, 240, 211, 228, 67, 228, 152, 31, 245, 167, 59, 210, 228, 98, 232, 152, 31, 52, 31, 246, 167, 68, 210, 315, 98, 232, 152, 31, 247, 167, 68, 210, 315, 98, 234, 152, 31, 248, 167, 246, 198, 57, 98, 173, 26, 129, 246, 198, 173, 98, 57, 26, 31, 249, 167, 51, 210, 218, 67, 241, 211, 231, 67, 247, 98, 250, 167, 247, 1, 231, 98, 251, 167, 315, 152, 31, 252, 167, 51, 210, 219, 67, 241, 211, 231, 67, 247, 98, 250, 167, 247, 1, 231, 98, 251, 167, 315, 152, 31, 253, 167, 153, 210, 198, 233, 98, 234, 26, 98, 82, 167, 130, 152, 31, 254, 167, 153, 210, 198, 234, 26, 98, 82, 167, 130, 152, 31, 158, 235, 57, 31, 255, 167, 189, 210, 223, 67, 239, 211, 230, 211, 231, 98, 210, 230, 98, 231, 152, 98, 210, 231, 98, 316, 152, 98, 210, 315, 98, 315, 152, 98, 210, 233, 98, 234, 152, 98, 210, 316, 98, 315, 152, 152, 31, 253, 167, 51, 210, 255, 98, 256, 167, 210, 315, 98, 316, 152, 98, 257, 167, 317, 152, 74, 244, 210, 130, 152, 31, 161, 31, 158, 236, 57, 31, 258, 167, 189, 210, 224, 67, 239, 211, 231, 98, 210, 231, 98, 152, 98, 210, 316, 98, 152, 98, 210, 315, 98, 152, 98, 210, 234, 98, 152, 98, 210, 315, 98, 152, 152, 31, 254, 167, 51, 210, 258, 98, 256, 167, 210, 315, 98, 152, 98, 257, 167, 317, 152, 74, 244, 210, 130, 152, 31, 161, 31, 121, 259, 139, 5, 210, 245, 152, 57, 31, 260, 167, 189, 210, 214, 67, 210, 242, 211, 229, 67, 241, 152, 211, 230, 98, 210, 228, 98, 230, 152, 98, 210, 229, 211, 230, 98, 316, 152, 98, 210, 259, 211, 232, 98, 315, 152, 98, 210, 232, 98, 233, 152, 98, 210, 316, 98, 315, 152, 152, 31, 261, 167, 189, 210, 215, 67, 210, 242, 211, 229, 67, 241, 152, 211, 230, 98, 210, 230, 98, 228, 152, 98, 210, 316, 98, 229, 211, 230, 152, 98, 210, 315, 98, 259, 211, 232, 152, 98, 210, 233, 98, 232, 152, 98, 210, 315, 98, 316, 152, 152, 31, 262, 167, 189, 210, 216, 67, 210, 242, 211, 229, 67, 241, 152, 211, 231, 98, 210, 228, 98, 231, 152, 98, 210, 229, 211, 231, 98, 316, 152, 98, 210, 259, 211, 232, 98, 315, 152, 98, 210, 232, 98, 234, 152, 98, 210, 316, 98, 315, 152, 152, 31, 263, 167, 189, 210, 220, 67, 210, 242, 211, 229, 67, 241, 152, 211, 231, 98, 210, 228, 98, 231, 152, 98, 210, 229, 211, 231, 98, 316, 152, 98, 210, 259, 211, 232, 98, 315, 152, 98, 210, 232, 98, 234, 152, 98, 210, 316, 98, 315, 152, 152, 31, 264, 167, 189, 210, 217, 67, 210, 242, 211, 229, 67, 241, 152, 98, 210, 228, 98, 152, 98, 210, 229, 98, 152, 98, 210, 259, 211, 232, 98, 152, 98, 210, 232, 98, 152, 98, 210, 315, 98, 152, 152, 31, 265, 167, 217, 67, 242, 211, 229, 67, 241, 67, 210, 228, 4, 316, 152, 211, 229, 158, 259, 69, 245, 4, 316, 29, 217, 67, 242, 211, 229, 67, 241, 67, 210, 259, 211, 232, 67, 232, 4, 316, 152, 211, 229, 31, 266, 167, 51, 210, 261, 98, 256, 167, 210, 315, 98, 316, 152, 98, 257, 167, 317, 152, 31, 267, 167, 51, 210, 262, 98, 256, 167, 210, 315, 98, 316, 152, 98, 257, 167, 317, 152, 31, 268, 167, 15, 210, 65, 210, 266, 152, 98, 253, 74, 244, 210, 266, 74, 82, 152, 98, 269, 167, 55, 152, 74, 244, 210, 130, 152, 67, 254, 198, 173, 98, 57, 26, 31, 268, 167, 176, 210, 210, 247, 1, 231, 152, 198, 173, 98, 57, 26, 98, 268, 98, 315, 152, 31, 270, 167, 191, 210, 268, 98, 271, 167, 316, 98, 272, 167, 142, 152, 41, 231, 31, 273, 167, 176, 210, 210, 247, 1, 231, 152, 198, 173, 98, 57, 26, 98, 268, 4, 270, 98, 315, 152, 31, 274, 167, 191, 210, 273, 211, 273, 98, 271, 167, 316, 98, 272, 167, 142, 152, 41, 231, 31, 275, 167, 316, 41, 116, 210, 274, 74, 244, 210, 130, 152, 67, 222, 152, 31, 276, 167, 210, 268, 4, 270, 152, 211, 275, 31, 267, 167, 276, 74, 244, 210, 266, 74, 82, 152, 211, 249, 198, 173, 98, 57, 26, 74, 244, 210, 266, 74, 82, 152, 67, 252, 198, 173, 98, 57, 26, 74, 244, 210, 266, 74, 82, 152, 4, 267, 74, 244, 210, 266, 74, 82, 152, 67, 65, 210, 266, 152, 31, 267, 167, 176, 210, 210, 247, 1, 231, 152, 198, 173, 98, 57, 26, 98, 267, 211, 249, 198, 173, 98, 57, 26, 74, 244, 210, 266, 74, 82, 152, 98, 315, 152, 31, 277, 167, 275, 211, 210, 231, 211, 267, 4, 191, 210, 267, 98, 271, 167, 316, 98, 272, 167, 142, 152, 4, 276, 74, 244, 210, 266, 74, 82, 152, 211, 191, 210, 267, 211, 276, 74, 244, 210, 266, 74, 82, 152, 98, 271, 167, 316, 98, 272, 167, 142, 152, 152, 41, 231, 31, 278, 167, 51, 210, 260, 98, 256, 167, 210, 315, 98, 316, 152, 98, 257, 167, 317, 152, 31, 279, 167, 51, 210, 264, 98, 256, 167, 210, 315, 98, 152, 98, 257, 167, 317, 152, 31, 278, 167, 210, 278, 211, 221, 152, 74, 244, 210, 266, 74, 82, 152, 31, 280, 167, 15, 210, 278, 98, 266, 98, 269, 167, 55, 152, 31, 280, 167, 176, 210, 248, 98, 280, 98, 315, 152, 31, 281, 167, 176, 210, 248, 98, 279, 198, 57, 98, 173, 26, 98, 315, 152, 31, 282, 167, 4, 15, 210, 279, 198, 57, 98, 173, 26, 211, 280, 74, 244, 210, 277, 74, 82, 152, 98, 277, 98, 269, 167, 55, 152, 31, 282, 149, 254, 198, 173, 98, 57, 26, 4, 15, 210, 281, 74, 244, 210, 277, 74, 82, 152, 98, 277, 98, 269, 167, 55, 152, 31, 282, 149, 15, 210, 278, 98, 253, 74, 244, 210, 278, 74, 82, 152, 98, 269, 167, 55, 152, 31, 283, 167, 51, 210, 265, 152, 31, 253, 167, 253, 4, 15, 210, 283, 211, 266, 98, 277, 74, 244, 210, 266, 74, 82, 152, 98, 269, 167, 55, 152, 31, 254, 167, 254, 4, 191, 210, 283, 211, 277, 74, 244, 210, 266, 74, 82, 152, 98, 271, 167, 315, 152, 31, 253, 167, 176, 210, 210, 247, 1, 231, 152, 198, 173, 98, 57, 26, 98, 253, 98, 315, 152, 31, 254, 167, 176, 210, 247, 1, 231, 98, 254, 98, 315, 152, 31, 10, 210, 263, 98, 282, 74, 244, 210, 263, 74, 82, 74, 104, 152, 98, 256, 167, 210, 315, 98, 316, 152, 152, 31, 70, 31, 158, 237, 57, 31, 284, 167, 189, 210, 225, 67, 239, 211, 230, 211, 231, 98, 210, 230, 98, 231, 152, 98, 210, 231, 98, 316, 152, 98, 210, 315, 98, 315, 152, 98, 210, 233, 98, 234, 152, 98, 210, 316, 98, 315, 152, 152, 31, 285, 167, 189, 210, 226, 67, 239, 211, 231, 98, 210, 231, 98, 152, 98, 210, 316, 98, 152, 98, 210, 315, 98, 152, 98, 210, 234, 98, 152, 98, 210, 315, 98, 152, 152, 31, 10, 210, 284, 98, 253, 74, 244, 210, 284, 74, 82, 74, 104, 152, 98, 256, 167, 210, 315, 98, 316, 152, 152, 31, 10, 210, 285, 98, 254, 74, 244, 210, 285, 74, 82, 74, 104, 152, 98, 256, 167, 210, 315, 98, 152, 152, 31, 161, 31, 3, 31]}, {"code": "def fused_chunk_ttt_linear_bwd_kernel_h(\n    k,\n    v,\n    v2,\n    x,\n    y,\n    r,\n    w,\n    b,\n    eta,\n    h0,\n    hb0,\n    h,\n    do,\n    dq,\n    scale,\n    eps,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    USE_INITIAL_STATE_B: tl.constexpr,\n):\n    i_nh = tl.program_id(0)\n    i_n, i_h = i_nh // H, i_nh % H\n    bos, _ = i_n * T, i_n * T + T\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\n\n    o_i = tl.arange(0, BT)\n    v_i = tl.arange(0, BV)\n    m_A = o_i[:, None] >= o_i[None, :]\n    b_w = tl.load(w + i_h * V + v_i, mask=v_i < V, other=0.0)\n    b_b = tl.load(b + i_h * V + v_i, mask=v_i < V, other=0.0)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    b_hb = tl.zeros([BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(\n            h0 + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)\n        )\n        b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)\n    if USE_INITIAL_STATE_B:\n        p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\n        b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option=\"zero\").to(tl.float32)\n\n    for i_t in range(NT):\n        p_h = tl.make_block_ptr(\n            h + ((boh + i_t) * H + i_h) * K * V,\n            (K, V),\n            (V, 1),\n            (0, 0),\n            (BK, BV),\n            (1, 0),\n        )\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n        )\n        p_v2 = tl.make_block_ptr(\n            v2 + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, 0),\n            (BT, BV),\n            (1, 0),\n        )\n        p_x = tl.make_block_ptr(\n            x + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n        )\n        p_y = tl.make_block_ptr(\n            y + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n        )\n        p_r = tl.make_block_ptr(\n            r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0)\n        )\n        p_e = tl.make_block_ptr(\n            eta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)\n        )\n        p_dq = tl.make_block_ptr(\n            dq + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, 0),\n            (BT, BK),\n            (1, 0),\n        )\n        p_do = tl.make_block_ptr(\n            do + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, 0),\n            (BT, BV),\n            (1, 0),\n        )\n        p_e_last = (\n            eta + bos * H + i_h + (T - 1) * H\n            if i_t == NT - 1\n            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H\n        )\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n\n        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")\n\n        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")\n\n        b_kh = (\n            tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32)\n            + b_hb[None, :]\n        )\n        b_kh = tl.where((v_i < V)[None, :], b_kh, 0.0)\n        mean = tl.sum(b_kh, axis=1, keep_dims=True) / V\n        xbar = tl.where((v_i < V)[None, :], b_kh - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V\n        rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)\n        b_kh_hat = (b_kh - mean) * rstd\n\n        b_v = (\n            b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype)\n            + b_b[None, :].to(b_k.dtype)\n            - b_v.to(b_k.dtype)\n            + tl.trans(b_k)\n        )\n        b_v = tl.where((v_i < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)\n        b_v2 = (\n            rstd\n            * (\n                V * b_v\n                - tl.sum(b_v, axis=1, keep_dims=True)\n                - b_kh_hat.to(b_k.dtype)\n                * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)\n            )\n            / V\n        )\n        tl.store(p_x, b_kh_hat.to(p_x.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_y, b_v.to(p_y.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_r, rstd.to(p_r.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_v2, b_v2.to(p_v2.dtype.element_ty), boundary_check=(0, 1))\n\n        b_e = tl.load(p_e, boundary_check=(0,), padding_option=\"zero\")\n        b_do = tl.load(p_do, boundary_check=(0, 1), padding_option=\"zero\")\n\n        b_v2 = tl.where((v_i < V)[None, :], b_v2, 0.0)\n        b_ds = tl.dot(b_do, tl.trans(b_v2).to(b_do.dtype))\n        b_ds = tl.where(m_A, b_ds, 0)\n        b_ds = b_ds.to(b_k.dtype)\n        b_dq = tl.dot(b_do, tl.trans(b_h).to(b_do.dtype))\n        b_dq -= tl.dot(b_ds, tl.trans(b_k)) * b_e[:, None]\n        b_dq *= scale\n\n        b_e_last = tl.load(p_e_last)\n        b_h = b_h - tl.dot(b_e_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)\n        b_hb = b_hb - tl.sum(b_e_last * b_v2.to(b_k.dtype), axis=0)\n        b_h = tl.where((v_i < V)[None, :], b_h, 0.0)\n        b_hb = tl.where((v_i < V), b_hb, 0.0)\n        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 314, 210, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 99, 224, 99, 225, 99, 226, 99, 227, 99, 228, 99, 229, 99, 230, 99, 231, 58, 6, 99, 232, 58, 6, 99, 233, 58, 6, 99, 234, 58, 6, 99, 235, 58, 6, 99, 236, 58, 6, 99, 237, 58, 6, 99, 238, 58, 6, 152, 58, 31, -1, 239, 167, 148, 210, 315, 152, 31, 240, 99, 241, 167, 210, 239, 46, 231, 99, 239, 194, 231, 152, 31, 242, 99, 243, 167, 210, 240, 211, 230, 99, 240, 211, 230, 68, 230, 152, 31, 244, 167, 60, 210, 230, 99, 234, 152, 31, 245, 167, 240, 211, 244, 31, 246, 167, 69, 210, 315, 99, 234, 152, 31, 247, 167, 69, 210, 315, 99, 236, 152, 31, 248, 167, 246, 198, 58, 99, 173, 26, 130, 246, 198, 173, 99, 58, 26, 31, 249, 167, 53, 210, 220, 68, 241, 211, 233, 68, 247, 99, 250, 167, 247, 1, 233, 99, 251, 167, 315, 152, 31, 252, 167, 53, 210, 221, 68, 241, 211, 233, 68, 247, 99, 250, 167, 247, 1, 233, 99, 251, 167, 315, 152, 31, 253, 167, 153, 210, 198, 235, 99, 236, 26, 99, 83, 167, 131, 152, 31, 254, 167, 153, 210, 198, 236, 26, 99, 83, 167, 131, 152, 31, 158, 237, 58, 31, 255, 167, 189, 210, 223, 68, 239, 211, 232, 211, 233, 99, 210, 232, 99, 233, 152, 99, 210, 233, 99, 316, 152, 99, 210, 315, 99, 315, 152, 99, 210, 235, 99, 236, 152, 99, 210, 316, 99, 315, 152, 152, 31, 253, 167, 53, 210, 255, 99, 256, 167, 210, 315, 99, 316, 152, 99, 257, 167, 317, 152, 75, 258, 210, 131, 152, 31, 161, 31, 158, 238, 58, 31, 259, 167, 189, 210, 224, 68, 239, 211, 233, 99, 210, 233, 99, 152, 99, 210, 316, 99, 152, 99, 210, 315, 99, 152, 99, 210, 236, 99, 152, 99, 210, 315, 99, 152, 152, 31, 254, 167, 53, 210, 259, 99, 256, 167, 210, 315, 99, 152, 99, 257, 167, 317, 152, 75, 258, 210, 131, 152, 31, 161, 31, 122, 260, 139, 5, 210, 244, 152, 58, 31, 261, 167, 189, 210, 225, 68, 210, 210, 245, 68, 260, 152, 211, 231, 68, 241, 152, 211, 232, 211, 233, 99, 210, 232, 99, 233, 152, 99, 210, 233, 99, 316, 152, 99, 210, 315, 99, 315, 152, 99, 210, 235, 99, 236, 152, 99, 210, 316, 99, 315, 152, 152, 31, 262, 167, 189, 210, 214, 68, 210, 242, 211, 231, 68, 241, 152, 211, 232, 99, 210, 232, 99, 230, 152, 99, 210, 316, 99, 231, 211, 232, 152, 99, 210, 315, 99, 260, 211, 234, 152, 99, 210, 235, 99, 234, 152, 99, 210, 315, 99, 316, 152, 152, 31, 263, 167, 189, 210, 215, 68, 210, 242, 211, 231, 68, 241, 152, 211, 233, 99, 210, 230, 99, 233, 152, 99, 210, 231, 211, 233, 99, 316, 152, 99, 210, 260, 211, 234, 99, 315, 152, 99, 210, 234, 99, 236, 152, 99, 210, 316, 99, 315, 152, 152, 31, 264, 167, 189, 210, 216, 68, 210, 242, 211, 231, 68, 241, 152, 211, 233, 99, 210, 230, 99, 233, 152, 99, 210, 231, 211, 233, 99, 316, 152, 99, 210, 260, 211, 234, 99, 315, 152, 99, 210, 234, 99, 236, 152, 99, 210, 316, 99, 315, 152, 152, 31, 265, 167, 189, 210, 217, 68, 210, 242, 211, 231, 68, 241, 152, 211, 233, 99, 210, 230, 99, 233, 152, 99, 210, 231, 211, 233, 99, 316, 152, 99, 210, 260, 211, 234, 99, 315, 152, 99, 210, 234, 99, 236, 152, 99, 210, 316, 99, 315, 152, 152, 31, 266, 167, 189, 210, 218, 68, 210, 242, 211, 231, 68, 241, 152, 211, 233, 99, 210, 230, 99, 233, 152, 99, 210, 231, 211, 233, 99, 316, 152, 99, 210, 260, 211, 234, 99, 315, 152, 99, 210, 234, 99, 236, 152, 99, 210, 316, 99, 315, 152, 152, 31, 267, 167, 189, 210, 219, 68, 242, 211, 231, 68, 241, 99, 210, 230, 99, 316, 152, 99, 210, 231, 99, 316, 152, 99, 210, 260, 211, 234, 99, 315, 152, 99, 210, 234, 99, 316, 152, 99, 210, 316, 99, 315, 152, 152, 31, 268, 167, 189, 210, 222, 68, 210, 242, 211, 231, 68, 241, 152, 99, 210, 230, 99, 152, 99, 210, 231, 99, 152, 99, 210, 260, 211, 234, 99, 152, 99, 210, 234, 99, 152, 99, 210, 315, 99, 152, 152, 31, 269, 167, 189, 210, 227, 68, 210, 242, 211, 231, 68, 241, 152, 211, 232, 99, 210, 230, 99, 232, 152, 99, 210, 231, 211, 232, 99, 316, 152, 99, 210, 260, 211, 234, 99, 315, 152, 99, 210, 234, 99, 235, 152, 99, 210, 316, 99, 315, 152, 152, 31, 270, 167, 189, 210, 226, 68, 210, 242, 211, 231, 68, 241, 152, 211, 233, 99, 210, 230, 99, 233, 152, 99, 210, 231, 211, 233, 99, 316, 152, 99, 210, 260, 211, 234, 99, 315, 152, 99, 210, 234, 99, 236, 152, 99, 210, 316, 99, 315, 152, 152, 31, 271, 167, 222, 68, 242, 211, 231, 68, 241, 68, 210, 230, 4, 316, 152, 211, 231, 158, 260, 70, 244, 4, 316, 29, 222, 68, 242, 211, 231, 68, 241, 68, 210, 260, 211, 234, 68, 234, 4, 316, 152, 211, 231, 31, 10, 210, 261, 99, 253, 75, 258, 210, 261, 75, 83, 75, 105, 152, 99, 256, 167, 210, 315, 99, 316, 152, 152, 31, 272, 167, 53, 210, 262, 99, 256, 167, 210, 315, 99, 316, 152, 99, 257, 167, 317, 152, 31, 273, 167, 53, 210, 263, 99, 256, 167, 210, 315, 99, 316, 152, 99, 257, 167, 317, 152, 31, 274, 167, 15, 210, 66, 210, 272, 152, 99, 253, 75, 258, 210, 272, 75, 83, 152, 99, 275, 167, 55, 152, 75, 258, 210, 131, 152, 68, 254, 198, 173, 99, 58, 26, 31, 274, 167, 176, 210, 210, 247, 1, 233, 152, 198, 173, 99, 58, 26, 99, 274, 99, 315, 152, 31, 276, 167, 191, 210, 274, 99, 277, 167, 316, 99, 278, 167, 142, 152, 41, 233, 31, 279, 167, 176, 210, 210, 247, 1, 233, 152, 198, 173, 99, 58, 26, 99, 274, 4, 276, 99, 315, 152, 31, 280, 167, 191, 210, 279, 211, 279, 99, 277, 167, 316, 99, 278, 167, 142, 152, 41, 233, 31, 281, 167, 316, 41, 117, 210, 280, 75, 258, 210, 131, 152, 68, 229, 152, 31, 282, 167, 210, 274, 4, 276, 152, 211, 281, 31, 273, 167, 282, 75, 258, 210, 272, 75, 83, 152, 211, 249, 198, 173, 99, 58, 26, 75, 258, 210, 272, 75, 83, 152, 68, 252, 198, 173, 99, 58, 26, 75, 258, 210, 272, 75, 83, 152, 4, 273, 75, 258, 210, 272, 75, 83, 152, 68, 66, 210, 272, 152, 31, 273, 167, 176, 210, 210, 247, 1, 233, 152, 198, 173, 99, 58, 26, 99, 273, 211, 249, 198, 173, 99, 58, 26, 75, 258, 210, 272, 75, 83, 152, 99, 315, 152, 31, 283, 167, 281, 211, 210, 233, 211, 273, 4, 191, 210, 273, 99, 277, 167, 316, 99, 278, 167, 142, 152, 4, 282, 75, 258, 210, 272, 75, 83, 152, 211, 191, 210, 273, 211, 282, 75, 258, 210, 272, 75, 83, 152, 99, 277, 167, 316, 99, 278, 167, 142, 152, 152, 41, 233, 31, 10, 210, 265, 99, 282, 75, 258, 210, 265, 75, 83, 75, 105, 152, 99, 256, 167, 210, 315, 99, 316, 152, 152, 31, 10, 210, 266, 99, 273, 75, 258, 210, 266, 75, 83, 75, 105, 152, 99, 256, 167, 210, 315, 99, 316, 152, 152, 31, 10, 210, 267, 99, 281, 75, 258, 210, 267, 75, 83, 75, 105, 152, 99, 256, 167, 210, 315, 99, 316, 152, 152, 31, 10, 210, 264, 99, 283, 75, 258, 210, 264, 75, 83, 75, 105, 152, 99, 256, 167, 210, 315, 99, 316, 152, 152, 31, 284, 167, 53, 210, 268, 99, 256, 167, 210, 315, 99, 152, 99, 257, 167, 317, 152, 31, 285, 167, 53, 210, 270, 99, 256, 167, 210, 315, 99, 316, 152, 99, 257, 167, 317, 152, 31, 283, 167, 176, 210, 210, 247, 1, 233, 152, 198, 173, 99, 58, 26, 99, 283, 99, 315, 152, 31, 286, 167, 15, 210, 285, 99, 66, 210, 283, 152, 75, 258, 210, 285, 75, 83, 152, 152, 31, 286, 167, 176, 210, 248, 99, 286, 99, 315, 152, 31, 286, 167, 286, 75, 258, 210, 272, 75, 83, 152, 31, 287, 167, 15, 210, 285, 99, 66, 210, 253, 152, 75, 258, 210, 285, 75, 83, 152, 152, 31, 287, 2, 15, 210, 286, 99, 66, 210, 272, 152, 152, 211, 284, 198, 58, 99, 173, 26, 31, 287, 23, 228, 31, 288, 167, 53, 210, 271, 152, 31, 253, 167, 253, 4, 15, 210, 288, 211, 272, 99, 283, 75, 258, 210, 272, 75, 83, 152, 99, 275, 167, 55, 152, 31, 254, 167, 254, 4, 191, 210, 288, 211, 283, 75, 258, 210, 272, 75, 83, 152, 99, 277, 167, 315, 152, 31, 253, 167, 176, 210, 210, 247, 1, 233, 152, 198, 173, 99, 58, 26, 99, 253, 99, 315, 152, 31, 254, 167, 176, 210, 247, 1, 233, 99, 254, 99, 315, 152, 31, 10, 210, 269, 99, 287, 75, 258, 210, 269, 75, 83, 75, 105, 152, 99, 256, 167, 210, 315, 99, 316, 152, 152, 31, 71, 31, 3, 31]}, {"code": "def fused_chunk_ttt_linear_bwd_kernel_dh(\n    q,\n    k,\n    v,\n    v2,\n    x,\n    y,\n    r,\n    w,\n    b,\n    eta,\n    h,\n    dht,\n    dhbt,\n    dh0,\n    dhb0,\n    do,\n    dk,\n    dv,\n    de,\n    dw,\n    db,\n    scale,\n    T,\n    H: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    USE_INITIAL_STATE_B: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT: tl.constexpr,\n    USE_FINAL_STATE_GRADIENT_B: tl.constexpr,\n):\n    i_nh = tl.program_id(0)\n    i_n, i_h = i_nh // H, i_nh % H\n    bos, _ = i_n * T, i_n * T + T\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n\n    b_dhb = tl.zeros([BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(\n            dht + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)\n        )\n        b_dh += tl.load(p_dht, boundary_check=(0, 1), padding_option=\"zero\")\n    if USE_FINAL_STATE_GRADIENT_B:\n        p_dhbt = tl.make_block_ptr(dhbt + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\n        b_dhb += tl.load(p_dhbt, boundary_check=(0,), padding_option=\"zero\")\n\n    o_i = tl.arange(0, BT)\n    v_i = tl.arange(0, BV)\n    m_A = o_i[:, None] >= o_i[None, :]\n    m_A_t = o_i[:, None] <= o_i[None, :]\n    b_w = tl.load(w + i_h * V + v_i, mask=v_i < V, other=0.0)\n    b_b = tl.load(b + i_h * V + v_i, mask=v_i < V, other=0.0)\n    b_dw = tl.zeros(\n        [\n            BV,\n        ],\n        dtype=b_w.dtype,\n    )\n    b_db = tl.zeros(\n        [\n            BV,\n        ],\n        dtype=b_b.dtype,\n    )\n    p_dw = tl.make_block_ptr(dw + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\n    p_db = tl.make_block_ptr(db + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\n\n    for i_t in range(NT - 1, -1, -1):\n        p_h = tl.make_block_ptr(\n            h + ((boh + i_t) * H + i_h) * K * V,\n            (V, K),\n            (1, V),\n            (0, 0),\n            (BV, BK),\n            (0, 1),\n        )\n        p_q = tl.make_block_ptr(\n            q + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1)\n        )\n        p_k = tl.make_block_ptr(\n            k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)\n        )\n        p_v = tl.make_block_ptr(\n            v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n        )\n        p_v2 = tl.make_block_ptr(\n            v2 + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, 0),\n            (BT, BV),\n            (1, 0),\n        )\n        p_x = tl.make_block_ptr(\n            x + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n        )\n        p_y = tl.make_block_ptr(\n            y + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)\n        )\n        p_r = tl.make_block_ptr(\n            r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0)\n        )\n        p_e = tl.make_block_ptr(\n            eta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)\n        )\n        p_dv = tl.make_block_ptr(\n            dv + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, 0),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dk = tl.make_block_ptr(\n            dk + (bos * H + i_h) * K,\n            (T, K),\n            (H * K, 1),\n            (i_t * BT, 0),\n            (BT, BK),\n            (1, 0),\n        )\n        p_do = tl.make_block_ptr(\n            do + (bos * H + i_h) * V,\n            (T, V),\n            (H * V, 1),\n            (i_t * BT, 0),\n            (BT, BV),\n            (1, 0),\n        )\n        p_de = tl.make_block_ptr(\n            de + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)\n        )\n        p_e_last = (\n            eta + bos * H + i_h + (T - 1) * H\n            if i_t == NT - 1\n            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H\n        )\n        b_q = tl.load(p_q, boundary_check=(0, 1), padding_option=\"zero\")\n        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")\n        b_e = tl.load(p_e, boundary_check=(0,), padding_option=\"zero\")\n        b_do = tl.load(p_do, boundary_check=(0, 1), padding_option=\"zero\")\n        b_e_last = tl.load(p_e_last)\n        b_A = tl.dot(b_k, b_q)\n        b_A = -tl.where(m_A_t, b_A * scale * b_e[None, :], 0).to(do.dtype.element_ty)\n        b_Ae = -tl.where(m_A_t, b_e[None, :], 0).to(do.dtype.element_ty)\n        b_dv_new = tl.dot(b_A.to(b_do.dtype), b_do) + tl.dot(b_Ae.to(b_do.dtype), b_do)\n        b_dv_new -= tl.dot(b_e_last * b_k, b_dh.to(b_k.dtype))\n        b_dv_new -= b_e_last * b_dhb.to(b_k.dtype)[None, :]\n\n        b_v2 = tl.load(p_v2, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)\n        b_x = tl.load(p_x, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)\n        b_y = tl.load(p_y, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)\n        b_rstd = tl.load(p_r, boundary_check=(0, 1), padding_option=\"zero\").to(\n            tl.float32\n        )\n        b_dy = (\n            b_rstd\n            * (\n                b_dv_new * V\n                - tl.sum(b_dv_new, axis=1, keep_dims=True)\n                - b_x * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)\n            )\n            / V\n        )\n        b_dx = (\n            -b_rstd\n            * (\n                b_dv_new * tl.sum(b_x * b_y, axis=1, keep_dims=True)\n                + b_y * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)\n            )\n            / V\n        )\n        b_drstd = tl.sum(\n            b_dv_new.to(b_rstd.dtype) * b_v2.to(b_rstd.dtype) / b_rstd,\n            axis=1,\n            keep_dims=True,\n        )\n\n        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")\n        b_w = b_w.to(b_k.dtype)\n        b_b = b_b.to(b_k.dtype)\n        b_dv = -b_w * b_dy.to(b_k.dtype)\n        b_dk = b_w * b_dy.to(b_k.dtype)\n        b_dw += tl.sum(\n            2 * b_w * b_x * b_dy.to(b_k.dtype)\n            + (b_b - b_v.to(b_k.dtype) + b_k) * b_dy.to(b_k.dtype),\n            axis=0,\n        ).to(b_dw.dtype)\n        b_db += tl.sum(b_w * b_dy.to(b_k.dtype), axis=0).to(b_db.dtype)\n        b_dx = b_dx.to(b_k.dtype) + b_w * b_w * b_dy.to(b_k.dtype)\n\n        b_h = tl.load(p_h, boundary_check=(0, 1), padding_option=\"zero\")\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_dkh = (\n            b_rstd\n            * (\n                V * b_dx\n                - tl.sum(b_dx, axis=1, keep_dims=True)\n                - b_x * tl.sum(b_x * b_dx, axis=1, keep_dims=True)\n            )\n            / V\n        )\n        b_dkh -= b_rstd * b_rstd * b_drstd * b_x / V\n        b_dkh = tl.where((v_i < V)[None, :] * (o_i < T - i_t * BT)[:, None], b_dkh, 0.0)\n        b_dk += tl.dot(b_dkh, b_h.to(b_dkh.dtype)).to(b_k.dtype)\n\n        b_ds = tl.dot(b_do, tl.trans(b_v2))\n        b_ds = tl.where(m_A, b_ds, 0)\n        b_ds = b_ds.to(b_k.dtype)\n        i_last = (BT - 1) if (i_t * BT + BT) <= T else (T % BT - 1)\n        mask = o_i == i_last\n        b_dk -= b_e_last * tl.dot(b_v2, tl.trans(b_dh).to(b_v2.dtype))\n        b_dk -= tl.dot(tl.trans(b_ds), tl.trans(b_q) * b_e[:, None])\n        b_de = mask * tl.sum(-b_dh * tl.trans(tl.dot(tl.trans(b_v2), b_k))).to(\n            b_k.dtype\n        )\n        b_de -= mask * tl.sum(b_dhb * tl.sum(b_v2, axis=0)).to(b_k.dtype)\n        b_de -= tl.sum(tl.dot(b_ds, b_k) * tl.trans(b_q).to(b_k.dtype), axis=1)\n        b_de -= tl.sum(b_ds, axis=1)\n        b_dh += tl.dot(b_q, b_do.to(b_q.dtype)) + tl.dot(\n            tl.trans(b_k).to(b_dkh.dtype), b_dkh\n        )\n        b_dhb += tl.sum(b_do + b_dkh, axis=0)\n        b_dh = tl.where((v_i < V)[None, :], b_dh, 0.0)\n        b_dhb = tl.where((v_i < V), b_dhb, 0.0)\n\n        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_de, b_de.to(p_de.dtype.element_ty), boundary_check=(0,))\n    tl.store(p_dw, b_dw.to(p_dw.dtype.element_ty), boundary_check=(0,))\n    tl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0,))\n\n    if USE_INITIAL_STATE:\n        p_dh0 = tl.make_block_ptr(\n            dh0 + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)\n        )\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))\n    if USE_INITIAL_STATE_B:\n        p_dhb0 = tl.make_block_ptr(dhb0 + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\n        tl.store(p_dhb0, b_dhb.to(p_dhb0.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 314, 210, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 98, 220, 98, 221, 98, 222, 98, 223, 98, 224, 98, 225, 98, 226, 98, 227, 98, 228, 98, 229, 98, 230, 98, 231, 98, 232, 98, 233, 98, 234, 98, 235, 98, 236, 98, 237, 57, 6, 98, 238, 57, 6, 98, 239, 57, 6, 98, 240, 57, 6, 98, 241, 57, 6, 98, 242, 57, 6, 98, 243, 57, 6, 98, 244, 57, 6, 98, 245, 57, 6, 98, 246, 57, 6, 152, 57, 31, -1, 247, 167, 148, 210, 315, 152, 31, 248, 98, 249, 167, 210, 247, 46, 237, 98, 247, 194, 237, 152, 31, 250, 98, 251, 167, 210, 248, 211, 236, 98, 248, 211, 236, 67, 236, 152, 31, 252, 167, 59, 210, 236, 98, 240, 152, 31, 253, 167, 248, 211, 252, 31, 254, 167, 153, 210, 198, 241, 98, 242, 26, 98, 82, 167, 130, 152, 31, 255, 167, 153, 210, 198, 242, 26, 98, 82, 167, 130, 152, 31, 158, 245, 57, 31, 256, 167, 189, 210, 225, 67, 247, 211, 238, 211, 239, 98, 210, 238, 98, 239, 152, 98, 210, 239, 98, 316, 152, 98, 210, 315, 98, 315, 152, 98, 210, 241, 98, 242, 152, 98, 210, 316, 98, 315, 152, 152, 31, 254, 149, 51, 210, 256, 98, 257, 167, 210, 315, 98, 316, 152, 98, 258, 167, 317, 152, 31, 161, 31, 158, 246, 57, 31, 259, 167, 189, 210, 226, 67, 247, 211, 239, 98, 210, 239, 98, 152, 98, 210, 316, 98, 152, 98, 210, 315, 98, 152, 98, 210, 242, 98, 152, 98, 210, 315, 98, 152, 152, 31, 255, 149, 51, 210, 259, 98, 257, 167, 210, 315, 98, 152, 98, 258, 167, 317, 152, 31, 161, 31, 260, 167, 68, 210, 315, 98, 240, 152, 31, 261, 167, 68, 210, 315, 98, 242, 152, 31, 262, 167, 260, 198, 57, 98, 173, 26, 129, 260, 198, 173, 98, 57, 26, 31, 263, 167, 260, 198, 57, 98, 173, 26, 188, 260, 198, 173, 98, 57, 26, 31, 264, 167, 51, 210, 221, 67, 249, 211, 239, 67, 261, 98, 265, 167, 261, 1, 239, 98, 266, 167, 315, 152, 31, 267, 167, 51, 210, 222, 67, 249, 211, 239, 67, 261, 98, 265, 167, 261, 1, 239, 98, 266, 167, 315, 152, 31, 268, 167, 153, 210, 198, 242, 26, 98, 82, 167, 264, 74, 82, 152, 31, 269, 167, 153, 210, 198, 242, 26, 98, 82, 167, 267, 74, 82, 152, 31, 270, 167, 189, 210, 233, 67, 247, 211, 239, 98, 210, 239, 98, 152, 98, 210, 316, 98, 152, 98, 210, 315, 98, 152, 98, 210, 242, 98, 152, 98, 210, 315, 98, 152, 152, 31, 271, 167, 189, 210, 234, 67, 247, 211, 239, 98, 210, 239, 98, 152, 98, 210, 316, 98, 152, 98, 210, 315, 98, 152, 98, 210, 242, 98, 152, 98, 210, 315, 98, 152, 152, 31, 121, 272, 139, 5, 210, 252, 4, 316, 98, 4, 316, 98, 4, 316, 152, 57, 31, 273, 167, 189, 210, 224, 67, 210, 210, 253, 67, 272, 152, 211, 237, 67, 249, 152, 211, 238, 211, 239, 98, 210, 239, 98, 238, 152, 98, 210, 316, 98, 239, 152, 98, 210, 315, 98, 315, 152, 98, 210, 242, 98, 241, 152, 98, 210, 315, 98, 316, 152, 152, 31, 274, 167, 189, 210, 214, 67, 210, 250, 211, 237, 67, 249, 152, 211, 238, 98, 210, 238, 98, 236, 152, 98, 210, 316, 98, 237, 211, 238, 152, 98, 210, 315, 98, 272, 211, 240, 152, 98, 210, 241, 98, 240, 152, 98, 210, 315, 98, 316, 152, 152, 31, 275, 167, 189, 210, 215, 67, 210, 250, 211, 237, 67, 249, 152, 211, 238, 98, 210, 236, 98, 238, 152, 98, 210, 237, 211, 238, 98, 316, 152, 98, 210, 272, 211, 240, 98, 315, 152, 98, 210, 240, 98, 241, 152, 98, 210, 316, 98, 315, 152, 152, 31, 276, 167, 189, 210, 216, 67, 210, 250, 211, 237, 67, 249, 152, 211, 239, 98, 210, 236, 98, 239, 152, 98, 210, 237, 211, 239, 98, 316, 152, 98, 210, 272, 211, 240, 98, 315, 152, 98, 210, 240, 98, 242, 152, 98, 210, 316, 98, 315, 152, 152, 31, 277, 167, 189, 210, 217, 67, 210, 250, 211, 237, 67, 249, 152, 211, 239, 98, 210, 236, 98, 239, 152, 98, 210, 237, 211, 239, 98, 316, 152, 98, 210, 272, 211, 240, 98, 315, 152, 98, 210, 240, 98, 242, 152, 98, 210, 316, 98, 315, 152, 152, 31, 278, 167, 189, 210, 218, 67, 210, 250, 211, 237, 67, 249, 152, 211, 239, 98, 210, 236, 98, 239, 152, 98, 210, 237, 211, 239, 98, 316, 152, 98, 210, 272, 211, 240, 98, 315, 152, 98, 210, 240, 98, 242, 152, 98, 210, 316, 98, 315, 152, 152, 31, 279, 167, 189, 210, 219, 67, 210, 250, 211, 237, 67, 249, 152, 211, 239, 98, 210, 236, 98, 239, 152, 98, 210, 237, 211, 239, 98, 316, 152, 98, 210, 272, 211, 240, 98, 315, 152, 98, 210, 240, 98, 242, 152, 98, 210, 316, 98, 315, 152, 152, 31, 280, 167, 189, 210, 220, 67, 250, 211, 237, 67, 249, 98, 210, 236, 98, 316, 152, 98, 210, 237, 98, 316, 152, 98, 210, 272, 211, 240, 98, 315, 152, 98, 210, 240, 98, 316, 152, 98, 210, 316, 98, 315, 152, 152, 31, 281, 167, 189, 210, 223, 67, 210, 250, 211, 237, 67, 249, 152, 98, 210, 236, 98, 152, 98, 210, 237, 98, 152, 98, 210, 272, 211, 240, 98, 152, 98, 210, 240, 98, 152, 98, 210, 315, 98, 152, 152, 31, 282, 167, 189, 210, 231, 67, 210, 250, 211, 237, 67, 249, 152, 211, 239, 98, 210, 236, 98, 239, 152, 98, 210, 237, 211, 239, 98, 316, 152, 98, 210, 272, 211, 240, 98, 315, 152, 98, 210, 240, 98, 242, 152, 98, 210, 316, 98, 315, 152, 152, 31, 283, 167, 189, 210, 230, 67, 210, 250, 211, 237, 67, 249, 152, 211, 238, 98, 210, 236, 98, 238, 152, 98, 210, 237, 211, 238, 98, 316, 152, 98, 210, 272, 211, 240, 98, 315, 152, 98, 210, 240, 98, 241, 152, 98, 210, 316, 98, 315, 152, 152, 31, 284, 167, 189, 210, 229, 67, 210, 250, 211, 237, 67, 249, 152, 211, 239, 98, 210, 236, 98, 239, 152, 98, 210, 237, 211, 239, 98, 316, 152, 98, 210, 272, 211, 240, 98, 315, 152, 98, 210, 240, 98, 242, 152, 98, 210, 316, 98, 315, 152, 152, 31, 285, 167, 189, 210, 232, 67, 210, 250, 211, 237, 67, 249, 152, 98, 210, 236, 98, 152, 98, 210, 237, 98, 152, 98, 210, 272, 211, 240, 98, 152, 98, 210, 240, 98, 152, 98, 210, 315, 98, 152, 152, 31, 286, 167, 223, 67, 250, 211, 237, 67, 249, 67, 210, 236, 4, 316, 152, 211, 237, 158, 272, 69, 252, 4, 316, 29, 223, 67, 250, 211, 237, 67, 249, 67, 210, 272, 211, 240, 67, 240, 4, 316, 152, 211, 237, 31, 287, 167, 51, 210, 274, 98, 257, 167, 210, 315, 98, 316, 152, 98, 258, 167, 317, 152, 31, 288, 167, 51, 210, 275, 98, 257, 167, 210, 315, 98, 316, 152, 98, 258, 167, 317, 152, 31, 289, 167, 51, 210, 281, 98, 257, 167, 210, 315, 98, 152, 98, 258, 167, 317, 152, 31, 290, 167, 51, 210, 284, 98, 257, 167, 210, 315, 98, 316, 152, 98, 258, 167, 317, 152, 31, 291, 167, 51, 210, 286, 152, 31, 292, 167, 15, 210, 288, 98, 287, 152, 31, 292, 167, 4, 176, 210, 263, 98, 292, 211, 235, 211, 289, 198, 173, 98, 57, 26, 98, 315, 152, 74, 293, 210, 229, 74, 82, 74, 104, 152, 31, 294, 167, 4, 176, 210, 263, 98, 289, 198, 173, 98, 57, 26, 98, 315, 152, 74, 293, 210, 229, 74, 82, 74, 104, 152, 31, 295, 167, 15, 210, 292, 74, 293, 210, 290, 74, 82, 152, 98, 290, 152, 67, 15, 210, 294, 74, 293, 210, 290, 74, 82, 152, 98, 290, 152, 31, 295, 2, 15, 210, 291, 211, 288, 98, 254, 74, 293, 210, 288, 74, 82, 152, 152, 31, 295, 2, 291, 211, 255, 74, 293, 210, 288, 74, 82, 152, 198, 173, 98, 57, 26, 31, 296, 167, 51, 210, 277, 98, 257, 167, 210, 315, 98, 316, 152, 98, 258, 167, 317, 152, 74, 293, 210, 288, 74, 82, 152, 31, 297, 167, 51, 210, 278, 98, 257, 167, 210, 315, 98, 316, 152, 98, 258, 167, 317, 152, 74, 293, 210, 288, 74, 82, 152, 31, 298, 167, 51, 210, 279, 98, 257, 167, 210, 315, 98, 316, 152, 98, 258, 167, 317, 152, 74, 293, 210, 288, 74, 82, 152, 31, 299, 167, 51, 210, 280, 98, 257, 167, 210, 315, 98, 316, 152, 98, 258, 167, 317, 152, 74, 293, 210, 130, 152, 31, 300, 167, 299, 211, 210, 295, 211, 239, 4, 191, 210, 295, 98, 301, 167, 316, 98, 302, 167, 142, 152, 4, 297, 211, 191, 210, 295, 211, 297, 98, 301, 167, 316, 98, 302, 167, 142, 152, 152, 41, 239, 31, 303, 167, 4, 299, 211, 210, 295, 211, 191, 210, 297, 211, 298, 98, 301, 167, 316, 98, 302, 167, 142, 152, 67, 298, 211, 191, 210, 295, 211, 297, 98, 301, 167, 316, 98, 302, 167, 142, 152, 152, 41, 239, 31, 304, 167, 191, 210, 295, 74, 293, 210, 299, 74, 82, 152, 211, 296, 74, 293, 210, 299, 74, 82, 152, 41, 299, 98, 301, 167, 316, 98, 302, 167, 142, 152, 31, 305, 167, 51, 210, 276, 98, 257, 167, 210, 315, 98, 316, 152, 98, 258, 167, 317, 152, 31, 264, 167, 264, 74, 293, 210, 288, 74, 82, 152, 31, 267, 167, 267, 74, 293, 210, 288, 74, 82, 152, 31, 306, 167, 4, 264, 211, 300, 74, 293, 210, 288, 74, 82, 152, 31, 307, 167, 264, 211, 300, 74, 293, 210, 288, 74, 82, 152, 31, 268, 149, 191, 210, 318, 211, 264, 211, 297, 211, 300, 74, 293, 210, 288, 74, 82, 152, 67, 210, 267, 4, 305, 74, 293, 210, 288, 74, 82, 152, 67, 288, 152, 211, 300, 74, 293, 210, 288, 74, 82, 152, 98, 301, 167, 315, 152, 74, 293, 210, 268, 74, 82, 152, 31, 269, 149, 191, 210, 264, 211, 300, 74, 293, 210, 288, 74, 82, 152, 98, 301, 167, 315, 152, 74, 293, 210, 269, 74, 82, 152, 31, 303, 167, 303, 74, 293, 210, 288, 74, 82, 152, 67, 264, 211, 264, 211, 300, 74, 293, 210, 288, 74, 82, 152, 31, 308, 167, 51, 210, 273, 98, 257, 167, 210, 315, 98, 316, 152, 98, 258, 167, 317, 152, 31, 287, 167, 210, 287, 211, 235, 152, 74, 293, 210, 287, 74, 82, 152, 31, 309, 167, 299, 211, 210, 239, 211, 303, 4, 191, 210, 303, 98, 301, 167, 316, 98, 302, 167, 142, 152, 4, 297, 211, 191, 210, 297, 211, 303, 98, 301, 167, 316, 98, 302, 167, 142, 152, 152, 41, 239, 31, 309, 2, 299, 211, 299, 211, 304, 211, 297, 41, 239, 31, 309, 167, 176, 210, 210, 261, 1, 239, 152, 198, 173, 98, 57, 26, 211, 210, 260, 1, 236, 4, 272, 211, 240, 152, 198, 57, 98, 173, 26, 98, 309, 98, 315, 152, 31, 307, 149, 15, 210, 309, 98, 308, 74, 293, 210, 309, 74, 82, 152, 152, 74, 293, 210, 288, 74, 82, 152, 31, 310, 167, 15, 210, 290, 98, 65, 210, 296, 152, 152, 31, 310, 167, 176, 210, 262, 98, 310, 98, 315, 152, 31, 310, 167, 310, 74, 293, 210, 288, 74, 82, 152, 31, 311, 167, 240, 4, 316, 158, 272, 211, 240, 67, 240, 188, 236, 29, 236, 194, 240, 4, 316, 31, 265, 167, 260, 69, 311, 31, 307, 2, 291, 211, 15, 210, 296, 98, 65, 210, 254, 152, 74, 293, 210, 296, 74, 82, 152, 152, 31, 307, 2, 15, 210, 65, 210, 310, 152, 98, 65, 210, 287, 152, 211, 289, 198, 57, 98, 173, 26, 152, 31, 312, 167, 265, 211, 191, 210, 4, 254, 211, 65, 210, 15, 210, 65, 210, 296, 152, 98, 288, 152, 152, 152, 74, 293, 210, 288, 74, 82, 152, 31, 312, 2, 265, 211, 191, 210, 255, 211, 191, 210, 296, 98, 301, 167, 315, 152, 152, 74, 293, 210, 288, 74, 82, 152, 31, 312, 2, 191, 210, 15, 210, 310, 98, 288, 152, 211, 65, 210, 287, 152, 74, 293, 210, 288, 74, 82, 152, 98, 301, 167, 316, 152, 31, 312, 2, 191, 210, 310, 98, 301, 167, 316, 152, 31, 254, 149, 15, 210, 287, 98, 290, 74, 293, 210, 287, 74, 82, 152, 152, 67, 15, 210, 65, 210, 288, 152, 74, 293, 210, 309, 74, 82, 152, 98, 309, 152, 31, 255, 149, 191, 210, 290, 67, 309, 98, 301, 167, 315, 152, 31, 254, 167, 176, 210, 210, 261, 1, 239, 152, 198, 173, 98, 57, 26, 98, 254, 98, 315, 152, 31, 255, 167, 176, 210, 261, 1, 239, 98, 255, 98, 315, 152, 31, 10, 210, 283, 98, 307, 74, 293, 210, 283, 74, 82, 74, 104, 152, 98, 257, 167, 210, 315, 98, 316, 152, 152, 31, 10, 210, 282, 98, 306, 74, 293, 210, 282, 74, 82, 74, 104, 152, 98, 257, 167, 210, 315, 98, 316, 152, 152, 31, 10, 210, 285, 98, 312, 74, 293, 210, 285, 74, 82, 74, 104, 152, 98, 257, 167, 210, 315, 98, 152, 152, 31, 70, 31, 10, 210, 270, 98, 268, 74, 293, 210, 270, 74, 82, 74, 104, 152, 98, 257, 167, 210, 315, 98, 152, 152, 31, 10, 210, 271, 98, 269, 74, 293, 210, 271, 74, 82, 74, 104, 152, 98, 257, 167, 210, 315, 98, 152, 152, 31, 158, 243, 57, 31, 313, 167, 189, 210, 227, 67, 247, 211, 238, 211, 239, 98, 210, 238, 98, 239, 152, 98, 210, 239, 98, 316, 152, 98, 210, 315, 98, 315, 152, 98, 210, 241, 98, 242, 152, 98, 210, 316, 98, 315, 152, 152, 31, 10, 210, 313, 98, 254, 74, 293, 210, 313, 74, 82, 74, 104, 152, 98, 257, 167, 210, 315, 98, 316, 152, 152, 31, 161, 31, 158, 244, 57, 31, 319, 167, 189, 210, 228, 67, 247, 211, 239, 98, 210, 239, 98, 152, 98, 210, 316, 98, 152, 98, 210, 315, 98, 152, 98, 210, 242, 98, 152, 98, 210, 315, 98, 152, 152, 31, 10, 210, 319, 98, 255, 74, 293, 210, 319, 74, 82, 74, 104, 152, 98, 257, 167, 210, 315, 98, 152, 152, 31, 161, 31, 3, 31]}, {"code": "def chunk_local_cumsum_scalar_kernel(\n    s,\n    o,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    BT: tl.constexpr,\n    REVERSE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    HEAD_FIRST: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    if HEAD_FIRST:\n        p_s = tl.make_block_ptr(\n            s + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,)\n        )\n        p_o = tl.make_block_ptr(\n            o + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,)\n        )\n    else:\n        p_s = tl.make_block_ptr(s + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))\n        p_o = tl.make_block_ptr(o + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))\n\n    b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\n    b_o = tl.cumsum(b_s, axis=0)\n    if REVERSE:\n        b_z = tl.sum(b_s, axis=0)\n        b_o = -b_o + b_z[None] + b_s\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 314, 210, 214, 99, 215, 99, 216, 99, 217, 99, 218, 99, 219, 58, 6, 99, 220, 58, 6, 99, 221, 58, 6, 99, 222, 58, 6, 99, 223, 58, 6, 99, 224, 58, 6, 152, 58, 31, -1, 225, 99, 226, 167, 210, 148, 210, 315, 152, 99, 148, 210, 316, 152, 152, 31, 227, 99, 228, 167, 210, 226, 46, 220, 99, 226, 194, 220, 152, 31, 158, 223, 58, 31, 229, 99, 225, 167, 210, 53, 210, 217, 68, 225, 211, 317, 152, 75, 230, 210, 57, 152, 99, 53, 210, 217, 68, 225, 211, 317, 68, 316, 152, 75, 230, 210, 57, 152, 152, 31, 231, 99, 232, 167, 210, 53, 210, 216, 68, 229, 152, 75, 230, 210, 57, 152, 99, 53, 210, 216, 68, 229, 68, 316, 152, 75, 230, 210, 57, 152, 152, 31, 218, 167, 232, 4, 231, 31, 161, 31, 29, 58, 31, 231, 99, 232, 167, 210, 227, 211, 218, 99, 227, 211, 218, 68, 218, 152, 31, 52, 31, 158, 224, 58, 31, 233, 167, 189, 210, 214, 68, 231, 211, 220, 68, 228, 211, 218, 99, 210, 218, 99, 152, 99, 210, 316, 99, 152, 99, 210, 225, 211, 221, 99, 152, 99, 210, 221, 99, 152, 99, 210, 315, 99, 152, 152, 31, 234, 167, 189, 210, 215, 68, 231, 211, 220, 68, 228, 211, 218, 99, 210, 218, 99, 152, 99, 210, 316, 99, 152, 99, 210, 225, 211, 221, 99, 152, 99, 210, 221, 99, 152, 99, 210, 315, 99, 152, 152, 31, 161, 31, 29, 58, 31, 233, 167, 189, 210, 214, 68, 231, 211, 220, 68, 228, 99, 210, 218, 99, 152, 99, 210, 220, 99, 152, 99, 210, 225, 211, 221, 99, 152, 99, 210, 221, 99, 152, 99, 210, 315, 99, 152, 152, 31, 234, 167, 189, 210, 215, 68, 231, 211, 220, 68, 228, 99, 210, 218, 99, 152, 99, 210, 220, 99, 152, 99, 210, 225, 211, 221, 99, 152, 99, 210, 221, 99, 152, 99, 210, 315, 99, 152, 152, 31, 52, 31, 235, 167, 53, 210, 233, 99, 236, 167, 210, 315, 99, 152, 152, 75, 230, 210, 131, 152, 31, 237, 167, 77, 210, 235, 99, 238, 167, 315, 152, 31, 158, 222, 58, 31, 239, 167, 191, 210, 235, 99, 238, 167, 315, 152, 31, 237, 167, 4, 237, 68, 239, 198, 173, 26, 68, 235, 31, 161, 31, 10, 210, 234, 99, 237, 75, 230, 210, 234, 75, 83, 75, 105, 152, 99, 236, 167, 210, 315, 99, 152, 152, 31, 3, 31]}, {"code": "def chunk_local_cumsum_vector_kernel(\n    s,\n    o,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    REVERSE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    HEAD_FIRST: tl.constexpr,\n):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    o_i = tl.arange(0, BT)\n    if REVERSE:\n        m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\n    else:\n        m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n\n    if HEAD_FIRST:\n        p_s = tl.make_block_ptr(\n            s + (bos * H + i_h * T) * S,\n            (T, S),\n            (S, 1),\n            (i_t * BT, i_s * BS),\n            (BT, BS),\n            (1, 0),\n        )\n        p_o = tl.make_block_ptr(\n            o + (bos * H + i_h * T) * S,\n            (T, S),\n            (S, 1),\n            (i_t * BT, i_s * BS),\n            (BT, BS),\n            (1, 0),\n        )\n    else:\n        p_s = tl.make_block_ptr(\n            s + (bos * H + i_h) * S,\n            (T, S),\n            (H * S, 1),\n            (i_t * BT, i_s * BS),\n            (BT, BS),\n            (1, 0),\n        )\n        p_o = tl.make_block_ptr(\n            o + (bos * H + i_h) * S,\n            (T, S),\n            (H * S, 1),\n            (i_t * BT, i_s * BS),\n            (BT, BS),\n            (1, 0),\n        )\n\n    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n    b_o = tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 314, 210, 214, 98, 215, 98, 216, 98, 217, 98, 218, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 98, 226, 57, 6, 152, 57, 31, -1, 227, 98, 228, 98, 229, 167, 210, 148, 210, 315, 152, 98, 148, 210, 316, 152, 98, 148, 210, 317, 152, 152, 31, 230, 98, 231, 167, 210, 229, 46, 220, 98, 229, 194, 220, 152, 31, 158, 225, 57, 31, 232, 98, 228, 167, 210, 51, 210, 217, 67, 228, 211, 317, 152, 74, 233, 210, 213, 152, 98, 51, 210, 217, 67, 228, 211, 317, 67, 316, 152, 74, 233, 210, 213, 152, 152, 31, 234, 98, 235, 167, 210, 51, 210, 216, 67, 232, 152, 74, 233, 210, 213, 152, 98, 51, 210, 216, 67, 232, 67, 316, 152, 74, 233, 210, 213, 152, 152, 31, 218, 167, 235, 4, 234, 31, 161, 31, 29, 57, 31, 234, 98, 235, 167, 210, 230, 211, 218, 98, 230, 211, 218, 67, 218, 152, 31, 52, 31, 236, 167, 68, 210, 315, 98, 222, 152, 31, 158, 224, 57, 31, 237, 167, 176, 210, 236, 198, 57, 98, 173, 26, 188, 236, 198, 173, 98, 57, 26, 98, 316, 98, 315, 152, 31, 161, 31, 29, 57, 31, 237, 167, 176, 210, 236, 198, 57, 98, 173, 26, 129, 236, 198, 173, 98, 57, 26, 98, 316, 98, 315, 152, 31, 52, 31, 158, 226, 57, 31, 238, 167, 189, 210, 214, 67, 210, 234, 211, 220, 67, 231, 211, 218, 152, 211, 221, 98, 210, 218, 98, 221, 152, 98, 210, 221, 98, 316, 152, 98, 210, 228, 211, 222, 98, 227, 211, 223, 152, 98, 210, 222, 98, 223, 152, 98, 210, 316, 98, 315, 152, 152, 31, 239, 167, 189, 210, 215, 67, 210, 234, 211, 220, 67, 231, 211, 218, 152, 211, 221, 98, 210, 218, 98, 221, 152, 98, 210, 221, 98, 316, 152, 98, 210, 228, 211, 222, 98, 227, 211, 223, 152, 98, 210, 222, 98, 223, 152, 98, 210, 316, 98, 315, 152, 152, 31, 161, 31, 29, 57, 31, 238, 167, 189, 210, 214, 67, 210, 234, 211, 220, 67, 231, 152, 211, 221, 98, 210, 218, 98, 221, 152, 98, 210, 220, 211, 221, 98, 316, 152, 98, 210, 228, 211, 222, 98, 227, 211, 223, 152, 98, 210, 222, 98, 223, 152, 98, 210, 316, 98, 315, 152, 152, 31, 239, 167, 189, 210, 215, 67, 210, 234, 211, 220, 67, 231, 152, 211, 221, 98, 210, 218, 98, 221, 152, 98, 210, 220, 211, 221, 98, 316, 152, 98, 210, 228, 211, 222, 98, 227, 211, 223, 152, 98, 210, 222, 98, 223, 152, 98, 210, 316, 98, 315, 152, 152, 31, 52, 31, 240, 167, 51, 210, 238, 98, 241, 167, 210, 315, 98, 316, 152, 152, 74, 233, 210, 130, 152, 31, 242, 167, 15, 210, 237, 98, 240, 98, 243, 167, 55, 152, 31, 10, 210, 239, 98, 242, 74, 233, 210, 239, 74, 82, 74, 104, 152, 98, 241, 167, 210, 315, 98, 316, 152, 152, 31, 3, 31]}, {"code": "def chunk_global_cumsum_scalar_kernel(\n    s,\n    o,\n    cu_seqlens,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    BT: tl.constexpr,\n    REVERSE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    HEAD_FIRST: tl.constexpr,\n):\n    i_nh = tl.program_id(0)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n    T = eos - bos\n\n    b_z = tl.zeros([], dtype=tl.float32)\n    NT = tl.cdiv(T, BT)\n    for i_c in range(NT):\n        i_t = NT - 1 - i_c if REVERSE else i_c\n        if HEAD_FIRST:\n            p_s = tl.make_block_ptr(\n                s + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,)\n            )\n            p_o = tl.make_block_ptr(\n                o + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,)\n            )\n        else:\n            p_s = tl.make_block_ptr(\n                s + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)\n            )\n            p_o = tl.make_block_ptr(\n                o + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)\n            )\n        b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\n        b_o = tl.cumsum(b_s, axis=0)\n        b_ss = tl.sum(b_s, 0)\n        if REVERSE:\n            b_o = -b_o + b_ss + b_s\n        b_o += b_z\n        if i_c >= 0:\n            b_z += b_ss\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 314, 210, 214, 99, 215, 99, 216, 99, 217, 99, 218, 58, 6, 99, 219, 58, 6, 99, 220, 58, 6, 99, 221, 58, 6, 99, 222, 58, 6, 99, 223, 58, 6, 152, 58, 31, -1, 224, 167, 148, 210, 315, 152, 31, 225, 99, 226, 167, 210, 224, 46, 219, 99, 224, 194, 219, 152, 31, 158, 222, 58, 31, 227, 99, 228, 167, 210, 53, 210, 216, 68, 225, 152, 75, 229, 210, 57, 152, 99, 53, 210, 216, 68, 225, 68, 316, 152, 75, 229, 210, 57, 152, 152, 31, 161, 31, 29, 58, 31, 227, 99, 228, 167, 210, 225, 211, 217, 99, 225, 211, 217, 68, 217, 152, 31, 52, 31, 217, 167, 228, 4, 227, 31, 230, 167, 153, 210, 198, 26, 99, 83, 167, 131, 152, 31, 231, 167, 60, 210, 217, 99, 220, 152, 31, 122, 232, 139, 5, 210, 231, 152, 58, 31, 233, 167, 231, 4, 316, 4, 232, 158, 221, 29, 232, 31, 158, 223, 58, 31, 234, 167, 189, 210, 214, 68, 227, 211, 219, 68, 226, 211, 217, 99, 210, 217, 99, 152, 99, 210, 316, 99, 152, 99, 210, 233, 211, 220, 99, 152, 99, 210, 220, 99, 152, 99, 210, 315, 99, 152, 152, 31, 235, 167, 189, 210, 215, 68, 227, 211, 219, 68, 226, 211, 217, 99, 210, 217, 99, 152, 99, 210, 316, 99, 152, 99, 210, 233, 211, 220, 99, 152, 99, 210, 220, 99, 152, 99, 210, 315, 99, 152, 152, 31, 161, 31, 29, 58, 31, 234, 167, 189, 210, 214, 68, 227, 211, 219, 68, 226, 99, 210, 217, 99, 152, 99, 210, 219, 99, 152, 99, 210, 233, 211, 220, 99, 152, 99, 210, 220, 99, 152, 99, 210, 315, 99, 152, 152, 31, 235, 167, 189, 210, 215, 68, 227, 211, 219, 68, 226, 99, 210, 217, 99, 152, 99, 210, 219, 99, 152, 99, 210, 233, 211, 220, 99, 152, 99, 210, 220, 99, 152, 99, 210, 315, 99, 152, 152, 31, 52, 31, 236, 167, 53, 210, 234, 99, 237, 167, 210, 315, 99, 152, 152, 75, 229, 210, 131, 152, 31, 238, 167, 77, 210, 236, 99, 239, 167, 315, 152, 31, 240, 167, 191, 210, 236, 99, 315, 152, 31, 158, 221, 58, 31, 238, 167, 4, 238, 68, 240, 68, 236, 31, 161, 31, 238, 149, 230, 31, 158, 232, 130, 315, 58, 31, 230, 149, 240, 31, 161, 31, 10, 210, 235, 99, 238, 75, 229, 210, 235, 75, 83, 75, 105, 152, 99, 237, 167, 210, 315, 99, 152, 152, 31, 71, 31, 3, 31]}, {"code": "def chunk_global_cumsum_vector_kernel(\n    s,\n    z,\n    cu_seqlens,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n    REVERSE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    HEAD_FIRST: tl.constexpr,\n):\n    i_s, i_nh = tl.program_id(0), tl.program_id(1)\n    i_n, i_h = i_nh // H, i_nh % H\n    if IS_VARLEN:\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n    else:\n        bos, eos = i_n * T, i_n * T + T\n    T = eos - bos\n\n    o_i = tl.arange(0, BT)\n    if REVERSE:\n        m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\n    else:\n        m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    NT = tl.cdiv(T, BT)\n    for i_c in range(NT):\n        i_t = NT - 1 - i_c if REVERSE else i_c\n        if HEAD_FIRST:\n            p_s = tl.make_block_ptr(\n                s + (bos * H + i_h * T) * S,\n                (T, S),\n                (S, 1),\n                (i_t * BT, i_s * BS),\n                (BT, BS),\n                (1, 0),\n            )\n            p_z = tl.make_block_ptr(\n                z + (bos * H + i_h * T) * S,\n                (T, S),\n                (S, 1),\n                (i_t * BT, i_s * BS),\n                (BT, BS),\n                (1, 0),\n            )\n        else:\n            p_s = tl.make_block_ptr(\n                s + (bos * H + i_h) * S,\n                (T, S),\n                (H * S, 1),\n                (i_t * BT, i_s * BS),\n                (BT, BS),\n                (1, 0),\n            )\n            p_z = tl.make_block_ptr(\n                z + (bos * H + i_h) * S,\n                (T, S),\n                (H * S, 1),\n                (i_t * BT, i_s * BS),\n                (BT, BS),\n                (1, 0),\n            )\n\n        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))\n        if i_c >= 0:\n            b_z += tl.sum(b_s, 0)", "encoded": [28, 314, 210, 214, 98, 215, 98, 216, 98, 217, 98, 218, 57, 6, 98, 219, 57, 6, 98, 220, 57, 6, 98, 221, 57, 6, 98, 222, 57, 6, 98, 223, 57, 6, 98, 224, 57, 6, 98, 225, 57, 6, 152, 57, 31, -1, 226, 98, 227, 167, 210, 148, 210, 315, 152, 98, 148, 210, 316, 152, 152, 31, 228, 98, 229, 167, 210, 227, 46, 219, 98, 227, 194, 219, 152, 31, 158, 224, 57, 31, 230, 98, 231, 167, 210, 51, 210, 216, 67, 228, 152, 74, 232, 210, 213, 152, 98, 51, 210, 216, 67, 228, 67, 316, 152, 74, 232, 210, 213, 152, 152, 31, 161, 31, 29, 57, 31, 230, 98, 231, 167, 210, 228, 211, 217, 98, 228, 211, 217, 67, 217, 152, 31, 52, 31, 217, 167, 231, 4, 230, 31, 233, 167, 68, 210, 315, 98, 221, 152, 31, 158, 223, 57, 31, 234, 167, 176, 210, 233, 198, 57, 98, 173, 26, 188, 233, 198, 173, 98, 57, 26, 98, 316, 98, 315, 152, 31, 161, 31, 29, 57, 31, 234, 167, 176, 210, 233, 198, 57, 98, 173, 26, 129, 233, 198, 173, 98, 57, 26, 98, 316, 98, 315, 152, 31, 52, 31, 235, 167, 153, 210, 198, 222, 26, 98, 82, 167, 130, 152, 31, 236, 167, 59, 210, 217, 98, 221, 152, 31, 121, 237, 139, 5, 210, 236, 152, 57, 31, 238, 167, 236, 4, 316, 4, 237, 158, 223, 29, 237, 31, 158, 225, 57, 31, 239, 167, 189, 210, 214, 67, 210, 230, 211, 219, 67, 229, 211, 217, 152, 211, 220, 98, 210, 217, 98, 220, 152, 98, 210, 220, 98, 316, 152, 98, 210, 238, 211, 221, 98, 226, 211, 222, 152, 98, 210, 221, 98, 222, 152, 98, 210, 316, 98, 315, 152, 152, 31, 240, 167, 189, 210, 215, 67, 210, 230, 211, 219, 67, 229, 211, 217, 152, 211, 220, 98, 210, 217, 98, 220, 152, 98, 210, 220, 98, 316, 152, 98, 210, 238, 211, 221, 98, 226, 211, 222, 152, 98, 210, 221, 98, 222, 152, 98, 210, 316, 98, 315, 152, 152, 31, 161, 31, 29, 57, 31, 239, 167, 189, 210, 214, 67, 210, 230, 211, 219, 67, 229, 152, 211, 220, 98, 210, 217, 98, 220, 152, 98, 210, 219, 211, 220, 98, 316, 152, 98, 210, 238, 211, 221, 98, 226, 211, 222, 152, 98, 210, 221, 98, 222, 152, 98, 210, 316, 98, 315, 152, 152, 31, 240, 167, 189, 210, 215, 67, 210, 230, 211, 219, 67, 229, 152, 211, 220, 98, 210, 217, 98, 220, 152, 98, 210, 219, 211, 220, 98, 316, 152, 98, 210, 238, 211, 221, 98, 226, 211, 222, 152, 98, 210, 221, 98, 222, 152, 98, 210, 316, 98, 315, 152, 152, 31, 52, 31, 241, 167, 51, 210, 239, 98, 242, 167, 210, 315, 98, 316, 152, 152, 74, 232, 210, 130, 152, 31, 243, 167, 235, 198, 173, 98, 57, 26, 67, 15, 210, 234, 98, 241, 98, 244, 167, 55, 152, 31, 10, 210, 240, 98, 243, 74, 232, 210, 240, 74, 82, 74, 104, 152, 98, 242, 167, 210, 315, 98, 316, 152, 152, 31, 158, 237, 129, 315, 57, 31, 235, 149, 191, 210, 241, 98, 315, 152, 31, 161, 31, 70, 31, 3, 31]}, {"code": "def prepare_position_ids_kernel(y, cu_seqlens, B: tl.constexpr):\n    i_n = tl.program_id(0)\n    bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(\n        tl.int32\n    )\n    T = eos - bos\n\n    o = tl.arange(0, B)\n    for i in range(0, tl.cdiv(T, B) * B, B):\n        o_i = o + i\n        tl.store(y + bos + o_i, o_i, o_i < T)", "encoded": [28, 314, 210, 214, 99, 215, 99, 216, 58, 6, 152, 58, 31, -1, 217, 167, 148, 210, 315, 152, 31, 218, 99, 219, 167, 210, 53, 210, 215, 68, 217, 152, 75, 220, 210, 57, 152, 99, 53, 210, 215, 68, 217, 68, 316, 152, 75, 220, 210, 57, 152, 152, 31, 221, 167, 219, 4, 218, 31, 222, 167, 69, 210, 315, 99, 216, 152, 31, 122, 223, 139, 5, 210, 315, 99, 60, 210, 221, 99, 216, 152, 211, 216, 99, 216, 152, 58, 31, 224, 167, 222, 68, 223, 31, 10, 210, 214, 68, 218, 68, 224, 99, 224, 99, 224, 1, 221, 152, 31, 71, 31, 3, 31]}, {"code": "def logcumsumexp_fwd_kernel(s, z, T, S: tl.constexpr, BT: tl.constexpr):\n    i_bh = tl.program_id(0)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n\n    b_mp = tl.full(\n        [\n            S,\n        ],\n        float(\"-inf\"),\n        dtype=tl.float32,\n    )\n    b_zp = tl.zeros(\n        [\n            S,\n        ],\n        dtype=tl.float32,\n    )\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(\n            s + i_bh * T * S, (T, S), (S, 1), (i_t * BT, 0), (BT, S), (1, 0)\n        )\n        p_z = tl.make_block_ptr(\n            z + i_bh * T * S, (T, S), (S, 1), (i_t * BT, 0), (BT, S), (1, 0)\n        )\n\n        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n\n        b_mc = tl.max(b_s, 0)\n        b_mc = tl.maximum(b_mp, b_mc)\n        b_zp = b_zp * exp(b_mp - b_mc)\n\n        b_s = exp(b_s - b_mc)\n        b_z = tl.dot(m_s, b_s, allow_tf32=False) + b_zp\n\n        b_zc = tl.max(b_z, 0)\n        b_mp = b_mc\n        b_zp = b_zc\n\n        b_z = log(tl.where(b_z != 0, b_z, 1e-20)) + b_mc\n        tl.store(p_z, b_z.to(p_z.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 314, 210, 214, 98, 215, 98, 216, 98, 217, 57, 6, 98, 218, 57, 6, 152, 57, 31, -1, 219, 167, 148, 210, 315, 152, 31, 220, 167, 68, 210, 315, 98, 218, 152, 31, 221, 167, 176, 210, 220, 198, 57, 98, 173, 26, 129, 220, 198, 173, 98, 57, 26, 98, 316, 98, 315, 152, 31, 222, 167, 204, 210, 198, 217, 26, 98, 223, 210, 317, 152, 98, 82, 167, 130, 152, 31, 224, 167, 153, 210, 198, 217, 26, 98, 82, 167, 130, 152, 31, 121, 225, 139, 5, 210, 59, 210, 216, 98, 218, 152, 152, 57, 31, 226, 167, 189, 210, 214, 67, 219, 211, 216, 211, 217, 98, 210, 216, 98, 217, 152, 98, 210, 217, 98, 316, 152, 98, 210, 225, 211, 218, 98, 315, 152, 98, 210, 218, 98, 217, 152, 98, 210, 316, 98, 315, 152, 152, 31, 227, 167, 189, 210, 215, 67, 219, 211, 216, 211, 217, 98, 210, 216, 98, 217, 152, 98, 210, 217, 98, 316, 152, 98, 210, 225, 211, 218, 98, 315, 152, 98, 210, 218, 98, 217, 152, 98, 210, 316, 98, 315, 152, 152, 31, 228, 167, 51, 210, 226, 98, 229, 167, 210, 315, 98, 316, 152, 152, 74, 230, 210, 130, 152, 31, 231, 167, 12, 210, 228, 98, 315, 152, 31, 231, 167, 166, 210, 222, 98, 231, 152, 31, 224, 167, 224, 211, 178, 210, 222, 4, 231, 152, 31, 228, 167, 178, 210, 228, 4, 231, 152, 31, 232, 167, 15, 210, 221, 98, 228, 98, 233, 167, 55, 152, 67, 224, 31, 234, 167, 12, 210, 232, 98, 315, 152, 31, 222, 167, 231, 31, 224, 167, 234, 31, 232, 167, 11, 210, 176, 210, 232, 162, 315, 98, 232, 98, 318, 152, 152, 67, 231, 31, 10, 210, 227, 98, 232, 74, 230, 210, 227, 74, 82, 74, 104, 152, 98, 229, 167, 210, 315, 98, 316, 152, 152, 31, 70, 31, 3, 31]}, {"code": "def logsumexp_fwd_kernel(\n    x, z, scale, D: tl.constexpr, B: tl.constexpr, HAS_SCALE: tl.constexpr\n):\n    i_n, i_d = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64)\n    o_d = i_d * B + tl.arange(0, B)\n    m_d = o_d < D\n\n    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float(\"inf\"))\n    if HAS_SCALE:\n        b_x = b_x * scale\n    b_m = tl.max(b_x, 0)\n    b_z = log(tl.sum(exp(b_x - b_m), 0)) + b_m\n    tl.store(z + i_n * tl.cdiv(D, B) + i_d, b_z)", "encoded": [28, 314, 210, 214, 99, 215, 99, 216, 99, 217, 58, 6, 99, 218, 58, 6, 99, 219, 58, 6, 152, 58, 31, -1, 220, 99, 221, 167, 210, 148, 210, 315, 152, 75, 222, 210, 157, 152, 99, 148, 210, 316, 152, 75, 222, 210, 157, 152, 152, 31, 223, 167, 221, 211, 218, 68, 69, 210, 315, 99, 218, 152, 31, 224, 167, 223, 1, 217, 31, 225, 167, 53, 210, 214, 68, 220, 211, 217, 68, 223, 99, 226, 167, 224, 99, 227, 167, 4, 228, 210, 317, 152, 152, 31, 158, 219, 58, 31, 225, 167, 225, 211, 216, 31, 161, 31, 229, 167, 12, 210, 225, 99, 315, 152, 31, 230, 167, 11, 210, 191, 210, 178, 210, 225, 4, 229, 152, 99, 315, 152, 152, 68, 229, 31, 10, 210, 215, 68, 220, 211, 60, 210, 217, 99, 218, 152, 68, 221, 99, 230, 152, 31, 3, 31]}, {"code": "def matmul_kernel(\n    a,\n    b,\n    c,\n    input,\n    alpha,\n    beta,\n    M,\n    N,\n    K,\n    stride_ab,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cb,\n    stride_cm,\n    stride_cn,\n    BM: tl.constexpr,\n    BK: tl.constexpr,\n    BN: tl.constexpr,\n    G: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n    HAS_INPUT: tl.constexpr,\n    HAS_ALPHA: tl.constexpr,\n    HAS_BETA: tl.constexpr,\n    ALLOW_TF32: tl.constexpr,\n    X_DIM: tl.constexpr = 1,\n):\n\n    i_b, i_m, i_n = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    NM, NN = tl.num_programs(1), tl.num_programs(2)\n    i_m, i_n = tl.swizzle2d(i_m, i_n, NM, NN, G)\n\n    a_batch_ptr = a + i_b * stride_ab\n    o_am = (i_m * BM + tl.arange(0, BM)) % M\n    o_bn = (i_n * BN + tl.arange(0, BN)) % N\n    o_k = tl.arange(0, BK)\n\n    p_a = a_batch_ptr + (o_am[:, None] * stride_am + o_k[None, :] * stride_ak)\n    p_b = b + (o_k[:, None] * stride_bk + o_bn[None, :] * stride_bn)\n\n    b_acc = tl.zeros((BM, BN), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BK)):\n\n        b_a = tl.load(p_a, mask=o_k[None, :] < K - k * BK, other=0.0)\n        b_b = tl.load(p_b, mask=o_k[:, None] < K - k * BK, other=0.0)\n\n        b_acc = tl.dot(b_a, b_b, acc=b_acc, allow_tf32=ALLOW_TF32)\n\n        p_a += BK * stride_ak\n        p_b += BK * stride_bk\n\n    o_cm = i_m * BM + tl.arange(0, BM)\n    o_cn = i_n * BN + tl.arange(0, BN)\n    mask = (o_cm[:, None] < M) & (o_cn[None, :] < N)\n\n    b_c = b_acc\n\n    if ACTIVATION == \"leaky_relu\":\n        b_c = leaky_relu(b_c)\n    elif ACTIVATION == \"relu\":\n        b_c = relu(b_c)\n    elif ACTIVATION == \"sigmoid\":\n        b_c = sigmoid(b_c)\n    elif ACTIVATION == \"tanh\":\n        b_c = tanh(b_c)\n\n    if HAS_ALPHA:\n        b_c *= tl.load(alpha)\n\n    if HAS_INPUT:\n        p_i = (\n            input\n            + (stride_cm * o_cm[:, None] if X_DIM == 2 else 0)\n            + stride_cn * o_cn[None, :]\n        )\n        mask_p = (o_cn[None, :] < N) if X_DIM == 1 else mask\n        b_i = tl.load(p_i, mask=mask_p, other=0.0).to(tl.float32)\n        if HAS_BETA:\n            b_i *= tl.load(beta)\n        b_c += b_i\n\n    c_batch_ptr = c + i_b * stride_cb\n    p_c = c_batch_ptr + stride_cm * o_cm[:, None] + stride_cn * o_cn[None, :]\n    tl.store(p_c, b_c.to(c.dtype.element_ty), mask=mask)", "encoded": [28, 315, 211, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 99, 222, 99, 223, 99, 224, 99, 225, 99, 226, 99, 227, 99, 228, 99, 229, 99, 230, 99, 231, 99, 232, 57, 6, 99, 233, 57, 6, 99, 234, 57, 6, 99, 235, 57, 6, 99, 236, 57, 6, 99, 237, 57, 6, 99, 238, 57, 6, 99, 239, 57, 6, 99, 240, 57, 6, 99, 241, 57, 6, 168, 316, 153, 57, 31, -1, 242, 99, 243, 99, 244, 168, 211, 149, 211, 317, 153, 99, 149, 211, 316, 153, 99, 149, 211, 318, 153, 153, 31, 245, 99, 246, 168, 211, 125, 211, 316, 153, 99, 125, 211, 318, 153, 153, 31, 243, 99, 244, 168, 63, 211, 243, 99, 244, 99, 245, 99, 246, 99, 235, 153, 31, 247, 168, 215, 68, 242, 212, 224, 31, 248, 168, 211, 243, 212, 232, 68, 69, 211, 317, 99, 232, 153, 153, 195, 221, 31, 249, 168, 211, 244, 212, 234, 68, 69, 211, 317, 99, 234, 153, 153, 195, 222, 31, 250, 168, 69, 211, 317, 99, 233, 153, 31, 251, 168, 247, 68, 211, 248, 199, 57, 99, 174, 26, 212, 225, 68, 250, 199, 174, 99, 57, 26, 212, 226, 153, 31, 252, 168, 216, 68, 211, 250, 199, 57, 99, 174, 26, 212, 227, 68, 249, 199, 174, 99, 57, 26, 212, 228, 153, 31, 253, 168, 154, 211, 211, 232, 99, 234, 153, 99, 83, 168, 131, 153, 31, 122, 254, 140, 5, 211, 317, 99, 59, 211, 223, 99, 233, 153, 153, 57, 31, 255, 168, 51, 211, 251, 99, 256, 168, 250, 199, 174, 99, 57, 26, 1, 223, 4, 254, 212, 233, 99, 257, 168, 317, 153, 31, 258, 168, 51, 211, 252, 99, 256, 168, 250, 199, 57, 99, 174, 26, 1, 223, 4, 254, 212, 233, 99, 257, 168, 317, 153, 31, 253, 168, 15, 211, 255, 99, 258, 99, 259, 168, 253, 99, 260, 168, 240, 153, 31, 251, 150, 233, 212, 226, 31, 252, 150, 233, 212, 227, 31, 71, 31, 261, 168, 243, 212, 232, 68, 69, 211, 317, 99, 232, 153, 31, 262, 168, 244, 212, 234, 68, 69, 211, 317, 99, 234, 153, 31, 256, 168, 211, 261, 199, 57, 99, 174, 26, 1, 221, 153, 151, 211, 262, 199, 174, 99, 57, 26, 1, 222, 153, 31, 263, 168, 253, 31, 159, 236, 70, 319, 57, 31, 263, 168, 264, 211, 263, 153, 31, 60, 31, 36, 236, 70, 320, 57, 31, 263, 168, 265, 211, 263, 153, 31, 60, 31, 36, 236, 70, 321, 57, 31, 263, 168, 266, 211, 263, 153, 31, 60, 31, 36, 236, 70, 322, 57, 31, 263, 168, 267, 211, 263, 153, 31, 162, 31, 159, 238, 57, 31, 263, 23, 51, 211, 219, 153, 31, 162, 31, 159, 237, 57, 31, 268, 168, 218, 68, 211, 230, 212, 261, 199, 57, 99, 174, 26, 159, 241, 70, 318, 29, 317, 153, 68, 231, 212, 262, 199, 174, 99, 57, 26, 31, 269, 168, 262, 199, 174, 99, 57, 26, 1, 222, 159, 241, 70, 316, 29, 256, 31, 270, 168, 51, 211, 268, 99, 256, 168, 269, 99, 257, 168, 317, 153, 75, 271, 211, 131, 153, 31, 159, 239, 57, 31, 270, 23, 51, 211, 220, 153, 31, 162, 31, 263, 150, 270, 31, 162, 31, 272, 168, 217, 68, 242, 212, 229, 31, 273, 168, 272, 68, 230, 212, 261, 199, 57, 99, 174, 26, 68, 231, 212, 262, 199, 174, 99, 57, 26, 31, 10, 211, 273, 99, 263, 75, 271, 211, 217, 75, 83, 75, 105, 153, 99, 256, 168, 256, 153, 31, 3, 31]}, {"code": "def packunpack_sequence_kernel(\n    x,\n    y,\n    cu_seqlens,\n    S,\n    D,\n    BD: tl.constexpr,\n    PADDING_SIDE: tl.constexpr,\n    PACK: tl.constexpr,\n):\n    i_d, i_s, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    bos, eos = tl.load(cu_seqlens + i_b), tl.load(cu_seqlens + i_b + 1)\n\n    T = eos - bos\n    if PADDING_SIDE == \"left\":\n        NP = S - T\n        if i_s < NP:\n            return\n        i_t = bos + (i_s - NP)\n    else:\n        if i_s >= T:\n            return\n        i_t = bos + i_s\n\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n\n    if PACK:\n        b_x = tl.load(x + (i_b * S + i_s) * D + o_d, mask=mask)\n        tl.store(y + i_t * D + o_d, b_x, mask=mask)\n    else:\n        b_x = tl.load(x + i_t * D + o_d, mask=mask)\n        tl.store(y + (i_b * S + i_s) * D + o_d, b_x, mask=mask)", "encoded": [28, 315, 211, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 58, 6, 99, 221, 58, 6, 99, 222, 58, 6, 153, 58, 31, -1, 223, 99, 224, 99, 225, 168, 211, 149, 211, 316, 153, 99, 149, 211, 317, 153, 99, 149, 211, 318, 153, 153, 31, 226, 99, 227, 168, 211, 53, 211, 217, 68, 225, 153, 99, 53, 211, 217, 68, 225, 68, 317, 153, 153, 31, 228, 168, 227, 4, 226, 31, 159, 221, 70, 319, 58, 31, 229, 168, 218, 4, 228, 31, 159, 224, 1, 229, 58, 31, 200, 31, 162, 31, 230, 168, 226, 68, 211, 224, 4, 229, 153, 31, 61, 31, 29, 58, 31, 159, 224, 130, 228, 58, 31, 200, 31, 162, 31, 230, 168, 226, 68, 224, 31, 231, 168, 223, 212, 220, 68, 69, 211, 316, 99, 220, 153, 31, 232, 168, 231, 1, 219, 31, 159, 222, 58, 31, 233, 168, 53, 211, 215, 68, 211, 225, 212, 218, 68, 224, 153, 212, 219, 68, 231, 99, 232, 168, 232, 153, 31, 10, 211, 216, 68, 230, 212, 219, 68, 231, 99, 233, 99, 232, 168, 232, 153, 31, 162, 31, 29, 58, 31, 233, 168, 53, 211, 215, 68, 230, 212, 219, 68, 231, 99, 232, 168, 232, 153, 31, 10, 211, 216, 68, 211, 225, 212, 218, 68, 224, 153, 212, 219, 68, 231, 99, 233, 99, 232, 168, 232, 153, 31, 52, 31, 3, 31]}, {"code": "def mean_pooling_fwd_kernel(\n    x,\n    o,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    D: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_d, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    p_x = tl.make_block_ptr(\n        x + (bos * H + i_h) * D,\n        (T, D),\n        (H * D, 1),\n        (i_t * BT, i_d * BD),\n        (BT, BD),\n        (1, 0),\n    )\n    p_o = tl.make_block_ptr(\n        o + (i_tg * H + i_h) * D, (D,), (1,), (i_d * BD,), (BD,), (0,)\n    )\n\n    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\n\n    b_o = tl.sum(b_x, axis=0) / min(BT, T - i_t * BT)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))", "encoded": [28, 315, 211, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 57, 6, 99, 221, 57, 6, 99, 222, 57, 6, 99, 223, 57, 6, 99, 224, 57, 6, 153, 57, 31, -1, 225, 99, 226, 99, 227, 168, 211, 149, 211, 316, 153, 99, 149, 211, 317, 153, 99, 149, 211, 318, 153, 153, 31, 228, 99, 229, 168, 211, 227, 46, 220, 99, 227, 195, 220, 153, 31, 159, 224, 57, 31, 230, 168, 226, 31, 231, 99, 226, 168, 211, 51, 211, 218, 68, 226, 212, 318, 153, 75, 232, 211, 214, 153, 99, 51, 211, 218, 68, 226, 212, 318, 68, 317, 153, 75, 232, 211, 214, 153, 153, 31, 233, 99, 234, 168, 211, 51, 211, 217, 68, 231, 153, 75, 232, 211, 214, 153, 99, 51, 211, 217, 68, 231, 68, 317, 153, 75, 232, 211, 214, 153, 153, 31, 219, 168, 234, 4, 233, 31, 235, 168, 59, 211, 219, 99, 222, 153, 31, 162, 31, 29, 57, 31, 235, 168, 59, 211, 219, 99, 222, 153, 31, 230, 168, 228, 212, 235, 68, 226, 31, 233, 99, 234, 168, 211, 228, 212, 219, 99, 228, 212, 219, 68, 219, 153, 31, 52, 31, 236, 168, 190, 211, 215, 68, 211, 233, 212, 220, 68, 229, 153, 212, 221, 99, 211, 219, 99, 221, 153, 99, 211, 220, 212, 221, 99, 317, 153, 99, 211, 226, 212, 222, 99, 225, 212, 223, 153, 99, 211, 222, 99, 223, 153, 99, 211, 317, 99, 316, 153, 153, 31, 237, 168, 190, 211, 216, 68, 211, 230, 212, 220, 68, 229, 153, 212, 221, 99, 211, 221, 99, 153, 99, 211, 317, 99, 153, 99, 211, 225, 212, 223, 99, 153, 99, 211, 223, 99, 153, 99, 211, 316, 99, 153, 153, 31, 238, 168, 51, 211, 236, 99, 239, 168, 211, 316, 99, 317, 153, 153, 75, 232, 211, 131, 153, 31, 240, 168, 192, 211, 238, 99, 241, 168, 316, 153, 41, 38, 211, 222, 99, 219, 4, 226, 212, 222, 153, 31, 10, 211, 237, 99, 240, 75, 232, 211, 237, 75, 83, 75, 105, 153, 99, 239, 168, 211, 316, 99, 153, 153, 31, 3, 31]}, {"code": "def mean_pooling_bwd_kernel(\n    do,\n    dx,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    D: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_d, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n        NT = tl.cdiv(T, BT)\n    else:\n        NT = tl.cdiv(T, BT)\n        i_tg = i_b * NT + i_t\n        bos, eos = i_b * T, i_b * T + T\n\n    p_dx = tl.make_block_ptr(\n        dx + (bos * H + i_h) * D,\n        (T, D),\n        (H * D, 1),\n        (i_t * BT, i_d * BD),\n        (BT, BD),\n        (1, 0),\n    )\n    p_do = tl.make_block_ptr(\n        do + (i_tg * H + i_h) * D, (D,), (1,), (i_d * BD,), (BD,), (0,)\n    )\n\n    b_do = tl.load(p_do, boundary_check=(0,)).to(tl.float32)\n\n    b_dx = b_do / tl.full((BT,), min(BT, T - i_t * BT), dtype=tl.float32)[:, None]\n    tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 315, 211, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 58, 6, 99, 221, 58, 6, 99, 222, 58, 6, 99, 223, 58, 6, 99, 224, 58, 6, 153, 58, 31, -1, 225, 99, 226, 99, 227, 168, 211, 149, 211, 316, 153, 99, 149, 211, 317, 153, 99, 149, 211, 318, 153, 153, 31, 228, 99, 229, 168, 211, 227, 46, 220, 99, 227, 195, 220, 153, 31, 159, 224, 58, 31, 230, 168, 226, 31, 231, 99, 226, 168, 211, 53, 211, 218, 68, 226, 212, 318, 153, 75, 232, 211, 57, 153, 99, 53, 211, 218, 68, 226, 212, 318, 68, 317, 153, 75, 232, 211, 57, 153, 153, 31, 233, 99, 234, 168, 211, 53, 211, 217, 68, 231, 153, 75, 232, 211, 57, 153, 99, 53, 211, 217, 68, 231, 68, 317, 153, 75, 232, 211, 57, 153, 153, 31, 219, 168, 234, 4, 233, 31, 235, 168, 60, 211, 219, 99, 222, 153, 31, 162, 31, 29, 58, 31, 235, 168, 60, 211, 219, 99, 222, 153, 31, 230, 168, 228, 212, 235, 68, 226, 31, 233, 99, 234, 168, 211, 228, 212, 219, 99, 228, 212, 219, 68, 219, 153, 31, 52, 31, 236, 168, 190, 211, 216, 68, 211, 233, 212, 220, 68, 229, 153, 212, 221, 99, 211, 219, 99, 221, 153, 99, 211, 220, 212, 221, 99, 317, 153, 99, 211, 226, 212, 222, 99, 225, 212, 223, 153, 99, 211, 222, 99, 223, 153, 99, 211, 317, 99, 316, 153, 153, 31, 237, 168, 190, 211, 215, 68, 211, 230, 212, 220, 68, 229, 153, 212, 221, 99, 211, 221, 99, 153, 99, 211, 317, 99, 153, 99, 211, 225, 212, 223, 99, 153, 99, 211, 223, 99, 153, 99, 211, 316, 99, 153, 153, 31, 238, 168, 53, 211, 237, 99, 239, 168, 211, 316, 99, 153, 153, 75, 232, 211, 131, 153, 31, 240, 168, 238, 41, 205, 211, 211, 222, 99, 153, 99, 38, 211, 222, 99, 219, 4, 226, 212, 222, 153, 99, 83, 168, 131, 153, 199, 58, 99, 174, 26, 31, 10, 211, 236, 99, 240, 75, 232, 211, 236, 75, 83, 75, 105, 153, 99, 239, 168, 211, 316, 99, 317, 153, 153, 31, 3, 31]}, {"code": "def softmax_fwd_kernel(x, p, D: tl.constexpr, B: tl.constexpr):\n    i_n = tl.program_id(0)\n    o_d = tl.arange(0, B)\n    m_d = o_d < D\n\n    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float(\"inf\"))\n    b_m = tl.max(b_x, 0)\n    b_x = exp(b_x - b_m)\n    b_p = b_x / tl.sum(b_x, 0)\n\n    tl.store(p + i_n * D + o_d, b_p.to(p.dtype.element_ty), mask=m_d)", "encoded": [28, 315, 211, 215, 99, 216, 99, 217, 57, 6, 99, 218, 57, 6, 153, 57, 31, -1, 219, 168, 149, 211, 316, 153, 31, 220, 168, 69, 211, 316, 99, 218, 153, 31, 221, 168, 220, 1, 217, 31, 222, 168, 51, 211, 215, 68, 219, 212, 217, 68, 220, 99, 223, 168, 221, 99, 224, 168, 4, 225, 211, 317, 153, 153, 31, 226, 168, 12, 211, 222, 99, 316, 153, 31, 222, 168, 179, 211, 222, 4, 226, 153, 31, 227, 168, 222, 41, 192, 211, 222, 99, 316, 153, 31, 10, 211, 216, 68, 219, 212, 217, 68, 220, 99, 227, 75, 228, 211, 216, 75, 83, 75, 105, 153, 99, 223, 168, 221, 153, 31, 3, 31]}, {"code": "def softmax_bwd_kernel(p, dp, ds, D: tl.constexpr, B: tl.constexpr):\n    i_n = tl.program_id(0)\n    o_d = tl.arange(0, B)\n    m_d = o_d < D\n\n    b_p = tl.load(p + i_n * D + o_d, mask=m_d, other=0.0)\n    b_dp = tl.load(dp + i_n * D + o_d, mask=m_d, other=0.0)\n    b_pp = tl.sum(b_p * b_dp, 0)\n    b_ds = b_p * b_dp - b_p * b_pp\n    tl.store(ds + i_n * D + o_d, b_ds.to(ds.dtype.element_ty), mask=m_d)", "encoded": [28, 315, 211, 215, 99, 216, 99, 217, 99, 218, 58, 6, 99, 219, 58, 6, 153, 58, 31, -1, 220, 168, 149, 211, 316, 153, 31, 221, 168, 69, 211, 316, 99, 219, 153, 31, 222, 168, 221, 1, 218, 31, 223, 168, 53, 211, 215, 68, 220, 212, 218, 68, 221, 99, 224, 168, 222, 99, 225, 168, 316, 153, 31, 226, 168, 53, 211, 216, 68, 220, 212, 218, 68, 221, 99, 224, 168, 222, 99, 225, 168, 316, 153, 31, 227, 168, 192, 211, 223, 212, 226, 99, 316, 153, 31, 228, 168, 223, 212, 226, 4, 223, 212, 227, 31, 10, 211, 217, 68, 220, 212, 218, 68, 221, 99, 228, 75, 229, 211, 217, 75, 83, 75, 105, 153, 99, 224, 168, 222, 153, 31, 3, 31]}, {"code": "def solve_tril_16x16_kernel(\n    A,\n    Ad,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    BT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    A = A + (bos * H + i_h) * BT\n    Ad = Ad + (bos * H + i_h) * 16\n\n    offset = (i_t * 16) % BT\n    p_A = tl.make_block_ptr(\n        A, (T, BT), (H * BT, 1), (i_t * 16, offset), (16, 16), (1, 0)\n    )\n    p_Ai = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 16, 0), (16, 16), (1, 0))\n    b_A = tl.load(p_A, boundary_check=(0, 1)).to(tl.float32)\n    b_A = -tl.where(tl.arange(0, 16)[:, None] > tl.arange(0, 16)[None, :], b_A, 0)\n\n    o_i = tl.arange(0, 16)\n    for i in range(1, min(16, T - i_t * 16)):\n        b_a = -tl.load(A + (i_t * 16 + i) * H * BT + o_i + offset)\n        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0)\n        mask = o_i == i\n        b_A = tl.where(mask[:, None], b_a, b_A)\n    b_A += o_i[:, None] == o_i[None, :]\n    tl.store(\n        p_Ai,\n        b_A.to(p_Ai.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )", "encoded": [28, 315, 211, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 57, 6, 99, 221, 57, 6, 99, 222, 57, 6, 153, 57, 31, -1, 223, 99, 224, 168, 211, 149, 211, 316, 153, 99, 149, 211, 317, 153, 153, 31, 225, 99, 226, 168, 211, 224, 46, 220, 99, 224, 195, 220, 153, 31, 159, 222, 57, 31, 227, 99, 223, 168, 211, 51, 211, 218, 68, 223, 212, 318, 153, 75, 228, 211, 214, 153, 99, 51, 211, 218, 68, 223, 212, 318, 68, 317, 153, 75, 228, 211, 214, 153, 153, 31, 229, 99, 230, 168, 211, 51, 211, 217, 68, 227, 153, 75, 228, 211, 214, 153, 99, 51, 211, 217, 68, 227, 68, 317, 153, 75, 228, 211, 214, 153, 153, 31, 219, 168, 230, 4, 229, 31, 162, 31, 29, 57, 31, 229, 99, 230, 168, 211, 225, 212, 219, 99, 225, 212, 219, 68, 219, 153, 31, 52, 31, 215, 168, 215, 68, 211, 229, 212, 220, 68, 226, 153, 212, 221, 31, 216, 168, 216, 68, 211, 229, 212, 220, 68, 226, 153, 212, 317, 319, 31, 231, 168, 223, 212, 317, 319, 195, 221, 31, 232, 168, 190, 211, 215, 99, 211, 219, 99, 221, 153, 99, 211, 220, 212, 221, 99, 317, 153, 99, 211, 223, 212, 317, 319, 99, 231, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 233, 168, 190, 211, 216, 99, 211, 219, 99, 317, 319, 153, 99, 211, 220, 212, 317, 319, 99, 317, 153, 99, 211, 223, 212, 317, 319, 99, 316, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 234, 168, 51, 211, 232, 99, 235, 168, 211, 316, 99, 317, 153, 153, 75, 228, 211, 131, 153, 31, 234, 168, 4, 177, 211, 69, 211, 316, 99, 317, 319, 153, 199, 57, 99, 174, 26, 113, 69, 211, 316, 99, 317, 319, 153, 199, 174, 99, 57, 26, 99, 234, 99, 316, 153, 31, 236, 168, 69, 211, 316, 99, 317, 319, 153, 31, 122, 237, 140, 5, 211, 317, 99, 38, 211, 317, 319, 99, 219, 4, 223, 212, 317, 319, 153, 153, 57, 31, 238, 168, 4, 51, 211, 215, 68, 211, 223, 212, 317, 319, 68, 237, 153, 212, 220, 212, 221, 68, 236, 68, 231, 153, 31, 238, 168, 238, 68, 192, 211, 238, 199, 57, 99, 174, 26, 212, 234, 99, 316, 153, 31, 239, 168, 236, 70, 237, 31, 234, 168, 177, 211, 239, 199, 57, 99, 174, 26, 99, 238, 99, 234, 153, 31, 71, 31, 234, 150, 236, 199, 57, 99, 174, 26, 70, 236, 199, 174, 99, 57, 26, 31, 10, 211, 233, 99, 234, 75, 228, 211, 233, 75, 83, 75, 105, 99, 146, 168, 320, 153, 99, 235, 168, 211, 316, 99, 317, 153, 153, 31, 3, 31]}, {"code": "def merge_16x16_to_32x32_inverse_kernel(\n    A,\n    Ad,\n    Ai,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    BT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    A += (bos * H + i_h) * 32\n    Ad += (bos * H + i_h) * 16\n    Ai += (bos * H + i_h) * 32\n\n    p_A_21 = tl.make_block_ptr(\n        A, (T, 32), (H * 32, 1), (i_t * 32 + 16, 0), (16, 16), (1, 0)\n    )\n    p_Ad_11 = tl.make_block_ptr(\n        Ad, (T, 16), (H * 16, 1), (i_t * 32, 0), (16, 16), (1, 0)\n    )\n    p_Ad_22 = tl.make_block_ptr(\n        Ad, (T, 16), (H * 16, 1), (i_t * 32 + 16, 0), (16, 16), (1, 0)\n    )\n    p_Ai_11 = tl.make_block_ptr(\n        Ai, (T, 32), (H * 32, 1), (i_t * 32, 0), (16, 16), (1, 0)\n    )\n    p_Ai_22 = tl.make_block_ptr(\n        Ai, (T, 32), (H * 32, 1), (i_t * 32 + 16, 16), (16, 16), (1, 0)\n    )\n    p_Ai_21 = tl.make_block_ptr(\n        Ai, (T, 32), (H * 32, 1), (i_t * 32 + 16, 0), (16, 16), (1, 0)\n    )\n\n    A_21 = tl.load(p_A_21, boundary_check=(0, 1)).to(tl.float32)\n    Ai_11 = tl.load(p_Ad_11, boundary_check=(0, 1)).to(tl.float32)\n    Ai_22 = tl.load(p_Ad_22, boundary_check=(0, 1)).to(tl.float32)\n    Ai_21 = -tl.dot(\n        tl.dot(Ai_22, A_21, input_precision=\"ieee\"), Ai_11, input_precision=\"ieee\"\n    )\n    tl.store(\n        p_Ai_11,\n        Ai_11.to(p_Ai_11.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_Ai_22,\n        Ai_22.to(p_Ai_22.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_Ai_21,\n        Ai_21.to(p_Ai_21.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )", "encoded": [28, 315, 211, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 58, 6, 99, 222, 58, 6, 99, 223, 58, 6, 153, 58, 31, -1, 224, 99, 225, 168, 211, 149, 211, 316, 153, 99, 149, 211, 317, 153, 153, 31, 226, 99, 227, 168, 211, 225, 46, 221, 99, 225, 195, 221, 153, 31, 159, 223, 58, 31, 228, 99, 224, 168, 211, 53, 211, 219, 68, 224, 212, 318, 153, 75, 229, 211, 57, 153, 99, 53, 211, 219, 68, 224, 212, 318, 68, 317, 153, 75, 229, 211, 57, 153, 153, 31, 230, 99, 231, 168, 211, 53, 211, 218, 68, 228, 153, 75, 229, 211, 57, 153, 99, 53, 211, 218, 68, 228, 68, 317, 153, 75, 229, 211, 57, 153, 153, 31, 220, 168, 231, 4, 230, 31, 162, 31, 29, 58, 31, 230, 99, 231, 168, 211, 226, 212, 220, 99, 226, 212, 220, 68, 220, 153, 31, 52, 31, 215, 150, 211, 230, 212, 221, 68, 227, 153, 212, 319, 318, 31, 216, 150, 211, 230, 212, 221, 68, 227, 153, 212, 317, 320, 31, 217, 150, 211, 230, 212, 221, 68, 227, 153, 212, 319, 318, 31, 232, 168, 190, 211, 215, 99, 211, 220, 99, 319, 318, 153, 99, 211, 221, 212, 319, 318, 99, 317, 153, 99, 211, 224, 212, 319, 318, 68, 317, 320, 99, 316, 153, 99, 211, 317, 320, 99, 317, 320, 153, 99, 211, 317, 99, 316, 153, 153, 31, 233, 168, 190, 211, 216, 99, 211, 220, 99, 317, 320, 153, 99, 211, 221, 212, 317, 320, 99, 317, 153, 99, 211, 224, 212, 319, 318, 99, 316, 153, 99, 211, 317, 320, 99, 317, 320, 153, 99, 211, 317, 99, 316, 153, 153, 31, 234, 168, 190, 211, 216, 99, 211, 220, 99, 317, 320, 153, 99, 211, 221, 212, 317, 320, 99, 317, 153, 99, 211, 224, 212, 319, 318, 68, 317, 320, 99, 316, 153, 99, 211, 317, 320, 99, 317, 320, 153, 99, 211, 317, 99, 316, 153, 153, 31, 235, 168, 190, 211, 217, 99, 211, 220, 99, 319, 318, 153, 99, 211, 221, 212, 319, 318, 99, 317, 153, 99, 211, 224, 212, 319, 318, 99, 316, 153, 99, 211, 317, 320, 99, 317, 320, 153, 99, 211, 317, 99, 316, 153, 153, 31, 236, 168, 190, 211, 217, 99, 211, 220, 99, 319, 318, 153, 99, 211, 221, 212, 319, 318, 99, 317, 153, 99, 211, 224, 212, 319, 318, 68, 317, 320, 99, 317, 320, 153, 99, 211, 317, 320, 99, 317, 320, 153, 99, 211, 317, 99, 316, 153, 153, 31, 237, 168, 190, 211, 217, 99, 211, 220, 99, 319, 318, 153, 99, 211, 221, 212, 319, 318, 99, 317, 153, 99, 211, 224, 212, 319, 318, 68, 317, 320, 99, 316, 153, 99, 211, 317, 320, 99, 317, 320, 153, 99, 211, 317, 99, 316, 153, 153, 31, 238, 168, 53, 211, 232, 99, 239, 168, 211, 316, 99, 317, 153, 153, 75, 229, 211, 131, 153, 31, 240, 168, 53, 211, 233, 99, 239, 168, 211, 316, 99, 317, 153, 153, 75, 229, 211, 131, 153, 31, 241, 168, 53, 211, 234, 99, 239, 168, 211, 316, 99, 317, 153, 153, 75, 229, 211, 131, 153, 31, 242, 168, 4, 15, 211, 15, 211, 241, 99, 238, 99, 243, 168, 321, 153, 99, 240, 99, 243, 168, 321, 153, 31, 10, 211, 235, 99, 240, 75, 229, 211, 235, 75, 83, 75, 105, 99, 146, 168, 322, 153, 99, 239, 168, 211, 316, 99, 317, 153, 153, 31, 10, 211, 236, 99, 241, 75, 229, 211, 236, 75, 83, 75, 105, 99, 146, 168, 322, 153, 99, 239, 168, 211, 316, 99, 317, 153, 153, 31, 10, 211, 237, 99, 242, 75, 229, 211, 237, 75, 83, 75, 105, 99, 146, 168, 322, 153, 99, 239, 168, 211, 316, 99, 317, 153, 153, 31, 3, 31]}, {"code": "def merge_16x16_to_64x64_inverse_kernel(\n    A,\n    Ad,\n    Ai,\n    cu_seqlens,\n    chunk_indices,\n    T,\n    H: tl.constexpr,\n    BT: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(\n            chunk_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(\n            cu_seqlens + i_n + 1\n        ).to(tl.int32)\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    A += (bos * H + i_h) * 64\n    Ad += (bos * H + i_h) * 16\n    Ai += (bos * H + i_h) * 64\n\n    p_A_21 = tl.make_block_ptr(\n        A, (T, 64), (H * 64, 1), (i_t * 64 + 16, 0), (16, 16), (1, 0)\n    )\n    p_A_32 = tl.make_block_ptr(\n        A, (T, 64), (H * 64, 1), (i_t * 64 + 32, 16), (16, 16), (1, 0)\n    )\n    p_A_31 = tl.make_block_ptr(\n        A, (T, 64), (H * 64, 1), (i_t * 64 + 32, 0), (16, 16), (1, 0)\n    )\n    p_A_43 = tl.make_block_ptr(\n        A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 32), (16, 16), (1, 0)\n    )\n    p_A_42 = tl.make_block_ptr(\n        A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 16), (16, 16), (1, 0)\n    )\n    p_A_41 = tl.make_block_ptr(\n        A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 0), (16, 16), (1, 0)\n    )\n    p_Ad_11 = tl.make_block_ptr(\n        Ad, (T, 16), (H * 16, 1), (i_t * 64, 0), (16, 16), (1, 0)\n    )\n    p_Ad_22 = tl.make_block_ptr(\n        Ad, (T, 16), (H * 16, 1), (i_t * 64 + 16, 0), (16, 16), (1, 0)\n    )\n    p_Ad_33 = tl.make_block_ptr(\n        Ad, (T, 16), (H * 16, 1), (i_t * 64 + 32, 0), (16, 16), (1, 0)\n    )\n    p_Ad_44 = tl.make_block_ptr(\n        Ad, (T, 16), (H * 16, 1), (i_t * 64 + 48, 0), (16, 16), (1, 0)\n    )\n\n    A_21 = tl.load(p_A_21, boundary_check=(0, 1)).to(tl.float32)\n    A_32 = tl.load(p_A_32, boundary_check=(0, 1)).to(tl.float32)\n    A_31 = tl.load(p_A_31, boundary_check=(0, 1)).to(tl.float32)\n    A_43 = tl.load(p_A_43, boundary_check=(0, 1)).to(tl.float32)\n    A_42 = tl.load(p_A_42, boundary_check=(0, 1)).to(tl.float32)\n    A_41 = tl.load(p_A_41, boundary_check=(0, 1)).to(tl.float32)\n\n    Ai_11 = tl.load(p_Ad_11, boundary_check=(0, 1)).to(tl.float32)\n    Ai_22 = tl.load(p_Ad_22, boundary_check=(0, 1)).to(tl.float32)\n    Ai_33 = tl.load(p_Ad_33, boundary_check=(0, 1)).to(tl.float32)\n    Ai_44 = tl.load(p_Ad_44, boundary_check=(0, 1)).to(tl.float32)\n\n    Ai_21 = -tl.dot(\n        tl.dot(Ai_22, A_21, input_precision=\"ieee\"), Ai_11, input_precision=\"ieee\"\n    )\n    Ai_32 = -tl.dot(\n        tl.dot(Ai_33, A_32, input_precision=\"ieee\"), Ai_22, input_precision=\"ieee\"\n    )\n    Ai_43 = -tl.dot(\n        tl.dot(Ai_44, A_43, input_precision=\"ieee\"), Ai_33, input_precision=\"ieee\"\n    )\n\n    Ai_31 = -tl.dot(\n        Ai_33,\n        tl.dot(A_31, Ai_11, input_precision=\"ieee\")\n        + tl.dot(A_32, Ai_21, input_precision=\"ieee\"),\n        input_precision=\"ieee\",\n    )\n    Ai_42 = -tl.dot(\n        Ai_44,\n        tl.dot(A_42, Ai_22, input_precision=\"ieee\")\n        + tl.dot(A_43, Ai_32, input_precision=\"ieee\"),\n        input_precision=\"ieee\",\n    )\n    Ai_41 = -tl.dot(\n        Ai_44,\n        tl.dot(A_41, Ai_11, input_precision=\"ieee\")\n        + tl.dot(A_42, Ai_21, input_precision=\"ieee\")\n        + tl.dot(A_43, Ai_31, input_precision=\"ieee\"),\n        input_precision=\"ieee\",\n    )\n\n    p_Ai_11 = tl.make_block_ptr(\n        Ai, (T, 64), (H * 64, 1), (i_t * 64, 0), (16, 16), (1, 0)\n    )\n    p_Ai_22 = tl.make_block_ptr(\n        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 16, 16), (16, 16), (1, 0)\n    )\n    p_Ai_33 = tl.make_block_ptr(\n        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 32), (16, 16), (1, 0)\n    )\n    p_Ai_44 = tl.make_block_ptr(\n        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 48), (16, 16), (1, 0)\n    )\n    p_Ai_21 = tl.make_block_ptr(\n        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 16, 0), (16, 16), (1, 0)\n    )\n    p_Ai_31 = tl.make_block_ptr(\n        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 0), (16, 16), (1, 0)\n    )\n    p_Ai_32 = tl.make_block_ptr(\n        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 16), (16, 16), (1, 0)\n    )\n    p_Ai_41 = tl.make_block_ptr(\n        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 0), (16, 16), (1, 0)\n    )\n    p_Ai_42 = tl.make_block_ptr(\n        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 16), (16, 16), (1, 0)\n    )\n    p_Ai_43 = tl.make_block_ptr(\n        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 32), (16, 16), (1, 0)\n    )\n    tl.store(\n        p_Ai_11,\n        Ai_11.to(p_Ai_11.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_Ai_22,\n        Ai_22.to(p_Ai_22.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_Ai_33,\n        Ai_33.to(p_Ai_33.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_Ai_44,\n        Ai_44.to(p_Ai_44.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_Ai_21,\n        Ai_21.to(p_Ai_21.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_Ai_31,\n        Ai_31.to(p_Ai_31.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_Ai_32,\n        Ai_32.to(p_Ai_32.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_Ai_41,\n        Ai_41.to(p_Ai_41.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_Ai_42,\n        Ai_42.to(p_Ai_42.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )\n    tl.store(\n        p_Ai_43,\n        Ai_43.to(p_Ai_43.dtype.element_ty, fp_downcast_rounding=\"rtne\"),\n        boundary_check=(0, 1),\n    )", "encoded": [28, 315, 211, 215, 99, 216, 99, 217, 99, 218, 99, 219, 99, 220, 99, 221, 57, 6, 99, 222, 57, 6, 99, 223, 57, 6, 153, 57, 31, -1, 224, 99, 225, 168, 211, 149, 211, 316, 153, 99, 149, 211, 317, 153, 153, 31, 226, 99, 227, 168, 211, 225, 46, 221, 99, 225, 195, 221, 153, 31, 159, 223, 57, 31, 228, 99, 224, 168, 211, 51, 211, 219, 68, 224, 212, 318, 153, 75, 229, 211, 214, 153, 99, 51, 211, 219, 68, 224, 212, 318, 68, 317, 153, 75, 229, 211, 214, 153, 153, 31, 230, 99, 231, 168, 211, 51, 211, 218, 68, 228, 153, 75, 229, 211, 214, 153, 99, 51, 211, 218, 68, 228, 68, 317, 153, 75, 229, 211, 214, 153, 153, 31, 220, 168, 231, 4, 230, 31, 162, 31, 29, 57, 31, 230, 99, 231, 168, 211, 226, 212, 220, 99, 226, 212, 220, 68, 220, 153, 31, 52, 31, 215, 150, 211, 230, 212, 221, 68, 227, 153, 212, 319, 320, 31, 216, 150, 211, 230, 212, 221, 68, 227, 153, 212, 317, 319, 31, 217, 150, 211, 230, 212, 221, 68, 227, 153, 212, 319, 320, 31, 232, 168, 190, 211, 215, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 317, 319, 99, 316, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 233, 168, 190, 211, 215, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 321, 318, 99, 317, 319, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 234, 168, 190, 211, 215, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 321, 318, 99, 316, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 235, 168, 190, 211, 215, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 320, 322, 99, 321, 318, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 236, 168, 190, 211, 215, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 320, 322, 99, 317, 319, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 237, 168, 190, 211, 215, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 320, 322, 99, 316, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 238, 168, 190, 211, 216, 99, 211, 220, 99, 317, 319, 153, 99, 211, 221, 212, 317, 319, 99, 317, 153, 99, 211, 224, 212, 319, 320, 99, 316, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 239, 168, 190, 211, 216, 99, 211, 220, 99, 317, 319, 153, 99, 211, 221, 212, 317, 319, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 317, 319, 99, 316, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 240, 168, 190, 211, 216, 99, 211, 220, 99, 317, 319, 153, 99, 211, 221, 212, 317, 319, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 321, 318, 99, 316, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 241, 168, 190, 211, 216, 99, 211, 220, 99, 317, 319, 153, 99, 211, 221, 212, 317, 319, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 320, 322, 99, 316, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 242, 168, 51, 211, 232, 99, 243, 168, 211, 316, 99, 317, 153, 153, 75, 229, 211, 131, 153, 31, 244, 168, 51, 211, 233, 99, 243, 168, 211, 316, 99, 317, 153, 153, 75, 229, 211, 131, 153, 31, 245, 168, 51, 211, 234, 99, 243, 168, 211, 316, 99, 317, 153, 153, 75, 229, 211, 131, 153, 31, 246, 168, 51, 211, 235, 99, 243, 168, 211, 316, 99, 317, 153, 153, 75, 229, 211, 131, 153, 31, 247, 168, 51, 211, 236, 99, 243, 168, 211, 316, 99, 317, 153, 153, 75, 229, 211, 131, 153, 31, 248, 168, 51, 211, 237, 99, 243, 168, 211, 316, 99, 317, 153, 153, 75, 229, 211, 131, 153, 31, 249, 168, 51, 211, 238, 99, 243, 168, 211, 316, 99, 317, 153, 153, 75, 229, 211, 131, 153, 31, 250, 168, 51, 211, 239, 99, 243, 168, 211, 316, 99, 317, 153, 153, 75, 229, 211, 131, 153, 31, 251, 168, 51, 211, 240, 99, 243, 168, 211, 316, 99, 317, 153, 153, 75, 229, 211, 131, 153, 31, 252, 168, 51, 211, 241, 99, 243, 168, 211, 316, 99, 317, 153, 153, 75, 229, 211, 131, 153, 31, 253, 168, 4, 15, 211, 15, 211, 250, 99, 242, 99, 254, 168, 323, 153, 99, 249, 99, 254, 168, 323, 153, 31, 255, 168, 4, 15, 211, 15, 211, 251, 99, 244, 99, 254, 168, 323, 153, 99, 250, 99, 254, 168, 323, 153, 31, 256, 168, 4, 15, 211, 15, 211, 252, 99, 246, 99, 254, 168, 323, 153, 99, 251, 99, 254, 168, 323, 153, 31, 257, 168, 4, 15, 211, 251, 99, 15, 211, 245, 99, 249, 99, 254, 168, 323, 153, 68, 15, 211, 244, 99, 253, 99, 254, 168, 323, 153, 99, 254, 168, 323, 153, 31, 258, 168, 4, 15, 211, 252, 99, 15, 211, 247, 99, 250, 99, 254, 168, 323, 153, 68, 15, 211, 246, 99, 255, 99, 254, 168, 323, 153, 99, 254, 168, 323, 153, 31, 259, 168, 4, 15, 211, 252, 99, 15, 211, 248, 99, 249, 99, 254, 168, 323, 153, 68, 15, 211, 247, 99, 253, 99, 254, 168, 323, 153, 68, 15, 211, 246, 99, 257, 99, 254, 168, 323, 153, 99, 254, 168, 323, 153, 31, 260, 168, 190, 211, 217, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 99, 316, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 261, 168, 190, 211, 217, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 317, 319, 99, 317, 319, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 262, 168, 190, 211, 217, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 321, 318, 99, 321, 318, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 263, 168, 190, 211, 217, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 320, 322, 99, 320, 322, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 264, 168, 190, 211, 217, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 317, 319, 99, 316, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 265, 168, 190, 211, 217, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 321, 318, 99, 316, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 266, 168, 190, 211, 217, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 321, 318, 99, 317, 319, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 267, 168, 190, 211, 217, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 320, 322, 99, 316, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 268, 168, 190, 211, 217, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 320, 322, 99, 317, 319, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 269, 168, 190, 211, 217, 99, 211, 220, 99, 319, 320, 153, 99, 211, 221, 212, 319, 320, 99, 317, 153, 99, 211, 224, 212, 319, 320, 68, 320, 322, 99, 321, 318, 153, 99, 211, 317, 319, 99, 317, 319, 153, 99, 211, 317, 99, 316, 153, 153, 31, 10, 211, 260, 99, 249, 75, 229, 211, 260, 75, 83, 75, 105, 99, 146, 168, 324, 153, 99, 243, 168, 211, 316, 99, 317, 153, 153, 31, 10, 211, 261, 99, 250, 75, 229, 211, 261, 75, 83, 75, 105, 99, 146, 168, 324, 153, 99, 243, 168, 211, 316, 99, 317, 153, 153, 31, 10, 211, 262, 99, 251, 75, 229, 211, 262, 75, 83, 75, 105, 99, 146, 168, 324, 153, 99, 243, 168, 211, 316, 99, 317, 153, 153, 31, 10, 211, 263, 99, 252, 75, 229, 211, 263, 75, 83, 75, 105, 99, 146, 168, 324, 153, 99, 243, 168, 211, 316, 99, 317, 153, 153, 31, 10, 211, 264, 99, 253, 75, 229, 211, 264, 75, 83, 75, 105, 99, 146, 168, 324, 153, 99, 243, 168, 211, 316, 99, 317, 153, 153, 31, 10, 211, 265, 99, 257, 75, 229, 211, 265, 75, 83, 75, 105, 99, 146, 168, 324, 153, 99, 243, 168, 211, 316, 99, 317, 153, 153, 31, 10, 211, 266, 99, 255, 75, 229, 211, 266, 75, 83, 75, 105, 99, 146, 168, 324, 153, 99, 243, 168, 211, 316, 99, 317, 153, 153, 31, 10, 211, 267, 99, 259, 75, 229, 211, 267, 75, 83, 75, 105, 99, 146, 168, 324, 153, 99, 243, 168, 211, 316, 99, 317, 153, 153, 31, 10, 211, 268, 99, 258, 75, 229, 211, 268, 75, 83, 75, 105, 99, 146, 168, 324, 153, 99, 243, 168, 211, 316, 99, 317, 153, 153, 31, 10, 211, 269, 99, 256, 75, 229, 211, 269, 75, 83, 75, 105, 99, 146, 168, 324, 153, 99, 243, 168, 211, 316, 99, 317, 153, 153, 31, 3, 31]}, {"code": "def parallel_nsa_compression_fwd_kernel(\n    q,\n    k,\n    v,\n    o,\n    lse,\n    scale,\n    offsets,\n    token_indices,\n    chunk_offsets,\n    T,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BC: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_OFFSETS: tl.constexpr,\n):\n    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if USE_OFFSETS:\n        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(\n            token_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(\n            tl.int32\n        )\n        T = eos - bos\n        boc = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        boc = i_b * tl.cdiv(T, BS)\n\n    p_q = tl.make_block_ptr(\n        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    TC = tl.cdiv(T, BS)\n\n    NC = (i_t + 1) // BS\n\n    p_o = tl.make_block_ptr(\n        o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0)\n    )\n\n    b_o = tl.zeros([G, BV], dtype=tl.float32)\n\n    b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)\n\n    b_acc = tl.zeros([G], dtype=tl.float32)\n\n    for i_c in range(0, NC, BC):\n        o_c = i_c + tl.arange(0, BC)\n\n        p_k = tl.make_block_ptr(\n            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + (boc * H + i_h) * V,\n            (TC, V),\n            (H * V, 1),\n            (i_c, i_v * BV),\n            (BC, BV),\n            (1, 0),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, b_k)\n        b_s = tl.where((o_c < NC)[None, :], b_s, float(\"-inf\"))\n\n        b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m\n        b_r = tl.exp(b_mp - b_m)\n\n        b_p = tl.exp(b_s - b_m[:, None])\n\n        b_acc = b_acc * b_r + tl.sum(b_p, 1)\n\n        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)\n\n        b_mp = b_m\n    if NC == 0:\n        b_lse = tl.zeros([G], dtype=tl.float32)\n    else:\n        b_o = b_o / b_acc[:, None]\n        b_lse = b_m + tl.log(b_acc)\n\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    if i_v == 0:\n        tl.store(\n            lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G),\n            b_lse.to(lse.dtype.element_ty),\n        )", "encoded": [28, 316, 212, 216, 100, 217, 100, 218, 100, 219, 100, 220, 100, 221, 100, 222, 100, 223, 100, 224, 100, 225, 100, 226, 59, 6, 100, 227, 59, 6, 100, 228, 59, 6, 100, 229, 59, 6, 100, 230, 59, 6, 100, 231, 59, 6, 100, 232, 59, 6, 100, 233, 59, 6, 100, 234, 59, 6, 100, 235, 59, 6, 154, 59, 31, -1, 236, 100, 237, 100, 238, 169, 212, 150, 212, 317, 154, 100, 150, 212, 318, 154, 100, 150, 212, 319, 154, 154, 31, 239, 100, 240, 169, 212, 238, 46, 226, 100, 238, 196, 226, 154, 31, 160, 235, 59, 31, 241, 100, 236, 169, 212, 54, 212, 223, 69, 236, 213, 319, 154, 76, 242, 212, 58, 154, 100, 54, 212, 223, 69, 236, 213, 319, 69, 318, 154, 76, 242, 212, 58, 154, 154, 31, 243, 100, 244, 169, 212, 54, 212, 222, 69, 241, 154, 76, 242, 212, 58, 154, 100, 54, 212, 222, 69, 241, 69, 318, 154, 76, 242, 212, 58, 154, 154, 31, 225, 169, 244, 4, 243, 31, 245, 169, 54, 212, 224, 69, 241, 154, 76, 242, 212, 58, 154, 31, 163, 31, 29, 59, 31, 243, 100, 244, 169, 212, 239, 213, 225, 100, 239, 213, 225, 69, 225, 154, 31, 245, 169, 239, 213, 61, 212, 225, 100, 232, 154, 31, 53, 31, 246, 169, 191, 212, 216, 69, 212, 243, 69, 236, 154, 213, 227, 213, 229, 100, 212, 227, 100, 229, 154, 100, 212, 229, 100, 318, 154, 100, 212, 240, 213, 228, 100, 317, 154, 100, 212, 228, 100, 233, 154, 100, 212, 318, 100, 317, 154, 154, 31, 247, 169, 54, 212, 246, 100, 248, 169, 212, 317, 100, 318, 154, 154, 31, 247, 169, 212, 247, 213, 221, 154, 76, 242, 212, 247, 76, 84, 154, 31, 249, 169, 61, 212, 225, 100, 232, 154, 31, 250, 169, 212, 236, 69, 318, 154, 46, 232, 31, 251, 169, 191, 212, 219, 69, 212, 243, 69, 236, 154, 213, 227, 213, 230, 100, 212, 227, 100, 230, 154, 100, 212, 230, 100, 318, 154, 100, 212, 240, 213, 228, 100, 237, 213, 234, 154, 100, 212, 228, 100, 234, 154, 100, 212, 318, 100, 317, 154, 154, 31, 252, 169, 155, 212, 200, 228, 100, 234, 26, 100, 84, 169, 132, 154, 31, 253, 169, 206, 212, 200, 228, 26, 100, 254, 212, 320, 154, 100, 84, 169, 132, 154, 31, 255, 169, 155, 212, 200, 228, 26, 100, 84, 169, 132, 154, 31, 123, 256, 141, 5, 212, 317, 100, 250, 100, 231, 154, 59, 31, 257, 169, 256, 69, 70, 212, 317, 100, 231, 154, 31, 258, 169, 191, 212, 217, 69, 212, 245, 213, 226, 69, 240, 154, 213, 229, 100, 212, 229, 100, 249, 154, 100, 212, 318, 100, 226, 213, 229, 154, 100, 212, 317, 100, 256, 154, 100, 212, 233, 100, 231, 154, 100, 212, 317, 100, 318, 154, 154, 31, 259, 169, 191, 212, 218, 69, 212, 245, 213, 226, 69, 240, 154, 213, 230, 100, 212, 249, 100, 230, 154, 100, 212, 226, 213, 230, 100, 318, 154, 100, 212, 256, 100, 237, 213, 234, 154, 100, 212, 231, 100, 234, 154, 100, 212, 318, 100, 317, 154, 154, 31, 260, 169, 54, 212, 258, 100, 248, 169, 212, 317, 100, 318, 154, 154, 31, 261, 169, 54, 212, 259, 100, 248, 169, 212, 317, 100, 318, 154, 154, 31, 262, 169, 15, 212, 247, 100, 260, 154, 31, 262, 169, 178, 212, 212, 257, 1, 250, 154, 200, 175, 100, 59, 26, 100, 262, 100, 254, 212, 320, 154, 154, 31, 253, 100, 263, 169, 212, 168, 212, 253, 100, 12, 212, 262, 100, 318, 154, 154, 100, 253, 154, 31, 264, 169, 99, 212, 263, 4, 253, 154, 31, 265, 169, 99, 212, 262, 4, 253, 200, 59, 100, 175, 26, 154, 31, 255, 169, 255, 213, 264, 69, 193, 212, 265, 100, 318, 154, 31, 252, 169, 252, 213, 264, 200, 59, 100, 175, 26, 69, 15, 212, 265, 76, 242, 212, 247, 76, 84, 154, 100, 261, 154, 31, 263, 169, 253, 31, 72, 31, 160, 250, 71, 317, 59, 31, 266, 169, 155, 212, 200, 228, 26, 100, 84, 169, 132, 154, 31, 163, 31, 29, 59, 31, 252, 169, 252, 41, 255, 200, 59, 100, 175, 26, 31, 266, 169, 253, 69, 48, 212, 255, 154, 31, 53, 31, 10, 212, 251, 100, 252, 76, 242, 212, 251, 76, 84, 76, 106, 154, 100, 248, 169, 212, 317, 100, 318, 154, 154, 31, 160, 237, 71, 317, 59, 31, 10, 212, 220, 69, 212, 243, 69, 236, 154, 213, 227, 69, 240, 213, 228, 69, 70, 212, 317, 100, 228, 154, 100, 266, 76, 242, 212, 220, 76, 84, 76, 106, 154, 154, 31, 163, 31, 3, 31]}, {"code": "def parallel_nsa_compression_bwd_kernel_dq(\n    q,\n    k,\n    v,\n    lse,\n    delta,\n    do,\n    dq,\n    scale,\n    offsets,\n    token_indices,\n    chunk_offsets,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BC: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_OFFSETS: tl.constexpr,\n):\n    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if USE_OFFSETS:\n        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(\n            token_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(\n            tl.int32\n        )\n        T = eos - bos\n        boc = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        boc = i_b * tl.cdiv(T, BS)\n\n    q += (bos + i_t) * HQ * K\n    do += (bos + i_t) * HQ * V\n    lse += (bos + i_t) * HQ\n    delta += (bos + i_t) * HQ\n    dq += (i_v * B * T + bos + i_t) * HQ * K\n\n    p_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\n    p_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    p_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\n    p_lse = lse + i_h * G + tl.arange(0, G)\n    p_delta = delta + i_h * G + tl.arange(0, G)\n\n    TC = tl.cdiv(T, BS)\n\n    NC = (i_t + 1) // BS\n\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n\n    b_lse = tl.load(p_lse)\n    b_delta = tl.load(p_delta)\n\n    b_dq = tl.zeros([G, BK], dtype=tl.float32)\n    for i_c in range(0, NC, BC):\n        o_c = i_c + tl.arange(0, BC)\n        p_k = tl.make_block_ptr(\n            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)\n        )\n        p_v = tl.make_block_ptr(\n            v + (boc * H + i_h) * V,\n            (V, TC),\n            (1, H * V),\n            (i_v * BV, i_c),\n            (BV, BC),\n            (0, 1),\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, b_k)\n        b_p = tl.exp(b_s - b_lse[:, None])\n        b_p = tl.where((o_c < NC)[None, :], b_p, 0)\n\n        b_dp = tl.dot(b_do, b_v)\n        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])\n\n        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))\n    b_dq *= scale\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 316, 212, 216, 100, 217, 100, 218, 100, 219, 100, 220, 100, 221, 100, 222, 100, 223, 100, 224, 100, 225, 100, 226, 100, 227, 100, 228, 58, 6, 100, 229, 58, 6, 100, 230, 58, 6, 100, 231, 58, 6, 100, 232, 58, 6, 100, 233, 58, 6, 100, 234, 58, 6, 100, 235, 58, 6, 100, 236, 58, 6, 100, 237, 58, 6, 100, 238, 58, 6, 154, 58, 31, -1, 239, 100, 240, 100, 241, 169, 212, 150, 212, 317, 154, 100, 150, 212, 318, 154, 100, 150, 212, 319, 154, 154, 31, 242, 100, 243, 169, 212, 241, 46, 229, 100, 241, 196, 229, 154, 31, 160, 238, 58, 31, 244, 100, 239, 169, 212, 52, 212, 225, 69, 239, 213, 319, 154, 76, 245, 212, 215, 154, 100, 52, 212, 225, 69, 239, 213, 319, 69, 318, 154, 76, 245, 212, 215, 154, 154, 31, 246, 100, 247, 169, 212, 52, 212, 224, 69, 244, 154, 76, 245, 212, 215, 154, 100, 52, 212, 224, 69, 244, 69, 318, 154, 76, 245, 212, 215, 154, 154, 31, 227, 169, 247, 4, 246, 31, 248, 169, 52, 212, 226, 69, 244, 154, 76, 245, 212, 215, 154, 31, 163, 31, 29, 58, 31, 246, 100, 247, 169, 212, 242, 213, 227, 100, 242, 213, 227, 69, 227, 154, 31, 248, 169, 242, 213, 60, 212, 227, 100, 235, 154, 31, 53, 31, 216, 151, 212, 246, 69, 239, 154, 213, 230, 213, 232, 31, 221, 151, 212, 246, 69, 239, 154, 213, 230, 213, 233, 31, 219, 151, 212, 246, 69, 239, 154, 213, 230, 31, 220, 151, 212, 246, 69, 239, 154, 213, 230, 31, 222, 151, 212, 240, 213, 228, 213, 227, 69, 246, 69, 239, 154, 213, 230, 213, 232, 31, 249, 169, 191, 212, 216, 100, 212, 230, 100, 232, 154, 100, 212, 232, 100, 318, 154, 100, 212, 243, 213, 231, 100, 317, 154, 100, 212, 231, 100, 236, 154, 100, 212, 318, 100, 317, 154, 154, 31, 250, 169, 191, 212, 222, 100, 212, 230, 100, 232, 154, 100, 212, 232, 100, 318, 154, 100, 212, 243, 213, 231, 100, 317, 154, 100, 212, 231, 100, 236, 154, 100, 212, 318, 100, 317, 154, 154, 31, 251, 169, 52, 212, 249, 100, 252, 169, 212, 317, 100, 318, 154, 154, 31, 251, 169, 212, 251, 213, 223, 154, 76, 245, 212, 251, 76, 84, 154, 31, 253, 169, 191, 212, 221, 100, 212, 230, 100, 233, 154, 100, 212, 233, 100, 318, 154, 100, 212, 243, 213, 231, 100, 240, 213, 237, 154, 100, 212, 231, 100, 237, 154, 100, 212, 318, 100, 317, 154, 154, 31, 254, 169, 219, 69, 243, 213, 231, 69, 70, 212, 317, 100, 231, 154, 31, 255, 169, 220, 69, 243, 213, 231, 69, 70, 212, 317, 100, 231, 154, 31, 256, 169, 60, 212, 227, 100, 235, 154, 31, 257, 169, 212, 239, 69, 318, 154, 46, 235, 31, 258, 169, 52, 212, 253, 100, 252, 169, 212, 317, 100, 318, 154, 154, 31, 259, 169, 52, 212, 254, 154, 31, 260, 169, 52, 212, 255, 154, 31, 261, 169, 155, 212, 200, 231, 100, 236, 26, 100, 84, 169, 132, 154, 31, 123, 262, 141, 5, 212, 317, 100, 257, 100, 234, 154, 58, 31, 263, 169, 262, 69, 70, 212, 317, 100, 234, 154, 31, 264, 169, 191, 212, 217, 69, 212, 248, 213, 229, 69, 243, 154, 213, 232, 100, 212, 232, 100, 256, 154, 100, 212, 318, 100, 229, 213, 232, 154, 100, 212, 317, 100, 262, 154, 100, 212, 236, 100, 234, 154, 100, 212, 317, 100, 318, 154, 154, 31, 265, 169, 191, 212, 218, 69, 212, 248, 213, 229, 69, 243, 154, 213, 233, 100, 212, 233, 100, 256, 154, 100, 212, 318, 100, 229, 213, 233, 154, 100, 212, 240, 213, 237, 100, 262, 154, 100, 212, 237, 100, 234, 154, 100, 212, 317, 100, 318, 154, 154, 31, 266, 169, 52, 212, 264, 100, 252, 169, 212, 317, 100, 318, 154, 154, 31, 267, 169, 52, 212, 265, 100, 252, 169, 212, 317, 100, 318, 154, 154, 31, 268, 169, 15, 212, 251, 100, 266, 154, 31, 269, 169, 99, 212, 268, 4, 259, 200, 58, 100, 175, 26, 154, 31, 269, 169, 178, 212, 212, 263, 1, 257, 154, 200, 175, 100, 58, 26, 100, 269, 100, 317, 154, 31, 270, 169, 15, 212, 258, 100, 267, 154, 31, 271, 169, 269, 213, 212, 270, 76, 245, 212, 132, 154, 4, 260, 200, 58, 100, 175, 26, 154, 31, 261, 151, 15, 212, 271, 76, 245, 212, 266, 76, 84, 154, 100, 67, 212, 266, 154, 154, 31, 72, 31, 261, 23, 223, 31, 10, 212, 250, 100, 261, 76, 245, 212, 250, 76, 84, 76, 106, 154, 100, 252, 169, 212, 317, 100, 318, 154, 154, 31, 3, 31]}, {"code": "def parallel_nsa_compression_bwd_kernel_dkv(\n    q,\n    k,\n    v,\n    lse,\n    delta,\n    do,\n    dk,\n    dv,\n    offsets,\n    chunk_indices,\n    chunk_offsets,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BC: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_OFFSETS: tl.constexpr,\n):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if USE_OFFSETS:\n        i_n, i_c = tl.load(chunk_indices + i_c * 2).to(tl.int32), tl.load(\n            chunk_indices + i_c * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(\n            tl.int32\n        )\n        T = eos - bos\n        boc = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        boc = i_b * tl.cdiv(T, BS)\n\n    TC = tl.cdiv(T, BS)\n\n    p_k = tl.make_block_ptr(\n        k + (boc * H + i_h) * K, (TC, K), (H * K, 1), (i_c * BC, 0), (BC, BK), (1, 0)\n    )\n    p_v = tl.make_block_ptr(\n        v + (boc * H + i_h) * V,\n        (TC, V),\n        (H * V, 1),\n        (i_c * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n    p_dk = tl.make_block_ptr(\n        dk + (i_v * B * T * H + boc * H + i_h) * K,\n        (TC, K),\n        (H * K, 1),\n        (i_c * BC, 0),\n        (BC, BK),\n        (1, 0),\n    )\n    p_dv = tl.make_block_ptr(\n        dv + (i_v * B * T * H + boc * H + i_h) * V,\n        (TC, V),\n        (H * V, 1),\n        (i_c * BC, i_v * BV),\n        (BC, BV),\n        (1, 0),\n    )\n\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BC, BV], dtype=tl.float32)\n\n    for i in range(i_c * BC * BS, T):\n        o_c = i_c * BC + tl.arange(0, BC)\n\n        p_q = tl.make_block_ptr(\n            q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n\n        p_do = tl.make_block_ptr(\n            do + (bos + i) * HQ * V,\n            (HQ, V),\n            (V, 1),\n            (i_h * G, i_v * BV),\n            (G, BV),\n            (1, 0),\n        )\n        p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n        p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_lse = tl.load(p_lse)\n        b_delta = tl.load(p_delta)\n\n        b_s = tl.dot(b_k, tl.trans(b_q))\n        b_p = tl.exp(b_s - b_lse[None, :])\n        b_p = tl.where((i >= max(0, (o_c + 1) * BS - 1))[:, None], b_p, 0)\n\n        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)\n\n        b_dp = tl.dot(b_v, tl.trans(b_do))\n\n        b_ds = b_p * (b_dp - b_delta[None, :])\n\n        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\n\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 316, 212, 216, 100, 217, 100, 218, 100, 219, 100, 220, 100, 221, 100, 222, 100, 223, 100, 224, 100, 225, 100, 226, 100, 227, 100, 228, 100, 229, 59, 6, 100, 230, 59, 6, 100, 231, 59, 6, 100, 232, 59, 6, 100, 233, 59, 6, 100, 234, 59, 6, 100, 235, 59, 6, 100, 236, 59, 6, 100, 237, 59, 6, 100, 238, 59, 6, 100, 239, 59, 6, 154, 59, 31, -1, 240, 100, 241, 100, 242, 169, 212, 150, 212, 317, 154, 100, 150, 212, 318, 154, 100, 150, 212, 319, 154, 154, 31, 243, 100, 244, 169, 212, 242, 46, 230, 100, 242, 196, 230, 154, 31, 160, 239, 59, 31, 245, 100, 241, 169, 212, 54, 212, 225, 69, 241, 213, 319, 154, 76, 246, 212, 58, 154, 100, 54, 212, 225, 69, 241, 213, 319, 69, 318, 154, 76, 246, 212, 58, 154, 154, 31, 247, 100, 248, 169, 212, 54, 212, 224, 69, 245, 154, 76, 246, 212, 58, 154, 100, 54, 212, 224, 69, 245, 69, 318, 154, 76, 246, 212, 58, 154, 154, 31, 228, 169, 248, 4, 247, 31, 249, 169, 54, 212, 226, 69, 245, 154, 76, 246, 212, 58, 154, 31, 163, 31, 29, 59, 31, 247, 100, 248, 169, 212, 243, 213, 228, 100, 243, 213, 228, 69, 228, 154, 31, 249, 169, 243, 213, 61, 212, 228, 100, 236, 154, 31, 53, 31, 250, 169, 61, 212, 228, 100, 236, 154, 31, 251, 169, 191, 212, 217, 69, 212, 249, 213, 230, 69, 244, 154, 213, 233, 100, 212, 250, 100, 233, 154, 100, 212, 230, 213, 233, 100, 318, 154, 100, 212, 241, 213, 235, 100, 317, 154, 100, 212, 235, 100, 237, 154, 100, 212, 318, 100, 317, 154, 154, 31, 252, 169, 191, 212, 218, 69, 212, 249, 213, 230, 69, 244, 154, 213, 234, 100, 212, 250, 100, 234, 154, 100, 212, 230, 213, 234, 100, 318, 154, 100, 212, 241, 213, 235, 100, 240, 213, 238, 154, 100, 212, 235, 100, 238, 154, 100, 212, 318, 100, 317, 154, 154, 31, 253, 169, 191, 212, 222, 69, 212, 240, 213, 229, 213, 228, 213, 230, 69, 249, 213, 230, 69, 244, 154, 213, 233, 100, 212, 250, 100, 233, 154, 100, 212, 230, 213, 233, 100, 318, 154, 100, 212, 241, 213, 235, 100, 317, 154, 100, 212, 235, 100, 237, 154, 100, 212, 318, 100, 317, 154, 154, 31, 254, 169, 191, 212, 223, 69, 212, 240, 213, 229, 213, 228, 213, 230, 69, 249, 213, 230, 69, 244, 154, 213, 234, 100, 212, 250, 100, 234, 154, 100, 212, 230, 213, 234, 100, 318, 154, 100, 212, 241, 213, 235, 100, 240, 213, 238, 154, 100, 212, 235, 100, 238, 154, 100, 212, 318, 100, 317, 154, 154, 31, 255, 169, 54, 212, 251, 100, 256, 169, 212, 317, 100, 318, 154, 154, 31, 257, 169, 155, 212, 200, 235, 100, 237, 26, 100, 84, 169, 132, 154, 31, 258, 169, 54, 212, 252, 100, 256, 169, 212, 317, 100, 318, 154, 154, 31, 259, 169, 155, 212, 200, 235, 100, 238, 26, 100, 84, 169, 132, 154, 31, 123, 260, 141, 5, 212, 241, 213, 235, 213, 236, 100, 228, 154, 59, 31, 261, 169, 241, 213, 235, 69, 70, 212, 317, 100, 235, 154, 31, 262, 169, 191, 212, 216, 69, 212, 247, 69, 260, 154, 213, 231, 213, 233, 100, 212, 231, 100, 233, 154, 100, 212, 233, 100, 318, 154, 100, 212, 244, 213, 232, 100, 317, 154, 100, 212, 232, 100, 237, 154, 100, 212, 318, 100, 317, 154, 154, 31, 263, 169, 54, 212, 262, 100, 256, 169, 212, 317, 100, 318, 154, 154, 31, 263, 169, 212, 263, 213, 227, 154, 76, 246, 212, 263, 76, 84, 154, 31, 264, 169, 191, 212, 221, 69, 212, 247, 69, 260, 154, 213, 231, 213, 234, 100, 212, 231, 100, 234, 154, 100, 212, 234, 100, 318, 154, 100, 212, 244, 213, 232, 100, 240, 213, 238, 154, 100, 212, 232, 100, 238, 154, 100, 212, 318, 100, 317, 154, 154, 31, 265, 169, 219, 69, 212, 247, 69, 260, 154, 213, 231, 69, 244, 213, 232, 69, 70, 212, 317, 100, 232, 154, 31, 266, 169, 220, 69, 212, 247, 69, 260, 154, 213, 231, 69, 244, 213, 232, 69, 70, 212, 317, 100, 232, 154, 31, 267, 169, 54, 212, 264, 100, 256, 169, 212, 317, 100, 318, 154, 154, 31, 268, 169, 54, 212, 265, 154, 31, 269, 169, 54, 212, 266, 154, 31, 270, 169, 15, 212, 255, 100, 67, 212, 263, 154, 154, 31, 271, 169, 99, 212, 270, 4, 268, 200, 175, 100, 59, 26, 154, 31, 271, 169, 178, 212, 212, 260, 131, 47, 212, 317, 100, 212, 261, 69, 318, 154, 213, 236, 4, 318, 154, 154, 200, 59, 100, 175, 26, 100, 271, 100, 317, 154, 31, 259, 151, 15, 212, 271, 76, 246, 212, 267, 76, 84, 154, 100, 267, 154, 31, 272, 169, 15, 212, 258, 100, 67, 212, 267, 154, 154, 31, 273, 169, 271, 213, 212, 272, 4, 269, 200, 175, 100, 59, 26, 154, 31, 257, 151, 15, 212, 273, 76, 246, 212, 263, 76, 84, 154, 100, 263, 154, 31, 72, 31, 10, 212, 253, 100, 257, 76, 246, 212, 253, 76, 84, 76, 106, 154, 100, 256, 169, 212, 317, 100, 318, 154, 154, 31, 10, 212, 254, 100, 259, 76, 246, 212, 254, 76, 84, 76, 106, 154, 100, 256, 169, 212, 317, 100, 318, 154, 154, 31, 3, 31]}, {"code": "def parallel_nsa_kernel_topk(\n    q,\n    k,\n    lse,\n    scale,\n    block_indices,\n    offsets,\n    token_indices,\n    chunk_offsets,\n    T,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    S: tl.constexpr,\n    BC: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    USE_OFFSETS: tl.constexpr,\n):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if USE_OFFSETS:\n        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(\n            token_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(\n            tl.int32\n        )\n        T = eos - bos\n        boc = tl.load(chunk_offsets + i_n).to(tl.int32)\n    else:\n        bos, eos = i_b * T, i_b * T + T\n        boc = i_b * tl.cdiv(T, BS)\n\n    p_q = tl.make_block_ptr(\n        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    TC = tl.cdiv(T, BS)\n\n    NC = (i_t + 1) // BS\n\n    if lse is not None:\n        b_lse = tl.load(lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G))\n    else:\n\n        b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)\n\n        b_acc = tl.zeros([G], dtype=tl.float32)\n        for i_c in range(0, NC, BC):\n            o_c = i_c + tl.arange(0, BC)\n\n            p_k = tl.make_block_ptr(\n                k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)\n            )\n\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n\n            b_s = tl.dot(b_q, b_k)\n            b_s = tl.where((o_c < NC)[None, :], b_s, float(\"-inf\"))\n\n            b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m\n            b_r = tl.exp(b_mp - b_m)\n\n            b_p = tl.exp(b_s - b_m[:, None])\n\n            b_acc = b_acc * b_r + tl.sum(b_p, 1)\n\n            b_mp = b_m\n        if NC == 0:\n            b_lse = tl.zeros([G], dtype=tl.float32)\n        else:\n            b_lse = b_m + tl.log(b_acc)\n\n    b_i = tl.full([BC], -1, dtype=tl.float32)\n    o_i = tl.zeros([BC], dtype=tl.int32)\n    m_i = tl.arange(0, BC) < BC // 2\n    for i_c in range(0, i_t // BS + 1, BC):\n        o_c = i_c + tl.arange(0, BC)\n\n        p_k = tl.make_block_ptr(\n            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)\n        )\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_s = tl.dot(b_q, b_k)\n        b_s = tl.where((i_t // BS > o_c)[None, :], b_s, float(\"-inf\"))\n\n        b_p = tl.where(\n            (i_t // BS == o_c)[None, :], float(1.0), tl.exp(b_s - b_lse[:, None])\n        )\n\n        b_i, b_ip = tl.sum(b_p, 0), b_i\n        o_i, o_ip = tl.where(o_c <= i_t // BS, o_c + 1, 0), o_i\n\n        n_dims: tl.constexpr = tl.standard._log2(b_i.shape[0])\n        for i in tl.static_range(1, n_dims):\n            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), i, 2, n_dims)\n\n        if i_c != 0:\n            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, False, n_dims)\n            b_i_new = b_ip * m_i + b_i * (1 - m_i)\n            o_i_new = o_ip * m_i + o_i * (1 - m_i)\n            b_i, o_i = _bitonic_merge(\n                b_i_new, o_i_new.to(tl.int32), n_dims, True, n_dims\n            )\n        else:\n            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, True, n_dims)\n\n    m_top = tl.arange(0, BC // S) == 0\n    b_top = tl.sum(m_top[:, None] * tl.reshape(o_i - 1, [BC // S, S]), 0)\n\n    p_b = tl.make_block_ptr(\n        block_indices + (bos + i_t) * H * S, (H * S,), (1,), (i_h * S,), (S,), (0,)\n    )\n    tl.store(p_b, b_top.to(p_b.dtype.element_ty))", "encoded": [28, 316, 212, 216, 100, 217, 100, 218, 100, 219, 100, 220, 100, 221, 100, 222, 100, 223, 100, 224, 100, 225, 58, 6, 100, 226, 58, 6, 100, 227, 58, 6, 100, 228, 58, 6, 100, 229, 58, 6, 100, 230, 58, 6, 100, 231, 58, 6, 100, 232, 58, 6, 100, 233, 58, 6, 154, 58, 31, -1, 234, 100, 235, 169, 212, 150, 212, 317, 154, 100, 150, 212, 318, 154, 154, 31, 236, 100, 237, 169, 212, 235, 46, 225, 100, 235, 196, 225, 154, 31, 160, 233, 58, 31, 238, 100, 234, 169, 212, 52, 212, 222, 69, 234, 213, 319, 154, 76, 239, 212, 215, 154, 100, 52, 212, 222, 69, 234, 213, 319, 69, 318, 154, 76, 239, 212, 215, 154, 154, 31, 240, 100, 241, 169, 212, 52, 212, 221, 69, 238, 154, 76, 239, 212, 215, 154, 100, 52, 212, 221, 69, 238, 69, 318, 154, 76, 239, 212, 215, 154, 154, 31, 224, 169, 241, 4, 240, 31, 242, 169, 52, 212, 223, 69, 238, 154, 76, 239, 212, 215, 154, 31, 163, 31, 29, 58, 31, 240, 100, 241, 169, 212, 236, 213, 224, 100, 236, 213, 224, 69, 224, 154, 31, 242, 169, 236, 213, 60, 212, 224, 100, 231, 154, 31, 53, 31, 243, 169, 191, 212, 216, 69, 212, 240, 69, 234, 154, 213, 226, 213, 228, 100, 212, 226, 100, 228, 154, 100, 212, 228, 100, 318, 154, 100, 212, 237, 213, 227, 100, 317, 154, 100, 212, 227, 100, 232, 154, 100, 212, 318, 100, 317, 154, 154, 31, 244, 169, 52, 212, 243, 100, 245, 169, 212, 317, 100, 318, 154, 154, 31, 244, 169, 212, 244, 213, 219, 154, 76, 239, 212, 244, 76, 84, 154, 31, 246, 169, 60, 212, 224, 100, 231, 154, 31, 247, 169, 212, 234, 69, 318, 154, 46, 231, 31, 160, 218, 102, 59, 175, 58, 31, 248, 169, 52, 212, 218, 69, 212, 240, 69, 234, 154, 213, 226, 69, 237, 213, 227, 69, 70, 212, 317, 100, 227, 154, 154, 31, 163, 31, 29, 58, 31, 249, 169, 206, 212, 200, 227, 26, 100, 250, 212, 320, 154, 100, 84, 169, 132, 154, 31, 251, 169, 155, 212, 200, 227, 26, 100, 84, 169, 132, 154, 31, 123, 252, 141, 5, 212, 317, 100, 247, 100, 230, 154, 58, 31, 253, 169, 252, 69, 70, 212, 317, 100, 230, 154, 31, 254, 169, 191, 212, 217, 69, 212, 242, 213, 225, 69, 237, 154, 213, 228, 100, 212, 228, 100, 246, 154, 100, 212, 318, 100, 225, 213, 228, 154, 100, 212, 317, 100, 252, 154, 100, 212, 232, 100, 230, 154, 100, 212, 317, 100, 318, 154, 154, 31, 255, 169, 52, 212, 254, 100, 245, 169, 212, 317, 100, 318, 154, 154, 31, 256, 169, 15, 212, 244, 100, 255, 154, 31, 256, 169, 178, 212, 212, 253, 1, 247, 154, 200, 175, 100, 58, 26, 100, 256, 100, 250, 212, 320, 154, 154, 31, 249, 100, 257, 169, 212, 168, 212, 249, 100, 12, 212, 256, 100, 318, 154, 154, 100, 249, 154, 31, 258, 169, 99, 212, 257, 4, 249, 154, 31, 259, 169, 99, 212, 256, 4, 249, 200, 58, 100, 175, 26, 154, 31, 251, 169, 251, 213, 258, 69, 193, 212, 259, 100, 318, 154, 31, 257, 169, 249, 31, 72, 31, 160, 247, 71, 317, 58, 31, 248, 169, 155, 212, 200, 227, 26, 100, 84, 169, 132, 154, 31, 163, 31, 29, 58, 31, 248, 169, 249, 69, 48, 212, 251, 154, 31, 53, 31, 53, 31, 260, 169, 206, 212, 200, 230, 26, 100, 4, 318, 100, 84, 169, 132, 154, 31, 261, 169, 155, 212, 200, 230, 26, 100, 84, 169, 215, 154, 31, 262, 169, 70, 212, 317, 100, 230, 154, 1, 230, 46, 319, 31, 123, 252, 141, 5, 212, 317, 100, 234, 46, 231, 69, 318, 100, 230, 154, 58, 31, 253, 169, 252, 69, 70, 212, 317, 100, 230, 154, 31, 254, 169, 191, 212, 217, 69, 212, 242, 213, 225, 69, 237, 154, 213, 228, 100, 212, 228, 100, 246, 154, 100, 212, 318, 100, 225, 213, 228, 154, 100, 212, 317, 100, 252, 154, 100, 212, 232, 100, 230, 154, 100, 212, 317, 100, 318, 154, 154, 31, 255, 169, 52, 212, 254, 100, 245, 169, 212, 317, 100, 318, 154, 154, 31, 256, 169, 15, 212, 244, 100, 255, 154, 31, 256, 169, 178, 212, 212, 234, 46, 231, 114, 253, 154, 200, 175, 100, 58, 26, 100, 256, 100, 250, 212, 320, 154, 154, 31, 259, 169, 178, 212, 212, 234, 46, 231, 71, 253, 154, 200, 175, 100, 58, 26, 100, 250, 212, 318, 154, 100, 99, 212, 256, 4, 248, 200, 58, 100, 175, 26, 154, 154, 31, 260, 100, 263, 169, 212, 193, 212, 259, 100, 317, 154, 100, 260, 154, 31, 261, 100, 264, 169, 212, 178, 212, 253, 190, 234, 46, 231, 100, 253, 69, 318, 100, 317, 154, 100, 261, 154, 31, 265, 58, 6, 169, 161, 212, 260, 76, 113, 200, 317, 26, 154, 31, 123, 266, 141, 182, 212, 318, 100, 265, 154, 58, 31, 260, 100, 261, 169, 267, 212, 260, 100, 261, 76, 239, 212, 215, 154, 100, 266, 100, 319, 100, 265, 154, 31, 72, 31, 160, 252, 164, 317, 58, 31, 260, 100, 261, 169, 267, 212, 260, 100, 261, 76, 239, 212, 215, 154, 100, 265, 100, 56, 100, 265, 154, 31, 268, 169, 263, 213, 262, 69, 260, 213, 212, 318, 4, 262, 154, 31, 269, 169, 264, 213, 262, 69, 261, 213, 212, 318, 4, 262, 154, 31, 260, 100, 261, 169, 267, 212, 268, 100, 269, 76, 239, 212, 215, 154, 100, 265, 100, 144, 100, 265, 154, 31, 163, 31, 29, 58, 31, 260, 100, 261, 169, 267, 212, 260, 100, 261, 76, 239, 212, 215, 154, 100, 265, 100, 144, 100, 265, 154, 31, 53, 31, 72, 31, 270, 169, 70, 212, 317, 100, 230, 46, 229, 154, 71, 317, 31, 271, 169, 193, 212, 270, 200, 58, 100, 175, 26, 213, 22, 212, 261, 4, 318, 100, 200, 230, 46, 229, 100, 229, 26, 154, 100, 317, 154, 31, 272, 169, 191, 212, 220, 69, 212, 240, 69, 234, 154, 213, 225, 213, 229, 100, 212, 225, 213, 229, 100, 154, 100, 212, 318, 100, 154, 100, 212, 237, 213, 229, 100, 154, 100, 212, 229, 100, 154, 100, 212, 317, 100, 154, 154, 31, 10, 212, 272, 100, 271, 76, 239, 212, 272, 76, 84, 76, 106, 154, 154, 31, 3, 31]}, {"code": "def parallel_nsa_fwd_kernel(\n    q,\n    k,\n    v,\n    o,\n    lse,\n    scale,\n    block_indices,\n    block_counts,\n    offsets,\n    token_indices,\n    T,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    S: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_OFFSETS: tl.constexpr,\n    USE_BLOCK_COUNTS: tl.constexpr,\n):\n    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if USE_OFFSETS:\n        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(\n            token_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(\n            tl.int32\n        )\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    k += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V\n    block_indices += (bos + i_t) * H * S + i_h * S\n\n    if USE_BLOCK_COUNTS:\n        NS = tl.load(block_counts + (bos + i_t) * H + i_h)\n    else:\n        NS = S\n\n    p_q = tl.make_block_ptr(\n        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    p_o = tl.make_block_ptr(\n        o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0)\n    )\n    p_lse = lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G)\n\n    b_o = tl.zeros([G, BV], dtype=tl.float32)\n\n    b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)\n    b_acc = tl.zeros([G], dtype=tl.float32)\n    for i in range(NS):\n        i_s = tl.load(block_indices + i).to(tl.int32) * BS\n        if i_s <= i_t and i_s >= 0:\n            p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))\n            p_v = tl.make_block_ptr(\n                v, (T, V), (H * V, 1), (i_s, i_v * BV), (BS, BV), (1, 0)\n            )\n\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n\n            b_s = tl.dot(b_q, b_k)\n            b_s = tl.where(\n                (i_t >= (i_s + tl.arange(0, BS)))[None, :], b_s, float(\"-inf\")\n            )\n\n            b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m\n            b_r = tl.exp(b_mp - b_m)\n\n            b_p = tl.exp(b_s - b_m[:, None])\n\n            b_acc = b_acc * b_r + tl.sum(b_p, 1)\n\n            b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)\n\n            b_mp = b_m\n    b_o = b_o / b_acc[:, None]\n    b_m += tl.log(b_acc)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_lse, b_m.to(p_lse.dtype.element_ty))", "encoded": [28, 316, 212, 216, 100, 217, 100, 218, 100, 219, 100, 220, 100, 221, 100, 222, 100, 223, 100, 224, 100, 225, 100, 226, 100, 227, 59, 6, 100, 228, 59, 6, 100, 229, 59, 6, 100, 230, 59, 6, 100, 231, 59, 6, 100, 232, 59, 6, 100, 233, 59, 6, 100, 234, 59, 6, 100, 235, 59, 6, 100, 236, 59, 6, 100, 237, 59, 6, 154, 59, 31, -1, 238, 100, 239, 100, 240, 169, 212, 150, 212, 317, 154, 100, 150, 212, 318, 154, 100, 150, 212, 319, 154, 154, 31, 241, 100, 242, 169, 212, 240, 46, 227, 100, 240, 196, 227, 154, 31, 160, 236, 59, 31, 243, 100, 238, 169, 212, 54, 212, 225, 69, 238, 213, 319, 154, 76, 244, 212, 58, 154, 100, 54, 212, 225, 69, 238, 213, 319, 69, 318, 154, 76, 244, 212, 58, 154, 154, 31, 245, 100, 246, 169, 212, 54, 212, 224, 69, 243, 154, 76, 244, 212, 58, 154, 100, 54, 212, 224, 69, 243, 69, 318, 154, 76, 244, 212, 58, 154, 154, 31, 226, 169, 246, 4, 245, 31, 163, 31, 29, 59, 31, 245, 100, 246, 169, 212, 241, 213, 226, 100, 241, 213, 226, 69, 226, 154, 31, 53, 31, 217, 151, 212, 245, 213, 227, 69, 242, 154, 213, 230, 31, 218, 151, 212, 245, 213, 227, 69, 242, 154, 213, 231, 31, 222, 151, 212, 245, 69, 238, 154, 213, 227, 213, 232, 69, 242, 213, 232, 31, 160, 237, 59, 31, 247, 169, 54, 212, 223, 69, 212, 245, 69, 238, 154, 213, 227, 69, 242, 154, 31, 163, 31, 29, 59, 31, 247, 169, 232, 31, 53, 31, 248, 169, 191, 212, 216, 69, 212, 245, 69, 238, 154, 213, 228, 213, 230, 100, 212, 228, 100, 230, 154, 100, 212, 230, 100, 318, 154, 100, 212, 242, 213, 229, 100, 317, 154, 100, 212, 229, 100, 234, 154, 100, 212, 318, 100, 317, 154, 154, 31, 249, 169, 54, 212, 248, 100, 250, 169, 212, 317, 100, 318, 154, 154, 31, 249, 169, 212, 249, 213, 221, 154, 76, 244, 212, 249, 76, 84, 154, 31, 251, 169, 191, 212, 219, 69, 212, 245, 69, 238, 154, 213, 228, 213, 231, 100, 212, 228, 100, 231, 154, 100, 212, 231, 100, 318, 154, 100, 212, 242, 213, 229, 100, 239, 213, 235, 154, 100, 212, 229, 100, 235, 154, 100, 212, 318, 100, 317, 154, 154, 31, 252, 169, 220, 69, 212, 245, 69, 238, 154, 213, 228, 69, 242, 213, 229, 69, 70, 212, 317, 100, 229, 154, 31, 253, 169, 155, 212, 200, 229, 100, 235, 26, 100, 84, 169, 132, 154, 31, 254, 169, 206, 212, 200, 229, 26, 100, 255, 212, 320, 154, 100, 84, 169, 132, 154, 31, 256, 169, 155, 212, 200, 229, 26, 100, 84, 169, 132, 154, 31, 123, 257, 141, 5, 212, 247, 154, 59, 31, 258, 169, 54, 212, 222, 69, 257, 154, 76, 244, 212, 58, 154, 213, 233, 31, 160, 258, 190, 238, 94, 258, 131, 317, 59, 31, 259, 169, 191, 212, 217, 100, 212, 230, 100, 226, 154, 100, 212, 318, 100, 227, 213, 230, 154, 100, 212, 317, 100, 258, 154, 100, 212, 234, 100, 233, 154, 100, 212, 317, 100, 318, 154, 154, 31, 260, 169, 191, 212, 218, 100, 212, 226, 100, 231, 154, 100, 212, 227, 213, 231, 100, 318, 154, 100, 212, 258, 100, 239, 213, 235, 154, 100, 212, 233, 100, 235, 154, 100, 212, 318, 100, 317, 154, 154, 31, 261, 169, 54, 212, 259, 100, 250, 169, 212, 317, 100, 318, 154, 154, 31, 262, 169, 54, 212, 260, 100, 250, 169, 212, 317, 100, 318, 154, 154, 31, 263, 169, 15, 212, 249, 100, 261, 154, 31, 263, 169, 178, 212, 212, 238, 131, 258, 69, 70, 212, 317, 100, 233, 154, 154, 200, 175, 100, 59, 26, 100, 263, 100, 255, 212, 320, 154, 154, 31, 254, 100, 264, 169, 212, 168, 212, 254, 100, 12, 212, 263, 100, 318, 154, 154, 100, 254, 154, 31, 265, 169, 99, 212, 264, 4, 254, 154, 31, 266, 169, 99, 212, 263, 4, 254, 200, 59, 100, 175, 26, 154, 31, 256, 169, 256, 213, 265, 69, 193, 212, 266, 100, 318, 154, 31, 253, 169, 253, 213, 265, 200, 59, 100, 175, 26, 69, 15, 212, 266, 76, 244, 212, 249, 76, 84, 154, 100, 262, 154, 31, 264, 169, 254, 31, 163, 31, 72, 31, 253, 169, 253, 41, 256, 200, 59, 100, 175, 26, 31, 254, 151, 48, 212, 256, 154, 31, 10, 212, 251, 100, 253, 76, 244, 212, 251, 76, 84, 76, 106, 154, 100, 250, 169, 212, 317, 100, 318, 154, 154, 31, 10, 212, 252, 100, 254, 76, 244, 212, 252, 76, 84, 76, 106, 154, 154, 31, 3, 31]}, {"code": "def parallel_nsa_kernel_mask(\n    block_indices,\n    block_counts,\n    block_mask,\n    T: tl.constexpr,\n    H: tl.constexpr,\n    S: tl.constexpr,\n    BS: tl.constexpr,\n    NS: tl.constexpr,\n    USE_BLOCK_COUNTS: tl.constexpr,\n):\n    i_t, i_b, i_hs = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_s = i_hs // S, i_hs % S\n\n    b_i = tl.load(block_indices + i_b * T * H * S + i_t * H * S + i_h * S + i_s)\n    if USE_BLOCK_COUNTS:\n        b_m = b_i * BS <= i_t and i_s < tl.load(\n            block_counts + i_b * T * H + i_t * H + i_h\n        )\n    else:\n        b_m = b_i * BS <= i_t\n\n    if b_i < NS and b_i >= 0:\n        tl.store(\n            block_mask + i_b * T * H * NS + i_t * H * NS + i_h * NS + b_i,\n            b_m.to(block_mask.dtype.element_ty),\n        )", "encoded": [28, 316, 212, 216, 100, 217, 100, 218, 100, 219, 58, 6, 100, 220, 58, 6, 100, 221, 58, 6, 100, 222, 58, 6, 100, 223, 58, 6, 100, 224, 58, 6, 154, 58, 31, -1, 225, 100, 226, 100, 227, 169, 212, 150, 212, 317, 154, 100, 150, 212, 318, 154, 100, 150, 212, 319, 154, 154, 31, 228, 100, 229, 169, 212, 227, 46, 221, 100, 227, 196, 221, 154, 31, 230, 169, 52, 212, 216, 69, 226, 213, 219, 213, 220, 213, 221, 69, 225, 213, 220, 213, 221, 69, 228, 213, 221, 69, 229, 154, 31, 160, 224, 58, 31, 231, 169, 230, 213, 222, 190, 225, 94, 229, 1, 52, 212, 217, 69, 226, 213, 219, 213, 220, 69, 225, 213, 220, 69, 228, 154, 31, 163, 31, 29, 58, 31, 231, 169, 230, 213, 222, 190, 225, 31, 53, 31, 160, 230, 1, 223, 94, 230, 131, 317, 58, 31, 10, 212, 218, 69, 226, 213, 219, 213, 220, 213, 223, 69, 225, 213, 220, 213, 223, 69, 228, 213, 223, 69, 230, 100, 231, 76, 232, 212, 218, 76, 84, 76, 106, 154, 154, 31, 163, 31, 3, 31]}, {"code": "def parallel_nsa_bwd_kernel_dq(\n    q,\n    k,\n    v,\n    lse,\n    delta,\n    do,\n    dq,\n    scale,\n    block_indices,\n    block_counts,\n    offsets,\n    token_indices,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    S: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_OFFSETS: tl.constexpr,\n    USE_BLOCK_COUNTS: tl.constexpr,\n):\n    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if USE_OFFSETS:\n        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(\n            token_indices + i_t * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(\n            tl.int32\n        )\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    q += (bos + i_t) * HQ * K\n    do += (bos + i_t) * HQ * V\n    lse += (bos + i_t) * HQ\n    delta += (bos + i_t) * HQ\n    dq += (i_v * B * T + bos + i_t) * HQ * K\n    block_indices += (bos + i_t) * H * S + i_h * S\n\n    if USE_BLOCK_COUNTS:\n        NS = tl.load(block_counts + (bos + i_t) * H + i_h)\n    else:\n        NS = S\n\n    k += (bos * H + i_h) * K\n    v += (bos * H + i_h) * V\n\n    p_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\n    p_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n\n    p_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\n    p_lse = lse + i_h * G + tl.arange(0, G)\n    p_delta = delta + i_h * G + tl.arange(0, G)\n\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n\n    b_lse = tl.load(p_lse)\n    b_delta = tl.load(p_delta)\n\n    b_dq = tl.zeros([G, BK], dtype=tl.float32)\n    for i in range(NS):\n        i_s = tl.load(block_indices + i).to(tl.int32) * BS\n        if i_s <= i_t and i_s >= 0:\n            p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))\n            p_v = tl.make_block_ptr(\n                v, (V, T), (1, H * V), (i_v * BV, i_s), (BV, BS), (0, 1)\n            )\n\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n\n            b_s = tl.dot(b_q, b_k)\n            b_p = tl.exp(b_s - b_lse[:, None])\n            b_p = tl.where((i_t >= (i_s + tl.arange(0, BS)))[None, :], b_p, 0)\n\n            b_dp = tl.dot(b_do, b_v)\n            b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])\n\n            b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))\n    b_dq *= scale\n\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 316, 212, 216, 100, 217, 100, 218, 100, 219, 100, 220, 100, 221, 100, 222, 100, 223, 100, 224, 100, 225, 100, 226, 100, 227, 100, 228, 100, 229, 59, 6, 100, 230, 59, 6, 100, 231, 59, 6, 100, 232, 59, 6, 100, 233, 59, 6, 100, 234, 59, 6, 100, 235, 59, 6, 100, 236, 59, 6, 100, 237, 59, 6, 100, 238, 59, 6, 100, 239, 59, 6, 100, 240, 59, 6, 154, 59, 31, -1, 241, 100, 242, 100, 243, 169, 212, 150, 212, 317, 154, 100, 150, 212, 318, 154, 100, 150, 212, 319, 154, 154, 31, 244, 100, 245, 169, 212, 243, 46, 230, 100, 243, 196, 230, 154, 31, 160, 239, 59, 31, 246, 100, 241, 169, 212, 54, 212, 227, 69, 241, 213, 319, 154, 76, 247, 212, 58, 154, 100, 54, 212, 227, 69, 241, 213, 319, 69, 318, 154, 76, 247, 212, 58, 154, 154, 31, 248, 100, 249, 169, 212, 54, 212, 226, 69, 246, 154, 76, 247, 212, 58, 154, 100, 54, 212, 226, 69, 246, 69, 318, 154, 76, 247, 212, 58, 154, 154, 31, 228, 169, 249, 4, 248, 31, 163, 31, 29, 59, 31, 248, 100, 249, 169, 212, 244, 213, 228, 100, 244, 213, 228, 69, 228, 154, 31, 53, 31, 216, 151, 212, 248, 69, 241, 154, 213, 231, 213, 233, 31, 221, 151, 212, 248, 69, 241, 154, 213, 231, 213, 234, 31, 219, 151, 212, 248, 69, 241, 154, 213, 231, 31, 220, 151, 212, 248, 69, 241, 154, 213, 231, 31, 222, 151, 212, 242, 213, 229, 213, 228, 69, 248, 69, 241, 154, 213, 231, 213, 233, 31, 224, 151, 212, 248, 69, 241, 154, 213, 230, 213, 235, 69, 245, 213, 235, 31, 160, 240, 59, 31, 250, 169, 54, 212, 225, 69, 212, 248, 69, 241, 154, 213, 230, 69, 245, 154, 31, 163, 31, 29, 59, 31, 250, 169, 235, 31, 53, 31, 217, 151, 212, 248, 213, 230, 69, 245, 154, 213, 233, 31, 218, 151, 212, 248, 213, 230, 69, 245, 154, 213, 234, 31, 251, 169, 191, 212, 216, 100, 212, 231, 100, 233, 154, 100, 212, 233, 100, 318, 154, 100, 212, 245, 213, 232, 100, 317, 154, 100, 212, 232, 100, 237, 154, 100, 212, 318, 100, 317, 154, 154, 31, 252, 169, 191, 212, 222, 100, 212, 231, 100, 233, 154, 100, 212, 233, 100, 318, 154, 100, 212, 245, 213, 232, 100, 317, 154, 100, 212, 232, 100, 237, 154, 100, 212, 318, 100, 317, 154, 154, 31, 253, 169, 54, 212, 251, 100, 254, 169, 212, 317, 100, 318, 154, 154, 31, 253, 169, 212, 253, 213, 223, 154, 76, 247, 212, 253, 76, 84, 154, 31, 255, 169, 191, 212, 221, 100, 212, 231, 100, 234, 154, 100, 212, 234, 100, 318, 154, 100, 212, 245, 213, 232, 100, 242, 213, 238, 154, 100, 212, 232, 100, 238, 154, 100, 212, 318, 100, 317, 154, 154, 31, 256, 169, 219, 69, 245, 213, 232, 69, 70, 212, 317, 100, 232, 154, 31, 257, 169, 220, 69, 245, 213, 232, 69, 70, 212, 317, 100, 232, 154, 31, 258, 169, 54, 212, 255, 100, 254, 169, 212, 317, 100, 318, 154, 154, 31, 259, 169, 54, 212, 256, 154, 31, 260, 169, 54, 212, 257, 154, 31, 261, 169, 155, 212, 200, 232, 100, 237, 26, 100, 84, 169, 132, 154, 31, 123, 262, 141, 5, 212, 250, 154, 59, 31, 263, 169, 54, 212, 224, 69, 262, 154, 76, 247, 212, 58, 154, 213, 236, 31, 160, 263, 190, 241, 94, 263, 131, 317, 59, 31, 264, 169, 191, 212, 217, 100, 212, 233, 100, 228, 154, 100, 212, 318, 100, 230, 213, 233, 154, 100, 212, 317, 100, 263, 154, 100, 212, 237, 100, 236, 154, 100, 212, 317, 100, 318, 154, 154, 31, 265, 169, 191, 212, 218, 100, 212, 234, 100, 228, 154, 100, 212, 318, 100, 230, 213, 234, 154, 100, 212, 242, 213, 238, 100, 263, 154, 100, 212, 238, 100, 236, 154, 100, 212, 317, 100, 318, 154, 154, 31, 266, 169, 54, 212, 264, 100, 254, 169, 212, 317, 100, 318, 154, 154, 31, 267, 169, 54, 212, 265, 100, 254, 169, 212, 317, 100, 318, 154, 154, 31, 268, 169, 15, 212, 253, 100, 266, 154, 31, 269, 169, 99, 212, 268, 4, 259, 200, 59, 100, 175, 26, 154, 31, 269, 169, 178, 212, 212, 241, 131, 263, 69, 70, 212, 317, 100, 236, 154, 154, 200, 175, 100, 59, 26, 100, 269, 100, 317, 154, 31, 270, 169, 15, 212, 258, 100, 267, 154, 31, 271, 169, 269, 213, 212, 270, 76, 247, 212, 132, 154, 4, 260, 200, 59, 100, 175, 26, 154, 31, 261, 151, 15, 212, 271, 76, 247, 212, 266, 76, 84, 154, 100, 67, 212, 266, 154, 154, 31, 163, 31, 72, 31, 261, 23, 223, 31, 10, 212, 252, 100, 261, 76, 247, 212, 252, 76, 84, 76, 106, 154, 100, 254, 169, 212, 317, 100, 318, 154, 154, 31, 3, 31]}, {"code": "def parallel_nsa_bwd_kernel_dkv(\n    q,\n    k,\n    v,\n    lse,\n    delta,\n    do,\n    dk,\n    dv,\n    block_mask,\n    offsets,\n    chunk_indices,\n    scale,\n    T,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    HQ: tl.constexpr,\n    G: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    M: tl.constexpr,\n    BS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    USE_OFFSETS: tl.constexpr,\n):\n    i_v, i_s, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n\n    if USE_OFFSETS:\n        i_n, i_s = tl.load(chunk_indices + i_s * 2).to(tl.int32), tl.load(\n            chunk_indices + i_s * 2 + 1\n        ).to(tl.int32)\n        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(\n            tl.int32\n        )\n        T = eos - bos\n    else:\n        bos, eos = i_b * T, i_b * T + T\n\n    p_k = tl.make_block_ptr(\n        k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_s * BS, 0), (BS, BK), (1, 0)\n    )\n    p_v = tl.make_block_ptr(\n        v + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_s * BS, i_v * BV),\n        (BS, BV),\n        (1, 0),\n    )\n    p_dk = tl.make_block_ptr(\n        dk + (i_v * B * T * H + bos * H + i_h) * K,\n        (T, K),\n        (H * K, 1),\n        (i_s * BS, 0),\n        (BS, BK),\n        (1, 0),\n    )\n    p_dv = tl.make_block_ptr(\n        dv + (bos * H + i_h) * V,\n        (T, V),\n        (H * V, 1),\n        (i_s * BS, i_v * BV),\n        (BS, BV),\n        (1, 0),\n    )\n\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dk = tl.zeros([BS, BK], dtype=tl.float32)\n\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BS, BV], dtype=tl.float32)\n\n    for i in range(i_s * BS, T):\n        b_m = tl.load(block_mask + (bos + i) * H * M + i_h * M + i_s)\n        if b_m:\n            p_q = tl.make_block_ptr(\n                q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)\n            )\n\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_q = (b_q * scale).to(b_q.dtype)\n\n            p_do = tl.make_block_ptr(\n                do + (bos + i) * HQ * V,\n                (HQ, V),\n                (V, 1),\n                (i_h * G, i_v * BV),\n                (G, BV),\n                (1, 0),\n            )\n            p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n            p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n\n            b_do = tl.load(p_do, boundary_check=(0, 1))\n\n            b_lse = tl.load(p_lse)\n            b_delta = tl.load(p_delta)\n\n            b_s = tl.dot(b_k, tl.trans(b_q))\n            b_p = tl.exp(b_s - b_lse[None, :])\n            b_p = tl.where((i >= (i_s * BS + tl.arange(0, BS)))[:, None], b_p, 0)\n\n            b_dv += tl.dot(b_p.to(b_do.dtype), b_do)\n\n            b_dp = tl.dot(b_v, tl.trans(b_do))\n\n            b_ds = b_p * (b_dp - b_delta[None, :])\n\n            b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\n\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))", "encoded": [28, 316, 212, 216, 100, 217, 100, 218, 100, 219, 100, 220, 100, 221, 100, 222, 100, 223, 100, 224, 100, 225, 100, 226, 100, 227, 100, 228, 100, 229, 58, 6, 100, 230, 58, 6, 100, 231, 58, 6, 100, 232, 58, 6, 100, 233, 58, 6, 100, 234, 58, 6, 100, 235, 58, 6, 100, 236, 58, 6, 100, 237, 58, 6, 100, 238, 58, 6, 100, 239, 58, 6, 154, 58, 31, -1, 240, 100, 241, 100, 242, 169, 212, 150, 212, 317, 154, 100, 150, 212, 318, 154, 100, 150, 212, 319, 154, 154, 31, 243, 100, 244, 169, 212, 242, 46, 230, 100, 242, 196, 230, 154, 31, 160, 239, 58, 31, 245, 100, 241, 169, 212, 52, 212, 226, 69, 241, 213, 319, 154, 76, 246, 212, 215, 154, 100, 52, 212, 226, 69, 241, 213, 319, 69, 318, 154, 76, 246, 212, 215, 154, 154, 31, 247, 100, 248, 169, 212, 52, 212, 225, 69, 245, 154, 76, 246, 212, 215, 154, 100, 52, 212, 225, 69, 245, 69, 318, 154, 76, 246, 212, 215, 154, 154, 31, 228, 169, 248, 4, 247, 31, 163, 31, 29, 58, 31, 247, 100, 248, 169, 212, 243, 213, 228, 100, 243, 213, 228, 69, 228, 154, 31, 53, 31, 249, 169, 191, 212, 217, 69, 212, 247, 213, 230, 69, 244, 154, 213, 233, 100, 212, 228, 100, 233, 154, 100, 212, 230, 213, 233, 100, 318, 154, 100, 212, 241, 213, 236, 100, 317, 154, 100, 212, 236, 100, 237, 154, 100, 212, 318, 100, 317, 154, 154, 31, 250, 169, 191, 212, 218, 69, 212, 247, 213, 230, 69, 244, 154, 213, 234, 100, 212, 228, 100, 234, 154, 100, 212, 230, 213, 234, 100, 318, 154, 100, 212, 241, 213, 236, 100, 240, 213, 238, 154, 100, 212, 236, 100, 238, 154, 100, 212, 318, 100, 317, 154, 154, 31, 251, 169, 191, 212, 222, 69, 212, 240, 213, 229, 213, 228, 213, 230, 69, 247, 213, 230, 69, 244, 154, 213, 233, 100, 212, 228, 100, 233, 154, 100, 212, 230, 213, 233, 100, 318, 154, 100, 212, 241, 213, 236, 100, 317, 154, 100, 212, 236, 100, 237, 154, 100, 212, 318, 100, 317, 154, 154, 31, 252, 169, 191, 212, 223, 69, 212, 247, 213, 230, 69, 244, 154, 213, 234, 100, 212, 228, 100, 234, 154, 100, 212, 230, 213, 234, 100, 318, 154, 100, 212, 241, 213, 236, 100, 240, 213, 238, 154, 100, 212, 236, 100, 238, 154, 100, 212, 318, 100, 317, 154, 154, 31, 253, 169, 52, 212, 249, 100, 254, 169, 212, 317, 100, 318, 154, 154, 31, 255, 169, 155, 212, 200, 236, 100, 237, 26, 100, 84, 169, 132, 154, 31, 256, 169, 52, 212, 250, 100, 254, 169, 212, 317, 100, 318, 154, 154, 31, 257, 169, 155, 212, 200, 236, 100, 238, 26, 100, 84, 169, 132, 154, 31, 123, 258, 141, 5, 212, 241, 213, 236, 100, 228, 154, 58, 31, 259, 169, 52, 212, 224, 69, 212, 247, 69, 258, 154, 213, 230, 213, 235, 69, 244, 213, 235, 69, 241, 154, 31, 160, 259, 58, 31, 260, 169, 191, 212, 216, 69, 212, 247, 69, 258, 154, 213, 231, 213, 233, 100, 212, 231, 100, 233, 154, 100, 212, 233, 100, 318, 154, 100, 212, 244, 213, 232, 100, 317, 154, 100, 212, 232, 100, 237, 154, 100, 212, 318, 100, 317, 154, 154, 31, 261, 169, 52, 212, 260, 100, 254, 169, 212, 317, 100, 318, 154, 154, 31, 261, 169, 212, 261, 213, 227, 154, 76, 246, 212, 261, 76, 84, 154, 31, 262, 169, 191, 212, 221, 69, 212, 247, 69, 258, 154, 213, 231, 213, 234, 100, 212, 231, 100, 234, 154, 100, 212, 234, 100, 318, 154, 100, 212, 244, 213, 232, 100, 240, 213, 238, 154, 100, 212, 232, 100, 238, 154, 100, 212, 318, 100, 317, 154, 154, 31, 263, 169, 219, 69, 212, 247, 69, 258, 154, 213, 231, 69, 244, 213, 232, 69, 70, 212, 317, 100, 232, 154, 31, 264, 169, 220, 69, 212, 247, 69, 258, 154, 213, 231, 69, 244, 213, 232, 69, 70, 212, 317, 100, 232, 154, 31, 265, 169, 52, 212, 262, 100, 254, 169, 212, 317, 100, 318, 154, 154, 31, 266, 169, 52, 212, 263, 154, 31, 267, 169, 52, 212, 264, 154, 31, 268, 169, 15, 212, 253, 100, 67, 212, 261, 154, 154, 31, 269, 169, 99, 212, 268, 4, 266, 200, 175, 100, 58, 26, 154, 31, 269, 169, 178, 212, 212, 258, 131, 241, 213, 236, 69, 70, 212, 317, 100, 236, 154, 154, 200, 58, 100, 175, 26, 100, 269, 100, 317, 154, 31, 257, 151, 15, 212, 269, 76, 246, 212, 265, 76, 84, 154, 100, 265, 154, 31, 270, 169, 15, 212, 256, 100, 67, 212, 265, 154, 154, 31, 271, 169, 269, 213, 212, 270, 4, 267, 200, 175, 100, 58, 26, 154, 31, 255, 151, 15, 212, 271, 76, 246, 212, 261, 76, 84, 154, 100, 261, 154, 31, 163, 31, 72, 31, 10, 212, 251, 100, 255, 76, 246, 212, 251, 76, 84, 76, 106, 154, 100, 254, 169, 212, 317, 100, 318, 154, 154, 31, 10, 212, 252, 100, 257, 76, 246, 212, 252, 76, 84, 76, 106, 154, 100, 254, 169, 212, 317, 100, 318, 154, 154, 31, 3, 31]}, {"code": "def act_func_forward_kernel(\n    input_pointer,\n    output_pointer,\n    size,\n    drop_p,\n    seed,\n    param,\n    act_func: tl.constexpr,\n    dropout: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n\n    input = tl.load(input_pointer + offset, mask=mask)\n    tl.store(\n        output_pointer + offset,\n        apply_act_func(input, drop_p, seed, offset, param, act_func, dropout),\n        mask=mask,\n    )", "encoded": [28, 316, 212, 216, 100, 217, 100, 187, 100, 218, 100, 219, 100, 220, 100, 221, 59, 6, 100, 222, 59, 6, 100, 223, 59, 6, 154, 59, 31, -1, 224, 169, 150, 212, 225, 169, 317, 154, 31, 226, 169, 224, 213, 223, 69, 70, 212, 317, 100, 223, 154, 31, 227, 169, 226, 1, 187, 31, 228, 169, 54, 212, 216, 69, 226, 100, 227, 169, 227, 154, 31, 10, 212, 217, 69, 226, 100, 229, 212, 228, 100, 218, 100, 219, 100, 226, 100, 220, 100, 221, 100, 222, 154, 100, 227, 169, 227, 154, 31, 3, 31]}, {"code": "def act_func_backward_kernel(\n    output_grad_pointer,\n    input_pointer,\n    input_grad_pointer,\n    size,\n    drop_p,\n    seed,\n    param,\n    act_func: tl.constexpr,\n    dropout: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n\n    output_grad = tl.load(output_grad_pointer + offset, mask=mask)\n    input = tl.load(input_pointer + offset, mask=mask)\n\n    tl.store(\n        input_grad_pointer + offset,\n        apply_act_func_grad(\n            output_grad, input, drop_p, seed, offset, param, act_func, dropout\n        ),\n        mask=mask,\n    )", "encoded": [28, 316, 212, 216, 100, 217, 100, 218, 100, 187, 100, 219, 100, 220, 100, 221, 100, 222, 58, 6, 100, 223, 58, 6, 100, 224, 58, 6, 154, 58, 31, -1, 225, 169, 150, 212, 226, 169, 317, 154, 31, 227, 169, 225, 213, 224, 69, 70, 212, 317, 100, 224, 154, 31, 228, 169, 227, 1, 187, 31, 229, 169, 52, 212, 216, 69, 227, 100, 228, 169, 228, 154, 31, 230, 169, 52, 212, 217, 69, 227, 100, 228, 169, 228, 154, 31, 10, 212, 218, 69, 227, 100, 231, 212, 229, 100, 230, 100, 219, 100, 220, 100, 227, 100, 221, 100, 222, 100, 223, 154, 100, 228, 169, 228, 154, 31, 3, 31]}, {"code": "def batch_norm_forward_kernel(\n    input_pointer,\n    weight_pointer,\n    bias_pointer,\n    mean_pointer,\n    inv_std_pointer,\n    pre_act_add_pointer,\n    pre_act_pointer,\n    output_pointer,\n    running_mean_pointer,\n    running_var_pointer,\n    batch_dim,\n    spatial_dim,\n    input_batch_stride,\n    input_feat_stride,\n    input_spatial_stride,\n    pre_act_add_batch_stride,\n    pre_act_add_feat_stride,\n    pre_act_add_spatial_stride,\n    pre_act_batch_stride,\n    pre_act_feat_stride,\n    pre_act_spatial_stride,\n    output_batch_stride,\n    output_feat_stride,\n    output_spatial_stride,\n    momentum,\n    eps,\n    param,\n    affine: tl.constexpr,\n    save_stats: tl.constexpr,\n    track_running_stats: tl.constexpr,\n    is_train: tl.constexpr,\n    add_pre_act: tl.constexpr,\n    act_func: tl.constexpr,\n    save_pre_act: tl.constexpr,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_SPATIAL: tl.constexpr,\n):\n\n    feat_pid = tl.program_id(axis=0)\n\n    batch_offset = tl.arange(0, BLOCK_SIZE_BATCH)\n    batch_mask = batch_offset < batch_dim\n\n    if is_train or not track_running_stats:\n        count = 0\n        mean = 0.0\n        var = 0.0\n\n        for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n            spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(\n                0, BLOCK_SIZE_SPATIAL\n            )\n            spatial_mask = spatial_offset < spatial_dim\n\n            curr_input_pointer = (\n                input_pointer\n                + input_feat_stride * feat_pid\n                + input_batch_stride * batch_offset[:, None]\n                + input_spatial_stride * spatial_offset[None, :]\n            )\n            curr_input = tl.load(\n                curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]\n            ).to(tl.float32)\n\n            spatial_count = min(\n                BLOCK_SIZE_SPATIAL, spatial_dim - block_ind * BLOCK_SIZE_SPATIAL\n            )\n            curr_count = spatial_count * batch_dim\n            count += curr_count\n\n            prev_mean = mean\n            mean += (tl.sum(curr_input) - curr_count * mean) / count\n            deltas = tl.where(\n                batch_mask[:, None] & spatial_mask[None, :],\n                (curr_input - mean) * (curr_input - prev_mean),\n                0.0,\n            )\n            var += tl.sum(deltas)\n\n        var /= count\n        inv_std = tl.rsqrt(var + eps)\n\n        if save_stats:\n            tl.store(feat_pid + mean_pointer, mean)\n            tl.store(feat_pid + inv_std_pointer, inv_std)\n\n        if track_running_stats:\n            running_mean_pointer += feat_pid\n            running_var_pointer += feat_pid\n\n            running_mean = tl.load(running_mean_pointer)\n            running_var = tl.load(running_var_pointer)\n\n            n = batch_dim * spatial_dim\n            tl.store(\n                running_mean_pointer, (1 - momentum) * running_mean + momentum * mean\n            )\n            tl.store(\n                running_var_pointer,\n                (1 - momentum) * running_var + momentum * var * n / (n - 1),\n            )\n\n    else:\n        mean = tl.load(feat_pid + running_mean_pointer)\n        inv_std = tl.rsqrt(tl.load(feat_pid + running_var_pointer) + eps)\n\n    if affine:\n        weight = tl.load(feat_pid + weight_pointer)\n        bias = tl.load(feat_pid + bias_pointer)\n\n    else:\n        weight = 1.0\n        bias = 0.0\n\n    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(\n            0, BLOCK_SIZE_SPATIAL\n        )\n        spatial_mask = spatial_offset < spatial_dim\n\n        curr_input_pointer = (\n            input_pointer\n            + input_feat_stride * feat_pid\n            + input_batch_stride * batch_offset[:, None]\n            + input_spatial_stride * spatial_offset[None, :]\n        )\n        curr_output_pointer = (\n            output_pointer\n            + output_feat_stride * feat_pid\n            + output_batch_stride * batch_offset[:, None]\n            + output_spatial_stride * spatial_offset[None, :]\n        )\n\n        curr_input = tl.load(\n            curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]\n        ).to(tl.float32)\n        output = weight * (curr_input - mean) * inv_std + bias\n\n        if add_pre_act:\n            curr_pre_act_add_pointer = (\n                pre_act_add_pointer\n                + pre_act_add_feat_stride * feat_pid\n                + pre_act_add_batch_stride * batch_offset[:, None]\n                + pre_act_add_spatial_stride * spatial_offset[None, :]\n            )\n            curr_pre_act_add = tl.load(\n                curr_pre_act_add_pointer,\n                mask=batch_mask[:, None] & spatial_mask[None, :],\n            )\n            output += curr_pre_act_add\n\n        if act_func is not None:\n            if save_pre_act:\n                curr_pre_act_pointer = (\n                    pre_act_pointer\n                    + pre_act_feat_stride * feat_pid\n                    + pre_act_batch_stride * batch_offset[:, None]\n                    + pre_act_spatial_stride * spatial_offset[None, :]\n                )\n                tl.store(\n                    curr_pre_act_pointer,\n                    output,\n                    mask=batch_mask[:, None] & spatial_mask[None, :],\n                )\n\n            output = apply_act_func(output, None, None, None, param, act_func, False)\n\n        tl.store(\n            curr_output_pointer,\n            output,\n            mask=batch_mask[:, None] & spatial_mask[None, :],\n        )", "encoded": [28, 317, 213, 217, 100, 218, 100, 219, 100, 220, 100, 221, 100, 222, 100, 223, 100, 224, 100, 225, 100, 226, 100, 227, 100, 228, 100, 229, 100, 230, 100, 231, 100, 232, 100, 233, 100, 234, 100, 235, 100, 236, 100, 237, 100, 238, 100, 239, 100, 240, 100, 241, 100, 242, 100, 243, 100, 244, 59, 6, 100, 245, 59, 6, 100, 246, 59, 6, 100, 247, 59, 6, 100, 248, 59, 6, 100, 249, 59, 6, 100, 250, 59, 6, 100, 251, 59, 6, 100, 252, 59, 6, 155, 59, 31, -1, 253, 170, 151, 213, 254, 170, 318, 155, 31, 255, 170, 70, 213, 318, 100, 251, 155, 31, 256, 170, 255, 1, 227, 31, 161, 247, 129, 60, 246, 59, 31, 257, 170, 318, 31, 258, 170, 318, 31, 259, 170, 318, 31, 124, 260, 142, 5, 213, 318, 100, 61, 213, 228, 100, 252, 155, 155, 59, 31, 261, 170, 260, 214, 252, 69, 70, 213, 318, 100, 252, 155, 31, 262, 170, 261, 1, 228, 31, 263, 170, 217, 69, 230, 214, 253, 69, 229, 214, 255, 201, 59, 100, 176, 26, 69, 231, 214, 261, 201, 176, 100, 59, 26, 31, 264, 170, 54, 213, 263, 100, 265, 170, 256, 201, 59, 100, 176, 26, 153, 262, 201, 176, 100, 59, 26, 155, 76, 266, 213, 133, 155, 31, 267, 170, 38, 213, 252, 100, 228, 4, 260, 214, 252, 155, 31, 268, 170, 267, 214, 227, 31, 257, 152, 268, 31, 269, 170, 258, 31, 258, 152, 213, 194, 213, 264, 155, 4, 268, 214, 258, 155, 41, 257, 31, 270, 170, 179, 213, 256, 201, 59, 100, 176, 26, 153, 262, 201, 176, 100, 59, 26, 100, 213, 264, 4, 258, 155, 214, 213, 264, 4, 269, 155, 100, 318, 155, 31, 259, 152, 194, 213, 270, 155, 31, 72, 31, 259, 8, 257, 31, 271, 170, 109, 213, 259, 69, 242, 155, 31, 161, 245, 59, 31, 10, 213, 253, 69, 220, 100, 258, 155, 31, 10, 213, 253, 69, 221, 100, 271, 155, 31, 164, 31, 161, 246, 59, 31, 225, 152, 253, 31, 226, 152, 253, 31, 272, 170, 54, 213, 225, 155, 31, 273, 170, 54, 213, 226, 155, 31, 274, 170, 227, 214, 228, 31, 10, 213, 225, 100, 213, 319, 4, 241, 155, 214, 272, 69, 241, 214, 258, 155, 31, 10, 213, 226, 100, 213, 319, 4, 241, 155, 214, 273, 69, 241, 214, 259, 214, 274, 41, 213, 274, 4, 319, 155, 155, 31, 164, 31, 164, 31, 29, 59, 31, 258, 170, 54, 213, 253, 69, 225, 155, 31, 271, 170, 109, 213, 54, 213, 253, 69, 226, 155, 69, 242, 155, 31, 53, 31, 161, 244, 59, 31, 275, 170, 54, 213, 253, 69, 218, 155, 31, 276, 170, 54, 213, 253, 69, 219, 155, 31, 164, 31, 29, 59, 31, 275, 170, 319, 31, 276, 170, 318, 31, 53, 31, 124, 260, 142, 5, 213, 318, 100, 61, 213, 228, 100, 252, 155, 155, 59, 31, 261, 170, 260, 214, 252, 69, 70, 213, 318, 100, 252, 155, 31, 262, 170, 261, 1, 228, 31, 263, 170, 217, 69, 230, 214, 253, 69, 229, 214, 255, 201, 59, 100, 176, 26, 69, 231, 214, 261, 201, 176, 100, 59, 26, 31, 277, 170, 224, 69, 239, 214, 253, 69, 238, 214, 255, 201, 59, 100, 176, 26, 69, 240, 214, 261, 201, 176, 100, 59, 26, 31, 264, 170, 54, 213, 263, 100, 265, 170, 256, 201, 59, 100, 176, 26, 153, 262, 201, 176, 100, 59, 26, 155, 76, 266, 213, 133, 155, 31, 278, 170, 275, 214, 213, 264, 4, 258, 155, 214, 271, 69, 276, 31, 161, 248, 59, 31, 279, 170, 222, 69, 233, 214, 253, 69, 232, 214, 255, 201, 59, 100, 176, 26, 69, 234, 214, 261, 201, 176, 100, 59, 26, 31, 280, 170, 54, 213, 279, 100, 265, 170, 256, 201, 59, 100, 176, 26, 153, 262, 201, 176, 100, 59, 26, 155, 31, 278, 152, 280, 31, 164, 31, 161, 249, 102, 60, 176, 59, 31, 161, 250, 59, 31, 281, 170, 223, 69, 236, 214, 253, 69, 235, 214, 255, 201, 59, 100, 176, 26, 69, 237, 214, 261, 201, 176, 100, 59, 26, 31, 10, 213, 281, 100, 278, 100, 265, 170, 256, 201, 59, 100, 176, 26, 153, 262, 201, 176, 100, 59, 26, 155, 31, 164, 31, 278, 170, 282, 213, 278, 100, 176, 100, 176, 100, 176, 100, 243, 100, 249, 100, 56, 155, 31, 164, 31, 10, 213, 277, 100, 278, 100, 265, 170, 256, 201, 59, 100, 176, 26, 153, 262, 201, 176, 100, 59, 26, 155, 31, 72, 31, 3, 31]}, {"code": "def batch_norm_backward_kernel(\n    output_grad_pointer,\n    input_pointer,\n    mean_pointer,\n    inv_std_pointer,\n    weight_pointer,\n    input_grad_pointer,\n    weight_grad_pointer,\n    bias_grad_pointer,\n    batch_dim,\n    spatial_dim,\n    output_grad_batch_stride,\n    output_grad_feat_stride,\n    output_grad_spatial_stride,\n    input_batch_stride,\n    input_feat_stride,\n    input_spatial_stride,\n    input_grad_batch_stride,\n    input_grad_feat_stride,\n    input_grad_spatial_stride,\n    affine: tl.constexpr,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_SPATIAL: tl.constexpr,\n):\n\n    feat_pid = tl.program_id(axis=0)\n\n    batch_offset = tl.arange(0, BLOCK_SIZE_BATCH)\n    batch_mask = batch_offset < batch_dim\n\n    mean = tl.load(feat_pid + mean_pointer)\n    inv_std = tl.load(feat_pid + inv_std_pointer)\n\n    term1 = 0.0\n    term2 = 0.0\n\n    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(\n            0, BLOCK_SIZE_SPATIAL\n        )\n        spatial_mask = spatial_offset < spatial_dim\n\n        curr_output_grad_pointer = (\n            output_grad_pointer\n            + output_grad_feat_stride * feat_pid\n            + output_grad_batch_stride * batch_offset[:, None]\n            + output_grad_spatial_stride * spatial_offset[None, :]\n        )\n        curr_input_pointer = (\n            input_pointer\n            + input_feat_stride * feat_pid\n            + input_batch_stride * batch_offset[:, None]\n            + input_spatial_stride * spatial_offset[None, :]\n        )\n\n        curr_input = tl.load(\n            curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]\n        ).to(tl.float32)\n        curr_pre_lin = (curr_input - mean) * inv_std\n        curr_output_grad = tl.load(\n            curr_output_grad_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]\n        ).to(tl.float32)\n\n        term1 += tl.sum(curr_pre_lin * curr_output_grad)\n        term2 += tl.sum(curr_output_grad)\n\n    if affine:\n        weight = tl.load(feat_pid + weight_pointer)\n        weight_grad = 0.0\n        bias_grad = 0.0\n\n    else:\n        weight = 1.0\n\n    count = batch_dim * spatial_dim\n    term1 *= weight / count\n    term2 *= weight / count\n\n    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(\n            0, BLOCK_SIZE_SPATIAL\n        )\n        spatial_mask = spatial_offset < spatial_dim\n\n        curr_output_grad_pointer = (\n            output_grad_pointer\n            + output_grad_feat_stride * feat_pid\n            + output_grad_batch_stride * batch_offset[:, None]\n            + output_grad_spatial_stride * spatial_offset[None, :]\n        )\n        curr_input_pointer = (\n            input_pointer\n            + input_feat_stride * feat_pid\n            + input_batch_stride * batch_offset[:, None]\n            + input_spatial_stride * spatial_offset[None, :]\n        )\n        curr_input_grad_pointer = (\n            input_grad_pointer\n            + input_grad_feat_stride * feat_pid\n            + input_grad_batch_stride * batch_offset[:, None]\n            + input_grad_spatial_stride * spatial_offset[None, :]\n        )\n\n        curr_input = tl.load(\n            curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]\n        ).to(tl.float32)\n        curr_pre_lin = (curr_input - mean) * inv_std\n        curr_output_grad = tl.load(\n            curr_output_grad_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]\n        ).to(tl.float32)\n        curr_input_grad = inv_std * (\n            weight * curr_output_grad - (term1 * curr_pre_lin + term2)\n        )\n        tl.store(\n            curr_input_grad_pointer,\n            curr_input_grad,\n            mask=batch_mask[:, None] & spatial_mask[None, :],\n        )\n\n        if affine:\n            weight_grad += tl.sum(curr_pre_lin * curr_output_grad)\n            bias_grad += tl.sum(curr_output_grad)\n\n    if affine:\n        tl.store(feat_pid + weight_grad_pointer, weight_grad)\n        tl.store(feat_pid + bias_grad_pointer, bias_grad)", "encoded": [28, 317, 213, 217, 100, 218, 100, 219, 100, 220, 100, 221, 100, 222, 100, 223, 100, 224, 100, 225, 100, 226, 100, 227, 100, 228, 100, 229, 100, 230, 100, 231, 100, 232, 100, 233, 100, 234, 100, 235, 100, 236, 58, 6, 100, 237, 58, 6, 100, 238, 58, 6, 155, 58, 31, -1, 239, 170, 151, 213, 240, 170, 318, 155, 31, 241, 170, 70, 213, 318, 100, 237, 155, 31, 242, 170, 241, 1, 225, 31, 243, 170, 52, 213, 239, 69, 219, 155, 31, 244, 170, 52, 213, 239, 69, 220, 155, 31, 245, 170, 318, 31, 246, 170, 318, 31, 124, 247, 142, 5, 213, 318, 100, 60, 213, 226, 100, 238, 155, 155, 58, 31, 248, 170, 247, 214, 238, 69, 70, 213, 318, 100, 238, 155, 31, 249, 170, 248, 1, 226, 31, 250, 170, 217, 69, 228, 214, 239, 69, 227, 214, 241, 201, 58, 100, 176, 26, 69, 229, 214, 248, 201, 176, 100, 58, 26, 31, 251, 170, 218, 69, 231, 214, 239, 69, 230, 214, 241, 201, 58, 100, 176, 26, 69, 232, 214, 248, 201, 176, 100, 58, 26, 31, 252, 170, 52, 213, 251, 100, 253, 170, 242, 201, 58, 100, 176, 26, 153, 249, 201, 176, 100, 58, 26, 155, 76, 254, 213, 133, 155, 31, 255, 170, 213, 252, 4, 243, 155, 214, 244, 31, 256, 170, 52, 213, 250, 100, 253, 170, 242, 201, 58, 100, 176, 26, 153, 249, 201, 176, 100, 58, 26, 155, 76, 254, 213, 133, 155, 31, 245, 152, 194, 213, 255, 214, 256, 155, 31, 246, 152, 194, 213, 256, 155, 31, 72, 31, 161, 236, 58, 31, 257, 170, 52, 213, 239, 69, 221, 155, 31, 258, 170, 318, 31, 259, 170, 318, 31, 164, 31, 29, 58, 31, 257, 170, 319, 31, 53, 31, 260, 170, 225, 214, 226, 31, 245, 23, 257, 41, 260, 31, 246, 23, 257, 41, 260, 31, 124, 247, 142, 5, 213, 318, 100, 60, 213, 226, 100, 238, 155, 155, 58, 31, 248, 170, 247, 214, 238, 69, 70, 213, 318, 100, 238, 155, 31, 249, 170, 248, 1, 226, 31, 250, 170, 217, 69, 228, 214, 239, 69, 227, 214, 241, 201, 58, 100, 176, 26, 69, 229, 214, 248, 201, 176, 100, 58, 26, 31, 251, 170, 218, 69, 231, 214, 239, 69, 230, 214, 241, 201, 58, 100, 176, 26, 69, 232, 214, 248, 201, 176, 100, 58, 26, 31, 261, 170, 222, 69, 234, 214, 239, 69, 233, 214, 241, 201, 58, 100, 176, 26, 69, 235, 214, 248, 201, 176, 100, 58, 26, 31, 252, 170, 52, 213, 251, 100, 253, 170, 242, 201, 58, 100, 176, 26, 153, 249, 201, 176, 100, 58, 26, 155, 76, 254, 213, 133, 155, 31, 255, 170, 213, 252, 4, 243, 155, 214, 244, 31, 256, 170, 52, 213, 250, 100, 253, 170, 242, 201, 58, 100, 176, 26, 153, 249, 201, 176, 100, 58, 26, 155, 76, 254, 213, 133, 155, 31, 262, 170, 244, 214, 213, 257, 214, 256, 4, 213, 245, 214, 255, 69, 246, 155, 155, 31, 10, 213, 261, 100, 262, 100, 253, 170, 242, 201, 58, 100, 176, 26, 153, 249, 201, 176, 100, 58, 26, 155, 31, 161, 236, 58, 31, 258, 152, 194, 213, 255, 214, 256, 155, 31, 259, 152, 194, 213, 256, 155, 31, 164, 31, 72, 31, 161, 236, 58, 31, 10, 213, 239, 69, 223, 100, 258, 155, 31, 10, 213, 239, 69, 224, 100, 259, 155, 31, 164, 31, 3, 31]}, {"code": "def conv2d_forward_kernel(\n    input_pointer,\n    weight_pointer,\n    output_pointer,\n    batch_dim,\n    in_feat_dim,\n    in_height,\n    in_width,\n    out_feat_dim,\n    out_height,\n    out_width,\n    input_batch_stride,\n    input_in_feat_stride,\n    input_height_stride,\n    input_width_stride,\n    weight_out_feat_stride,\n    weight_in_feat_stride,\n    weight_height_stride,\n    weight_width_stride,\n    output_batch_stride,\n    output_out_feat_stride,\n    output_height_stride,\n    output_width_stride,\n    kernel_height: tl.constexpr,\n    kernel_width: tl.constexpr,\n    stride_height: tl.constexpr,\n    stride_width: tl.constexpr,\n    padding_height: tl.constexpr,\n    padding_width: tl.constexpr,\n    groups: tl.constexpr,\n    fp16: tl.constexpr,\n    tf32: tl.constexpr,\n    BLOCK_SIZE_BATCH_HEIGHT_WIDTH: tl.constexpr,\n    BLOCK_SIZE_IN_FEAT: tl.constexpr,\n    BLOCK_SIZE_OUT_FEAT: tl.constexpr,\n):\n\n    batch_height_width_pid = tl.program_id(0)\n    out_feat_pid = tl.program_id(1)\n    group_pid = tl.program_id(2)\n\n    in_group_dim = in_feat_dim // groups\n    out_group_dim = out_feat_dim // groups\n\n    batch_height_width_offset = (\n        batch_height_width_pid * BLOCK_SIZE_BATCH_HEIGHT_WIDTH\n        + tl.arange(0, BLOCK_SIZE_BATCH_HEIGHT_WIDTH)\n    )\n    batch_height_offset = batch_height_width_offset // out_width\n    batch_offset = batch_height_offset // out_height\n\n    output_feat_offset = out_feat_pid * BLOCK_SIZE_OUT_FEAT + tl.arange(\n        0, BLOCK_SIZE_OUT_FEAT\n    )\n    output_height_offset = batch_height_offset % out_height\n    output_width_offset = batch_height_width_offset % out_width\n\n    input_pointer += (\n        input_batch_stride * batch_offset\n        + input_in_feat_stride * group_pid * in_group_dim\n    )[:, None]\n    weight_pointer += (\n        weight_out_feat_stride * output_feat_offset\n        + weight_out_feat_stride * group_pid * out_group_dim\n    )[None, :]\n\n    accum = tl.zeros(\n        (BLOCK_SIZE_BATCH_HEIGHT_WIDTH, BLOCK_SIZE_OUT_FEAT), dtype=tl.float32\n    )\n\n    for h in range(kernel_height):\n        for w in range(kernel_width):\n            for c in range(0, in_group_dim, BLOCK_SIZE_IN_FEAT):\n                input_feat_offset = c + tl.arange(0, BLOCK_SIZE_IN_FEAT)\n                input_height_offset = (\n                    h - padding_height + stride_height * output_height_offset\n                )\n                input_width_offset = (\n                    w - padding_width + stride_width * output_width_offset\n                )\n\n                curr_input_pointer = (\n                    input_pointer\n                    + (input_in_feat_stride * input_feat_offset)[None, :]\n                    + (input_height_stride * input_height_offset)[:, None]\n                    + (input_width_stride * input_width_offset)[:, None]\n                )\n                curr_weight_pointer = (\n                    weight_pointer\n                    + (weight_in_feat_stride * input_feat_offset)[:, None]\n                    + (weight_height_stride * h)\n                    + (weight_width_stride * w)\n                )\n\n                input_mask = (\n                    (batch_offset < batch_dim)[:, None]\n                    & (input_feat_offset < in_group_dim)[None, :]\n                    & (0 <= input_height_offset)[:, None]\n                    & (input_height_offset < in_height)[:, None]\n                    & (0 <= input_width_offset)[:, None]\n                    & (input_width_offset < in_width)[:, None]\n                )\n                weight_mask = (input_feat_offset < in_group_dim)[:, None] & (\n                    output_feat_offset < out_group_dim\n                )[None, :]\n\n                input_block = tl.load(curr_input_pointer, mask=input_mask)\n                weight_block = tl.load(curr_weight_pointer, mask=weight_mask)\n\n                if fp16:\n                    input_block = input_block.to(tl.float16)\n                    weight_block = weight_block.to(tl.float16)\n\n                accum += tl.dot(input_block, weight_block, allow_tf32=tf32)\n\n    output_pointer += (\n        (output_batch_stride * batch_offset)[:, None]\n        + (output_out_feat_stride * (group_pid * out_group_dim + output_feat_offset))[\n            None, :\n        ]\n        + (output_height_stride * output_height_offset)[:, None]\n        + (output_width_stride * output_width_offset)[:, None]\n    )\n    output_mask = (\n        (batch_offset < batch_dim)[:, None]\n        & (output_feat_offset < out_group_dim)[None, :]\n        & (output_height_offset < out_height)[:, None]\n        & (output_width_offset < out_width)[:, None]\n    )\n\n    tl.store(output_pointer, accum, mask=output_mask)", "encoded": [29, 318, 214, 218, 101, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 227, 101, 228, 101, 229, 101, 230, 101, 231, 101, 232, 101, 233, 101, 234, 101, 235, 101, 236, 101, 237, 101, 238, 101, 239, 101, 240, 60, 6, 101, 241, 60, 6, 101, 242, 60, 6, 101, 243, 60, 6, 101, 244, 60, 6, 101, 245, 60, 6, 101, 246, 60, 6, 101, 247, 60, 6, 101, 248, 60, 6, 101, 249, 60, 6, 101, 250, 60, 6, 101, 251, 60, 6, 156, 60, 32, -1, 252, 171, 152, 214, 319, 156, 32, 253, 171, 152, 214, 320, 156, 32, 254, 171, 152, 214, 321, 156, 32, 255, 171, 222, 47, 246, 32, 256, 171, 225, 47, 246, 32, 257, 171, 252, 215, 249, 70, 71, 214, 319, 101, 249, 156, 32, 258, 171, 257, 47, 227, 32, 259, 171, 258, 47, 226, 32, 260, 171, 253, 215, 251, 70, 71, 214, 319, 101, 251, 156, 32, 261, 171, 258, 198, 226, 32, 262, 171, 257, 198, 227, 32, 218, 153, 214, 228, 215, 259, 70, 229, 215, 254, 215, 255, 156, 202, 60, 101, 177, 27, 32, 219, 153, 214, 232, 215, 260, 70, 232, 215, 254, 215, 256, 156, 202, 177, 101, 60, 27, 32, 263, 171, 157, 214, 214, 249, 101, 251, 156, 101, 85, 171, 134, 156, 32, 125, 264, 143, 5, 214, 240, 156, 60, 32, 125, 265, 143, 5, 214, 241, 156, 60, 32, 125, 266, 143, 5, 214, 319, 101, 255, 101, 250, 156, 60, 32, 267, 171, 266, 70, 71, 214, 319, 101, 250, 156, 32, 268, 171, 264, 4, 244, 70, 242, 215, 261, 32, 269, 171, 265, 4, 245, 70, 243, 215, 262, 32, 270, 171, 218, 70, 214, 229, 215, 267, 156, 202, 177, 101, 60, 27, 70, 214, 230, 215, 268, 156, 202, 60, 101, 177, 27, 70, 214, 231, 215, 269, 156, 202, 60, 101, 177, 27, 32, 271, 171, 219, 70, 214, 233, 215, 267, 156, 202, 60, 101, 177, 27, 70, 234, 215, 264, 70, 235, 215, 265, 32, 272, 171, 214, 259, 1, 221, 156, 202, 60, 101, 177, 27, 154, 214, 267, 1, 255, 156, 202, 177, 101, 60, 27, 154, 214, 319, 192, 268, 156, 202, 60, 101, 177, 27, 154, 214, 268, 1, 223, 156, 202, 60, 101, 177, 27, 154, 214, 319, 192, 269, 156, 202, 60, 101, 177, 27, 154, 214, 269, 1, 224, 156, 202, 60, 101, 177, 27, 32, 273, 171, 214, 267, 1, 255, 156, 202, 60, 101, 177, 27, 154, 214, 260, 1, 256, 156, 202, 177, 101, 60, 27, 32, 274, 171, 55, 214, 270, 101, 275, 171, 272, 156, 32, 276, 171, 55, 214, 271, 101, 275, 171, 273, 156, 32, 162, 247, 60, 32, 274, 171, 274, 77, 277, 214, 21, 156, 32, 276, 171, 276, 77, 277, 214, 21, 156, 32, 165, 32, 263, 153, 15, 214, 274, 101, 276, 101, 278, 171, 248, 156, 32, 73, 32, 73, 32, 73, 32, 220, 153, 214, 236, 215, 259, 156, 202, 60, 101, 177, 27, 70, 214, 237, 215, 214, 254, 215, 256, 70, 260, 156, 156, 202, 177, 101, 60, 27, 70, 214, 238, 215, 261, 156, 202, 60, 101, 177, 27, 70, 214, 239, 215, 262, 156, 202, 60, 101, 177, 27, 32, 279, 171, 214, 259, 1, 221, 156, 202, 60, 101, 177, 27, 154, 214, 260, 1, 256, 156, 202, 177, 101, 60, 27, 154, 214, 261, 1, 226, 156, 202, 60, 101, 177, 27, 154, 214, 262, 1, 227, 156, 202, 60, 101, 177, 27, 32, 10, 214, 220, 101, 263, 101, 275, 171, 279, 156, 32, 3, 32]}, {"code": "def cross_entropy_loss_forward_kernel(\n    input_pointer,\n    target_pointer,\n    weight_pointer,\n    sum_weights_pointer,\n    output_pointer,\n    batch_dim,\n    feat_dim,\n    input_batch_stride,\n    input_feat_stride,\n    weighted: tl.constexpr,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEAT: tl.constexpr,\n):\n\n    batch_pid = tl.program_id(axis=0)\n\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n\n    target = tl.load(target_pointer + batch_offset, mask=batch_mask)\n\n    pred_pointer = (\n        input_pointer + input_feat_stride * target + input_batch_stride * batch_offset\n    )\n    input_pointer += (\n        input_batch_stride * batch_offset[:, None]\n        + input_feat_stride * feat_offset[None, :]\n    )\n\n    input = tl.load(\n        input_pointer,\n        mask=batch_mask[:, None] & feat_mask[None, :],\n        other=-float(\"inf\"),\n    ).to(tl.float32)\n    pred = tl.load(pred_pointer, mask=batch_mask).to(tl.float32)\n    mx = tl.max(input, axis=1)\n    input -= mx[:, None]\n    loss = tl.log(tl.sum(tl.exp(input), axis=1)) - pred + mx\n\n    if weighted:\n        weight = tl.load(weight_pointer + target, mask=batch_mask).to(tl.float32)\n        loss *= weight\n        tl.store(sum_weights_pointer + batch_pid, tl.sum(weight))\n\n    else:\n        loss /= batch_dim\n\n    tl.store(output_pointer + batch_pid, tl.sum(loss))", "encoded": [29, 318, 214, 218, 101, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 227, 59, 6, 101, 228, 59, 6, 101, 229, 59, 6, 156, 59, 32, -1, 230, 171, 152, 214, 231, 171, 319, 156, 32, 232, 171, 230, 215, 228, 70, 71, 214, 319, 101, 228, 156, 32, 233, 171, 71, 214, 319, 101, 229, 156, 32, 234, 171, 232, 1, 223, 32, 235, 171, 233, 1, 224, 32, 236, 171, 53, 214, 219, 70, 232, 101, 237, 171, 234, 156, 32, 238, 171, 218, 70, 226, 215, 236, 70, 225, 215, 232, 32, 218, 153, 225, 215, 232, 202, 59, 101, 177, 27, 70, 226, 215, 233, 202, 177, 101, 59, 27, 32, 239, 171, 53, 214, 218, 101, 237, 171, 234, 202, 59, 101, 177, 27, 154, 235, 202, 177, 101, 59, 27, 101, 240, 171, 4, 241, 214, 320, 156, 156, 77, 242, 214, 134, 156, 32, 243, 171, 53, 214, 238, 101, 237, 171, 234, 156, 77, 242, 214, 134, 156, 32, 244, 171, 12, 214, 239, 101, 231, 171, 321, 156, 32, 239, 2, 244, 202, 59, 101, 177, 27, 32, 245, 171, 49, 214, 195, 214, 100, 214, 239, 156, 101, 231, 171, 321, 156, 156, 4, 243, 70, 244, 32, 162, 227, 59, 32, 246, 171, 53, 214, 220, 70, 236, 101, 237, 171, 234, 156, 77, 242, 214, 134, 156, 32, 245, 24, 246, 32, 10, 214, 221, 70, 230, 101, 195, 214, 246, 156, 156, 32, 165, 32, 30, 59, 32, 245, 8, 223, 32, 54, 32, 10, 214, 222, 70, 230, 101, 195, 214, 245, 156, 156, 32, 3, 32]}, {"code": "def cross_entropy_loss_backward_kernel(\n    output_grad_pointer,\n    target_pointer,\n    input_pointer,\n    weight_pointer,\n    sum_weights_pointer,\n    input_grad_pointer,\n    batch_dim,\n    feat_dim,\n    input_batch_stride,\n    input_feat_stride,\n    input_grad_batch_stride,\n    input_grad_feat_stride,\n    weighted: tl.constexpr,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEAT: tl.constexpr,\n):\n\n    batch_pid = tl.program_id(axis=0)\n\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n\n    input_pointer += (\n        input_batch_stride * batch_offset[:, None]\n        + input_feat_stride * feat_offset[None, :]\n    )\n    input_grad_pointer += (\n        input_grad_batch_stride * batch_offset[:, None]\n        + input_grad_feat_stride * feat_offset[None, :]\n    )\n\n    input = tl.load(\n        input_pointer,\n        mask=batch_mask[:, None] & feat_mask[None, :],\n        other=-float(\"inf\"),\n    ).to(tl.float32)\n    input -= tl.max(input, axis=1)[:, None]\n    numerator = tl.exp(input)\n    softmax = numerator / tl.sum(numerator, axis=1)[:, None]\n\n    output_grad = tl.load(output_grad_pointer).to(tl.float32)\n    target = tl.load(target_pointer + batch_offset, mask=batch_mask)\n    broadcasted_feat_offset = tl.broadcast_to(\n        feat_offset[None, :], (BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT)\n    )\n    broadcasted_target = tl.broadcast_to(\n        target[:, None], (BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT)\n    )\n    input_grad = output_grad * (\n        softmax - (broadcasted_feat_offset == broadcasted_target)\n    )\n\n    if weighted:\n        weight = tl.load(weight_pointer + target, mask=batch_mask).to(tl.float32)\n        sum_weights = tl.load(sum_weights_pointer)\n        input_grad *= weight[:, None] / sum_weights\n\n    else:\n        input_grad /= batch_dim\n\n    tl.store(\n        input_grad_pointer, input_grad, mask=batch_mask[:, None] & feat_mask[None, :]\n    )", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 227, 101, 228, 101, 229, 101, 230, 101, 231, 60, 6, 101, 232, 60, 6, 101, 233, 60, 6, 156, 60, 32, -1, 234, 172, 152, 215, 235, 172, 320, 156, 32, 236, 172, 234, 216, 232, 70, 71, 215, 320, 101, 232, 156, 32, 237, 172, 71, 215, 320, 101, 233, 156, 32, 238, 172, 236, 1, 225, 32, 239, 172, 237, 1, 226, 32, 221, 153, 227, 216, 236, 203, 60, 101, 178, 27, 70, 228, 216, 237, 203, 178, 101, 60, 27, 32, 224, 153, 229, 216, 236, 203, 60, 101, 178, 27, 70, 230, 216, 237, 203, 178, 101, 60, 27, 32, 240, 172, 55, 215, 221, 101, 241, 172, 238, 203, 60, 101, 178, 27, 154, 239, 203, 178, 101, 60, 27, 101, 242, 172, 4, 243, 215, 321, 156, 156, 77, 244, 215, 134, 156, 32, 240, 2, 12, 215, 240, 101, 235, 172, 322, 156, 203, 60, 101, 178, 27, 32, 245, 172, 100, 215, 240, 156, 32, 246, 172, 245, 42, 196, 215, 245, 101, 235, 172, 322, 156, 203, 60, 101, 178, 27, 32, 247, 172, 55, 215, 219, 156, 77, 244, 215, 134, 156, 32, 248, 172, 55, 215, 220, 70, 236, 101, 241, 172, 238, 156, 32, 249, 172, 171, 215, 237, 203, 178, 101, 60, 27, 101, 215, 232, 101, 233, 156, 156, 32, 250, 172, 171, 215, 248, 203, 60, 101, 178, 27, 101, 215, 232, 101, 233, 156, 156, 32, 251, 172, 247, 216, 215, 246, 4, 215, 249, 72, 250, 156, 156, 32, 162, 231, 60, 32, 252, 172, 55, 215, 222, 70, 248, 101, 241, 172, 238, 156, 77, 244, 215, 134, 156, 32, 253, 172, 55, 215, 223, 156, 32, 251, 24, 252, 203, 60, 101, 178, 27, 42, 253, 32, 165, 32, 30, 60, 32, 251, 8, 225, 32, 54, 32, 10, 215, 224, 101, 251, 101, 241, 172, 238, 203, 60, 101, 178, 27, 154, 239, 203, 178, 101, 60, 27, 156, 32, 3, 32]}, {"code": "def dropout_forward_kernel(\n    input_pointer,\n    output_pointer,\n    size,\n    drop_p,\n    seed,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n\n    input = tl.load(input_pointer + offset, mask=mask)\n    output = apply_dropout(input, drop_p, seed, offset)\n    tl.store(output_pointer + offset, output, mask=mask)", "encoded": [29, 319, 215, 219, 101, 220, 101, 190, 101, 221, 101, 222, 101, 223, 59, 6, 156, 59, 32, -1, 224, 172, 152, 215, 225, 172, 320, 156, 32, 226, 172, 224, 216, 223, 70, 71, 215, 320, 101, 223, 156, 32, 227, 172, 226, 1, 190, 32, 228, 172, 53, 215, 219, 70, 226, 101, 227, 172, 227, 156, 32, 229, 172, 230, 215, 228, 101, 221, 101, 222, 101, 226, 156, 32, 10, 215, 220, 70, 226, 101, 229, 101, 227, 172, 227, 156, 32, 3, 32]}, {"code": "def dropout_backward_kernel(\n    output_grad_pointer,\n    input_grad_pointer,\n    size,\n    drop_p,\n    seed,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n\n    output_grad = tl.load(output_grad_pointer + offset, mask=mask)\n    input_grad = apply_dropout_grad(output_grad, drop_p, seed, offset)\n    tl.store(input_grad_pointer + offset, input_grad, mask=mask)", "encoded": [29, 319, 215, 219, 101, 220, 101, 190, 101, 221, 101, 222, 101, 223, 60, 6, 156, 60, 32, -1, 224, 172, 152, 215, 225, 172, 320, 156, 32, 226, 172, 224, 216, 223, 70, 71, 215, 320, 101, 223, 156, 32, 227, 172, 226, 1, 190, 32, 228, 172, 55, 215, 219, 70, 226, 101, 227, 172, 227, 156, 32, 229, 172, 230, 215, 228, 101, 221, 101, 222, 101, 226, 156, 32, 10, 215, 220, 70, 226, 101, 229, 101, 227, 172, 227, 156, 32, 3, 32]}, {"code": "def glu_forward_kernel(\n    input1_pointer,\n    input2_pointer,\n    output_pointer,\n    size,\n    param,\n    act_func: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n\n    input1 = tl.load(input1_pointer + offset, mask=mask)\n    input2 = tl.load(input2_pointer + offset, mask=mask)\n\n    output = input1 * apply_act_func(input2, None, None, None, param, act_func, False)\n    tl.store(output_pointer + offset, output, mask=mask)", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 190, 101, 222, 101, 223, 59, 6, 101, 224, 59, 6, 156, 59, 32, -1, 225, 172, 152, 215, 226, 172, 320, 156, 32, 227, 172, 225, 216, 224, 70, 71, 215, 320, 101, 224, 156, 32, 228, 172, 227, 1, 190, 32, 229, 172, 53, 215, 219, 70, 227, 101, 228, 172, 228, 156, 32, 230, 172, 53, 215, 220, 70, 227, 101, 228, 172, 228, 156, 32, 231, 172, 229, 216, 232, 215, 230, 101, 178, 101, 178, 101, 178, 101, 222, 101, 223, 101, 57, 156, 32, 10, 215, 221, 70, 227, 101, 231, 101, 228, 172, 228, 156, 32, 3, 32]}, {"code": "def glu_backward_kernel(\n    output_grad_pointer,\n    input1_pointer,\n    input2_pointer,\n    input1_grad_pointer,\n    input2_grad_pointer,\n    size,\n    param,\n    act_func: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n\n    output_grad = tl.load(output_grad_pointer + offset, mask=mask)\n    input1 = tl.load(input1_pointer + offset, mask=mask)\n    input2 = tl.load(input2_pointer + offset, mask=mask)\n\n    input1_grad = output_grad * apply_act_func(\n        input2, None, None, None, param, act_func, False\n    )\n    input2_grad = (\n        output_grad\n        * input1\n        * apply_act_func_grad(1, input2, None, None, None, param, act_func, False)\n    )\n\n    tl.store(input1_grad_pointer + offset, input1_grad, mask=mask)\n    tl.store(input2_grad_pointer + offset, input2_grad, mask=mask)", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 190, 101, 224, 101, 225, 60, 6, 101, 226, 60, 6, 156, 60, 32, -1, 227, 172, 152, 215, 228, 172, 320, 156, 32, 229, 172, 227, 216, 226, 70, 71, 215, 320, 101, 226, 156, 32, 230, 172, 229, 1, 190, 32, 231, 172, 55, 215, 219, 70, 229, 101, 230, 172, 230, 156, 32, 232, 172, 55, 215, 220, 70, 229, 101, 230, 172, 230, 156, 32, 233, 172, 55, 215, 221, 70, 229, 101, 230, 172, 230, 156, 32, 234, 172, 231, 216, 235, 215, 233, 101, 178, 101, 178, 101, 178, 101, 224, 101, 225, 101, 57, 156, 32, 236, 172, 231, 216, 232, 216, 237, 215, 321, 101, 233, 101, 178, 101, 178, 101, 178, 101, 224, 101, 225, 101, 57, 156, 32, 10, 215, 222, 70, 229, 101, 234, 101, 230, 172, 230, 156, 32, 10, 215, 223, 70, 229, 101, 236, 101, 230, 172, 230, 156, 32, 3, 32]}, {"code": "def layer_norm_forward_kernel(\n    input_pointer,\n    weight_pointer,\n    bias_pointer,\n    mean_pointer,\n    inv_std_pointer,\n    output_pointer,\n    batch_dim,\n    feat_dim,\n    input_batch_stride,\n    input_feat_stride,\n    output_batch_stride,\n    output_feat_stride,\n    eps,\n    scale_by_weight: tl.constexpr,\n    add_bias: tl.constexpr,\n    save_stats: tl.constexpr,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEAT: tl.constexpr,\n):\n\n    batch_pid = tl.program_id(axis=0)\n\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n\n    input_pointer += (\n        input_batch_stride * batch_offset[:, None]\n        + input_feat_stride * feat_offset[None, :]\n    )\n    output_pointer += (\n        output_batch_stride * batch_offset[:, None]\n        + output_feat_stride * feat_offset[None, :]\n    )\n\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(\n        tl.float32\n    )\n    mean = tl.sum(input, axis=1) / feat_dim\n    diff = tl.where(feat_mask[None, :], input - mean[:, None], 0)\n    inv_std = tl.rsqrt(tl.sum(diff * diff, axis=1) / feat_dim + eps)\n\n    if save_stats:\n        tl.store(mean_pointer + batch_offset, mean, mask=batch_mask)\n        tl.store(inv_std_pointer + batch_offset, inv_std, mask=batch_mask)\n\n    output = diff * inv_std[:, None]\n    if scale_by_weight:\n        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n        output *= weight\n        if add_bias:\n            bias = tl.load(bias_pointer + feat_offset, mask=feat_mask)\n            output += bias\n\n    tl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[None, :])", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 227, 101, 228, 101, 229, 101, 230, 101, 231, 101, 232, 59, 6, 101, 233, 59, 6, 101, 234, 59, 6, 101, 235, 59, 6, 101, 236, 59, 6, 156, 59, 32, -1, 237, 172, 152, 215, 238, 172, 320, 156, 32, 239, 172, 237, 216, 235, 70, 71, 215, 320, 101, 235, 156, 32, 240, 172, 71, 215, 320, 101, 236, 156, 32, 241, 172, 239, 1, 225, 32, 242, 172, 240, 1, 226, 32, 219, 153, 227, 216, 239, 203, 59, 101, 178, 27, 70, 228, 216, 240, 203, 178, 101, 59, 27, 32, 224, 153, 229, 216, 239, 203, 59, 101, 178, 27, 70, 230, 216, 240, 203, 178, 101, 59, 27, 32, 243, 172, 53, 215, 219, 101, 244, 172, 241, 203, 59, 101, 178, 27, 154, 242, 203, 178, 101, 59, 27, 156, 77, 245, 215, 134, 156, 32, 246, 172, 196, 215, 243, 101, 238, 172, 321, 156, 42, 226, 32, 247, 172, 181, 215, 242, 203, 178, 101, 59, 27, 101, 243, 4, 246, 203, 59, 101, 178, 27, 101, 320, 156, 32, 248, 172, 110, 215, 196, 215, 247, 216, 247, 101, 238, 172, 321, 156, 42, 226, 70, 231, 156, 32, 162, 234, 59, 32, 10, 215, 222, 70, 239, 101, 246, 101, 244, 172, 241, 156, 32, 10, 215, 223, 70, 239, 101, 248, 101, 244, 172, 241, 156, 32, 165, 32, 249, 172, 247, 216, 248, 203, 59, 101, 178, 27, 32, 162, 232, 59, 32, 250, 172, 53, 215, 220, 70, 240, 101, 244, 172, 242, 156, 32, 249, 24, 250, 32, 162, 233, 59, 32, 251, 172, 53, 215, 221, 70, 240, 101, 244, 172, 242, 156, 32, 249, 153, 251, 32, 165, 32, 165, 32, 10, 215, 224, 101, 249, 101, 244, 172, 241, 203, 59, 101, 178, 27, 154, 242, 203, 178, 101, 59, 27, 156, 32, 3, 32]}, {"code": "def layer_norm_backward_kernel(\n    output_grad_pointer,\n    input_pointer,\n    mean_pointer,\n    inv_std_pointer,\n    weight_pointer,\n    input_grad_pointer,\n    weight_grad_pointer,\n    bias_grad_pointer,\n    batch_dim,\n    feat_dim,\n    output_grad_batch_stride,\n    output_grad_feat_stride,\n    input_batch_stride,\n    input_feat_stride,\n    input_grad_batch_stride,\n    input_grad_feat_stride,\n    weight_grad_batch_stride,\n    weight_grad_feat_stride,\n    bias_grad_batch_stride,\n    bias_grad_feat_stride,\n    scale_by_weight: tl.constexpr,\n    add_bias: tl.constexpr,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEAT: tl.constexpr,\n):\n\n    batch_pid = tl.program_id(axis=0)\n\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n\n    output_grad_pointer += (\n        output_grad_batch_stride * batch_offset[:, None]\n        + output_grad_feat_stride * feat_offset[None, :]\n    )\n    input_pointer += (\n        input_batch_stride * batch_offset[:, None]\n        + input_feat_stride * feat_offset[None, :]\n    )\n    input_grad_pointer += (\n        input_grad_batch_stride * batch_offset[:, None]\n        + input_grad_feat_stride * feat_offset[None, :]\n    )\n\n    output_grad = tl.load(\n        output_grad_pointer, mask=batch_mask[:, None] & feat_mask[None, :]\n    ).to(tl.float32)\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(\n        tl.float32\n    )\n    mean = tl.load(mean_pointer + batch_offset, mask=batch_mask)\n    inv_std = tl.load(inv_std_pointer + batch_offset, mask=batch_mask)\n    pre_lin = (input - mean[:, None]) * inv_std[:, None]\n\n    if scale_by_weight:\n        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n        weight_output_grad_prod = weight * output_grad\n\n    else:\n        weight_output_grad_prod = output_grad\n\n    term1 = tl.sum(pre_lin * weight_output_grad_prod, axis=1) / feat_dim\n    term1 = pre_lin * term1[:, None]\n    term2 = tl.sum(weight_output_grad_prod, axis=1) / feat_dim\n    input_grad = inv_std[:, None] * (weight_output_grad_prod - (term1 + term2[:, None]))\n\n    tl.store(\n        input_grad_pointer, input_grad, mask=batch_mask[:, None] & feat_mask[None, :]\n    )\n\n    if scale_by_weight:\n        weight_grad_pointer += (\n            weight_grad_batch_stride * batch_pid + weight_grad_feat_stride * feat_offset\n        )\n        tl.store(\n            weight_grad_pointer, tl.sum(output_grad * pre_lin, axis=0), mask=feat_mask\n        )\n\n        if add_bias:\n            bias_grad_pointer += (\n                bias_grad_batch_stride * batch_pid + bias_grad_feat_stride * feat_offset\n            )\n            tl.store(bias_grad_pointer, tl.sum(output_grad, axis=0), mask=feat_mask)", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 227, 101, 228, 101, 229, 101, 230, 101, 231, 101, 232, 101, 233, 101, 234, 101, 235, 101, 236, 101, 237, 101, 238, 101, 239, 60, 6, 101, 240, 60, 6, 101, 241, 60, 6, 101, 242, 60, 6, 156, 60, 32, -1, 243, 172, 152, 215, 244, 172, 320, 156, 32, 245, 172, 243, 216, 241, 70, 71, 215, 320, 101, 241, 156, 32, 246, 172, 71, 215, 320, 101, 242, 156, 32, 247, 172, 245, 1, 227, 32, 248, 172, 246, 1, 228, 32, 219, 153, 229, 216, 245, 203, 60, 101, 178, 27, 70, 230, 216, 246, 203, 178, 101, 60, 27, 32, 220, 153, 231, 216, 245, 203, 60, 101, 178, 27, 70, 232, 216, 246, 203, 178, 101, 60, 27, 32, 224, 153, 233, 216, 245, 203, 60, 101, 178, 27, 70, 234, 216, 246, 203, 178, 101, 60, 27, 32, 249, 172, 55, 215, 219, 101, 250, 172, 247, 203, 60, 101, 178, 27, 154, 248, 203, 178, 101, 60, 27, 156, 77, 251, 215, 134, 156, 32, 252, 172, 55, 215, 220, 101, 250, 172, 247, 203, 60, 101, 178, 27, 154, 248, 203, 178, 101, 60, 27, 156, 77, 251, 215, 134, 156, 32, 253, 172, 55, 215, 221, 70, 245, 101, 250, 172, 247, 156, 32, 254, 172, 55, 215, 222, 70, 245, 101, 250, 172, 247, 156, 32, 255, 172, 215, 252, 4, 253, 203, 60, 101, 178, 27, 156, 216, 254, 203, 60, 101, 178, 27, 32, 162, 239, 60, 32, 256, 172, 55, 215, 223, 70, 246, 101, 250, 172, 248, 156, 32, 257, 172, 256, 216, 249, 32, 165, 32, 30, 60, 32, 257, 172, 249, 32, 54, 32, 258, 172, 196, 215, 255, 216, 257, 101, 244, 172, 321, 156, 42, 228, 32, 258, 172, 255, 216, 258, 203, 60, 101, 178, 27, 32, 259, 172, 196, 215, 257, 101, 244, 172, 321, 156, 42, 228, 32, 260, 172, 254, 203, 60, 101, 178, 27, 216, 215, 257, 4, 215, 258, 70, 259, 203, 60, 101, 178, 27, 156, 156, 32, 10, 215, 224, 101, 260, 101, 250, 172, 247, 203, 60, 101, 178, 27, 154, 248, 203, 178, 101, 60, 27, 156, 32, 162, 239, 60, 32, 225, 153, 235, 216, 243, 70, 236, 216, 246, 32, 10, 215, 225, 101, 196, 215, 249, 216, 255, 101, 244, 172, 320, 156, 101, 250, 172, 248, 156, 32, 162, 240, 60, 32, 226, 153, 237, 216, 243, 70, 238, 216, 246, 32, 10, 215, 226, 101, 196, 215, 249, 101, 244, 172, 320, 156, 101, 250, 172, 248, 156, 32, 165, 32, 165, 32, 3, 32]}, {"code": "def linear_forward_kernel(\n    input_pointer,\n    weight_pointer,\n    bias_pointer,\n    pre_act_pointer,\n    output_pointer,\n    batch_dim,\n    in_feat_dim,\n    out_feat_dim,\n    input_batch_stride,\n    input_in_feat_stride,\n    weight_in_feat_stride,\n    weight_out_feat_stride,\n    pre_act_batch_stride,\n    pre_act_out_feat_stride,\n    output_batch_stride,\n    output_out_feat_stride,\n    param,\n    add_bias: tl.constexpr,\n    act_func: tl.constexpr,\n    save_pre_act: tl.constexpr,\n    fp16: tl.constexpr,\n    tf32: tl.constexpr,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_IN_FEAT: tl.constexpr,\n    BLOCK_SIZE_OUT_FEAT: tl.constexpr,\n    GROUP_SIZE_BATCH: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    n_batch_pids = tl.cdiv(batch_dim, BLOCK_SIZE_BATCH)\n    n_out_feat_pids = tl.cdiv(out_feat_dim, BLOCK_SIZE_OUT_FEAT)\n    pids_per_group = GROUP_SIZE_BATCH * n_out_feat_pids\n    group_id = pid // pids_per_group\n    first_batch_pid = group_id * GROUP_SIZE_BATCH\n    GROUP_SIZE_BATCH = min(n_batch_pids - first_batch_pid, GROUP_SIZE_BATCH)\n    batch_pid = first_batch_pid + (pid % GROUP_SIZE_BATCH)\n    out_feat_pid = (pid % pids_per_group) // GROUP_SIZE_BATCH\n\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    out_feat_offset = out_feat_pid * BLOCK_SIZE_OUT_FEAT + tl.arange(\n        0, BLOCK_SIZE_OUT_FEAT\n    )\n\n    batch_mask = batch_offset < batch_dim\n    out_feat_mask = out_feat_offset < out_feat_dim\n\n    input_pointer += input_batch_stride * batch_offset[:, None]\n    weight_pointer += weight_out_feat_stride * out_feat_offset[None, :]\n\n    accum = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_OUT_FEAT), dtype=tl.float32)\n\n    for block_ind in range(0, tl.cdiv(in_feat_dim, BLOCK_SIZE_IN_FEAT)):\n        in_feat_offset = block_ind * BLOCK_SIZE_IN_FEAT + tl.arange(\n            0, BLOCK_SIZE_IN_FEAT\n        )\n        in_feat_mask = in_feat_offset < in_feat_dim\n\n        curr_input_pointer = (\n            input_pointer + input_in_feat_stride * in_feat_offset[None, :]\n        )\n        curr_weight_pointer = (\n            weight_pointer + weight_in_feat_stride * in_feat_offset[:, None]\n        )\n\n        input_block = tl.load(\n            curr_input_pointer, mask=batch_mask[:, None] & in_feat_mask[None, :]\n        )\n        weight_block = tl.load(\n            curr_weight_pointer, mask=out_feat_mask[None, :] & in_feat_mask[:, None]\n        )\n\n        if fp16:\n            input_block = input_block.to(tl.float16)\n            weight_block = weight_block.to(tl.float16)\n\n        accum += tl.dot(input_block, weight_block, allow_tf32=tf32)\n\n    if add_bias:\n        bias = tl.load(bias_pointer + out_feat_offset, mask=out_feat_mask)\n\n        if fp16:\n            bias = bias.to(tl.float16)\n\n        accum += bias[None, :]\n\n    if act_func is not None:\n        if save_pre_act:\n            pre_act_pointer += (\n                pre_act_batch_stride * batch_offset[:, None]\n                + pre_act_out_feat_stride * out_feat_offset[None, :]\n            )\n            tl.store(\n                pre_act_pointer,\n                accum,\n                mask=batch_mask[:, None] & out_feat_mask[None, :],\n            )\n\n        accum = apply_act_func(accum, None, None, None, param, act_func, False)\n\n    output_pointer += (\n        output_batch_stride * batch_offset[:, None]\n        + output_out_feat_stride * out_feat_offset[None, :]\n    )\n    tl.store(output_pointer, accum, mask=batch_mask[:, None] & out_feat_mask[None, :])", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 227, 101, 228, 101, 229, 101, 230, 101, 231, 101, 232, 101, 233, 101, 234, 101, 235, 101, 236, 59, 6, 101, 237, 59, 6, 101, 238, 59, 6, 101, 239, 59, 6, 101, 240, 59, 6, 101, 241, 59, 6, 101, 242, 59, 6, 101, 243, 59, 6, 101, 244, 59, 6, 156, 59, 32, -1, 245, 172, 152, 215, 246, 172, 320, 156, 32, 247, 172, 61, 215, 224, 101, 241, 156, 32, 248, 172, 61, 215, 226, 101, 243, 156, 32, 249, 172, 244, 216, 248, 32, 250, 172, 245, 47, 249, 32, 251, 172, 250, 216, 244, 32, 244, 172, 39, 215, 247, 4, 251, 101, 244, 156, 32, 252, 172, 251, 70, 245, 199, 244, 32, 253, 172, 245, 199, 249, 47, 244, 32, 254, 172, 252, 216, 241, 70, 71, 215, 320, 101, 241, 156, 32, 255, 172, 253, 216, 243, 70, 71, 215, 320, 101, 243, 156, 32, 256, 172, 254, 1, 224, 32, 257, 172, 255, 1, 226, 32, 219, 153, 227, 216, 254, 203, 59, 101, 178, 27, 32, 220, 153, 230, 216, 255, 203, 178, 101, 59, 27, 32, 258, 172, 157, 215, 215, 241, 101, 243, 156, 101, 85, 172, 134, 156, 32, 125, 259, 143, 5, 215, 320, 101, 61, 215, 225, 101, 242, 156, 156, 59, 32, 260, 172, 259, 216, 242, 70, 71, 215, 320, 101, 242, 156, 32, 261, 172, 260, 1, 225, 32, 262, 172, 219, 70, 228, 216, 260, 203, 178, 101, 59, 27, 32, 263, 172, 220, 70, 229, 216, 260, 203, 59, 101, 178, 27, 32, 264, 172, 53, 215, 262, 101, 265, 172, 256, 203, 59, 101, 178, 27, 154, 261, 203, 178, 101, 59, 27, 156, 32, 266, 172, 53, 215, 263, 101, 265, 172, 257, 203, 178, 101, 59, 27, 154, 261, 203, 59, 101, 178, 27, 156, 32, 162, 239, 59, 32, 264, 172, 264, 77, 267, 215, 21, 156, 32, 266, 172, 266, 77, 267, 215, 21, 156, 32, 165, 32, 258, 153, 15, 215, 264, 101, 266, 101, 268, 172, 240, 156, 32, 73, 32, 162, 236, 59, 32, 269, 172, 53, 215, 221, 70, 255, 101, 265, 172, 257, 156, 32, 162, 239, 59, 32, 269, 172, 269, 77, 267, 215, 21, 156, 32, 165, 32, 258, 153, 269, 203, 178, 101, 59, 27, 32, 165, 32, 162, 237, 103, 60, 178, 59, 32, 162, 238, 59, 32, 222, 153, 231, 216, 254, 203, 59, 101, 178, 27, 70, 232, 216, 255, 203, 178, 101, 59, 27, 32, 10, 215, 222, 101, 258, 101, 265, 172, 256, 203, 59, 101, 178, 27, 154, 257, 203, 178, 101, 59, 27, 156, 32, 165, 32, 258, 172, 270, 215, 258, 101, 178, 101, 178, 101, 178, 101, 235, 101, 237, 101, 57, 156, 32, 165, 32, 223, 153, 233, 216, 254, 203, 59, 101, 178, 27, 70, 234, 216, 255, 203, 178, 101, 59, 27, 32, 10, 215, 223, 101, 258, 101, 265, 172, 256, 203, 59, 101, 178, 27, 154, 257, 203, 178, 101, 59, 27, 156, 32, 3, 32]}, {"code": "def nll_loss_forward_kernel(\n    input_pointer,\n    target_pointer,\n    weight_pointer,\n    sum_weights_pointer,\n    output_pointer,\n    batch_dim,\n    spatial_dim,\n    input_batch_stride,\n    input_feat_stride,\n    input_spatial_stride,\n    target_batch_stride,\n    target_spatial_stride,\n    output_batch_stride,\n    output_spatial_stride,\n    reduction: tl.constexpr,\n    weighted: tl.constexpr,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_SPATIAL: tl.constexpr,\n):\n\n    batch_pid = tl.program_id(axis=0)\n\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    spatial_offset = tl.arange(0, BLOCK_SIZE_SPATIAL)\n\n    batch_mask = batch_offset < batch_dim\n    spatial_mask = spatial_offset < spatial_dim\n\n    target_pointer += (\n        target_batch_stride * batch_offset[:, None]\n        + target_spatial_stride * spatial_offset[None, :]\n    )\n    target = tl.load(target_pointer, mask=batch_mask[:, None] & spatial_mask[None, :])\n\n    input_pointer += (\n        input_feat_stride * target\n        + input_batch_stride * batch_offset[:, None]\n        + input_spatial_stride * spatial_offset[None, :]\n    )\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]).to(\n        tl.float32\n    )\n\n    output = -input\n    if weighted:\n        weight = tl.load(\n            weight_pointer + target, mask=batch_mask[:, None] & spatial_mask[None, :]\n        ).to(tl.float32)\n        output *= weight\n\n    if reduction == \"none\":\n        output_pointer += (\n            output_batch_stride * batch_offset[:, None]\n            + output_spatial_stride * spatial_offset[None, :]\n        )\n        tl.store(\n            output_pointer, output, mask=batch_mask[:, None] & spatial_mask[None, :]\n        )\n\n    elif reduction == \"mean\":\n        if weighted:\n            tl.store(sum_weights_pointer + batch_pid, tl.sum(weight))\n            tl.store(output_pointer + batch_pid, tl.sum(output))\n\n        else:\n            tl.store(\n                output_pointer + batch_pid, tl.sum(output) / (batch_dim * spatial_dim)\n            )\n\n    elif reduction == \"sum\":\n        tl.store(output_pointer + batch_pid, tl.sum(output))", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 227, 101, 228, 101, 229, 101, 230, 101, 231, 101, 232, 101, 233, 60, 6, 101, 234, 60, 6, 101, 235, 60, 6, 101, 236, 60, 6, 156, 60, 32, -1, 237, 172, 152, 215, 238, 172, 320, 156, 32, 239, 172, 237, 216, 235, 70, 71, 215, 320, 101, 235, 156, 32, 240, 172, 71, 215, 320, 101, 236, 156, 32, 241, 172, 239, 1, 224, 32, 242, 172, 240, 1, 225, 32, 220, 153, 229, 216, 239, 203, 60, 101, 178, 27, 70, 230, 216, 240, 203, 178, 101, 60, 27, 32, 243, 172, 55, 215, 220, 101, 244, 172, 241, 203, 60, 101, 178, 27, 154, 242, 203, 178, 101, 60, 27, 156, 32, 219, 153, 227, 216, 243, 70, 226, 216, 239, 203, 60, 101, 178, 27, 70, 228, 216, 240, 203, 178, 101, 60, 27, 32, 245, 172, 55, 215, 219, 101, 244, 172, 241, 203, 60, 101, 178, 27, 154, 242, 203, 178, 101, 60, 27, 156, 77, 246, 215, 134, 156, 32, 247, 172, 4, 245, 32, 162, 234, 60, 32, 248, 172, 55, 215, 221, 70, 243, 101, 244, 172, 241, 203, 60, 101, 178, 27, 154, 242, 203, 178, 101, 60, 27, 156, 77, 246, 215, 134, 156, 32, 247, 24, 248, 32, 165, 32, 162, 233, 72, 321, 60, 32, 223, 153, 231, 216, 239, 203, 60, 101, 178, 27, 70, 232, 216, 240, 203, 178, 101, 60, 27, 32, 10, 215, 223, 101, 247, 101, 244, 172, 241, 203, 60, 101, 178, 27, 154, 242, 203, 178, 101, 60, 27, 156, 32, 63, 32, 37, 233, 72, 322, 60, 32, 162, 234, 60, 32, 10, 215, 222, 70, 237, 101, 196, 215, 248, 156, 156, 32, 10, 215, 223, 70, 237, 101, 196, 215, 247, 156, 156, 32, 165, 32, 30, 60, 32, 10, 215, 223, 70, 237, 101, 196, 215, 247, 156, 42, 215, 224, 216, 225, 156, 156, 32, 54, 32, 63, 32, 37, 233, 72, 323, 60, 32, 10, 215, 223, 70, 237, 101, 196, 215, 247, 156, 156, 32, 165, 32, 3, 32]}, {"code": "def nll_loss_backward_kernel(\n    output_grad_pointer,\n    target_pointer,\n    weight_pointer,\n    sum_weights_pointer,\n    input_grad_pointer,\n    batch_dim,\n    spatial_dim,\n    output_grad_batch_stride,\n    output_grad_feat_stride,\n    target_batch_stride,\n    target_spatial_stride,\n    input_grad_batch_stride,\n    input_grad_feat_stride,\n    input_grad_spatial_stride,\n    reduction: tl.constexpr,\n    weighted: tl.constexpr,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_SPATIAL: tl.constexpr,\n):\n\n    batch_pid = tl.program_id(axis=0)\n\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    spatial_offset = tl.arange(0, BLOCK_SIZE_SPATIAL)\n\n    batch_mask = batch_offset < batch_dim\n    spatial_mask = spatial_offset < spatial_dim\n\n    output_grad_mask = None\n    if reduction == \"none\":\n        output_grad_pointer += (\n            output_grad_batch_stride * batch_offset[:, None]\n            + output_grad_feat_stride * spatial_offset[None, :]\n        )\n        output_grad_mask = batch_mask[:, None] & spatial_mask[None, :]\n\n    output_grad = tl.load(output_grad_pointer, mask=output_grad_mask).to(tl.float32)\n    input_grad = -output_grad\n\n    target_pointer += (\n        target_batch_stride * batch_offset[:, None]\n        + target_spatial_stride * spatial_offset[None, :]\n    )\n    target = tl.load(target_pointer, mask=batch_mask[:, None] & spatial_mask[None, :])\n\n    if weighted:\n        weight = tl.load(\n            weight_pointer + target, mask=batch_mask[:, None] & spatial_mask[None, :]\n        ).to(tl.float32)\n        input_grad *= weight\n\n        if reduction == \"mean\":\n            input_grad /= tl.load(sum_weights_pointer)\n\n    elif reduction == \"mean\":\n        input_grad /= batch_dim * spatial_dim\n\n    input_grad_pointer += (\n        input_grad_feat_stride * target\n        + input_grad_batch_stride * batch_offset[:, None]\n        + input_grad_spatial_stride * spatial_offset[None, :]\n    )\n    tl.store(\n        input_grad_pointer, input_grad, mask=batch_mask[:, None] & spatial_mask[None, :]\n    )", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 227, 101, 228, 101, 229, 101, 230, 101, 231, 101, 232, 101, 233, 59, 6, 101, 234, 59, 6, 101, 235, 59, 6, 101, 236, 59, 6, 156, 59, 32, -1, 237, 172, 152, 215, 238, 172, 320, 156, 32, 239, 172, 237, 216, 235, 70, 71, 215, 320, 101, 235, 156, 32, 240, 172, 71, 215, 320, 101, 236, 156, 32, 241, 172, 239, 1, 224, 32, 242, 172, 240, 1, 225, 32, 243, 172, 178, 32, 162, 233, 72, 321, 59, 32, 219, 153, 226, 216, 239, 203, 59, 101, 178, 27, 70, 227, 216, 240, 203, 178, 101, 59, 27, 32, 243, 172, 241, 203, 59, 101, 178, 27, 154, 242, 203, 178, 101, 59, 27, 32, 165, 32, 244, 172, 53, 215, 219, 101, 245, 172, 243, 156, 77, 246, 215, 134, 156, 32, 247, 172, 4, 244, 32, 220, 153, 228, 216, 239, 203, 59, 101, 178, 27, 70, 229, 216, 240, 203, 178, 101, 59, 27, 32, 248, 172, 53, 215, 220, 101, 245, 172, 241, 203, 59, 101, 178, 27, 154, 242, 203, 178, 101, 59, 27, 156, 32, 162, 234, 59, 32, 249, 172, 53, 215, 221, 70, 248, 101, 245, 172, 241, 203, 59, 101, 178, 27, 154, 242, 203, 178, 101, 59, 27, 156, 77, 246, 215, 134, 156, 32, 247, 24, 249, 32, 162, 233, 72, 322, 59, 32, 247, 8, 53, 215, 222, 156, 32, 165, 32, 62, 32, 37, 233, 72, 322, 59, 32, 247, 8, 224, 216, 225, 32, 165, 32, 223, 153, 231, 216, 248, 70, 230, 216, 239, 203, 59, 101, 178, 27, 70, 232, 216, 240, 203, 178, 101, 59, 27, 32, 10, 215, 223, 101, 247, 101, 245, 172, 241, 203, 59, 101, 178, 27, 154, 242, 203, 178, 101, 59, 27, 156, 32, 3, 32]}, {"code": "def p_loss_forward_kernel(\n    input_pointer,\n    target_pointer,\n    output_pointer,\n    param,\n    size,\n    p_loss: tl.constexpr,\n    reduction: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n\n    input = tl.load(input_pointer + offset, mask=mask).to(tl.float32)\n    target = tl.load(target_pointer + offset, mask=mask).to(tl.float32)\n    diff = input - target\n\n    if p_loss == 0:\n        error = tl.where(\n            diff < param, 0.5 * diff * diff / param, tl.abs(diff) - 0.5 * param\n        )\n\n    elif p_loss == 1:\n        error = tl.abs(diff)\n\n    elif p_loss == 2:\n        error = diff * diff\n\n    elif p_loss == 3:\n        error = tl.where(\n            diff < param, 0.5 * diff * diff, param * (tl.abs(diff) - 0.5 * param)\n        )\n\n    if reduction == \"none\":\n        tl.store(output_pointer + offset, error, mask=mask)\n\n    elif reduction == \"mean\":\n        tl.store(output_pointer + pid, tl.sum(error) / size)\n\n    elif reduction == \"sum\":\n        tl.store(output_pointer + pid, tl.sum(error))", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 190, 101, 223, 60, 6, 101, 224, 60, 6, 101, 225, 60, 6, 156, 60, 32, -1, 226, 172, 152, 215, 227, 172, 320, 156, 32, 228, 172, 226, 216, 225, 70, 71, 215, 320, 101, 225, 156, 32, 229, 172, 228, 1, 190, 32, 230, 172, 55, 215, 219, 70, 228, 101, 229, 172, 229, 156, 77, 231, 215, 134, 156, 32, 232, 172, 55, 215, 220, 70, 228, 101, 229, 172, 229, 156, 77, 231, 215, 134, 156, 32, 233, 172, 230, 4, 232, 32, 162, 223, 72, 320, 60, 32, 234, 172, 181, 215, 233, 1, 222, 101, 321, 216, 233, 216, 233, 42, 222, 101, 93, 215, 233, 156, 4, 321, 216, 222, 156, 32, 63, 32, 37, 223, 72, 322, 60, 32, 234, 172, 93, 215, 233, 156, 32, 63, 32, 37, 223, 72, 323, 60, 32, 234, 172, 233, 216, 233, 32, 63, 32, 37, 223, 72, 324, 60, 32, 234, 172, 181, 215, 233, 1, 222, 101, 321, 216, 233, 216, 233, 101, 222, 216, 215, 93, 215, 233, 156, 4, 321, 216, 222, 156, 156, 32, 165, 32, 162, 224, 72, 325, 60, 32, 10, 215, 221, 70, 228, 101, 234, 101, 229, 172, 229, 156, 32, 63, 32, 37, 224, 72, 326, 60, 32, 10, 215, 221, 70, 226, 101, 196, 215, 234, 156, 42, 190, 156, 32, 63, 32, 37, 224, 72, 327, 60, 32, 10, 215, 221, 70, 226, 101, 196, 215, 234, 156, 156, 32, 165, 32, 3, 32]}, {"code": "def p_loss_backward_kernel(\n    output_grad_pointer,\n    input_pointer,\n    target_pointer,\n    input_grad_pointer,\n    target_grad_pointer,\n    param,\n    size,\n    p_loss: tl.constexpr,\n    reduction: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n\n    output_grad_mask = None\n    if reduction == \"none\":\n        output_grad_pointer += offset\n        output_grad_mask = mask\n\n    input = tl.load(input_pointer + offset, mask=mask).to(tl.float32)\n    target = tl.load(target_pointer + offset, mask=mask).to(tl.float32)\n    diff = input - target\n    output_grad = tl.load(output_grad_pointer, mask=output_grad_mask).to(tl.float32)\n\n    if p_loss == 0:\n        input_grad = tl.where(diff < param, diff / param, tl.where(0 <= diff, 1, -1))\n\n    elif p_loss == 1:\n        input_grad = tl.where(0 <= diff, 1, -1)\n\n    elif p_loss == 2:\n        input_grad = 2 * diff\n\n    elif p_loss == 3:\n        input_grad = tl.where(diff < param, diff, param * tl.where(0 <= diff, 1, -1))\n\n    if reduction == \"mean\":\n        input_grad /= size\n\n    input_grad *= output_grad\n    tl.store(input_grad_pointer + offset, input_grad, mask=mask)\n    tl.store(target_grad_pointer + offset, -input_grad, mask=mask)", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 190, 101, 225, 59, 6, 101, 226, 59, 6, 101, 227, 59, 6, 156, 59, 32, -1, 228, 172, 152, 215, 229, 172, 320, 156, 32, 230, 172, 228, 216, 227, 70, 71, 215, 320, 101, 227, 156, 32, 231, 172, 230, 1, 190, 32, 232, 172, 178, 32, 162, 226, 72, 321, 59, 32, 219, 153, 230, 32, 232, 172, 231, 32, 165, 32, 233, 172, 53, 215, 220, 70, 230, 101, 231, 172, 231, 156, 77, 234, 215, 134, 156, 32, 235, 172, 53, 215, 221, 70, 230, 101, 231, 172, 231, 156, 77, 234, 215, 134, 156, 32, 236, 172, 233, 4, 235, 32, 237, 172, 53, 215, 219, 101, 231, 172, 232, 156, 77, 234, 215, 134, 156, 32, 162, 225, 72, 320, 59, 32, 238, 172, 181, 215, 236, 1, 224, 101, 236, 42, 224, 101, 181, 215, 320, 193, 236, 101, 322, 101, 4, 322, 156, 156, 32, 62, 32, 37, 225, 72, 322, 59, 32, 238, 172, 181, 215, 320, 193, 236, 101, 322, 101, 4, 322, 156, 32, 62, 32, 37, 225, 72, 323, 59, 32, 238, 172, 323, 216, 236, 32, 62, 32, 37, 225, 72, 324, 59, 32, 238, 172, 181, 215, 236, 1, 224, 101, 236, 101, 224, 216, 181, 215, 320, 193, 236, 101, 322, 101, 4, 322, 156, 156, 32, 165, 32, 162, 226, 72, 325, 59, 32, 238, 8, 190, 32, 165, 32, 238, 24, 237, 32, 10, 215, 222, 70, 230, 101, 238, 101, 231, 172, 231, 156, 32, 10, 215, 223, 70, 230, 101, 4, 238, 101, 231, 172, 231, 156, 32, 3, 32]}, {"code": "def rms_norm_forward_kernel(\n    input_pointer,\n    weight_pointer,\n    inv_rms_pointer,\n    output_pointer,\n    batch_dim,\n    feat_dim,\n    input_batch_stride,\n    input_feat_stride,\n    output_batch_stride,\n    output_feat_stride,\n    eps,\n    scale_by_weight: tl.constexpr,\n    save_stats: tl.constexpr,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEAT: tl.constexpr,\n):\n\n    batch_pid = tl.program_id(axis=0)\n\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n\n    input_pointer += (\n        input_batch_stride * batch_offset[:, None]\n        + input_feat_stride * feat_offset[None, :]\n    )\n    output_pointer += (\n        output_batch_stride * batch_offset[:, None]\n        + output_feat_stride * feat_offset[None, :]\n    )\n\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(\n        tl.float32\n    )\n    inv_rms = tl.rsqrt(tl.sum(input * input, axis=1) / feat_dim + eps)\n    output = input * inv_rms[:, None]\n\n    if save_stats:\n        tl.store(inv_rms_pointer + batch_offset, inv_rms, mask=batch_mask)\n\n    if scale_by_weight:\n        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n        output *= weight\n\n    tl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[None, :])", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 227, 101, 228, 101, 229, 101, 230, 60, 6, 101, 231, 60, 6, 101, 232, 60, 6, 101, 233, 60, 6, 156, 60, 32, -1, 234, 172, 152, 215, 235, 172, 320, 156, 32, 236, 172, 234, 216, 232, 70, 71, 215, 320, 101, 232, 156, 32, 237, 172, 71, 215, 320, 101, 233, 156, 32, 238, 172, 236, 1, 223, 32, 239, 172, 237, 1, 224, 32, 219, 153, 225, 216, 236, 203, 60, 101, 178, 27, 70, 226, 216, 237, 203, 178, 101, 60, 27, 32, 222, 153, 227, 216, 236, 203, 60, 101, 178, 27, 70, 228, 216, 237, 203, 178, 101, 60, 27, 32, 240, 172, 55, 215, 219, 101, 241, 172, 238, 203, 60, 101, 178, 27, 154, 239, 203, 178, 101, 60, 27, 156, 77, 242, 215, 134, 156, 32, 243, 172, 110, 215, 196, 215, 240, 216, 240, 101, 235, 172, 321, 156, 42, 224, 70, 229, 156, 32, 244, 172, 240, 216, 243, 203, 60, 101, 178, 27, 32, 162, 231, 60, 32, 10, 215, 221, 70, 236, 101, 243, 101, 241, 172, 238, 156, 32, 165, 32, 162, 230, 60, 32, 245, 172, 55, 215, 220, 70, 237, 101, 241, 172, 239, 156, 32, 244, 24, 245, 32, 165, 32, 10, 215, 222, 101, 244, 101, 241, 172, 238, 203, 60, 101, 178, 27, 154, 239, 203, 178, 101, 60, 27, 156, 32, 3, 32]}, {"code": "def rms_norm_backward_kernel(\n    output_grad_pointer,\n    input_pointer,\n    inv_rms_pointer,\n    weight_pointer,\n    input_grad_pointer,\n    weight_grad_pointer,\n    batch_dim,\n    feat_dim,\n    output_grad_batch_stride,\n    output_grad_feat_stride,\n    input_batch_stride,\n    input_feat_stride,\n    input_grad_batch_stride,\n    input_grad_feat_stride,\n    weight_grad_batch_stride,\n    weight_grad_feat_stride,\n    scale_by_weight: tl.constexpr,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEAT: tl.constexpr,\n):\n\n    batch_pid = tl.program_id(axis=0)\n\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n\n    output_grad_pointer += (\n        output_grad_batch_stride * batch_offset[:, None]\n        + output_grad_feat_stride * feat_offset[None, :]\n    )\n    input_pointer += (\n        input_batch_stride * batch_offset[:, None]\n        + input_feat_stride * feat_offset[None, :]\n    )\n    input_grad_pointer += (\n        input_grad_batch_stride * batch_offset[:, None]\n        + input_grad_feat_stride * feat_offset[None, :]\n    )\n\n    output_grad = tl.load(\n        output_grad_pointer, mask=batch_mask[:, None] & feat_mask[None, :]\n    ).to(tl.float32)\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(\n        tl.float32\n    )\n    inv_rms = tl.load(inv_rms_pointer + batch_offset, mask=batch_mask)\n    pre_lin = input * inv_rms[:, None]\n\n    if scale_by_weight:\n        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n        weight_output_grad_prod = weight * output_grad\n\n    else:\n        weight_output_grad_prod = output_grad\n\n    term1 = input * tl.sum(input * weight_output_grad_prod, axis=1)\n    term2 = inv_rms[:, None] * inv_rms[:, None]\n    input_grad = inv_rms[:, None] * (weight_output_grad_prod - term1 * term2 / feat_dim)\n\n    tl.store(\n        input_grad_pointer, input_grad, mask=batch_mask[:, None] & feat_mask[None, :]\n    )\n\n    if scale_by_weight:\n        weight_grad_pointer += (\n            weight_grad_batch_stride * batch_pid + weight_grad_feat_stride * feat_offset\n        )\n        tl.store(\n            weight_grad_pointer, tl.sum(output_grad * pre_lin, axis=0), mask=feat_mask\n        )", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 227, 101, 228, 101, 229, 101, 230, 101, 231, 101, 232, 101, 233, 101, 234, 101, 235, 59, 6, 101, 236, 59, 6, 101, 237, 59, 6, 156, 59, 32, -1, 238, 172, 152, 215, 239, 172, 320, 156, 32, 240, 172, 238, 216, 236, 70, 71, 215, 320, 101, 236, 156, 32, 241, 172, 71, 215, 320, 101, 237, 156, 32, 242, 172, 240, 1, 225, 32, 243, 172, 241, 1, 226, 32, 219, 153, 227, 216, 240, 203, 59, 101, 178, 27, 70, 228, 216, 241, 203, 178, 101, 59, 27, 32, 220, 153, 229, 216, 240, 203, 59, 101, 178, 27, 70, 230, 216, 241, 203, 178, 101, 59, 27, 32, 223, 153, 231, 216, 240, 203, 59, 101, 178, 27, 70, 232, 216, 241, 203, 178, 101, 59, 27, 32, 244, 172, 53, 215, 219, 101, 245, 172, 242, 203, 59, 101, 178, 27, 154, 243, 203, 178, 101, 59, 27, 156, 77, 246, 215, 134, 156, 32, 247, 172, 53, 215, 220, 101, 245, 172, 242, 203, 59, 101, 178, 27, 154, 243, 203, 178, 101, 59, 27, 156, 77, 246, 215, 134, 156, 32, 248, 172, 53, 215, 221, 70, 240, 101, 245, 172, 242, 156, 32, 249, 172, 247, 216, 248, 203, 59, 101, 178, 27, 32, 162, 235, 59, 32, 250, 172, 53, 215, 222, 70, 241, 101, 245, 172, 243, 156, 32, 251, 172, 250, 216, 244, 32, 165, 32, 30, 59, 32, 251, 172, 244, 32, 54, 32, 252, 172, 247, 216, 196, 215, 247, 216, 251, 101, 239, 172, 321, 156, 32, 253, 172, 248, 203, 59, 101, 178, 27, 216, 248, 203, 59, 101, 178, 27, 32, 254, 172, 248, 203, 59, 101, 178, 27, 216, 215, 251, 4, 252, 216, 253, 42, 226, 156, 32, 10, 215, 223, 101, 254, 101, 245, 172, 242, 203, 59, 101, 178, 27, 154, 243, 203, 178, 101, 59, 27, 156, 32, 162, 235, 59, 32, 224, 153, 233, 216, 238, 70, 234, 216, 241, 32, 10, 215, 224, 101, 196, 215, 244, 216, 249, 101, 239, 172, 320, 156, 101, 245, 172, 243, 156, 32, 165, 32, 3, 32]}, {"code": "def softmax_forward_kernel(\n    input_pointer,\n    output_pointer,\n    batch_dim,\n    feat_dim,\n    input_batch_stride,\n    input_feat_stride,\n    output_batch_stride,\n    output_feat_stride,\n    log: tl.constexpr,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEAT: tl.constexpr,\n):\n\n    batch_pid = tl.program_id(axis=0)\n\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n\n    input_pointer += (\n        input_batch_stride * batch_offset[:, None]\n        + input_feat_stride * feat_offset[None, :]\n    )\n    output_pointer += (\n        output_batch_stride * batch_offset[:, None]\n        + output_feat_stride * feat_offset[None, :]\n    )\n\n    input = tl.load(\n        input_pointer,\n        mask=batch_mask[:, None] & feat_mask[None, :],\n        other=-float(\"inf\"),\n    ).to(tl.float32)\n    input -= tl.max(input, axis=1)[:, None]\n    numerator = tl.exp(input)\n    denominator = tl.sum(numerator, axis=1)[:, None]\n\n    if log:\n        output = input - tl.log(denominator)\n\n    else:\n        output = numerator / denominator\n\n    tl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[None, :])", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 11, 60, 6, 101, 227, 60, 6, 101, 228, 60, 6, 156, 60, 32, -1, 229, 172, 152, 215, 230, 172, 320, 156, 32, 231, 172, 229, 216, 227, 70, 71, 215, 320, 101, 227, 156, 32, 232, 172, 71, 215, 320, 101, 228, 156, 32, 233, 172, 231, 1, 221, 32, 234, 172, 232, 1, 222, 32, 219, 153, 223, 216, 231, 203, 60, 101, 178, 27, 70, 224, 216, 232, 203, 178, 101, 60, 27, 32, 220, 153, 225, 216, 231, 203, 60, 101, 178, 27, 70, 226, 216, 232, 203, 178, 101, 60, 27, 32, 235, 172, 55, 215, 219, 101, 236, 172, 233, 203, 60, 101, 178, 27, 154, 234, 203, 178, 101, 60, 27, 101, 237, 172, 4, 238, 215, 321, 156, 156, 77, 239, 215, 134, 156, 32, 235, 2, 12, 215, 235, 101, 230, 172, 322, 156, 203, 60, 101, 178, 27, 32, 240, 172, 100, 215, 235, 156, 32, 241, 172, 196, 215, 240, 101, 230, 172, 322, 156, 203, 60, 101, 178, 27, 32, 162, 11, 60, 32, 242, 172, 235, 4, 49, 215, 241, 156, 32, 165, 32, 30, 60, 32, 242, 172, 240, 42, 241, 32, 54, 32, 10, 215, 220, 101, 242, 101, 236, 172, 233, 203, 60, 101, 178, 27, 154, 234, 203, 178, 101, 60, 27, 156, 32, 3, 32]}, {"code": "def softmax_backward_kernel(\n    output_grad_pointer,\n    output_pointer,\n    input_grad_pointer,\n    batch_dim,\n    feat_dim,\n    output_grad_batch_stride,\n    output_grad_feat_stride,\n    output_batch_stride,\n    output_feat_stride,\n    input_grad_batch_stride,\n    input_grad_feat_stride,\n    log: tl.constexpr,\n    BLOCK_SIZE_BATCH: tl.constexpr,\n    BLOCK_SIZE_FEAT: tl.constexpr,\n):\n\n    batch_pid = tl.program_id(axis=0)\n\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n\n    output_grad_pointer += (\n        output_grad_batch_stride * batch_offset[:, None]\n        + output_grad_feat_stride * feat_offset[None, :]\n    )\n    output_pointer += (\n        output_batch_stride * batch_offset[:, None]\n        + output_feat_stride * feat_offset[None, :]\n    )\n    input_grad_pointer += (\n        input_grad_batch_stride * batch_offset[:, None]\n        + input_grad_feat_stride * feat_offset[None, :]\n    )\n\n    output_grad = tl.load(\n        output_grad_pointer, mask=batch_mask[:, None] & feat_mask[None, :]\n    ).to(tl.float32)\n    output = tl.load(output_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(\n        tl.float32\n    )\n\n    if log:\n        input_grad = output_grad - tl.exp(output) * tl.sum(output_grad, axis=1)[:, None]\n\n    else:\n        input_grad = output * (\n            output_grad - tl.sum(output_grad * output, axis=1)[:, None]\n        )\n\n    tl.store(\n        input_grad_pointer, input_grad, mask=batch_mask[:, None] & feat_mask[None, :]\n    )", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 227, 101, 228, 101, 229, 101, 11, 59, 6, 101, 230, 59, 6, 101, 231, 59, 6, 156, 59, 32, -1, 232, 172, 152, 215, 233, 172, 320, 156, 32, 234, 172, 232, 216, 230, 70, 71, 215, 320, 101, 230, 156, 32, 235, 172, 71, 215, 320, 101, 231, 156, 32, 236, 172, 234, 1, 222, 32, 237, 172, 235, 1, 223, 32, 219, 153, 224, 216, 234, 203, 59, 101, 178, 27, 70, 225, 216, 235, 203, 178, 101, 59, 27, 32, 220, 153, 226, 216, 234, 203, 59, 101, 178, 27, 70, 227, 216, 235, 203, 178, 101, 59, 27, 32, 221, 153, 228, 216, 234, 203, 59, 101, 178, 27, 70, 229, 216, 235, 203, 178, 101, 59, 27, 32, 238, 172, 53, 215, 219, 101, 239, 172, 236, 203, 59, 101, 178, 27, 154, 237, 203, 178, 101, 59, 27, 156, 77, 240, 215, 134, 156, 32, 241, 172, 53, 215, 220, 101, 239, 172, 236, 203, 59, 101, 178, 27, 154, 237, 203, 178, 101, 59, 27, 156, 77, 240, 215, 134, 156, 32, 162, 11, 59, 32, 242, 172, 238, 4, 100, 215, 241, 156, 216, 196, 215, 238, 101, 233, 172, 321, 156, 203, 59, 101, 178, 27, 32, 165, 32, 30, 59, 32, 242, 172, 241, 216, 215, 238, 4, 196, 215, 238, 216, 241, 101, 233, 172, 321, 156, 203, 59, 101, 178, 27, 156, 32, 54, 32, 10, 215, 221, 101, 242, 101, 239, 172, 236, 203, 59, 101, 178, 27, 154, 237, 203, 178, 101, 59, 27, 156, 32, 3, 32]}, {"code": "def _paged_attn_kernel(\n    m_i_ptr,\n    l_i_ptr,\n    out_ptr,\n    q_ptr,\n    k_cache_ptr,\n    v_cache_ptr,\n    context_lens_ptr,\n    block_tables_ptr,\n    attn_scale,\n    stride_bt0,\n    stride_bt1,\n    stride_q0,\n    stride_q1,\n    stride_q2,\n    stride_kv0,\n    stride_kv1,\n    stride_kv2,\n    stride_kv3,\n    stride_o0,\n    stride_o1,\n    stride_o2,\n    stride_o3,\n    stride_o4,\n    HEAD_SIZE: tl.constexpr,\n    QUERY_GROUP_SIZE: tl.constexpr,\n    PADDED_QUERY_GROUP_SIZE: tl.constexpr,\n    NUM_KV_HEADS: tl.constexpr,\n    KV_BLOCK_SIZE: tl.constexpr,\n    PARTITION_SIZE: tl.constexpr,\n):\n    seq_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n    part_idx = tl.program_id(2)\n    max_num_partitions = tl.num_programs(2)\n\n    log2e: tl.constexpr = 1.4426950408889634\n\n    USE_PARTITIONING = PARTITION_SIZE > 0\n    context_len = tl.load(context_lens_ptr + seq_idx)\n    if USE_PARTITIONING:\n        context_start_idx = part_idx * PARTITION_SIZE\n        if context_start_idx >= context_len:\n            return\n        context_end_idx = tl.minimum(context_start_idx + PARTITION_SIZE, context_len)\n        num_blocks = tl.cdiv(context_end_idx - context_start_idx, KV_BLOCK_SIZE)\n    else:\n        num_blocks = tl.cdiv(context_len, KV_BLOCK_SIZE)\n\n    block_offset = tl.arange(0, KV_BLOCK_SIZE)\n    head_offset = tl.arange(0, HEAD_SIZE)\n    padding_group_offset = tl.arange(0, PADDED_QUERY_GROUP_SIZE)\n\n    kv_offset = (\n        kv_head_idx * stride_kv1\n        + block_offset[:, None] * stride_kv2\n        + head_offset[None, :] * stride_kv3\n    )\n\n    q_offset = (\n        seq_idx * stride_q0\n        + (kv_head_idx * QUERY_GROUP_SIZE + padding_group_offset[:, None]) * stride_q1\n        + head_offset[None, :] * stride_q2\n    )\n    group_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE\n\n    q = tl.load(q_ptr + q_offset, mask=group_mask, other=0.0)\n\n    m_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32)\n    acc = tl.zeros([PADDED_QUERY_GROUP_SIZE, HEAD_SIZE], dtype=tl.float32)\n\n    num_prev_blocks = part_idx * (PARTITION_SIZE // KV_BLOCK_SIZE)\n    for i in range(num_blocks):\n        block_idx = num_prev_blocks + i\n        block_number = tl.load(\n            block_tables_ptr + seq_idx * stride_bt0 + block_idx * stride_bt1\n        )\n\n        kv_block_offset = block_number * stride_kv0 + kv_offset\n        mask_offset = block_idx * KV_BLOCK_SIZE + block_offset\n        kv_mask = mask_offset[:, None] < context_len\n\n        k = tl.load(k_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)\n\n        if PADDED_QUERY_GROUP_SIZE == 1:\n            qk = tl.sum(q[:, None, :] * k[None, :, :], axis=2)\n        else:\n            qk = tl.dot(q, k.T, out_dtype=tl.float32)\n\n        qk *= attn_scale\n        qk = tl.where(mask_offset < context_len, qk, float(\"-inf\"))\n\n        m_i_new = tl.maximum(m_i, tl.max(qk, axis=1))\n\n        p = tl.math.exp2((qk - m_i_new[:, None]) * log2e)\n        alpha = tl.math.exp2((m_i - m_i_new) * log2e)\n        acc *= alpha[:, None]\n\n        v = tl.load(v_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)\n\n        if PADDED_QUERY_GROUP_SIZE == 1:\n            acc += tl.sum(p.T[:, :, None] * v[:, None, :], axis=0)\n        else:\n            p = p.to(v.dtype)\n            acc += tl.dot(p, v, out_dtype=tl.float32)\n\n        l_i = l_i * alpha + tl.sum(p, axis=1)\n        m_i = m_i_new\n    acc = acc / l_i[:, None]\n\n    if USE_PARTITIONING:\n        part_offset = (\n            (seq_idx * NUM_KV_HEADS + kv_head_idx)\n            * max_num_partitions\n            * QUERY_GROUP_SIZE\n            + part_idx * QUERY_GROUP_SIZE\n            + padding_group_offset\n        )\n        mask = padding_group_offset < QUERY_GROUP_SIZE\n        tl.store(m_i_ptr + part_offset, m_i, mask=mask)\n        tl.store(l_i_ptr + part_offset, l_i, mask=mask)\n\n    out_offset = seq_idx * stride_o0\n    if USE_PARTITIONING:\n        out_offset += kv_head_idx * stride_o1\n    else:\n        out_offset += kv_head_idx * QUERY_GROUP_SIZE * stride_o1\n    out_offset += (\n        part_idx * stride_o2\n        + padding_group_offset[:, None] * stride_o3\n        + head_offset[None, :] * stride_o4\n    )\n\n    group_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE\n    tl.store(out_ptr + out_offset, acc, mask=group_mask)", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 227, 101, 228, 101, 229, 101, 230, 101, 231, 101, 232, 101, 233, 101, 234, 101, 235, 101, 236, 101, 237, 101, 238, 101, 239, 101, 240, 101, 241, 101, 242, 60, 6, 101, 243, 60, 6, 101, 244, 60, 6, 101, 245, 60, 6, 101, 246, 60, 6, 101, 247, 60, 6, 156, 60, 32, -1, 248, 172, 152, 215, 320, 156, 32, 249, 172, 152, 215, 321, 156, 32, 250, 172, 152, 215, 322, 156, 32, 251, 172, 128, 215, 322, 156, 32, 252, 60, 6, 172, 323, 32, 253, 172, 247, 116, 320, 32, 254, 172, 55, 215, 225, 70, 248, 156, 32, 162, 253, 60, 32, 255, 172, 250, 216, 247, 32, 162, 255, 133, 254, 60, 32, 204, 32, 165, 32, 256, 172, 64, 215, 255, 70, 247, 101, 254, 156, 32, 257, 172, 62, 215, 256, 4, 255, 101, 246, 156, 32, 165, 32, 30, 60, 32, 257, 172, 62, 215, 254, 101, 246, 156, 32, 54, 32, 258, 172, 71, 215, 320, 101, 246, 156, 32, 259, 172, 71, 215, 320, 101, 242, 156, 32, 260, 172, 71, 215, 320, 101, 244, 156, 32, 261, 172, 249, 216, 234, 70, 258, 203, 60, 101, 178, 27, 216, 235, 70, 259, 203, 178, 101, 60, 27, 216, 236, 32, 262, 172, 248, 216, 230, 70, 215, 249, 216, 243, 70, 260, 203, 60, 101, 178, 27, 156, 216, 231, 70, 259, 203, 178, 101, 60, 27, 216, 232, 32, 263, 172, 260, 203, 60, 101, 178, 27, 1, 243, 32, 264, 172, 55, 215, 222, 70, 262, 101, 265, 172, 263, 101, 266, 172, 320, 156, 32, 267, 172, 157, 215, 203, 244, 27, 101, 85, 172, 134, 156, 4, 268, 215, 324, 156, 32, 269, 172, 157, 215, 203, 244, 27, 101, 85, 172, 134, 156, 32, 270, 172, 157, 215, 203, 244, 101, 242, 27, 101, 85, 172, 134, 156, 32, 271, 172, 250, 216, 215, 247, 47, 246, 156, 32, 125, 272, 143, 5, 215, 257, 156, 60, 32, 273, 172, 271, 70, 272, 32, 274, 172, 55, 215, 226, 70, 248, 216, 228, 70, 273, 216, 229, 156, 32, 275, 172, 274, 216, 233, 70, 261, 32, 276, 172, 273, 216, 246, 70, 258, 32, 277, 172, 276, 203, 60, 101, 178, 27, 1, 254, 32, 278, 172, 55, 215, 223, 70, 275, 101, 265, 172, 277, 101, 266, 172, 320, 156, 32, 162, 244, 72, 321, 60, 32, 279, 172, 196, 215, 264, 203, 60, 101, 178, 101, 60, 27, 216, 278, 203, 178, 101, 60, 101, 60, 27, 101, 280, 172, 322, 156, 32, 165, 32, 30, 60, 32, 279, 172, 15, 215, 264, 101, 278, 77, 281, 101, 282, 172, 134, 156, 32, 54, 32, 279, 24, 227, 32, 279, 172, 181, 215, 276, 1, 254, 101, 279, 101, 268, 215, 325, 156, 156, 32, 283, 172, 170, 215, 267, 101, 12, 215, 279, 101, 280, 172, 321, 156, 156, 32, 284, 172, 141, 215, 215, 279, 4, 283, 203, 60, 101, 178, 27, 156, 216, 252, 156, 32, 285, 172, 141, 215, 215, 267, 4, 283, 156, 216, 252, 156, 32, 270, 24, 285, 203, 60, 101, 178, 27, 32, 286, 172, 55, 215, 224, 70, 275, 101, 265, 172, 277, 101, 266, 172, 320, 156, 32, 162, 244, 72, 321, 60, 32, 270, 153, 196, 215, 284, 77, 281, 203, 60, 101, 60, 101, 178, 27, 216, 286, 203, 60, 101, 178, 101, 60, 27, 101, 280, 172, 320, 156, 32, 165, 32, 30, 60, 32, 284, 172, 284, 77, 287, 215, 286, 77, 85, 156, 32, 270, 153, 15, 215, 284, 101, 286, 101, 282, 172, 134, 156, 32, 54, 32, 269, 172, 269, 216, 285, 70, 196, 215, 284, 101, 280, 172, 321, 156, 32, 267, 172, 283, 32, 73, 32, 270, 172, 270, 42, 269, 203, 60, 101, 178, 27, 32, 162, 253, 60, 32, 288, 172, 215, 248, 216, 245, 70, 249, 156, 216, 251, 216, 243, 70, 250, 216, 243, 70, 260, 32, 265, 172, 260, 1, 243, 32, 10, 215, 219, 70, 288, 101, 267, 101, 265, 172, 265, 156, 32, 10, 215, 220, 70, 288, 101, 269, 101, 265, 172, 265, 156, 32, 165, 32, 289, 172, 248, 216, 237, 32, 162, 253, 60, 32, 289, 153, 249, 216, 238, 32, 165, 32, 30, 60, 32, 289, 153, 249, 216, 243, 216, 238, 32, 54, 32, 289, 153, 250, 216, 239, 70, 260, 203, 60, 101, 178, 27, 216, 240, 70, 259, 203, 178, 101, 60, 27, 216, 241, 32, 263, 172, 260, 203, 60, 101, 178, 27, 1, 243, 32, 10, 215, 221, 70, 289, 101, 270, 101, 265, 172, 263, 156, 32, 3, 32]}, {"code": "def bmm_kernel(\n    x_ptr,\n    y_ptr,\n    o_ptr,\n    M,\n    N,\n    K,\n    stride_al,\n    stride_am,\n    stride_ak,\n    stride_bl,\n    stride_bk,\n    stride_bn,\n    stride_ol,\n    stride_om,\n    stride_on,\n    **meta,\n):\n    BLOCK_SIZE_M = meta[\"BLOCK_SIZE_M\"]\n    BLOCK_SIZE_N = meta[\"BLOCK_SIZE_N\"]\n    BLOCK_SIZE_K = meta[\"BLOCK_SIZE_K\"]\n    GROUP_SIZE_M = 8\n\n    pid_batch = tl.program_id(0)\n    pid = tl.program_id(1)\n\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (\n        offs_am[:, None] * stride_am\n        + offs_k[None, :] * stride_ak\n        + pid_batch * stride_al\n    )\n    y_ptrs = y_ptr + (\n        offs_k[:, None] * stride_bk\n        + offs_bn[None, :] * stride_bn\n        + pid_batch * stride_bl\n    )\n\n    o = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        x = tl.load(x_ptrs)\n        y = tl.load(y_ptrs)\n        o += tl.dot(x, y)\n\n        x_ptrs += BLOCK_SIZE_K * stride_ak\n        y_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if exists(meta[\"ACTIVATION\"]):\n        o = meta[\"ACTIVATION\"](o)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n\n    o_ptrs = (\n        o_ptr\n        + stride_om * offs_m[:, None]\n        + stride_on * offs_n[None, :]\n        + stride_ol * pid_batch\n    )\n    tl.store(o_ptrs, o, mask=mask)", "encoded": [29, 319, 215, 219, 101, 220, 101, 221, 101, 222, 101, 223, 101, 224, 101, 225, 101, 226, 101, 227, 101, 228, 101, 229, 101, 230, 101, 231, 101, 232, 101, 233, 101, 7, 234, 156, 59, 32, -1, 235, 172, 234, 203, 320, 27, 32, 236, 172, 234, 203, 321, 27, 32, 237, 172, 234, 203, 322, 27, 32, 238, 172, 323, 32, 239, 172, 152, 215, 324, 156, 32, 240, 172, 152, 215, 325, 156, 32, 241, 172, 61, 215, 222, 101, 235, 156, 32, 242, 172, 61, 215, 223, 101, 236, 156, 32, 243, 172, 238, 216, 242, 32, 244, 172, 240, 47, 243, 32, 245, 172, 244, 216, 238, 32, 246, 172, 39, 215, 241, 4, 245, 101, 238, 156, 32, 247, 172, 245, 70, 240, 199, 246, 32, 248, 172, 240, 199, 243, 47, 246, 32, 249, 172, 247, 216, 235, 70, 71, 215, 324, 101, 235, 156, 32, 250, 172, 248, 216, 236, 70, 71, 215, 324, 101, 236, 156, 32, 251, 172, 71, 215, 324, 101, 237, 156, 32, 252, 172, 219, 70, 215, 249, 203, 59, 101, 178, 27, 216, 226, 70, 251, 203, 178, 101, 59, 27, 216, 227, 70, 239, 216, 225, 156, 32, 253, 172, 220, 70, 215, 251, 203, 59, 101, 178, 27, 216, 229, 70, 250, 203, 178, 101, 59, 27, 216, 230, 70, 239, 216, 228, 156, 32, 254, 172, 157, 215, 215, 235, 101, 236, 156, 101, 85, 172, 134, 156, 32, 125, 255, 143, 5, 215, 324, 101, 224, 101, 237, 156, 59, 32, 256, 172, 53, 215, 252, 156, 32, 257, 172, 53, 215, 253, 156, 32, 254, 153, 15, 215, 256, 101, 257, 156, 32, 252, 153, 237, 216, 227, 32, 253, 153, 237, 216, 229, 32, 73, 32, 162, 258, 215, 234, 203, 326, 27, 156, 59, 32, 254, 172, 234, 203, 326, 27, 215, 254, 156, 32, 165, 32, 259, 172, 247, 216, 235, 70, 71, 215, 324, 101, 235, 156, 32, 260, 172, 248, 216, 236, 70, 71, 215, 324, 101, 236, 156, 32, 261, 172, 215, 259, 203, 59, 101, 178, 27, 1, 222, 156, 154, 215, 260, 203, 178, 101, 59, 27, 1, 223, 156, 32, 262, 172, 221, 70, 232, 216, 259, 203, 59, 101, 178, 27, 70, 233, 216, 260, 203, 178, 101, 59, 27, 70, 231, 216, 239, 32, 10, 215, 262, 101, 254, 101, 261, 172, 261, 156, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n\n    if require_x_boundary_check:\n        input = tl.load(input_block_ptr, boundary_check=(1,))\n        condition = tl.arange(0, x_block_size) < x_size\n        input = tl.where(condition, input, float(\"-inf\"))\n    else:\n        input = tl.load(input_block_ptr)\n\n    output = tl.argmax(input, 1)\n    tl.store(output_block_ptr, output.to(dtype))", "encoded": [29, 321, 217, 221, 60, 91, 102, 222, 60, 91, 102, 223, 60, 59, 102, 224, 60, 59, 102, 225, 60, 59, 102, 226, 60, 59, 102, 85, 60, 6, 102, 227, 60, 6, 102, 228, 60, 6, 157, 60, 32, -1, 229, 174, 153, 217, 322, 157, 32, 230, 174, 196, 217, 221, 102, 116, 174, 217, 223, 102, 157, 102, 231, 174, 217, 323, 102, 157, 102, 232, 174, 217, 229, 102, 157, 102, 233, 174, 217, 323, 102, 157, 102, 234, 174, 217, 322, 102, 157, 157, 32, 235, 174, 196, 217, 222, 102, 116, 174, 217, 223, 102, 224, 157, 102, 231, 174, 217, 225, 102, 226, 157, 102, 232, 174, 217, 229, 102, 322, 157, 102, 233, 174, 217, 323, 102, 227, 157, 102, 234, 174, 217, 323, 102, 322, 157, 157, 32, 163, 228, 60, 32, 236, 174, 55, 217, 235, 102, 237, 174, 217, 323, 102, 157, 157, 32, 238, 174, 71, 217, 322, 102, 227, 157, 1, 224, 32, 236, 174, 183, 217, 238, 102, 236, 102, 239, 217, 324, 157, 157, 32, 166, 32, 30, 60, 32, 236, 174, 55, 217, 235, 157, 32, 54, 32, 240, 174, 170, 217, 236, 102, 323, 157, 32, 10, 217, 230, 102, 240, 77, 241, 217, 85, 157, 157, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    denominator_ptr: tl.tensor,\n    numerator_ptr: tl.tensor,\n    x1_ptr: tl.tensor,\n    x2_ptr: tl.tensor,\n    z_size: tl.int32,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    z_stride: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    eps: tl.float32,\n    size_along_dim: tl.int32,\n    output_y_size: tl.int32,\n    output_x_size: tl.int32,\n    dtype: tl.constexpr,\n    block_size: tl.constexpr,\n    require_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n\n    num_output_y = pid // output_x_size\n    num_output_x = pid % output_x_size\n\n    x1_block_ptr = tl.make_block_ptr(\n        x1_ptr,\n        shape=(z_size, y_size, x_size),\n        strides=(z_stride, y_stride, x_stride),\n        offsets=(0, num_output_y, num_output_x),\n        block_shape=(block_size, 1, 1),\n        order=(2, 1, 0),\n    )\n    x2_block_ptr = tl.make_block_ptr(\n        x2_ptr,\n        shape=(z_size, y_size, x_size),\n        strides=(z_stride, y_stride, x_stride),\n        offsets=(0, num_output_y, num_output_x),\n        block_shape=(block_size, 1, 1),\n        order=(2, 1, 0),\n    )\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(output_y_size, output_x_size),\n        strides=(output_x_size, 1),\n        offsets=(num_output_y, num_output_x),\n        block_shape=(1, 1),\n        order=(1, 0),\n    )\n    denominator_block_ptr = tl.make_block_ptr(\n        denominator_ptr,\n        shape=(output_y_size, output_x_size),\n        strides=(output_x_size, 1),\n        offsets=(num_output_y, num_output_x),\n        block_shape=(1, 1),\n        order=(1, 0),\n    )\n    numerator_block_ptr = tl.make_block_ptr(\n        numerator_ptr,\n        shape=(output_y_size, output_x_size),\n        strides=(output_x_size, 1),\n        offsets=(num_output_y, num_output_x),\n        block_shape=(1, 1),\n        order=(1, 0),\n    )\n\n    denominator_accumulation1 = tl.zeros((block_size, 1, 1), tl.float32)\n    denominator_accumulation2 = tl.zeros((block_size, 1, 1), tl.float32)\n    numerator_accumulation = tl.zeros((block_size, 1, 1), tl.float32)\n\n    for _ in range(0, size_along_dim, block_size):\n        if require_boundary_check:\n            x1 = tl.load(x1_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n            x2 = tl.load(x2_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n        else:\n            x1 = tl.load(x1_block_ptr)\n            x2 = tl.load(x2_block_ptr)\n\n        denominator_accumulation1 += x1 * x1\n        denominator_accumulation2 += x2 * x2\n        numerator_accumulation += x1 * x2\n\n        x1_block_ptr = tl.advance(x1_block_ptr, (block_size, 0, 0))\n        x2_block_ptr = tl.advance(x2_block_ptr, (block_size, 0, 0))\n\n    denominator1 = tl.sum(denominator_accumulation1, 0)\n    denominator2 = tl.sum(denominator_accumulation2, 0)\n    denominator = tl.sqrt(denominator1) * tl.sqrt(denominator2)\n\n    numerator = tl.sum(numerator_accumulation, 0)\n    output = numerator / tl.math.max(denominator, eps)\n\n    tl.store(output_block_ptr, output.to(dtype))\n    tl.store(denominator_block_ptr, denominator.to(dtype))\n    tl.store(numerator_block_ptr, numerator.to(dtype))", "encoded": [29, 322, 218, 222, 60, 92, 103, 223, 60, 92, 103, 224, 60, 92, 103, 225, 60, 92, 103, 226, 60, 92, 103, 227, 60, 221, 103, 228, 60, 221, 103, 229, 60, 221, 103, 230, 60, 221, 103, 231, 60, 221, 103, 232, 60, 221, 103, 233, 60, 136, 103, 234, 60, 221, 103, 235, 60, 221, 103, 236, 60, 221, 103, 86, 60, 6, 103, 237, 60, 6, 103, 238, 60, 6, 158, 60, 32, -1, 239, 175, 154, 218, 323, 158, 32, 240, 175, 239, 48, 236, 32, 241, 175, 239, 202, 236, 32, 242, 175, 197, 218, 225, 103, 117, 175, 218, 227, 103, 228, 103, 229, 158, 103, 243, 175, 218, 230, 103, 231, 103, 232, 158, 103, 244, 175, 218, 323, 103, 240, 103, 241, 158, 103, 245, 175, 218, 237, 103, 324, 103, 324, 158, 103, 246, 175, 218, 325, 103, 324, 103, 323, 158, 158, 32, 247, 175, 197, 218, 226, 103, 117, 175, 218, 227, 103, 228, 103, 229, 158, 103, 243, 175, 218, 230, 103, 231, 103, 232, 158, 103, 244, 175, 218, 323, 103, 240, 103, 241, 158, 103, 245, 175, 218, 237, 103, 324, 103, 324, 158, 103, 246, 175, 218, 325, 103, 324, 103, 323, 158, 158, 32, 248, 175, 197, 218, 222, 103, 117, 175, 218, 235, 103, 236, 158, 103, 243, 175, 218, 236, 103, 324, 158, 103, 244, 175, 218, 240, 103, 241, 158, 103, 245, 175, 218, 324, 103, 324, 158, 103, 246, 175, 218, 324, 103, 323, 158, 158, 32, 249, 175, 197, 218, 223, 103, 117, 175, 218, 235, 103, 236, 158, 103, 243, 175, 218, 236, 103, 324, 158, 103, 244, 175, 218, 240, 103, 241, 158, 103, 245, 175, 218, 324, 103, 324, 158, 103, 246, 175, 218, 324, 103, 323, 158, 158, 32, 250, 175, 197, 218, 224, 103, 117, 175, 218, 235, 103, 236, 158, 103, 243, 175, 218, 236, 103, 324, 158, 103, 244, 175, 218, 240, 103, 241, 158, 103, 245, 175, 218, 324, 103, 324, 158, 103, 246, 175, 218, 324, 103, 323, 158, 158, 32, 251, 175, 159, 218, 218, 237, 103, 324, 103, 324, 158, 103, 136, 158, 32, 252, 175, 159, 218, 218, 237, 103, 324, 103, 324, 158, 103, 136, 158, 32, 253, 175, 159, 218, 218, 237, 103, 324, 103, 324, 158, 103, 136, 158, 32, 127, 254, 145, 5, 218, 323, 103, 234, 103, 237, 158, 60, 32, 164, 238, 60, 32, 255, 175, 54, 218, 242, 103, 256, 175, 218, 323, 103, 158, 103, 257, 175, 326, 158, 32, 258, 175, 54, 218, 247, 103, 256, 175, 218, 323, 103, 158, 103, 257, 175, 326, 158, 32, 167, 32, 30, 60, 32, 255, 175, 54, 218, 242, 158, 32, 258, 175, 54, 218, 247, 158, 32, 55, 32, 251, 155, 255, 219, 255, 32, 252, 155, 258, 219, 258, 32, 253, 155, 255, 219, 258, 32, 242, 175, 134, 218, 242, 103, 218, 237, 103, 323, 103, 323, 158, 158, 32, 247, 175, 134, 218, 247, 103, 218, 237, 103, 323, 103, 323, 158, 158, 32, 74, 32, 259, 175, 199, 218, 251, 103, 323, 158, 32, 260, 175, 199, 218, 252, 103, 323, 158, 32, 261, 175, 122, 218, 259, 158, 219, 122, 218, 260, 158, 32, 262, 175, 199, 218, 253, 103, 323, 158, 32, 263, 175, 262, 42, 43, 218, 261, 103, 233, 158, 32, 10, 218, 248, 103, 263, 78, 264, 218, 86, 158, 158, 32, 10, 218, 249, 103, 261, 78, 264, 218, 86, 158, 158, 32, 10, 218, 250, 103, 262, 78, 264, 218, 86, 158, 158, 32, 3, 32]}, {"code": "def backward(\n    grad_x1_ptr: tl.tensor,\n    grad_x2_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    denominator_ptr: tl.tensor,\n    numerator_ptr: tl.tensor,\n    x1_ptr: tl.tensor,\n    x2_ptr: tl.tensor,\n    z_size: tl.int32,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    z_stride: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    size_along_dim: tl.int32,\n    output_y_size: tl.int32,\n    output_x_size: tl.int32,\n    dtype: tl.constexpr,\n    block_size: tl.constexpr,\n    require_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_output_y = pid // output_x_size\n    num_output_x = pid % output_x_size\n\n    grad_x1_block_ptr = tl.make_block_ptr(\n        grad_x1_ptr,\n        shape=(z_size, y_size, x_size),\n        strides=(z_stride, y_stride, x_stride),\n        offsets=(0, num_output_y, num_output_x),\n        block_shape=(block_size, 1, 1),\n        order=(2, 1, 0),\n    )\n    grad_x2_block_ptr = tl.make_block_ptr(\n        grad_x2_ptr,\n        shape=(z_size, y_size, x_size),\n        strides=(z_stride, y_stride, x_stride),\n        offsets=(0, num_output_y, num_output_x),\n        block_shape=(block_size, 1, 1),\n        order=(2, 1, 0),\n    )\n    grad_output_block_ptr = tl.make_block_ptr(\n        grad_output_ptr,\n        shape=(output_y_size, output_x_size),\n        strides=(output_x_size, 1),\n        offsets=(num_output_y, num_output_x),\n        block_shape=(1, 1),\n        order=(1, 0),\n    )\n    x1_block_ptr = tl.make_block_ptr(\n        x1_ptr,\n        shape=(z_size, y_size, x_size),\n        strides=(z_stride, y_stride, x_stride),\n        offsets=(0, num_output_y, num_output_x),\n        block_shape=(block_size, 1, 1),\n        order=(2, 1, 0),\n    )\n    x2_block_ptr = tl.make_block_ptr(\n        x2_ptr,\n        shape=(z_size, y_size, x_size),\n        strides=(z_stride, y_stride, x_stride),\n        offsets=(0, num_output_y, num_output_x),\n        block_shape=(block_size, 1, 1),\n        order=(2, 1, 0),\n    )\n    denominator_block_ptr = tl.make_block_ptr(\n        denominator_ptr,\n        shape=(output_y_size, output_x_size),\n        strides=(output_x_size, 1),\n        offsets=(num_output_y, num_output_x),\n        block_shape=(1, 1),\n        order=(1, 0),\n    )\n    numerator_block_ptr = tl.make_block_ptr(\n        numerator_ptr,\n        shape=(output_y_size, output_x_size),\n        strides=(output_x_size, 1),\n        offsets=(num_output_y, num_output_x),\n        block_shape=(1, 1),\n        order=(1, 0),\n    )\n\n    for _ in range(0, size_along_dim, block_size):\n        if require_boundary_check:\n            x1 = tl.load(\n                x1_block_ptr, boundary_check=(0,), padding_option=\"zero\"\n            ).to(tl.float32)\n            x2 = tl.load(\n                x2_block_ptr, boundary_check=(0,), padding_option=\"zero\"\n            ).to(tl.float32)\n        else:\n            x1 = tl.load(x1_block_ptr)\n            x2 = tl.load(x2_block_ptr)\n\n        denominator = tl.load(denominator_block_ptr)\n        numerator = tl.load(numerator_block_ptr)\n        grad_output = tl.load(grad_output_block_ptr)\n\n        squared_x1 = x1 * x1\n        squared_x2 = x2 * x2\n        squared_x1_sum = tl.sum(squared_x1, 0)\n        squared_x2_sum = tl.sum(squared_x2, 0)\n\n        grad_denominator = (\n            grad_output * numerator * (-1 / (denominator * denominator))\n        )\n\n        grad_mul1 = grad_denominator * tl.sqrt(tl.sum(squared_x2, 0))\n        grad_mul2 = grad_denominator * tl.sqrt(tl.sum(squared_x1, 0))\n\n        grad_sqrt1 = grad_mul1 / (2 * tl.sqrt(squared_x1_sum))\n        grad_sqrt2 = grad_mul2 / (2 * tl.sqrt(squared_x2_sum))\n\n        grad_to_dot = grad_output / denominator\n\n        grad_x1 = (grad_sqrt1 * 2 * x1) + (grad_to_dot * x2)\n        grad_x2 = (grad_sqrt2 * 2 * x2) + (grad_to_dot * x1)\n\n        if require_boundary_check:\n            tl.store(grad_x1_block_ptr, grad_x1.to(dtype), boundary_check=(0,))\n            tl.store(grad_x2_block_ptr, grad_x2.to(dtype), boundary_check=(0,))\n        else:\n            tl.store(grad_x1_block_ptr, grad_x1.to(dtype))\n            tl.store(grad_x2_block_ptr, grad_x2.to(dtype))\n\n        x1_block_ptr = tl.advance(x1_block_ptr, (block_size, 0, 0))\n        x2_block_ptr = tl.advance(x2_block_ptr, (block_size, 0, 0))\n        grad_x1_block_ptr = tl.advance(grad_x1_block_ptr, (block_size, 0, 0))\n        grad_x2_block_ptr = tl.advance(grad_x2_block_ptr, (block_size, 0, 0))", "encoded": [29, 322, 218, 222, 61, 92, 103, 223, 61, 92, 103, 224, 61, 92, 103, 225, 61, 92, 103, 226, 61, 92, 103, 227, 61, 92, 103, 228, 61, 92, 103, 229, 61, 60, 103, 230, 61, 60, 103, 231, 61, 60, 103, 232, 61, 60, 103, 233, 61, 60, 103, 234, 61, 60, 103, 235, 61, 60, 103, 236, 61, 60, 103, 237, 61, 60, 103, 86, 61, 6, 103, 238, 61, 6, 103, 239, 61, 6, 158, 61, 32, -1, 240, 175, 154, 218, 323, 158, 32, 241, 175, 240, 48, 237, 32, 242, 175, 240, 202, 237, 32, 243, 175, 197, 218, 222, 103, 117, 175, 218, 229, 103, 230, 103, 231, 158, 103, 244, 175, 218, 232, 103, 233, 103, 234, 158, 103, 245, 175, 218, 323, 103, 241, 103, 242, 158, 103, 246, 175, 218, 238, 103, 324, 103, 324, 158, 103, 247, 175, 218, 325, 103, 324, 103, 323, 158, 158, 32, 248, 175, 197, 218, 223, 103, 117, 175, 218, 229, 103, 230, 103, 231, 158, 103, 244, 175, 218, 232, 103, 233, 103, 234, 158, 103, 245, 175, 218, 323, 103, 241, 103, 242, 158, 103, 246, 175, 218, 238, 103, 324, 103, 324, 158, 103, 247, 175, 218, 325, 103, 324, 103, 323, 158, 158, 32, 249, 175, 197, 218, 224, 103, 117, 175, 218, 236, 103, 237, 158, 103, 244, 175, 218, 237, 103, 324, 158, 103, 245, 175, 218, 241, 103, 242, 158, 103, 246, 175, 218, 324, 103, 324, 158, 103, 247, 175, 218, 324, 103, 323, 158, 158, 32, 250, 175, 197, 218, 227, 103, 117, 175, 218, 229, 103, 230, 103, 231, 158, 103, 244, 175, 218, 232, 103, 233, 103, 234, 158, 103, 245, 175, 218, 323, 103, 241, 103, 242, 158, 103, 246, 175, 218, 238, 103, 324, 103, 324, 158, 103, 247, 175, 218, 325, 103, 324, 103, 323, 158, 158, 32, 251, 175, 197, 218, 228, 103, 117, 175, 218, 229, 103, 230, 103, 231, 158, 103, 244, 175, 218, 232, 103, 233, 103, 234, 158, 103, 245, 175, 218, 323, 103, 241, 103, 242, 158, 103, 246, 175, 218, 238, 103, 324, 103, 324, 158, 103, 247, 175, 218, 325, 103, 324, 103, 323, 158, 158, 32, 252, 175, 197, 218, 225, 103, 117, 175, 218, 236, 103, 237, 158, 103, 244, 175, 218, 237, 103, 324, 158, 103, 245, 175, 218, 241, 103, 242, 158, 103, 246, 175, 218, 324, 103, 324, 158, 103, 247, 175, 218, 324, 103, 323, 158, 158, 32, 253, 175, 197, 218, 226, 103, 117, 175, 218, 236, 103, 237, 158, 103, 244, 175, 218, 237, 103, 324, 158, 103, 245, 175, 218, 241, 103, 242, 158, 103, 246, 175, 218, 324, 103, 324, 158, 103, 247, 175, 218, 324, 103, 323, 158, 158, 32, 127, 254, 145, 5, 218, 323, 103, 235, 103, 238, 158, 61, 32, 164, 239, 61, 32, 255, 175, 56, 218, 250, 103, 256, 175, 218, 323, 103, 158, 103, 257, 175, 326, 158, 78, 258, 218, 136, 158, 32, 259, 175, 56, 218, 251, 103, 256, 175, 218, 323, 103, 158, 103, 257, 175, 326, 158, 78, 258, 218, 136, 158, 32, 167, 32, 30, 61, 32, 255, 175, 56, 218, 250, 158, 32, 259, 175, 56, 218, 251, 158, 32, 55, 32, 260, 175, 56, 218, 252, 158, 32, 261, 175, 56, 218, 253, 158, 32, 262, 175, 56, 218, 249, 158, 32, 263, 175, 255, 219, 255, 32, 264, 175, 259, 219, 259, 32, 265, 175, 199, 218, 263, 103, 323, 158, 32, 266, 175, 199, 218, 264, 103, 323, 158, 32, 267, 175, 262, 219, 261, 219, 218, 4, 324, 42, 218, 260, 219, 260, 158, 158, 32, 268, 175, 267, 219, 122, 218, 199, 218, 264, 103, 323, 158, 158, 32, 269, 175, 267, 219, 122, 218, 199, 218, 263, 103, 323, 158, 158, 32, 270, 175, 268, 42, 218, 325, 219, 122, 218, 265, 158, 158, 32, 271, 175, 269, 42, 218, 325, 219, 122, 218, 266, 158, 158, 32, 272, 175, 262, 42, 260, 32, 273, 175, 270, 219, 325, 219, 255, 71, 272, 219, 259, 32, 274, 175, 271, 219, 325, 219, 259, 71, 272, 219, 255, 32, 164, 239, 61, 32, 10, 218, 243, 103, 273, 78, 258, 218, 86, 158, 103, 256, 175, 218, 323, 103, 158, 158, 32, 10, 218, 248, 103, 274, 78, 258, 218, 86, 158, 103, 256, 175, 218, 323, 103, 158, 158, 32, 167, 32, 30, 61, 32, 10, 218, 243, 103, 273, 78, 258, 218, 86, 158, 158, 32, 10, 218, 248, 103, 274, 78, 258, 218, 86, 158, 158, 32, 55, 32, 250, 175, 134, 218, 250, 103, 218, 238, 103, 323, 103, 323, 158, 158, 32, 251, 175, 134, 218, 251, 103, 218, 238, 103, 323, 103, 323, 158, 158, 32, 243, 175, 134, 218, 243, 103, 218, 238, 103, 323, 103, 323, 158, 158, 32, 248, 175, 134, 218, 248, 103, 218, 238, 103, 323, 103, 323, 158, 158, 32, 74, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    x_size: tl.int32,\n    p: tl.float32,\n    seed: tl.int32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    x_offset = pid * x_block_size\n\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(x_size,),\n        strides=(1,),\n        offsets=(x_offset,),\n        block_shape=(x_block_size,),\n        order=(0,),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr,\n        shape=(x_size,),\n        strides=(1,),\n        offsets=(x_offset,),\n        block_shape=(x_block_size,),\n        order=(0,),\n    )\n\n    if require_x_boundary_check:\n        input = tl.load(input_block_ptr, boundary_check=(0,))\n    else:\n        input = tl.load(input_block_ptr)\n\n    condition = tl.rand(seed, tl.arange(0, x_block_size) + x_offset) > p\n    output = tl.where(condition, input / (1.0 - p + language.eps), 0.0)\n\n    if require_x_boundary_check:\n        tl.store(output_block_ptr, output.to(dtype), boundary_check=(0,))\n    else:\n        tl.store(output_block_ptr, output.to(dtype))", "encoded": [29, 323, 219, 223, 60, 93, 104, 224, 60, 93, 104, 225, 60, 222, 104, 226, 60, 137, 104, 227, 60, 222, 104, 87, 60, 6, 104, 228, 60, 6, 104, 229, 60, 6, 159, 60, 32, -1, 230, 176, 155, 219, 324, 159, 32, 231, 176, 230, 220, 228, 32, 232, 176, 198, 219, 223, 104, 118, 176, 219, 225, 104, 159, 104, 233, 176, 219, 325, 104, 159, 104, 234, 176, 219, 231, 104, 159, 104, 235, 176, 219, 228, 104, 159, 104, 236, 176, 219, 324, 104, 159, 159, 32, 237, 176, 198, 219, 224, 104, 118, 176, 219, 225, 104, 159, 104, 233, 176, 219, 325, 104, 159, 104, 234, 176, 219, 231, 104, 159, 104, 235, 176, 219, 228, 104, 159, 104, 236, 176, 219, 324, 104, 159, 159, 32, 165, 229, 60, 32, 238, 176, 54, 219, 237, 104, 239, 176, 219, 324, 104, 159, 159, 32, 168, 32, 30, 60, 32, 238, 176, 54, 219, 237, 159, 32, 55, 32, 240, 176, 66, 219, 227, 104, 73, 219, 324, 104, 228, 159, 72, 231, 159, 119, 226, 32, 241, 176, 185, 219, 240, 104, 238, 42, 219, 325, 4, 226, 72, 153, 79, 242, 159, 104, 324, 159, 32, 165, 229, 60, 32, 10, 219, 232, 104, 241, 79, 243, 219, 87, 159, 104, 239, 176, 219, 324, 104, 159, 159, 32, 168, 32, 30, 60, 32, 10, 219, 232, 104, 241, 79, 243, 219, 87, 159, 159, 32, 55, 32, 3, 32]}, {"code": "def backward(\n    grad_input_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    output_ptr: tl.tensor,\n    x_size: tl.int32,\n    p: tl.float32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    x_offset = pid * x_block_size\n\n    grad_input_block_ptr = tl.make_block_ptr(\n        grad_input_ptr,\n        shape=(x_size,),\n        strides=(1,),\n        offsets=(x_offset,),\n        block_shape=(x_block_size,),\n        order=(0,),\n    )\n    grad_output_block_ptr = tl.make_block_ptr(\n        grad_output_ptr,\n        shape=(x_size,),\n        strides=(1,),\n        offsets=(x_offset,),\n        block_shape=(x_block_size,),\n        order=(0,),\n    )\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(x_size,),\n        strides=(1,),\n        offsets=(x_offset,),\n        block_shape=(x_block_size,),\n        order=(0,),\n    )\n\n    if require_x_boundary_check:\n        grad_output = tl.load(grad_output_block_ptr, boundary_check=(0,))\n        output = tl.load(output_block_ptr, boundary_check=(0,))\n    else:\n        grad_output = tl.load(grad_output_block_ptr)\n        output = tl.load(output_block_ptr)\n\n    condition = (p == 0.0) | (output > 0.0)\n    grad_input = tl.where(condition, grad_output * (1.0 - p + language.eps), 0.0)\n\n    if require_x_boundary_check:\n        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(0,))\n    else:\n        tl.store(grad_input_block_ptr, grad_input.to(dtype))", "encoded": [29, 323, 219, 223, 61, 93, 104, 224, 61, 93, 104, 225, 61, 93, 104, 226, 61, 60, 104, 227, 61, 137, 104, 87, 61, 6, 104, 228, 61, 6, 104, 229, 61, 6, 159, 61, 32, -1, 230, 176, 155, 219, 324, 159, 32, 231, 176, 230, 220, 228, 32, 232, 176, 198, 219, 223, 104, 118, 176, 219, 226, 104, 159, 104, 233, 176, 219, 325, 104, 159, 104, 234, 176, 219, 231, 104, 159, 104, 235, 176, 219, 228, 104, 159, 104, 236, 176, 219, 324, 104, 159, 159, 32, 237, 176, 198, 219, 224, 104, 118, 176, 219, 226, 104, 159, 104, 233, 176, 219, 325, 104, 159, 104, 234, 176, 219, 231, 104, 159, 104, 235, 176, 219, 228, 104, 159, 104, 236, 176, 219, 324, 104, 159, 159, 32, 238, 176, 198, 219, 225, 104, 118, 176, 219, 226, 104, 159, 104, 233, 176, 219, 325, 104, 159, 104, 234, 176, 219, 231, 104, 159, 104, 235, 176, 219, 228, 104, 159, 104, 236, 176, 219, 324, 104, 159, 159, 32, 165, 229, 61, 32, 239, 176, 56, 219, 237, 104, 240, 176, 219, 324, 104, 159, 159, 32, 241, 176, 56, 219, 238, 104, 240, 176, 219, 324, 104, 159, 159, 32, 168, 32, 30, 61, 32, 239, 176, 56, 219, 237, 159, 32, 241, 176, 56, 219, 238, 159, 32, 55, 32, 242, 176, 219, 227, 74, 324, 159, 141, 219, 241, 119, 324, 159, 32, 243, 176, 185, 219, 242, 104, 239, 220, 219, 325, 4, 227, 72, 153, 79, 244, 159, 104, 324, 159, 32, 165, 229, 61, 32, 10, 219, 232, 104, 243, 79, 245, 219, 87, 159, 104, 240, 176, 219, 324, 104, 159, 159, 32, 168, 32, 30, 61, 32, 10, 219, 232, 104, 243, 79, 245, 219, 87, 159, 159, 32, 55, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    state_gate_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    weight_ptr: tl.tensor,\n    bias_ptr: tl.tensor,\n    m_size: tl.int32,\n    n_size: tl.int32,\n    k_size: tl.int32,\n    x_size: tl.int32,\n    input_batch_stride: tl.int32,\n    input_m_stride: tl.int32,\n    input_k_stride: tl.int32,\n    weight_n_stride: tl.int32,\n    weight_k_stride: tl.int32,\n    use_accelerator: tl.constexpr,\n    dtype: tl.constexpr,\n    m_block_size: tl.constexpr,\n    k_block_size: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_m_boundary_check: tl.constexpr,\n    require_k_boundary_check: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_m_blocks = tl.cdiv(m_size, m_block_size)\n    num_x_blocks = tl.cdiv(x_size, x_block_size)\n    num_blocks = num_m_blocks * num_x_blocks\n    batch = pid // num_blocks\n    block = pid % num_blocks\n    m_block = block // num_x_blocks\n    x_block = block % num_x_blocks\n    m_offset = m_block * m_block_size\n    x_offset = x_block * x_block_size\n\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr + batch * m_size * x_size,\n        shape=(m_size, x_size),\n        strides=(x_size, 1),\n        offsets=(m_offset, x_offset),\n        block_shape=(m_block_size, x_block_size),\n        order=(1, 0),\n    )\n    state_block_ptr = tl.make_block_ptr(\n        state_gate_ptr + batch * m_size * n_size,\n        shape=(m_size, n_size),\n        strides=(n_size, 1),\n        offsets=(m_offset, x_offset),\n        block_shape=(m_block_size, x_block_size),\n        order=(1, 0),\n    )\n    gate_block_ptr = tl.make_block_ptr(\n        state_gate_ptr + batch * m_size * n_size,\n        shape=(m_size, n_size),\n        strides=(n_size, 1),\n        offsets=(m_offset, x_offset + x_size),\n        block_shape=(m_block_size, x_block_size),\n        order=(1, 0),\n    )\n\n    state = language.Linear.forward(\n        input_ptr + batch * input_batch_stride,\n        weight_ptr,\n        bias_ptr,\n        m_size,\n        n_size,\n        k_size,\n        input_m_stride,\n        input_k_stride,\n        weight_n_stride,\n        weight_k_stride,\n        m_offset,\n        x_offset,\n        use_accelerator,\n        m_block_size,\n        x_block_size,\n        k_block_size,\n        require_m_boundary_check,\n        require_x_boundary_check,\n        require_k_boundary_check,\n        dtype,\n    )\n    gate = language.Linear.forward(\n        input_ptr + batch * input_batch_stride,\n        weight_ptr,\n        bias_ptr,\n        m_size,\n        n_size,\n        k_size,\n        input_m_stride,\n        input_k_stride,\n        weight_n_stride,\n        weight_k_stride,\n        m_offset,\n        x_offset + x_size,\n        use_accelerator,\n        m_block_size,\n        x_block_size,\n        k_block_size,\n        require_m_boundary_check,\n        require_x_boundary_check,\n        require_k_boundary_check,\n        dtype,\n    )\n    output = state * language.math.GELU.forward(gate)\n\n    if require_m_boundary_check & require_x_boundary_check:\n        tl.store(output_block_ptr, output.to(dtype))\n        tl.store(state_block_ptr, state.to(dtype))\n        tl.store(gate_block_ptr, gate.to(dtype))\n    else:\n        tl.store(output_block_ptr, output.to(dtype), boundary_check=(0, 1))\n        tl.store(state_block_ptr, state.to(dtype), boundary_check=(0, 1))\n        tl.store(gate_block_ptr, gate.to(dtype), boundary_check=(0, 1))", "encoded": [29, 323, 219, 223, 60, 93, 104, 224, 60, 93, 104, 225, 60, 93, 104, 226, 60, 93, 104, 227, 60, 93, 104, 228, 60, 222, 104, 229, 60, 222, 104, 230, 60, 222, 104, 231, 60, 222, 104, 232, 60, 222, 104, 233, 60, 222, 104, 234, 60, 222, 104, 235, 60, 222, 104, 236, 60, 222, 104, 237, 60, 6, 104, 87, 60, 6, 104, 238, 60, 6, 104, 239, 60, 6, 104, 240, 60, 6, 104, 241, 60, 6, 104, 242, 60, 6, 104, 243, 60, 6, 159, 60, 32, -1, 244, 176, 155, 219, 324, 159, 32, 245, 176, 62, 219, 228, 104, 238, 159, 32, 246, 176, 62, 219, 231, 104, 240, 159, 32, 247, 176, 245, 220, 246, 32, 248, 176, 244, 48, 247, 32, 249, 176, 244, 203, 247, 32, 250, 176, 249, 48, 246, 32, 251, 176, 249, 203, 246, 32, 252, 176, 250, 220, 238, 32, 253, 176, 251, 220, 240, 32, 254, 176, 198, 219, 223, 72, 248, 220, 228, 220, 231, 104, 118, 176, 219, 228, 104, 231, 159, 104, 255, 176, 219, 231, 104, 325, 159, 104, 256, 176, 219, 252, 104, 253, 159, 104, 257, 176, 219, 238, 104, 240, 159, 104, 258, 176, 219, 325, 104, 324, 159, 159, 32, 259, 176, 198, 219, 224, 72, 248, 220, 228, 220, 229, 104, 118, 176, 219, 228, 104, 229, 159, 104, 255, 176, 219, 229, 104, 325, 159, 104, 256, 176, 219, 252, 104, 253, 159, 104, 257, 176, 219, 238, 104, 240, 159, 104, 258, 176, 219, 325, 104, 324, 159, 159, 32, 260, 176, 198, 219, 224, 72, 248, 220, 228, 220, 229, 104, 118, 176, 219, 228, 104, 229, 159, 104, 255, 176, 219, 229, 104, 325, 159, 104, 256, 176, 219, 252, 104, 253, 72, 231, 159, 104, 257, 176, 219, 238, 104, 240, 159, 104, 258, 176, 219, 325, 104, 324, 159, 159, 32, 261, 176, 153, 79, 262, 79, 323, 219, 225, 72, 248, 220, 232, 104, 226, 104, 227, 104, 228, 104, 229, 104, 230, 104, 233, 104, 234, 104, 235, 104, 236, 104, 252, 104, 253, 104, 237, 104, 238, 104, 240, 104, 239, 104, 241, 104, 243, 104, 242, 104, 87, 159, 32, 263, 176, 153, 79, 262, 79, 323, 219, 225, 72, 248, 220, 232, 104, 226, 104, 227, 104, 228, 104, 229, 104, 230, 104, 233, 104, 234, 104, 235, 104, 236, 104, 252, 104, 253, 72, 231, 104, 237, 104, 238, 104, 240, 104, 239, 104, 241, 104, 243, 104, 242, 104, 87, 159, 32, 264, 176, 261, 220, 153, 79, 265, 79, 266, 79, 323, 219, 263, 159, 32, 165, 241, 157, 243, 60, 32, 10, 219, 254, 104, 264, 79, 267, 219, 87, 159, 159, 32, 10, 219, 259, 104, 261, 79, 267, 219, 87, 159, 159, 32, 10, 219, 260, 104, 263, 79, 267, 219, 87, 159, 159, 32, 168, 32, 30, 60, 32, 10, 219, 254, 104, 264, 79, 267, 219, 87, 159, 104, 268, 176, 219, 324, 104, 325, 159, 159, 32, 10, 219, 259, 104, 261, 79, 267, 219, 87, 159, 104, 268, 176, 219, 324, 104, 325, 159, 159, 32, 10, 219, 260, 104, 263, 79, 267, 219, 87, 159, 104, 268, 176, 219, 324, 104, 325, 159, 159, 32, 55, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    rstd_ptr: tl.tensor,\n    mean_ptr: tl.tensor,\n    group_size: tl.int32,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    num_groups: tl.int32,\n    weight_ptr: tl.tensor,\n    bias_ptr: tl.tensor,\n    eps: tl.float32,\n    dtype: tl.constexpr,\n    group_block_size: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_group_boundary_check: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    batch = pid // num_groups\n    group = pid % num_groups\n    num_elements = group_size * x_size\n    batch_offset = batch * num_groups * num_elements\n    group_offset = batch_offset + group * num_elements\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr + group_offset,\n        shape=(group_size, x_size),\n        strides=(x_size, 1),\n        offsets=(0, 0),\n        block_shape=(group_block_size, x_block_size),\n        order=(1, 0),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr + group_offset,\n        shape=(group_size, x_size),\n        strides=(x_size, 1),\n        offsets=(0, 0),\n        block_shape=(group_block_size, x_block_size),\n        order=(1, 0),\n    )\n    rstd_block_ptr = tl.make_block_ptr(\n        rstd_ptr + batch * num_groups,\n        shape=(group_size,),\n        strides=(1,),\n        offsets=(group,),\n        block_shape=(1,),\n        order=(0,),\n    )\n    mean_block_ptr = tl.make_block_ptr(\n        mean_ptr + batch * num_groups,\n        shape=(group_size,),\n        strides=(1,),\n        offsets=(group,),\n        block_shape=(1,),\n        order=(0,),\n    )\n\n    if require_group_boundary_check | require_x_boundary_check:\n        input = tl.load(\n            input_block_ptr, boundary_check=(0, 1), padding_option=\"zero\"\n        )\n        mean = tl.sum(\n            tl.view(input / num_elements, (1, group_block_size * x_block_size)), 1\n        )\n        group_condition = tl.arange(0, group_block_size) < group_size\n        x_condition = tl.arange(0, x_block_size) < x_size\n        condition = group_condition[:, None] & x_condition[None, :]\n        centered_mean = tl.where(condition, input - mean, 0)\n    else:\n        input = tl.load(input_block_ptr)\n        mean = tl.sum(\n            tl.view(input / num_elements, (1, group_block_size * x_block_size)), 1\n        )\n        centered_mean = input - mean\n\n    var = tl.sum(\n        tl.view(\n            centered_mean * centered_mean / num_elements,\n            (1, group_block_size * x_block_size),\n        ),\n        1,\n    )\n    rstd = tl.math.rsqrt(var + eps)\n    output = centered_mean * rstd\n\n    if weight_ptr is not None:\n        weight_block_ptr = tl.make_block_ptr(\n            weight_ptr,\n            shape=(y_size, 1),\n            strides=(1, y_size),\n            offsets=(group * group_size, 0),\n            block_shape=(group_block_size, 1),\n            order=(0, 1),\n        )\n        weight = tl.load(weight_block_ptr, boundary_check=(0,))\n        output *= weight\n\n    if bias_ptr is not None:\n        bias_block_ptr = tl.make_block_ptr(\n            bias_ptr,\n            shape=(y_size, 1),\n            strides=(1, y_size),\n            offsets=(group * group_size, 0),\n            block_shape=(group_block_size, 1),\n            order=(0, 1),\n        )\n        bias = tl.load(bias_block_ptr, boundary_check=(0,))\n        output += bias\n\n    if require_group_boundary_check | require_x_boundary_check:\n        tl.store(output_block_ptr, output.to(dtype), boundary_check=(0, 1))\n    else:\n        tl.store(output_block_ptr, output.to(dtype))\n\n    tl.store(rstd_block_ptr, rstd.to(dtype))\n    tl.store(mean_block_ptr, mean.to(dtype))", "encoded": [29, 325, 221, 225, 62, 94, 105, 226, 62, 94, 105, 227, 62, 94, 105, 228, 62, 94, 105, 229, 62, 60, 105, 230, 62, 60, 105, 231, 62, 60, 105, 232, 62, 60, 105, 233, 62, 94, 105, 234, 62, 94, 105, 235, 62, 139, 105, 88, 62, 6, 105, 236, 62, 6, 105, 237, 62, 6, 105, 238, 62, 6, 105, 239, 62, 6, 161, 62, 32, -1, 240, 178, 157, 221, 326, 161, 32, 241, 178, 240, 48, 232, 32, 242, 178, 240, 205, 232, 32, 243, 178, 229, 222, 231, 32, 244, 178, 241, 222, 232, 222, 243, 32, 245, 178, 244, 73, 242, 222, 243, 32, 246, 178, 200, 221, 225, 73, 245, 105, 120, 178, 221, 229, 105, 231, 161, 105, 247, 178, 221, 231, 105, 327, 161, 105, 248, 178, 221, 326, 105, 326, 161, 105, 249, 178, 221, 236, 105, 237, 161, 105, 250, 178, 221, 327, 105, 326, 161, 161, 32, 251, 178, 200, 221, 226, 73, 245, 105, 120, 178, 221, 229, 105, 231, 161, 105, 247, 178, 221, 231, 105, 327, 161, 105, 248, 178, 221, 326, 105, 326, 161, 105, 249, 178, 221, 236, 105, 237, 161, 105, 250, 178, 221, 327, 105, 326, 161, 161, 32, 252, 178, 200, 221, 227, 73, 241, 222, 232, 105, 120, 178, 221, 229, 105, 161, 105, 247, 178, 221, 327, 105, 161, 105, 248, 178, 221, 242, 105, 161, 105, 249, 178, 221, 327, 105, 161, 105, 250, 178, 221, 326, 105, 161, 161, 32, 253, 178, 200, 221, 228, 73, 241, 222, 232, 105, 120, 178, 221, 229, 105, 161, 105, 247, 178, 221, 327, 105, 161, 105, 248, 178, 221, 242, 105, 161, 105, 249, 178, 221, 327, 105, 161, 105, 250, 178, 221, 326, 105, 161, 161, 32, 167, 238, 143, 239, 62, 32, 254, 178, 56, 221, 251, 105, 255, 178, 221, 326, 105, 327, 161, 105, 256, 178, 328, 161, 32, 257, 178, 202, 221, 113, 221, 254, 42, 243, 105, 221, 327, 105, 236, 222, 237, 161, 161, 105, 327, 161, 32, 258, 178, 74, 221, 326, 105, 236, 161, 1, 229, 32, 259, 178, 74, 221, 326, 105, 237, 161, 1, 231, 32, 260, 178, 258, 209, 62, 105, 184, 27, 159, 259, 209, 184, 105, 62, 27, 32, 261, 178, 187, 221, 260, 105, 254, 4, 257, 105, 326, 161, 32, 170, 32, 30, 62, 32, 254, 178, 56, 221, 251, 161, 32, 257, 178, 202, 221, 113, 221, 254, 42, 243, 105, 221, 327, 105, 236, 222, 237, 161, 161, 105, 327, 161, 32, 261, 178, 254, 4, 257, 32, 55, 32, 262, 178, 202, 221, 113, 221, 261, 222, 261, 42, 243, 105, 221, 327, 105, 236, 222, 237, 161, 161, 105, 327, 161, 32, 263, 178, 61, 221, 262, 73, 235, 161, 32, 264, 178, 261, 222, 263, 32, 167, 233, 107, 63, 184, 62, 32, 265, 178, 200, 221, 233, 105, 120, 178, 221, 230, 105, 327, 161, 105, 247, 178, 221, 327, 105, 230, 161, 105, 248, 178, 221, 242, 222, 229, 105, 326, 161, 105, 249, 178, 221, 236, 105, 327, 161, 105, 250, 178, 221, 326, 105, 327, 161, 161, 32, 266, 178, 56, 221, 265, 105, 255, 178, 221, 326, 105, 161, 161, 32, 264, 24, 266, 32, 170, 32, 167, 234, 107, 63, 184, 62, 32, 267, 178, 200, 221, 234, 105, 120, 178, 221, 230, 105, 327, 161, 105, 247, 178, 221, 327, 105, 230, 161, 105, 248, 178, 221, 242, 222, 229, 105, 326, 161, 105, 249, 178, 221, 236, 105, 327, 161, 105, 250, 178, 221, 326, 105, 327, 161, 161, 32, 268, 178, 56, 221, 267, 105, 255, 178, 221, 326, 105, 161, 161, 32, 264, 158, 268, 32, 170, 32, 167, 238, 143, 239, 62, 32, 10, 221, 246, 105, 264, 80, 269, 221, 88, 161, 105, 255, 178, 221, 326, 105, 327, 161, 161, 32, 170, 32, 30, 62, 32, 10, 221, 246, 105, 264, 80, 269, 221, 88, 161, 161, 32, 55, 32, 10, 221, 252, 105, 263, 80, 269, 221, 88, 161, 161, 32, 10, 221, 253, 105, 257, 80, 269, 221, 88, 161, 161, 32, 3, 32]}, {"code": "def backward(\n    grad_input_ptr: tl.tensor,\n    grad_weight_staging_ptr: tl.tensor,\n    grad_bias_staging_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    group_size: tl.int32,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    num_groups: tl.int32,\n    weight_ptr: tl.tensor,\n    rstd_ptr: tl.tensor,\n    mean_ptr: tl.tensor,\n    dtype: tl.constexpr,\n    group_block_size: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_group_boundary_check: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    batch = pid // num_groups\n    group = pid % num_groups\n    num_elements = group_size * x_size\n    batch_offset = batch * num_groups * num_elements\n    group_offset = batch_offset + group * num_elements\n    grad_input_block_ptr = tl.make_block_ptr(\n        grad_input_ptr + group_offset,\n        shape=(group_size, x_size),\n        strides=(x_size, 1),\n        offsets=(0, 0),\n        block_shape=(group_block_size, x_block_size),\n        order=(1, 0),\n    )\n    grad_output_block_ptr = tl.make_block_ptr(\n        grad_output_ptr + group_offset,\n        shape=(group_size, x_size),\n        strides=(x_size, 1),\n        offsets=(0, 0),\n        block_shape=(group_block_size, x_block_size),\n        order=(1, 0),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr + group_offset,\n        shape=(group_size, x_size),\n        strides=(x_size, 1),\n        offsets=(0, 0),\n        block_shape=(group_block_size, x_block_size),\n        order=(1, 0),\n    )\n    rstd_block_ptr = tl.make_block_ptr(\n        rstd_ptr + batch * num_groups,\n        shape=(group_size,),\n        strides=(1,),\n        offsets=(group,),\n        block_shape=(1,),\n        order=(0,),\n    )\n    mean_block_ptr = tl.make_block_ptr(\n        mean_ptr + batch * num_groups,\n        shape=(group_size,),\n        strides=(1,),\n        offsets=(group,),\n        block_shape=(1,),\n        order=(0,),\n    )\n\n    rstd = tl.load(rstd_block_ptr)\n    mean = tl.load(mean_block_ptr)\n\n    if require_group_boundary_check | require_x_boundary_check:\n        grad_output = tl.load(grad_output_block_ptr, boundary_check=(0, 1))\n    else:\n        grad_output = tl.load(grad_output_block_ptr)\n\n    if weight_ptr is not None:\n        weight_block_ptr = tl.make_block_ptr(\n            weight_ptr,\n            shape=(y_size, 1),\n            strides=(1, y_size),\n            offsets=(group * group_size, 0),\n            block_shape=(group_block_size, 1),\n            order=(0, 1),\n        )\n        weight = tl.load(weight_block_ptr, boundary_check=(0,))\n        grad_norm = weight * grad_output\n    else:\n        grad_norm = grad_output\n\n    if require_group_boundary_check | require_x_boundary_check:\n        input = tl.load(input_block_ptr, boundary_check=(0, 1))\n        group_condition = tl.arange(0, group_block_size) < group_size\n        x_condition = tl.arange(0, x_block_size) < x_size\n        condition = group_condition[:, None] & x_condition[None, :]\n        centered_mean = tl.where(condition, input - mean, 0)\n        grad_std = tl.sum(\n            tl.view(\n                grad_norm * centered_mean, (1, group_block_size * x_block_size)\n            ),\n            1,\n        )\n        grad_var = grad_std * -(0.5 * rstd * rstd * rstd) / (x_size * group_size)\n        grad_distance = 2 * centered_mean * grad_var\n        grad_centered_mean = tl.where(\n            condition, grad_norm * rstd + grad_distance, 0\n        )\n        grad_mean = (\n            -tl.sum(\n                tl.view(grad_centered_mean, (1, group_block_size * x_block_size)), 1\n            )\n            / num_elements\n        )\n        grad_input = grad_centered_mean + grad_mean\n        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(0, 1))\n    else:\n        input = tl.load(input_block_ptr)\n        centered_mean = input - mean\n        grad_std = tl.sum(\n            tl.view(\n                grad_norm * centered_mean, (1, group_block_size * x_block_size)\n            ),\n            1,\n        )\n        grad_var = grad_std * -(0.5 * rstd * rstd * rstd) / (x_size * group_size)\n        grad_distance = 2 * centered_mean * grad_var\n        grad_centered_mean = grad_norm * rstd + grad_distance\n        grad_mean = (\n            -tl.sum(\n                tl.view(grad_centered_mean, (1, group_block_size * x_block_size)), 1\n            )\n            / num_elements\n        )\n        grad_input = grad_centered_mean + grad_mean\n        tl.store(grad_input_block_ptr, grad_input.to(dtype))\n\n    if grad_weight_staging_ptr is not None:\n        norm = centered_mean * rstd\n        grad_weight = tl.sum(norm * grad_output, 1)\n        offset = batch * y_size + group * group_size\n        grad_weight_staging_block_ptr = tl.make_block_ptr(\n            grad_weight_staging_ptr + offset,\n            shape=(group_size,),\n            strides=(1,),\n            offsets=(0,),\n            block_shape=(group_block_size,),\n            order=(0,),\n        )\n\n        if require_group_boundary_check:\n            tl.store(\n                grad_weight_staging_block_ptr,\n                grad_weight.to(dtype),\n                boundary_check=(0,),\n            )\n        else:\n            tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype))\n\n    if grad_bias_staging_ptr is not None:\n        grad_bias = tl.sum(grad_output, 1)\n        offset = batch * y_size + group * group_size\n        grad_bias_staging_block_ptr = tl.make_block_ptr(\n            grad_bias_staging_ptr + offset,\n            shape=(group_size,),\n            strides=(1,),\n            offsets=(0,),\n            block_shape=(group_block_size,),\n            order=(0,),\n        )\n\n        if require_group_boundary_check:\n            tl.store(\n                grad_bias_staging_block_ptr,\n                grad_bias.to(dtype),\n                boundary_check=(0,),\n            )\n        else:\n            tl.store(grad_bias_staging_block_ptr, grad_bias.to(dtype))", "encoded": [29, 325, 221, 225, 61, 94, 105, 226, 61, 94, 105, 227, 61, 94, 105, 228, 61, 94, 105, 229, 61, 94, 105, 230, 61, 224, 105, 231, 61, 224, 105, 232, 61, 224, 105, 233, 61, 224, 105, 234, 61, 94, 105, 235, 61, 94, 105, 236, 61, 94, 105, 88, 61, 6, 105, 237, 61, 6, 105, 238, 61, 6, 105, 239, 61, 6, 105, 240, 61, 6, 161, 61, 32, -1, 241, 178, 157, 221, 326, 161, 32, 242, 178, 241, 48, 233, 32, 243, 178, 241, 205, 233, 32, 244, 178, 230, 222, 232, 32, 245, 178, 242, 222, 233, 222, 244, 32, 246, 178, 245, 73, 243, 222, 244, 32, 247, 178, 200, 221, 225, 73, 246, 105, 120, 178, 221, 230, 105, 232, 161, 105, 248, 178, 221, 232, 105, 327, 161, 105, 249, 178, 221, 326, 105, 326, 161, 105, 250, 178, 221, 237, 105, 238, 161, 105, 251, 178, 221, 327, 105, 326, 161, 161, 32, 252, 178, 200, 221, 228, 73, 246, 105, 120, 178, 221, 230, 105, 232, 161, 105, 248, 178, 221, 232, 105, 327, 161, 105, 249, 178, 221, 326, 105, 326, 161, 105, 250, 178, 221, 237, 105, 238, 161, 105, 251, 178, 221, 327, 105, 326, 161, 161, 32, 253, 178, 200, 221, 229, 73, 246, 105, 120, 178, 221, 230, 105, 232, 161, 105, 248, 178, 221, 232, 105, 327, 161, 105, 249, 178, 221, 326, 105, 326, 161, 105, 250, 178, 221, 237, 105, 238, 161, 105, 251, 178, 221, 327, 105, 326, 161, 161, 32, 254, 178, 200, 221, 235, 73, 242, 222, 233, 105, 120, 178, 221, 230, 105, 161, 105, 248, 178, 221, 327, 105, 161, 105, 249, 178, 221, 243, 105, 161, 105, 250, 178, 221, 327, 105, 161, 105, 251, 178, 221, 326, 105, 161, 161, 32, 255, 178, 200, 221, 236, 73, 242, 222, 233, 105, 120, 178, 221, 230, 105, 161, 105, 248, 178, 221, 327, 105, 161, 105, 249, 178, 221, 243, 105, 161, 105, 250, 178, 221, 327, 105, 161, 105, 251, 178, 221, 326, 105, 161, 161, 32, 256, 178, 54, 221, 254, 161, 32, 257, 178, 54, 221, 255, 161, 32, 167, 239, 143, 240, 61, 32, 258, 178, 54, 221, 252, 105, 259, 178, 221, 326, 105, 327, 161, 161, 32, 170, 32, 30, 61, 32, 258, 178, 54, 221, 252, 161, 32, 55, 32, 167, 234, 107, 62, 184, 61, 32, 260, 178, 200, 221, 234, 105, 120, 178, 221, 231, 105, 327, 161, 105, 248, 178, 221, 327, 105, 231, 161, 105, 249, 178, 221, 243, 222, 230, 105, 326, 161, 105, 250, 178, 221, 237, 105, 327, 161, 105, 251, 178, 221, 326, 105, 327, 161, 161, 32, 261, 178, 54, 221, 260, 105, 259, 178, 221, 326, 105, 161, 161, 32, 262, 178, 261, 222, 258, 32, 170, 32, 30, 61, 32, 262, 178, 258, 32, 55, 32, 167, 239, 143, 240, 61, 32, 263, 178, 54, 221, 253, 105, 259, 178, 221, 326, 105, 327, 161, 161, 32, 264, 178, 74, 221, 326, 105, 237, 161, 1, 230, 32, 265, 178, 74, 221, 326, 105, 238, 161, 1, 232, 32, 266, 178, 264, 209, 61, 105, 184, 27, 159, 265, 209, 184, 105, 61, 27, 32, 267, 178, 187, 221, 266, 105, 263, 4, 257, 105, 326, 161, 32, 268, 178, 202, 221, 113, 221, 262, 222, 267, 105, 221, 327, 105, 237, 222, 238, 161, 161, 105, 327, 161, 32, 269, 178, 268, 222, 4, 221, 328, 222, 256, 222, 256, 222, 256, 161, 42, 221, 232, 222, 230, 161, 32, 270, 178, 329, 222, 267, 222, 269, 32, 271, 178, 187, 221, 266, 105, 262, 222, 256, 73, 270, 105, 326, 161, 32, 272, 178, 4, 202, 221, 113, 221, 271, 105, 221, 327, 105, 237, 222, 238, 161, 161, 105, 327, 161, 42, 244, 32, 273, 178, 271, 73, 272, 32, 10, 221, 247, 105, 273, 80, 274, 221, 88, 161, 105, 259, 178, 221, 326, 105, 327, 161, 161, 32, 170, 32, 30, 61, 32, 263, 178, 54, 221, 253, 161, 32, 267, 178, 263, 4, 257, 32, 268, 178, 202, 221, 113, 221, 262, 222, 267, 105, 221, 327, 105, 237, 222, 238, 161, 161, 105, 327, 161, 32, 269, 178, 268, 222, 4, 221, 328, 222, 256, 222, 256, 222, 256, 161, 42, 221, 232, 222, 230, 161, 32, 270, 178, 329, 222, 267, 222, 269, 32, 271, 178, 262, 222, 256, 73, 270, 32, 272, 178, 4, 202, 221, 113, 221, 271, 105, 221, 327, 105, 237, 222, 238, 161, 161, 105, 327, 161, 42, 244, 32, 273, 178, 271, 73, 272, 32, 10, 221, 247, 105, 273, 80, 274, 221, 88, 161, 161, 32, 55, 32, 167, 226, 107, 62, 184, 61, 32, 275, 178, 267, 222, 256, 32, 276, 178, 202, 221, 275, 222, 258, 105, 327, 161, 32, 277, 178, 242, 222, 231, 73, 243, 222, 230, 32, 278, 178, 200, 221, 226, 73, 277, 105, 120, 178, 221, 230, 105, 161, 105, 248, 178, 221, 327, 105, 161, 105, 249, 178, 221, 326, 105, 161, 105, 250, 178, 221, 237, 105, 161, 105, 251, 178, 221, 326, 105, 161, 161, 32, 167, 239, 61, 32, 10, 221, 278, 105, 276, 80, 274, 221, 88, 161, 105, 259, 178, 221, 326, 105, 161, 161, 32, 170, 32, 30, 61, 32, 10, 221, 278, 105, 276, 80, 274, 221, 88, 161, 161, 32, 55, 32, 170, 32, 167, 227, 107, 62, 184, 61, 32, 279, 178, 202, 221, 258, 105, 327, 161, 32, 277, 178, 242, 222, 231, 73, 243, 222, 230, 32, 280, 178, 200, 221, 227, 73, 277, 105, 120, 178, 221, 230, 105, 161, 105, 248, 178, 221, 327, 105, 161, 105, 249, 178, 221, 326, 105, 161, 105, 250, 178, 221, 237, 105, 161, 105, 251, 178, 221, 326, 105, 161, 161, 32, 167, 239, 61, 32, 10, 221, 280, 105, 279, 80, 274, 221, 88, 161, 105, 259, 178, 221, 326, 105, 161, 161, 32, 170, 32, 30, 61, 32, 10, 221, 280, 105, 279, 80, 274, 221, 88, 161, 161, 32, 55, 32, 170, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    rstd_ptr: tl.tensor,\n    mean_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    weight_ptr: tl.tensor,\n    bias_ptr: tl.tensor,\n    eps: tl.float32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(y_size, x_size),\n        strides=(x_size, 1),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    rstd_block_ptr = tl.make_block_ptr(\n        rstd_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n    mean_block_ptr = tl.make_block_ptr(\n        mean_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr,\n        shape=(y_size, x_size),\n        strides=(x_size, 1),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n\n    if require_x_boundary_check:\n        input = tl.load(input_block_ptr, boundary_check=(1,), padding_option=\"zero\")\n        mean = tl.sum(input / x_size, 1)\n        condition = tl.arange(0, x_block_size) < x_size\n        centered_mean = tl.where(condition, input - mean, 0)\n    else:\n        input = tl.load(input_block_ptr)\n        mean = tl.sum(input / x_size, 1)\n        centered_mean = input - mean\n\n    var = tl.sum(centered_mean * centered_mean / x_size, 1)\n    rstd = tl.math.rsqrt(var + eps)\n    output = centered_mean * rstd\n\n    if weight_ptr is not None:\n        weight_block_ptr = tl.make_block_ptr(\n            weight_ptr,\n            shape=(x_size,),\n            strides=(1,),\n            offsets=(0,),\n            block_shape=(x_block_size,),\n            order=(0,),\n        )\n\n        if require_x_boundary_check:\n            weight = tl.load(weight_block_ptr, boundary_check=(0,))\n        else:\n            weight = tl.load(weight_block_ptr)\n\n        output *= weight\n\n    if bias_ptr is not None:\n        bias_block_ptr = tl.make_block_ptr(\n            bias_ptr,\n            shape=(x_size,),\n            strides=(1,),\n            offsets=(0,),\n            block_shape=(x_block_size,),\n            order=(0,),\n        )\n\n        if require_x_boundary_check:\n            bias = tl.load(bias_block_ptr, boundary_check=(0,))\n        else:\n            bias = tl.load(bias_block_ptr)\n\n        output += bias\n\n    if require_x_boundary_check:\n        tl.store(output_block_ptr, output.to(dtype), boundary_check=(1,))\n    else:\n        tl.store(output_block_ptr, output.to(dtype))\n\n    tl.store(rstd_block_ptr, rstd.to(dtype))\n    tl.store(mean_block_ptr, mean.to(dtype))", "encoded": [29, 325, 221, 225, 62, 94, 105, 226, 62, 94, 105, 227, 62, 94, 105, 228, 62, 94, 105, 229, 62, 61, 105, 230, 62, 61, 105, 231, 62, 94, 105, 232, 62, 94, 105, 233, 62, 139, 105, 88, 62, 6, 105, 234, 62, 6, 105, 235, 62, 6, 161, 62, 32, -1, 236, 178, 157, 221, 326, 161, 32, 237, 178, 200, 221, 225, 105, 120, 178, 221, 229, 105, 230, 161, 105, 238, 178, 221, 230, 105, 327, 161, 105, 239, 178, 221, 236, 105, 326, 161, 105, 240, 178, 221, 327, 105, 234, 161, 105, 241, 178, 221, 327, 105, 326, 161, 161, 32, 242, 178, 200, 221, 226, 105, 120, 178, 221, 229, 105, 161, 105, 238, 178, 221, 327, 105, 161, 105, 239, 178, 221, 236, 105, 161, 105, 240, 178, 221, 327, 105, 161, 105, 241, 178, 221, 326, 105, 161, 161, 32, 243, 178, 200, 221, 227, 105, 120, 178, 221, 229, 105, 161, 105, 238, 178, 221, 327, 105, 161, 105, 239, 178, 221, 236, 105, 161, 105, 240, 178, 221, 327, 105, 161, 105, 241, 178, 221, 326, 105, 161, 161, 32, 244, 178, 200, 221, 228, 105, 120, 178, 221, 229, 105, 230, 161, 105, 238, 178, 221, 230, 105, 327, 161, 105, 239, 178, 221, 236, 105, 326, 161, 105, 240, 178, 221, 327, 105, 234, 161, 105, 241, 178, 221, 327, 105, 326, 161, 161, 32, 167, 235, 62, 32, 245, 178, 56, 221, 244, 105, 246, 178, 221, 327, 105, 161, 105, 247, 178, 328, 161, 32, 248, 178, 202, 221, 245, 42, 230, 105, 327, 161, 32, 249, 178, 74, 221, 326, 105, 234, 161, 1, 230, 32, 250, 178, 187, 221, 249, 105, 245, 4, 248, 105, 326, 161, 32, 170, 32, 30, 62, 32, 245, 178, 56, 221, 244, 161, 32, 248, 178, 202, 221, 245, 42, 230, 105, 327, 161, 32, 250, 178, 245, 4, 248, 32, 55, 32, 251, 178, 202, 221, 250, 222, 250, 42, 230, 105, 327, 161, 32, 252, 178, 60, 221, 251, 73, 233, 161, 32, 253, 178, 250, 222, 252, 32, 167, 231, 107, 63, 184, 62, 32, 254, 178, 200, 221, 231, 105, 120, 178, 221, 230, 105, 161, 105, 238, 178, 221, 327, 105, 161, 105, 239, 178, 221, 326, 105, 161, 105, 240, 178, 221, 234, 105, 161, 105, 241, 178, 221, 326, 105, 161, 161, 32, 167, 235, 62, 32, 255, 178, 56, 221, 254, 105, 246, 178, 221, 326, 105, 161, 161, 32, 170, 32, 30, 62, 32, 255, 178, 56, 221, 254, 161, 32, 55, 32, 253, 24, 255, 32, 170, 32, 167, 232, 107, 63, 184, 62, 32, 256, 178, 200, 221, 232, 105, 120, 178, 221, 230, 105, 161, 105, 238, 178, 221, 327, 105, 161, 105, 239, 178, 221, 326, 105, 161, 105, 240, 178, 221, 234, 105, 161, 105, 241, 178, 221, 326, 105, 161, 161, 32, 167, 235, 62, 32, 257, 178, 56, 221, 256, 105, 246, 178, 221, 326, 105, 161, 161, 32, 170, 32, 30, 62, 32, 257, 178, 56, 221, 256, 161, 32, 55, 32, 253, 158, 257, 32, 170, 32, 167, 235, 62, 32, 10, 221, 237, 105, 253, 80, 258, 221, 88, 161, 105, 246, 178, 221, 327, 105, 161, 161, 32, 170, 32, 30, 62, 32, 10, 221, 237, 105, 253, 80, 258, 221, 88, 161, 161, 32, 55, 32, 10, 221, 242, 105, 252, 80, 258, 221, 88, 161, 161, 32, 10, 221, 243, 105, 248, 80, 258, 221, 88, 161, 161, 32, 3, 32]}, {"code": "def backward(\n    grad_input_ptr: tl.tensor,\n    grad_weight_staging_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    weight_ptr: tl.tensor,\n    rstd_ptr: tl.tensor,\n    mean_ptr: tl.tensor,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    grad_input_block_ptr = tl.make_block_ptr(\n        grad_input_ptr,\n        shape=(y_size, x_size),\n        strides=(x_size, 1),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    grad_output_block_ptr = tl.make_block_ptr(\n        grad_output_ptr,\n        shape=(y_size, x_size),\n        strides=(x_size, 1),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr,\n        shape=(y_size, x_size),\n        strides=(x_size, 1),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    rstd_block_ptr = tl.make_block_ptr(\n        rstd_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n    mean_block_ptr = tl.make_block_ptr(\n        mean_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n\n    if require_x_boundary_check:\n        grad_output = tl.load(\n            grad_output_block_ptr, boundary_check=(1,), padding_option=\"zero\"\n        )\n        input = tl.load(input_block_ptr, boundary_check=(1,), padding_option=\"zero\")\n    else:\n        grad_output = tl.load(grad_output_block_ptr)\n        input = tl.load(input_block_ptr)\n\n    rstd = tl.load(rstd_block_ptr)\n    mean = tl.load(mean_block_ptr)\n    centered_mean = input - mean\n\n    if weight_ptr is not None:\n        weight_block_ptr = tl.make_block_ptr(\n            weight_ptr,\n            shape=(1, x_size),\n            strides=(x_size, 1),\n            offsets=(0, 0),\n            block_shape=(1, x_block_size),\n            order=(1, 0),\n        )\n\n        if require_x_boundary_check:\n            weight = tl.load(weight_block_ptr, boundary_check=(1,))\n        else:\n            weight = tl.load(weight_block_ptr)\n\n        grad_norm = weight * grad_output\n    else:\n        grad_norm = grad_output\n\n    grad_std = tl.sum(grad_norm * centered_mean, 1)\n    grad_var = grad_std * -(0.5 * rstd * rstd * rstd) / x_size\n    grad_distance = 2 * centered_mean * grad_var\n    grad_centered_mean = grad_norm * rstd + grad_distance\n    grad_mean = -tl.sum(grad_centered_mean, 1) / x_size\n    grad_input = grad_centered_mean + grad_mean\n\n    if require_x_boundary_check:\n        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1,))\n    else:\n        tl.store(grad_input_block_ptr, grad_input.to(dtype))\n\n    if grad_weight_staging_ptr is not None:\n        grad_weight_staging_block_ptr = tl.make_block_ptr(\n            grad_weight_staging_ptr,\n            shape=(y_size, x_size),\n            strides=(x_size, 1),\n            offsets=(y_offset, 0),\n            block_shape=(1, x_block_size),\n            order=(1, 0),\n        )\n\n        norm = centered_mean * rstd\n        grad_weight = norm * grad_output\n\n        if require_x_boundary_check:\n            tl.store(\n                grad_weight_staging_block_ptr,\n                grad_weight.to(dtype),\n                boundary_check=(1,),\n            )\n        else:\n            tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype))", "encoded": [29, 325, 221, 225, 61, 94, 105, 226, 61, 94, 105, 227, 61, 94, 105, 228, 61, 94, 105, 229, 61, 224, 105, 230, 61, 224, 105, 231, 61, 94, 105, 232, 61, 94, 105, 233, 61, 94, 105, 88, 61, 6, 105, 234, 61, 6, 105, 235, 61, 6, 161, 61, 32, -1, 236, 178, 157, 221, 326, 161, 32, 237, 178, 200, 221, 225, 105, 120, 178, 221, 229, 105, 230, 161, 105, 238, 178, 221, 230, 105, 327, 161, 105, 239, 178, 221, 236, 105, 326, 161, 105, 240, 178, 221, 327, 105, 234, 161, 105, 241, 178, 221, 327, 105, 326, 161, 161, 32, 242, 178, 200, 221, 227, 105, 120, 178, 221, 229, 105, 230, 161, 105, 238, 178, 221, 230, 105, 327, 161, 105, 239, 178, 221, 236, 105, 326, 161, 105, 240, 178, 221, 327, 105, 234, 161, 105, 241, 178, 221, 327, 105, 326, 161, 161, 32, 243, 178, 200, 221, 228, 105, 120, 178, 221, 229, 105, 230, 161, 105, 238, 178, 221, 230, 105, 327, 161, 105, 239, 178, 221, 236, 105, 326, 161, 105, 240, 178, 221, 327, 105, 234, 161, 105, 241, 178, 221, 327, 105, 326, 161, 161, 32, 244, 178, 200, 221, 232, 105, 120, 178, 221, 229, 105, 161, 105, 238, 178, 221, 327, 105, 161, 105, 239, 178, 221, 236, 105, 161, 105, 240, 178, 221, 327, 105, 161, 105, 241, 178, 221, 326, 105, 161, 161, 32, 245, 178, 200, 221, 233, 105, 120, 178, 221, 229, 105, 161, 105, 238, 178, 221, 327, 105, 161, 105, 239, 178, 221, 236, 105, 161, 105, 240, 178, 221, 327, 105, 161, 105, 241, 178, 221, 326, 105, 161, 161, 32, 167, 235, 61, 32, 246, 178, 54, 221, 242, 105, 247, 178, 221, 327, 105, 161, 105, 248, 178, 328, 161, 32, 249, 178, 54, 221, 243, 105, 247, 178, 221, 327, 105, 161, 105, 248, 178, 328, 161, 32, 170, 32, 30, 61, 32, 246, 178, 54, 221, 242, 161, 32, 249, 178, 54, 221, 243, 161, 32, 55, 32, 250, 178, 54, 221, 244, 161, 32, 251, 178, 54, 221, 245, 161, 32, 252, 178, 249, 4, 251, 32, 167, 231, 107, 62, 184, 61, 32, 253, 178, 200, 221, 231, 105, 120, 178, 221, 327, 105, 230, 161, 105, 238, 178, 221, 230, 105, 327, 161, 105, 239, 178, 221, 326, 105, 326, 161, 105, 240, 178, 221, 327, 105, 234, 161, 105, 241, 178, 221, 327, 105, 326, 161, 161, 32, 167, 235, 61, 32, 254, 178, 54, 221, 253, 105, 247, 178, 221, 327, 105, 161, 161, 32, 170, 32, 30, 61, 32, 254, 178, 54, 221, 253, 161, 32, 55, 32, 255, 178, 254, 222, 246, 32, 170, 32, 30, 61, 32, 255, 178, 246, 32, 55, 32, 256, 178, 202, 221, 255, 222, 252, 105, 327, 161, 32, 257, 178, 256, 222, 4, 221, 329, 222, 250, 222, 250, 222, 250, 161, 42, 230, 32, 258, 178, 330, 222, 252, 222, 257, 32, 259, 178, 255, 222, 250, 73, 258, 32, 260, 178, 4, 202, 221, 259, 105, 327, 161, 42, 230, 32, 261, 178, 259, 73, 260, 32, 167, 235, 61, 32, 10, 221, 237, 105, 261, 80, 262, 221, 88, 161, 105, 247, 178, 221, 327, 105, 161, 161, 32, 170, 32, 30, 61, 32, 10, 221, 237, 105, 261, 80, 262, 221, 88, 161, 161, 32, 55, 32, 167, 226, 107, 62, 184, 61, 32, 263, 178, 200, 221, 226, 105, 120, 178, 221, 229, 105, 230, 161, 105, 238, 178, 221, 230, 105, 327, 161, 105, 239, 178, 221, 236, 105, 326, 161, 105, 240, 178, 221, 327, 105, 234, 161, 105, 241, 178, 221, 327, 105, 326, 161, 161, 32, 264, 178, 252, 222, 250, 32, 265, 178, 264, 222, 246, 32, 167, 235, 61, 32, 10, 221, 263, 105, 265, 80, 262, 221, 88, 161, 105, 247, 178, 221, 327, 105, 161, 161, 32, 170, 32, 30, 61, 32, 10, 221, 263, 105, 265, 80, 262, 221, 88, 161, 161, 32, 55, 32, 170, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    weight_ptr: tl.tensor,\n    bias_ptr: tl.tensor,\n    m_size: tl.int32,\n    n_size: tl.int32,\n    k_size: tl.int32,\n    input_batch_stride: tl.int32,\n    input_m_stride: tl.int32,\n    input_k_stride: tl.int32,\n    weight_n_stride: tl.int32,\n    weight_k_stride: tl.int32,\n    use_accelerator: tl.constexpr,\n    dtype: tl.constexpr,\n    m_block_size: tl.constexpr,\n    n_block_size: tl.constexpr,\n    k_block_size: tl.constexpr,\n    require_m_boundary_check: tl.constexpr,\n    require_n_boundary_check: tl.constexpr,\n    require_k_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_m_blocks = tl.cdiv(m_size, m_block_size)\n    num_n_blocks = tl.cdiv(n_size, n_block_size)\n    num_blocks = num_m_blocks * num_n_blocks\n    batch = pid // num_blocks\n    block = pid % num_blocks\n    m_block = block // num_n_blocks\n    n_block = block % num_n_blocks\n    m_offset = m_block * m_block_size\n    n_offset = n_block * n_block_size\n\n    output = language.Linear.forward(\n        input_ptr + batch * input_batch_stride,\n        weight_ptr,\n        bias_ptr,\n        m_size,\n        n_size,\n        k_size,\n        input_m_stride,\n        input_k_stride,\n        weight_n_stride,\n        weight_k_stride,\n        m_offset,\n        n_offset,\n        use_accelerator,\n        m_block_size,\n        n_block_size,\n        k_block_size,\n        require_m_boundary_check,\n        require_n_boundary_check,\n        require_k_boundary_check,\n        dtype,\n    )\n\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr + batch * m_size * n_size,\n        shape=(m_size, n_size),\n        strides=(n_size, 1),\n        offsets=(m_offset, n_offset),\n        block_shape=(m_block_size, n_block_size),\n        order=(1, 0),\n    )\n    if require_m_boundary_check | require_n_boundary_check:\n        tl.store(output_block_ptr, output, boundary_check=(0, 1))\n    else:\n        tl.store(output_block_ptr, output)", "encoded": [29, 325, 221, 225, 62, 94, 105, 226, 62, 94, 105, 227, 62, 94, 105, 228, 62, 94, 105, 229, 62, 61, 105, 230, 62, 61, 105, 231, 62, 61, 105, 232, 62, 61, 105, 233, 62, 61, 105, 234, 62, 61, 105, 235, 62, 61, 105, 236, 62, 61, 105, 237, 62, 6, 105, 88, 62, 6, 105, 238, 62, 6, 105, 239, 62, 6, 105, 240, 62, 6, 105, 241, 62, 6, 105, 242, 62, 6, 105, 243, 62, 6, 161, 62, 32, -1, 244, 178, 157, 221, 326, 161, 32, 245, 178, 64, 221, 229, 105, 238, 161, 32, 246, 178, 64, 221, 230, 105, 239, 161, 32, 247, 178, 245, 222, 246, 32, 248, 178, 244, 48, 247, 32, 249, 178, 244, 205, 247, 32, 250, 178, 249, 48, 246, 32, 251, 178, 249, 205, 246, 32, 252, 178, 250, 222, 238, 32, 253, 178, 251, 222, 239, 32, 254, 178, 155, 80, 255, 80, 325, 221, 226, 73, 248, 222, 232, 105, 227, 105, 228, 105, 229, 105, 230, 105, 231, 105, 233, 105, 234, 105, 235, 105, 236, 105, 252, 105, 253, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 88, 161, 32, 256, 178, 200, 221, 225, 73, 248, 222, 229, 222, 230, 105, 120, 178, 221, 229, 105, 230, 161, 105, 257, 178, 221, 230, 105, 327, 161, 105, 258, 178, 221, 252, 105, 253, 161, 105, 259, 178, 221, 238, 105, 239, 161, 105, 260, 178, 221, 327, 105, 326, 161, 161, 32, 167, 241, 143, 242, 62, 32, 10, 221, 256, 105, 254, 105, 261, 178, 221, 326, 105, 327, 161, 161, 32, 170, 32, 30, 62, 32, 10, 221, 256, 105, 254, 161, 32, 55, 32, 3, 32]}, {"code": "def backward(\n    grad_input_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    weight_ptr: tl.tensor,\n    m_size: tl.int32,\n    n_size: tl.int32,\n    k_size: tl.int32,\n    input_m_stride: tl.int32,\n    input_k_stride: tl.int32,\n    weight_n_stride: tl.int32,\n    weight_k_stride: tl.int32,\n    use_accelerator: tl.constexpr,\n    dtype: tl.constexpr,\n    m_block_size: tl.constexpr,\n    n_block_size: tl.constexpr,\n    k_block_size: tl.constexpr,\n    require_m_boundary_check: tl.constexpr,\n    require_n_boundary_check: tl.constexpr,\n    require_k_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_m_blocks = tl.cdiv(m_size, m_block_size)\n    num_k_blocks = tl.cdiv(k_size, k_block_size)\n    num_blocks = num_m_blocks * num_k_blocks\n    batch = pid // num_blocks\n    block = pid % num_blocks\n    m_block = block // num_k_blocks\n    k_block = block % num_k_blocks\n    m_offset = m_block * m_block_size\n    k_offset = k_block * k_block_size\n\n    grad_input = language.Linear.backward(\n        grad_output_ptr + batch * m_size * n_size,\n        weight_ptr,\n        m_size,\n        n_size,\n        k_size,\n        weight_n_stride,\n        weight_k_stride,\n        m_offset,\n        k_offset,\n        use_accelerator,\n        m_block_size,\n        n_block_size,\n        k_block_size,\n        require_m_boundary_check,\n        require_n_boundary_check,\n        require_k_boundary_check,\n        dtype,\n    )\n\n    grad_input_block_ptr = tl.make_block_ptr(\n        grad_input_ptr + batch * m_size * k_size,\n        shape=(m_size, k_size),\n        strides=(input_m_stride, input_k_stride),\n        offsets=(m_offset, k_offset),\n        block_shape=(m_block_size, k_block_size),\n        order=(1, 0),\n    )\n\n    if require_m_boundary_check | require_k_boundary_check:\n        tl.store(grad_input_block_ptr, grad_input, boundary_check=(0, 1))\n    else:\n        tl.store(grad_input_block_ptr, grad_input)", "encoded": [29, 325, 221, 225, 61, 94, 105, 226, 61, 94, 105, 227, 61, 94, 105, 228, 61, 224, 105, 229, 61, 224, 105, 230, 61, 224, 105, 231, 61, 224, 105, 232, 61, 224, 105, 233, 61, 224, 105, 234, 61, 224, 105, 235, 61, 6, 105, 88, 61, 6, 105, 236, 61, 6, 105, 237, 61, 6, 105, 238, 61, 6, 105, 239, 61, 6, 105, 240, 61, 6, 105, 241, 61, 6, 161, 61, 32, -1, 242, 178, 157, 221, 326, 161, 32, 243, 178, 63, 221, 228, 105, 236, 161, 32, 244, 178, 63, 221, 230, 105, 238, 161, 32, 245, 178, 243, 222, 244, 32, 246, 178, 242, 48, 245, 32, 247, 178, 242, 205, 245, 32, 248, 178, 247, 48, 244, 32, 249, 178, 247, 205, 244, 32, 250, 178, 248, 222, 236, 32, 251, 178, 249, 222, 238, 32, 252, 178, 155, 80, 253, 80, 325, 221, 226, 73, 246, 222, 228, 222, 229, 105, 227, 105, 228, 105, 229, 105, 230, 105, 233, 105, 234, 105, 250, 105, 251, 105, 235, 105, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 88, 161, 32, 254, 178, 200, 221, 225, 73, 246, 222, 228, 222, 230, 105, 120, 178, 221, 228, 105, 230, 161, 105, 255, 178, 221, 231, 105, 232, 161, 105, 256, 178, 221, 250, 105, 251, 161, 105, 257, 178, 221, 236, 105, 238, 161, 105, 258, 178, 221, 327, 105, 326, 161, 161, 32, 167, 239, 143, 241, 61, 32, 10, 221, 254, 105, 252, 105, 259, 178, 221, 326, 105, 327, 161, 161, 32, 170, 32, 30, 61, 32, 10, 221, 254, 105, 252, 161, 32, 55, 32, 3, 32]}, {"code": "def backward_weight(\n    grad_weight_staging_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    m_size: tl.int32,\n    n_size: tl.int32,\n    k_size: tl.int32,\n    input_batch_stride: tl.int32,\n    input_m_stride: tl.int32,\n    input_k_stride: tl.int32,\n    use_accelerator: tl.constexpr,\n    dtype: tl.constexpr,\n    m_block_size: tl.constexpr,\n    n_block_size: tl.constexpr,\n    k_block_size: tl.constexpr,\n    require_m_boundary_check: tl.constexpr,\n    require_n_boundary_check: tl.constexpr,\n    require_k_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_n_blocks = tl.cdiv(n_size, n_block_size)\n    num_k_blocks = tl.cdiv(k_size, k_block_size)\n    num_blocks = num_n_blocks * num_k_blocks\n    batch = pid // num_blocks\n    block = pid % num_blocks\n    n_block = block // num_k_blocks\n    k_block = block % num_k_blocks\n    n_offset = n_block * n_block_size\n    k_offset = k_block * k_block_size\n\n    grad_weight = language.Linear.backward_weight(\n        grad_output_ptr + batch * m_size * n_size,\n        input_ptr + batch * input_batch_stride,\n        m_size,\n        n_size,\n        k_size,\n        input_m_stride,\n        input_k_stride,\n        n_offset,\n        k_offset,\n        use_accelerator,\n        m_block_size,\n        n_block_size,\n        k_block_size,\n        require_m_boundary_check,\n        require_n_boundary_check,\n        require_k_boundary_check,\n        dtype,\n    )\n\n    grad_weight_staging_block_ptr = tl.make_block_ptr(\n        grad_weight_staging_ptr + batch * n_size * k_size,\n        shape=(n_size, k_size),\n        strides=(k_size, 1),\n        offsets=(n_offset, k_offset),\n        block_shape=(n_block_size, k_block_size),\n        order=(1, 0),\n    )\n\n    if require_n_boundary_check | require_k_boundary_check:\n        tl.store(grad_weight_staging_block_ptr, grad_weight, boundary_check=(0, 1))\n    else:\n        tl.store(grad_weight_staging_block_ptr, grad_weight)", "encoded": [29, 325, 221, 225, 62, 94, 105, 226, 62, 94, 105, 227, 62, 94, 105, 228, 62, 61, 105, 229, 62, 61, 105, 230, 62, 61, 105, 231, 62, 61, 105, 232, 62, 61, 105, 233, 62, 61, 105, 234, 62, 6, 105, 88, 62, 6, 105, 235, 62, 6, 105, 236, 62, 6, 105, 237, 62, 6, 105, 238, 62, 6, 105, 239, 62, 6, 105, 240, 62, 6, 161, 62, 32, -1, 241, 178, 157, 221, 326, 161, 32, 242, 178, 64, 221, 229, 105, 236, 161, 32, 243, 178, 64, 221, 230, 105, 237, 161, 32, 244, 178, 242, 222, 243, 32, 245, 178, 241, 48, 244, 32, 246, 178, 241, 205, 244, 32, 247, 178, 246, 48, 243, 32, 248, 178, 246, 205, 243, 32, 249, 178, 247, 222, 236, 32, 250, 178, 248, 222, 237, 32, 251, 178, 155, 80, 252, 80, 325, 221, 226, 73, 245, 222, 228, 222, 229, 105, 227, 73, 245, 222, 231, 105, 228, 105, 229, 105, 230, 105, 232, 105, 233, 105, 249, 105, 250, 105, 234, 105, 235, 105, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 88, 161, 32, 253, 178, 200, 221, 225, 73, 245, 222, 229, 222, 230, 105, 120, 178, 221, 229, 105, 230, 161, 105, 254, 178, 221, 230, 105, 327, 161, 105, 255, 178, 221, 249, 105, 250, 161, 105, 256, 178, 221, 236, 105, 237, 161, 105, 257, 178, 221, 327, 105, 326, 161, 161, 32, 167, 239, 143, 240, 62, 32, 10, 221, 253, 105, 251, 105, 258, 178, 221, 326, 105, 327, 161, 161, 32, 170, 32, 30, 62, 32, 10, 221, 253, 105, 251, 161, 32, 55, 32, 3, 32]}, {"code": "def backward_bias(\n    grad_bias_staging_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    m_size: tl.int32,\n    n_size: tl.int32,\n    dtype: tl.constexpr,\n    m_block_size: tl.constexpr,\n    require_m_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    batch = pid // n_size\n    n_offset = pid % n_size\n    grad_bias = language.Linear.backward_bias(\n        grad_output_ptr + batch * m_size * n_size,\n        m_size,\n        n_size,\n        n_offset,\n        m_block_size,\n        require_m_boundary_check,\n        dtype,\n    )\n\n    grad_bias_staging_block_ptr = tl.make_block_ptr(\n        grad_bias_staging_ptr + batch * n_size,\n        shape=(n_size,),\n        strides=(1,),\n        offsets=(n_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n    tl.store(grad_bias_staging_block_ptr, grad_bias)", "encoded": [29, 325, 221, 225, 61, 94, 105, 226, 61, 94, 105, 227, 61, 224, 105, 228, 61, 224, 105, 88, 61, 6, 105, 229, 61, 6, 105, 230, 61, 6, 161, 61, 32, -1, 231, 178, 157, 221, 326, 161, 32, 232, 178, 231, 48, 228, 32, 233, 178, 231, 205, 228, 32, 234, 178, 155, 80, 235, 80, 325, 221, 226, 73, 232, 222, 227, 222, 228, 105, 227, 105, 228, 105, 233, 105, 229, 105, 230, 105, 88, 161, 32, 236, 178, 200, 221, 225, 73, 232, 222, 228, 105, 120, 178, 221, 228, 105, 161, 105, 237, 178, 221, 327, 105, 161, 105, 238, 178, 221, 233, 105, 161, 105, 239, 178, 221, 327, 105, 161, 105, 240, 178, 221, 326, 105, 161, 161, 32, 10, 221, 236, 105, 234, 161, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    mask_ptr: tl.tensor,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    mask_block_ptr = tl.make_block_ptr(\n        mask_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n\n    if require_x_boundary_check:\n        input = tl.load(input_block_ptr, boundary_check=(1,))\n        mask = tl.load(mask_block_ptr, boundary_check=(1,))\n        condition = tl.arange(0, x_block_size) < x_size\n        mask = tl.where(condition, mask, 1)\n    else:\n        input = tl.load(input_block_ptr)\n        mask = tl.load(mask_block_ptr)\n\n    input = tl.where(mask > language.eps, float(\"-inf\"), input)\n    max = tl.max(input, 1)\n    numerator = tl.math.fast_expf(input - max)\n    output = numerator / tl.sum(numerator)\n\n    if require_x_boundary_check:\n        tl.store(output_block_ptr, output.to(dtype), boundary_check=(1,))\n    else:\n        tl.store(output_block_ptr, output.to(dtype))", "encoded": [29, 326, 222, 226, 62, 94, 105, 227, 62, 94, 105, 228, 62, 61, 105, 229, 62, 61, 105, 230, 62, 61, 105, 231, 62, 61, 105, 232, 62, 94, 105, 88, 62, 6, 105, 233, 62, 6, 105, 234, 62, 6, 162, 62, 32, -1, 235, 179, 158, 222, 327, 162, 32, 236, 179, 201, 222, 226, 105, 120, 179, 222, 228, 105, 229, 162, 105, 237, 179, 222, 230, 105, 231, 162, 105, 238, 179, 222, 235, 105, 327, 162, 105, 239, 179, 222, 328, 105, 233, 162, 105, 240, 179, 222, 328, 105, 327, 162, 162, 32, 241, 179, 201, 222, 227, 105, 120, 179, 222, 228, 105, 229, 162, 105, 237, 179, 222, 230, 105, 231, 162, 105, 238, 179, 222, 235, 105, 327, 162, 105, 239, 179, 222, 328, 105, 233, 162, 105, 240, 179, 222, 328, 105, 327, 162, 162, 32, 242, 179, 201, 222, 232, 105, 120, 179, 222, 228, 105, 229, 162, 105, 237, 179, 222, 230, 105, 231, 162, 105, 238, 179, 222, 235, 105, 327, 162, 105, 239, 179, 222, 328, 105, 233, 162, 105, 240, 179, 222, 328, 105, 327, 162, 162, 32, 168, 234, 62, 32, 243, 179, 56, 222, 241, 105, 244, 179, 222, 328, 105, 162, 162, 32, 245, 179, 56, 222, 242, 105, 244, 179, 222, 328, 105, 162, 162, 32, 246, 179, 74, 222, 327, 105, 233, 162, 1, 229, 32, 245, 179, 188, 222, 246, 105, 245, 105, 328, 162, 32, 171, 32, 30, 62, 32, 243, 179, 56, 222, 241, 162, 32, 245, 179, 56, 222, 242, 162, 32, 55, 32, 243, 179, 188, 222, 245, 121, 156, 80, 247, 105, 248, 222, 329, 162, 105, 243, 162, 32, 49, 179, 12, 222, 243, 105, 328, 162, 32, 249, 179, 125, 222, 243, 4, 49, 162, 32, 250, 179, 249, 42, 203, 222, 249, 162, 32, 168, 234, 62, 32, 10, 222, 236, 105, 250, 80, 251, 222, 88, 162, 105, 244, 179, 222, 328, 105, 162, 162, 32, 171, 32, 30, 62, 32, 10, 222, 236, 105, 250, 80, 251, 222, 88, 162, 162, 32, 55, 32, 3, 32]}, {"code": "def backward(\n    grad_input_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    output_ptr: tl.tensor,\n    delta_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    grad_input_block_ptr = tl.make_block_ptr(\n        grad_input_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    grad_output_block_ptr = tl.make_block_ptr(\n        grad_output_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    delta_block_ptr = tl.make_block_ptr(\n        delta_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n\n    if require_x_boundary_check:\n        output = tl.load(output_block_ptr, boundary_check=(1,))\n        grad_output = tl.load(grad_output_block_ptr, boundary_check=(1,))\n    else:\n        output = tl.load(output_block_ptr)\n        grad_output = tl.load(grad_output_block_ptr)\n\n    delta = tl.load(delta_block_ptr)\n    grad_input = output * (grad_output - delta)\n\n    if require_x_boundary_check:\n        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1,))\n    else:\n        tl.store(grad_input_block_ptr, grad_input.to(dtype))", "encoded": [29, 326, 222, 226, 61, 94, 105, 227, 61, 94, 105, 228, 61, 94, 105, 229, 61, 94, 105, 230, 61, 225, 105, 231, 61, 225, 105, 232, 61, 225, 105, 233, 61, 225, 105, 88, 61, 6, 105, 234, 61, 6, 105, 235, 61, 6, 162, 61, 32, -1, 236, 179, 158, 222, 327, 162, 32, 237, 179, 201, 222, 226, 105, 120, 179, 222, 230, 105, 231, 162, 105, 238, 179, 222, 232, 105, 233, 162, 105, 239, 179, 222, 236, 105, 327, 162, 105, 240, 179, 222, 328, 105, 234, 162, 105, 241, 179, 222, 328, 105, 327, 162, 162, 32, 242, 179, 201, 222, 227, 105, 120, 179, 222, 230, 105, 231, 162, 105, 238, 179, 222, 232, 105, 233, 162, 105, 239, 179, 222, 236, 105, 327, 162, 105, 240, 179, 222, 328, 105, 234, 162, 105, 241, 179, 222, 328, 105, 327, 162, 162, 32, 243, 179, 201, 222, 228, 105, 120, 179, 222, 230, 105, 231, 162, 105, 238, 179, 222, 232, 105, 233, 162, 105, 239, 179, 222, 236, 105, 327, 162, 105, 240, 179, 222, 328, 105, 234, 162, 105, 241, 179, 222, 328, 105, 327, 162, 162, 32, 244, 179, 201, 222, 229, 105, 120, 179, 222, 230, 105, 162, 105, 238, 179, 222, 328, 105, 162, 105, 239, 179, 222, 236, 105, 162, 105, 240, 179, 222, 328, 105, 162, 105, 241, 179, 222, 327, 105, 162, 162, 32, 168, 235, 61, 32, 245, 179, 54, 222, 243, 105, 246, 179, 222, 328, 105, 162, 162, 32, 247, 179, 54, 222, 242, 105, 246, 179, 222, 328, 105, 162, 162, 32, 171, 32, 30, 61, 32, 245, 179, 54, 222, 243, 162, 32, 247, 179, 54, 222, 242, 162, 32, 55, 32, 248, 179, 54, 222, 244, 162, 32, 249, 179, 245, 223, 222, 247, 4, 248, 162, 32, 168, 235, 61, 32, 10, 222, 237, 105, 249, 80, 250, 222, 88, 162, 105, 246, 179, 222, 328, 105, 162, 162, 32, 171, 32, 30, 61, 32, 10, 222, 237, 105, 249, 80, 250, 222, 88, 162, 162, 32, 55, 32, 3, 32]}, {"code": "def backward_delta(\n    delta_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    output_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    delta_block_ptr = tl.make_block_ptr(\n        delta_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n    grad_output_block_ptr = tl.make_block_ptr(\n        grad_output_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n\n    if require_x_boundary_check:\n        grad_output = tl.load(\n            grad_output_block_ptr, boundary_check=(1,), padding_option=\"zero\"\n        )\n        output = tl.load(output_block_ptr, boundary_check=(1,))\n    else:\n        grad_output = tl.load(grad_output_block_ptr)\n        output = tl.load(output_block_ptr)\n\n    delta = tl.sum(grad_output * output, 1)\n    tl.store(delta_block_ptr, delta.to(dtype))", "encoded": [29, 326, 222, 226, 62, 94, 105, 227, 62, 94, 105, 228, 62, 94, 105, 229, 62, 61, 105, 230, 62, 61, 105, 231, 62, 61, 105, 232, 62, 61, 105, 88, 62, 6, 105, 233, 62, 6, 105, 234, 62, 6, 162, 62, 32, -1, 235, 179, 158, 222, 327, 162, 32, 236, 179, 201, 222, 226, 105, 120, 179, 222, 229, 105, 162, 105, 237, 179, 222, 328, 105, 162, 105, 238, 179, 222, 235, 105, 162, 105, 239, 179, 222, 328, 105, 162, 105, 240, 179, 222, 327, 105, 162, 162, 32, 241, 179, 201, 222, 227, 105, 120, 179, 222, 229, 105, 230, 162, 105, 237, 179, 222, 231, 105, 232, 162, 105, 238, 179, 222, 235, 105, 327, 162, 105, 239, 179, 222, 328, 105, 233, 162, 105, 240, 179, 222, 328, 105, 327, 162, 162, 32, 242, 179, 201, 222, 228, 105, 120, 179, 222, 229, 105, 230, 162, 105, 237, 179, 222, 231, 105, 232, 162, 105, 238, 179, 222, 235, 105, 327, 162, 105, 239, 179, 222, 328, 105, 233, 162, 105, 240, 179, 222, 328, 105, 327, 162, 162, 32, 168, 234, 62, 32, 243, 179, 56, 222, 241, 105, 244, 179, 222, 328, 105, 162, 105, 245, 179, 329, 162, 32, 246, 179, 56, 222, 242, 105, 244, 179, 222, 328, 105, 162, 162, 32, 171, 32, 30, 62, 32, 243, 179, 56, 222, 241, 162, 32, 246, 179, 56, 222, 242, 162, 32, 55, 32, 247, 179, 203, 222, 243, 223, 246, 105, 328, 162, 32, 10, 222, 236, 105, 247, 80, 248, 222, 88, 162, 162, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n\n    output = language.Mean.forward(\n        input_ptr,\n        y_size,\n        x_size,\n        y_stride,\n        x_stride,\n        y_offset,\n        dtype,\n        x_block_size,\n        require_x_boundary_check,\n    )\n    tl.store(output_block_ptr, output)", "encoded": [29, 326, 222, 226, 61, 94, 105, 227, 61, 94, 105, 228, 61, 225, 105, 229, 61, 225, 105, 230, 61, 225, 105, 231, 61, 225, 105, 88, 61, 6, 105, 232, 61, 6, 105, 233, 61, 6, 162, 61, 32, -1, 234, 179, 158, 222, 327, 162, 32, 235, 179, 201, 222, 226, 105, 120, 179, 222, 228, 105, 162, 105, 236, 179, 222, 328, 105, 162, 105, 237, 179, 222, 234, 105, 162, 105, 238, 179, 222, 328, 105, 162, 105, 239, 179, 222, 327, 105, 162, 162, 32, 240, 179, 156, 80, 241, 80, 326, 222, 227, 105, 228, 105, 229, 105, 230, 105, 231, 105, 234, 105, 88, 105, 232, 105, 233, 162, 32, 10, 222, 235, 105, 240, 162, 32, 3, 32]}, {"code": "def backward(\n    grad_input_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_x_blocks = tl.cdiv(x_size, x_block_size)\n    y_offset = pid // num_x_blocks\n    x = pid % num_x_blocks\n    x_offset = x * x_block_size\n\n    grad_input_block_ptr = tl.make_block_ptr(\n        grad_input_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, x_offset),\n        block_shape=(1, x_block_size),\n        order=(0, 1),\n    )\n\n    grad_input = language.Mean.backward(\n        grad_output_ptr, y_size, x_size, y_offset, dtype, x_block_size\n    )\n\n    if require_x_boundary_check:\n        tl.store(grad_input_block_ptr, grad_input, boundary_check=(1,))\n    else:\n        tl.store(grad_input_block_ptr, grad_input)", "encoded": [29, 326, 222, 226, 62, 94, 105, 227, 62, 94, 105, 228, 62, 61, 105, 229, 62, 61, 105, 230, 62, 61, 105, 231, 62, 61, 105, 88, 62, 6, 105, 232, 62, 6, 105, 233, 62, 6, 162, 62, 32, -1, 234, 179, 158, 222, 327, 162, 32, 235, 179, 64, 222, 229, 105, 232, 162, 32, 236, 179, 234, 48, 235, 32, 237, 179, 234, 206, 235, 32, 238, 179, 237, 223, 232, 32, 239, 179, 201, 222, 226, 105, 120, 179, 222, 228, 105, 229, 162, 105, 240, 179, 222, 230, 105, 231, 162, 105, 241, 179, 222, 236, 105, 238, 162, 105, 242, 179, 222, 328, 105, 232, 162, 105, 243, 179, 222, 327, 105, 328, 162, 162, 32, 244, 179, 156, 80, 245, 80, 326, 222, 227, 105, 228, 105, 229, 105, 236, 105, 88, 105, 232, 162, 32, 168, 233, 62, 32, 10, 222, 239, 105, 244, 105, 246, 179, 222, 328, 105, 162, 162, 32, 171, 32, 30, 62, 32, 10, 222, 239, 105, 244, 162, 32, 55, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    weight_ptr: tl.tensor,\n    num_batches: tl.int32,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    batch_stride: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    dtype: tl.constexpr,\n    y_block_size: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_y_boundary_check: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_y_blocks = tl.cdiv(y_size, y_block_size)\n    num_x_blocks = tl.cdiv(x_size, x_block_size)\n    num_blocks = num_y_blocks * num_x_blocks\n    batch_offset = pid // num_blocks\n    block = pid % num_blocks\n    y_block = block // num_x_blocks\n    x_block = block % num_x_blocks\n    y_offset = y_block * y_block_size\n    x_offset = x_block * x_block_size\n\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(num_batches, y_size, x_size),\n        strides=(batch_stride, y_stride, x_stride),\n        offsets=(batch_offset, y_offset, x_offset),\n        block_shape=(1, y_block_size, x_block_size),\n        order=(2, 1, 0),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr,\n        shape=(num_batches, y_size, x_size),\n        strides=(batch_stride, y_stride, x_stride),\n        offsets=(batch_offset, y_offset, x_offset),\n        block_shape=(1, y_block_size, x_block_size),\n        order=(2, 1, 0),\n    )\n    weight_block_ptr = tl.make_block_ptr(\n        weight_ptr,\n        shape=(y_size, 1),\n        strides=(1, 0),\n        offsets=(y_offset, 0),\n        block_shape=(y_block_size, 1),\n        order=(1, 0),\n    )\n\n    if require_y_boundary_check | require_x_boundary_check:\n        input = tl.load(input_block_ptr, boundary_check=(1, 2))\n    else:\n        input = tl.load(input_block_ptr)\n\n    if require_y_boundary_check:\n        weight = tl.load(weight_block_ptr, boundary_check=(0,))\n    else:\n        weight = tl.load(weight_block_ptr)\n\n    output = language.math.LeakyReLU.forward(input, weight)\n\n    if require_y_boundary_check | require_x_boundary_check:\n        tl.store(output_block_ptr, output.to(dtype), boundary_check=(1, 2))\n    else:\n        tl.store(output_block_ptr, output.to(dtype))", "encoded": [29, 326, 222, 226, 61, 94, 105, 227, 61, 94, 105, 228, 61, 94, 105, 229, 61, 225, 105, 230, 61, 225, 105, 231, 61, 225, 105, 232, 61, 225, 105, 233, 61, 225, 105, 234, 61, 225, 105, 88, 61, 6, 105, 235, 61, 6, 105, 236, 61, 6, 105, 237, 61, 6, 105, 238, 61, 6, 162, 61, 32, -1, 239, 179, 158, 222, 327, 162, 32, 240, 179, 63, 222, 230, 105, 235, 162, 32, 241, 179, 63, 222, 231, 105, 236, 162, 32, 242, 179, 240, 223, 241, 32, 243, 179, 239, 48, 242, 32, 244, 179, 239, 206, 242, 32, 245, 179, 244, 48, 241, 32, 246, 179, 244, 206, 241, 32, 247, 179, 245, 223, 235, 32, 248, 179, 246, 223, 236, 32, 249, 179, 201, 222, 226, 105, 120, 179, 222, 229, 105, 230, 105, 231, 162, 105, 250, 179, 222, 232, 105, 233, 105, 234, 162, 105, 251, 179, 222, 243, 105, 247, 105, 248, 162, 105, 252, 179, 222, 328, 105, 235, 105, 236, 162, 105, 253, 179, 222, 329, 105, 328, 105, 327, 162, 162, 32, 254, 179, 201, 222, 227, 105, 120, 179, 222, 229, 105, 230, 105, 231, 162, 105, 250, 179, 222, 232, 105, 233, 105, 234, 162, 105, 251, 179, 222, 243, 105, 247, 105, 248, 162, 105, 252, 179, 222, 328, 105, 235, 105, 236, 162, 105, 253, 179, 222, 329, 105, 328, 105, 327, 162, 162, 32, 255, 179, 201, 222, 228, 105, 120, 179, 222, 230, 105, 328, 162, 105, 250, 179, 222, 328, 105, 327, 162, 105, 251, 179, 222, 247, 105, 327, 162, 105, 252, 179, 222, 235, 105, 328, 162, 105, 253, 179, 222, 328, 105, 327, 162, 162, 32, 168, 237, 144, 238, 61, 32, 256, 179, 54, 222, 254, 105, 257, 179, 222, 328, 105, 329, 162, 162, 32, 171, 32, 30, 61, 32, 256, 179, 54, 222, 254, 162, 32, 55, 32, 168, 237, 61, 32, 258, 179, 54, 222, 255, 105, 257, 179, 222, 327, 105, 162, 162, 32, 171, 32, 30, 61, 32, 258, 179, 54, 222, 255, 162, 32, 55, 32, 259, 179, 156, 80, 260, 80, 261, 80, 326, 222, 256, 105, 258, 162, 32, 168, 237, 144, 238, 61, 32, 10, 222, 249, 105, 259, 80, 262, 222, 88, 162, 105, 257, 179, 222, 328, 105, 329, 162, 162, 32, 171, 32, 30, 61, 32, 10, 222, 249, 105, 259, 80, 262, 222, 88, 162, 162, 32, 55, 32, 3, 32]}, {"code": "def backward(\n    grad_input_ptr: tl.tensor,\n    grad_weight_staging_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    weight_ptr: tl.tensor,\n    num_batches: tl.int32,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    batch_stride: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    dtype: tl.constexpr,\n    y_block_size: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_y_boundary_check: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_y_blocks = tl.cdiv(y_size, y_block_size)\n    num_x_blocks = tl.cdiv(x_size, x_block_size)\n    num_blocks = num_y_blocks * num_x_blocks\n    batch_offset = pid // num_blocks\n    block = pid % num_blocks\n    y_block = block // num_x_blocks\n    x_block = block % num_x_blocks\n    y_offset = y_block * y_block_size\n    x_offset = x_block * x_block_size\n\n    grad_input_block_ptr = tl.make_block_ptr(\n        grad_input_ptr,\n        shape=(num_batches, y_size, x_size),\n        strides=(batch_stride, y_stride, x_stride),\n        offsets=(batch_offset, y_offset, x_offset),\n        block_shape=(1, y_block_size, x_block_size),\n        order=(2, 1, 0),\n    )\n    grad_weight_staging_block_ptr = tl.make_block_ptr(\n        grad_weight_staging_ptr,\n        shape=(num_batches, y_size, x_size),\n        strides=(batch_stride, y_stride, x_stride),\n        offsets=(batch_offset, y_offset, x_offset),\n        block_shape=(1, y_block_size, x_block_size),\n        order=(2, 1, 0),\n    )\n    grad_output_block_ptr = tl.make_block_ptr(\n        grad_output_ptr,\n        shape=(num_batches, y_size, x_size),\n        strides=(batch_stride, y_stride, x_stride),\n        offsets=(batch_offset, y_offset, x_offset),\n        block_shape=(1, y_block_size, x_block_size),\n        order=(2, 1, 0),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr,\n        shape=(num_batches, y_size, x_size),\n        strides=(batch_stride, y_stride, x_stride),\n        offsets=(batch_offset, y_offset, x_offset),\n        block_shape=(1, y_block_size, x_block_size),\n        order=(2, 1, 0),\n    )\n    weight_block_ptr = tl.make_block_ptr(\n        weight_ptr,\n        shape=(y_size, 1),\n        strides=(1, 0),\n        offsets=(y_offset, 0),\n        block_shape=(y_block_size, 1),\n        order=(1, 0),\n    )\n    if require_y_boundary_check | require_x_boundary_check:\n        input = tl.load(input_block_ptr, boundary_check=(1, 2))\n        grad_output = tl.load(grad_output_block_ptr, boundary_check=(1, 2))\n    else:\n        input = tl.load(input_block_ptr)\n        grad_output = tl.load(grad_output_block_ptr)\n\n    weight = tl.load(weight_block_ptr)\n    grad_input = language.math.LeakyReLU.backward(grad_output, input, weight)\n    grad_weight = grad_output * tl.where(input > 0, 0, input)\n\n    if require_y_boundary_check | require_x_boundary_check:\n        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1, 2))\n        tl.store(\n            grad_weight_staging_block_ptr,\n            grad_weight.to(dtype),\n            boundary_check=(1, 2),\n        )\n    else:\n        tl.store(grad_input_block_ptr, grad_input.to(dtype))\n        tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype))", "encoded": [29, 326, 222, 226, 62, 94, 105, 227, 62, 94, 105, 228, 62, 94, 105, 229, 62, 94, 105, 230, 62, 94, 105, 231, 62, 61, 105, 232, 62, 61, 105, 233, 62, 61, 105, 234, 62, 61, 105, 235, 62, 61, 105, 236, 62, 61, 105, 88, 62, 6, 105, 237, 62, 6, 105, 238, 62, 6, 105, 239, 62, 6, 105, 240, 62, 6, 162, 62, 32, -1, 241, 179, 158, 222, 327, 162, 32, 242, 179, 64, 222, 232, 105, 237, 162, 32, 243, 179, 64, 222, 233, 105, 238, 162, 32, 244, 179, 242, 223, 243, 32, 245, 179, 241, 48, 244, 32, 246, 179, 241, 206, 244, 32, 247, 179, 246, 48, 243, 32, 248, 179, 246, 206, 243, 32, 249, 179, 247, 223, 237, 32, 250, 179, 248, 223, 238, 32, 251, 179, 201, 222, 226, 105, 120, 179, 222, 231, 105, 232, 105, 233, 162, 105, 252, 179, 222, 234, 105, 235, 105, 236, 162, 105, 253, 179, 222, 245, 105, 249, 105, 250, 162, 105, 254, 179, 222, 328, 105, 237, 105, 238, 162, 105, 255, 179, 222, 329, 105, 328, 105, 327, 162, 162, 32, 256, 179, 201, 222, 227, 105, 120, 179, 222, 231, 105, 232, 105, 233, 162, 105, 252, 179, 222, 234, 105, 235, 105, 236, 162, 105, 253, 179, 222, 245, 105, 249, 105, 250, 162, 105, 254, 179, 222, 328, 105, 237, 105, 238, 162, 105, 255, 179, 222, 329, 105, 328, 105, 327, 162, 162, 32, 257, 179, 201, 222, 228, 105, 120, 179, 222, 231, 105, 232, 105, 233, 162, 105, 252, 179, 222, 234, 105, 235, 105, 236, 162, 105, 253, 179, 222, 245, 105, 249, 105, 250, 162, 105, 254, 179, 222, 328, 105, 237, 105, 238, 162, 105, 255, 179, 222, 329, 105, 328, 105, 327, 162, 162, 32, 258, 179, 201, 222, 229, 105, 120, 179, 222, 231, 105, 232, 105, 233, 162, 105, 252, 179, 222, 234, 105, 235, 105, 236, 162, 105, 253, 179, 222, 245, 105, 249, 105, 250, 162, 105, 254, 179, 222, 328, 105, 237, 105, 238, 162, 105, 255, 179, 222, 329, 105, 328, 105, 327, 162, 162, 32, 259, 179, 201, 222, 230, 105, 120, 179, 222, 232, 105, 328, 162, 105, 252, 179, 222, 328, 105, 327, 162, 105, 253, 179, 222, 249, 105, 327, 162, 105, 254, 179, 222, 237, 105, 328, 162, 105, 255, 179, 222, 328, 105, 327, 162, 162, 32, 168, 239, 144, 240, 62, 32, 260, 179, 56, 222, 258, 105, 261, 179, 222, 328, 105, 329, 162, 162, 32, 262, 179, 56, 222, 257, 105, 261, 179, 222, 328, 105, 329, 162, 162, 32, 171, 32, 30, 62, 32, 260, 179, 56, 222, 258, 162, 32, 262, 179, 56, 222, 257, 162, 32, 55, 32, 263, 179, 56, 222, 259, 162, 32, 264, 179, 156, 80, 265, 80, 266, 80, 326, 222, 262, 105, 260, 105, 263, 162, 32, 267, 179, 262, 223, 188, 222, 260, 121, 327, 105, 327, 105, 260, 162, 32, 168, 239, 144, 240, 62, 32, 10, 222, 251, 105, 264, 80, 268, 222, 88, 162, 105, 261, 179, 222, 328, 105, 329, 162, 162, 32, 10, 222, 256, 105, 267, 80, 268, 222, 88, 162, 105, 261, 179, 222, 328, 105, 329, 162, 162, 32, 171, 32, 30, 62, 32, 10, 222, 251, 105, 264, 80, 268, 222, 88, 162, 162, 32, 10, 222, 256, 105, 267, 80, 268, 222, 88, 162, 162, 32, 55, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    rms_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    partial_size: tl.constexpr,\n    weight_ptr: tl.tensor,\n    bias_ptr: tl.tensor,\n    eps: tl.float32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    rms_block_ptr = tl.make_block_ptr(\n        rms_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    weight_block_ptr = tl.make_block_ptr(\n        weight_ptr,\n        shape=(x_size,),\n        strides=(1,),\n        offsets=(0,),\n        block_shape=(x_block_size,),\n        order=(0,),\n    )\n\n    if require_x_boundary_check:\n        input = tl.load(input_block_ptr, boundary_check=(1,))\n    else:\n        input = tl.load(input_block_ptr)\n\n    if x_block_size != partial_size:\n        condition = tl.arange(0, x_block_size) < partial_size\n        partial_input = tl.where(condition, input, 0)\n    else:\n        partial_input = input\n\n    rms = tl.math.sqrt(tl.sum(partial_input * partial_input / partial_size, 1))\n    norm = input / (rms + eps)\n\n    if require_x_boundary_check:\n        weight = tl.load(weight_block_ptr, boundary_check=(0,))\n    else:\n        weight = tl.load(weight_block_ptr)\n\n    output = norm * weight\n\n    if bias_ptr is not None:\n        bias_block_ptr = tl.make_block_ptr(\n            bias_ptr,\n            shape=(1, x_size),\n            strides=(x_stride, 1),\n            offsets=(0, 0),\n            block_shape=(1, x_block_size),\n            order=(1, 0),\n        )\n\n        if require_x_boundary_check:\n            bias = tl.load(bias_block_ptr, boundary_check=(1,))\n        else:\n            bias = tl.load(bias_block_ptr)\n\n        output += bias\n\n    tl.store(rms_block_ptr, rms.to(dtype))\n\n    if require_x_boundary_check:\n        tl.store(output_block_ptr, output.to(dtype), boundary_check=(1,))\n    else:\n        tl.store(output_block_ptr, output.to(dtype))", "encoded": [29, 327, 223, 227, 61, 94, 105, 228, 61, 94, 105, 229, 61, 94, 105, 230, 61, 226, 105, 231, 61, 226, 105, 232, 61, 226, 105, 233, 61, 226, 105, 234, 61, 6, 105, 235, 61, 94, 105, 236, 61, 94, 105, 237, 61, 140, 105, 88, 61, 6, 105, 238, 61, 6, 105, 239, 61, 6, 163, 61, 32, -1, 240, 180, 159, 223, 328, 163, 32, 241, 180, 202, 223, 227, 105, 120, 180, 223, 230, 105, 231, 163, 105, 242, 180, 223, 232, 105, 233, 163, 105, 243, 180, 223, 240, 105, 328, 163, 105, 244, 180, 223, 329, 105, 238, 163, 105, 245, 180, 223, 329, 105, 328, 163, 163, 32, 246, 180, 202, 223, 228, 105, 120, 180, 223, 230, 105, 163, 105, 242, 180, 223, 329, 105, 163, 105, 243, 180, 223, 240, 105, 163, 105, 244, 180, 223, 329, 105, 163, 105, 245, 180, 223, 328, 105, 163, 163, 32, 247, 180, 202, 223, 229, 105, 120, 180, 223, 230, 105, 231, 163, 105, 242, 180, 223, 232, 105, 233, 163, 105, 243, 180, 223, 240, 105, 328, 163, 105, 244, 180, 223, 329, 105, 238, 163, 105, 245, 180, 223, 329, 105, 328, 163, 163, 32, 248, 180, 202, 223, 235, 105, 120, 180, 223, 231, 105, 163, 105, 242, 180, 223, 329, 105, 163, 105, 243, 180, 223, 328, 105, 163, 105, 244, 180, 223, 238, 105, 163, 105, 245, 180, 223, 328, 105, 163, 163, 32, 169, 239, 61, 32, 249, 180, 54, 223, 247, 105, 250, 180, 223, 329, 105, 163, 163, 32, 172, 32, 30, 61, 32, 249, 180, 54, 223, 247, 163, 32, 55, 32, 169, 238, 173, 234, 61, 32, 251, 180, 74, 223, 328, 105, 238, 163, 1, 234, 32, 252, 180, 189, 223, 251, 105, 249, 105, 328, 163, 32, 172, 32, 30, 61, 32, 252, 180, 249, 32, 55, 32, 253, 180, 153, 223, 204, 223, 252, 224, 252, 42, 234, 105, 329, 163, 163, 32, 254, 180, 249, 42, 223, 253, 73, 237, 163, 32, 169, 239, 61, 32, 255, 180, 54, 223, 248, 105, 250, 180, 223, 328, 105, 163, 163, 32, 172, 32, 30, 61, 32, 255, 180, 54, 223, 248, 163, 32, 55, 32, 256, 180, 254, 224, 255, 32, 169, 236, 107, 62, 186, 61, 32, 257, 180, 202, 223, 236, 105, 120, 180, 223, 329, 105, 231, 163, 105, 242, 180, 223, 233, 105, 329, 163, 105, 243, 180, 223, 328, 105, 328, 163, 105, 244, 180, 223, 329, 105, 238, 163, 105, 245, 180, 223, 329, 105, 328, 163, 163, 32, 169, 239, 61, 32, 258, 180, 54, 223, 257, 105, 250, 180, 223, 329, 105, 163, 163, 32, 172, 32, 30, 61, 32, 258, 180, 54, 223, 257, 163, 32, 55, 32, 256, 160, 258, 32, 172, 32, 10, 223, 246, 105, 253, 80, 259, 223, 88, 163, 163, 32, 169, 239, 61, 32, 10, 223, 241, 105, 256, 80, 259, 223, 88, 163, 105, 250, 180, 223, 329, 105, 163, 163, 32, 172, 32, 30, 61, 32, 10, 223, 241, 105, 256, 80, 259, 223, 88, 163, 163, 32, 55, 32, 3, 32]}, {"code": "def backward(\n    grad_input_ptr: tl.tensor,\n    grad_weight_staging: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    rms_ptr: tl.tensor,\n    partial_size: tl.constexpr,\n    weight_ptr: tl.tensor,\n    eps: tl.float32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    grad_input_block_ptr = tl.make_block_ptr(\n        grad_input_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    grad_weight_staging_block_ptr = tl.make_block_ptr(\n        grad_weight_staging,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    grad_output_block_ptr = tl.make_block_ptr(\n        grad_output_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    rms_block_ptr = tl.make_block_ptr(\n        rms_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n    weight_block_ptr = tl.make_block_ptr(\n        weight_ptr,\n        shape=(x_size,),\n        strides=(1,),\n        offsets=(0,),\n        block_shape=(x_block_size,),\n        order=(0,),\n    )\n\n    if require_x_boundary_check:\n        grad_output = tl.load(grad_output_block_ptr, boundary_check=(1,))\n        input = tl.load(input_block_ptr, boundary_check=(1,))\n    else:\n        grad_output = tl.load(grad_output_block_ptr)\n        input = tl.load(input_block_ptr)\n\n    rms = tl.load(rms_block_ptr)\n\n    if require_x_boundary_check:\n        weight = tl.load(weight_block_ptr, boundary_check=(0,))\n    else:\n        weight = tl.load(weight_block_ptr)\n\n    grad_norm = grad_output * weight\n    norm = input / (rms + eps)\n    grad_weight = grad_output * norm\n\n    if require_x_boundary_check:\n        tl.store(\n            grad_weight_staging_block_ptr,\n            grad_weight.to(dtype),\n            boundary_check=(1,),\n        )\n    else:\n        tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype))\n\n    grad_rms = grad_norm * -input / (rms * rms + eps)\n\n    if require_x_boundary_check:\n        condition = tl.arange(0, x_block_size) < x_size\n        grad_rms = tl.where(condition, grad_rms, 0.0)\n\n    grad_rms = tl.sum(grad_rms, 1)\n    grad_mean_square = grad_rms / (2 * rms)\n    grad_partial_input = 2 * input * grad_mean_square / partial_size\n\n    if x_block_size != partial_size:\n        condition = tl.arange(0, x_block_size) < partial_size\n        grad_partial_input = tl.where(condition, grad_partial_input, 0)\n\n    grad_input = (grad_norm / (rms + eps)) + grad_partial_input\n\n    if require_x_boundary_check:\n        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1,))\n    else:\n        tl.store(grad_input_block_ptr, grad_input.to(dtype))", "encoded": [29, 327, 223, 227, 62, 94, 105, 228, 62, 94, 105, 229, 62, 94, 105, 230, 62, 94, 105, 231, 62, 61, 105, 232, 62, 61, 105, 233, 62, 61, 105, 234, 62, 61, 105, 235, 62, 94, 105, 236, 62, 6, 105, 237, 62, 94, 105, 238, 62, 140, 105, 88, 62, 6, 105, 239, 62, 6, 105, 240, 62, 6, 163, 62, 32, -1, 241, 180, 159, 223, 328, 163, 32, 242, 180, 202, 223, 227, 105, 120, 180, 223, 231, 105, 232, 163, 105, 243, 180, 223, 233, 105, 234, 163, 105, 244, 180, 223, 241, 105, 328, 163, 105, 245, 180, 223, 329, 105, 239, 163, 105, 246, 180, 223, 329, 105, 328, 163, 163, 32, 247, 180, 202, 223, 228, 105, 120, 180, 223, 231, 105, 232, 163, 105, 243, 180, 223, 233, 105, 234, 163, 105, 244, 180, 223, 241, 105, 328, 163, 105, 245, 180, 223, 329, 105, 239, 163, 105, 246, 180, 223, 329, 105, 328, 163, 163, 32, 248, 180, 202, 223, 229, 105, 120, 180, 223, 231, 105, 232, 163, 105, 243, 180, 223, 233, 105, 234, 163, 105, 244, 180, 223, 241, 105, 328, 163, 105, 245, 180, 223, 329, 105, 239, 163, 105, 246, 180, 223, 329, 105, 328, 163, 163, 32, 249, 180, 202, 223, 230, 105, 120, 180, 223, 231, 105, 232, 163, 105, 243, 180, 223, 233, 105, 234, 163, 105, 244, 180, 223, 241, 105, 328, 163, 105, 245, 180, 223, 329, 105, 239, 163, 105, 246, 180, 223, 329, 105, 328, 163, 163, 32, 250, 180, 202, 223, 235, 105, 120, 180, 223, 231, 105, 163, 105, 243, 180, 223, 329, 105, 163, 105, 244, 180, 223, 241, 105, 163, 105, 245, 180, 223, 329, 105, 163, 105, 246, 180, 223, 328, 105, 163, 163, 32, 251, 180, 202, 223, 237, 105, 120, 180, 223, 232, 105, 163, 105, 243, 180, 223, 329, 105, 163, 105, 244, 180, 223, 328, 105, 163, 105, 245, 180, 223, 239, 105, 163, 105, 246, 180, 223, 328, 105, 163, 163, 32, 169, 240, 62, 32, 252, 180, 56, 223, 248, 105, 253, 180, 223, 329, 105, 163, 163, 32, 254, 180, 56, 223, 249, 105, 253, 180, 223, 329, 105, 163, 163, 32, 172, 32, 30, 62, 32, 252, 180, 56, 223, 248, 163, 32, 254, 180, 56, 223, 249, 163, 32, 55, 32, 255, 180, 56, 223, 250, 163, 32, 169, 240, 62, 32, 256, 180, 56, 223, 251, 105, 253, 180, 223, 328, 105, 163, 163, 32, 172, 32, 30, 62, 32, 256, 180, 56, 223, 251, 163, 32, 55, 32, 257, 180, 252, 224, 256, 32, 258, 180, 254, 42, 223, 255, 73, 238, 163, 32, 259, 180, 252, 224, 258, 32, 169, 240, 62, 32, 10, 223, 247, 105, 259, 80, 260, 223, 88, 163, 105, 253, 180, 223, 329, 105, 163, 163, 32, 172, 32, 30, 62, 32, 10, 223, 247, 105, 259, 80, 260, 223, 88, 163, 163, 32, 55, 32, 261, 180, 257, 224, 4, 254, 42, 223, 255, 224, 255, 73, 238, 163, 32, 169, 240, 62, 32, 262, 180, 74, 223, 328, 105, 239, 163, 1, 232, 32, 261, 180, 189, 223, 262, 105, 261, 105, 328, 163, 32, 172, 32, 261, 180, 204, 223, 261, 105, 329, 163, 32, 263, 180, 261, 42, 223, 330, 224, 255, 163, 32, 264, 180, 330, 224, 254, 224, 263, 42, 236, 32, 169, 239, 173, 236, 62, 32, 262, 180, 74, 223, 328, 105, 239, 163, 1, 236, 32, 264, 180, 189, 223, 262, 105, 264, 105, 328, 163, 32, 172, 32, 265, 180, 257, 42, 223, 255, 73, 238, 163, 73, 264, 32, 169, 240, 62, 32, 10, 223, 242, 105, 265, 80, 260, 223, 88, 163, 105, 253, 180, 223, 329, 105, 163, 163, 32, 172, 32, 30, 62, 32, 10, 223, 242, 105, 265, 80, 260, 223, 88, 163, 163, 32, 55, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    x_size: tl.int32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    x_offset = tl.program_id(0) * x_block_size\n\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(x_size,),\n        strides=(1,),\n        offsets=(x_offset,),\n        block_shape=(x_block_size,),\n        order=(0,),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr,\n        shape=(x_size,),\n        strides=(1,),\n        offsets=(x_offset,),\n        block_shape=(x_block_size,),\n        order=(0,),\n    )\n\n    if require_x_boundary_check:\n        input = tl.load(input_block_ptr, boundary_check=(0,))\n    else:\n        input = tl.load(input_block_ptr)\n\n    sigma = 1 / (1 + tl.math.fast_expf(-input.to(tl.float32)))\n    output = input * sigma\n\n    if require_x_boundary_check:\n        tl.store(output_block_ptr, output.to(dtype), boundary_check=(0,))\n    else:\n        tl.store(output_block_ptr, output.to(dtype))", "encoded": [29, 327, 223, 227, 61, 94, 105, 228, 61, 94, 105, 229, 61, 226, 105, 88, 61, 6, 105, 230, 61, 6, 105, 231, 61, 6, 163, 61, 32, -1, 232, 180, 159, 223, 328, 163, 224, 230, 32, 233, 180, 202, 223, 227, 105, 120, 180, 223, 229, 105, 163, 105, 234, 180, 223, 329, 105, 163, 105, 235, 180, 223, 232, 105, 163, 105, 236, 180, 223, 230, 105, 163, 105, 237, 180, 223, 328, 105, 163, 163, 32, 238, 180, 202, 223, 228, 105, 120, 180, 223, 229, 105, 163, 105, 234, 180, 223, 329, 105, 163, 105, 235, 180, 223, 232, 105, 163, 105, 236, 180, 223, 230, 105, 163, 105, 237, 180, 223, 328, 105, 163, 163, 32, 169, 231, 61, 32, 239, 180, 54, 223, 238, 105, 240, 180, 223, 328, 105, 163, 163, 32, 172, 32, 30, 61, 32, 239, 180, 54, 223, 238, 163, 32, 55, 32, 241, 180, 329, 42, 223, 329, 73, 125, 223, 4, 239, 80, 242, 223, 140, 163, 163, 163, 32, 243, 180, 239, 224, 241, 32, 169, 231, 61, 32, 10, 223, 233, 105, 243, 80, 242, 223, 88, 163, 105, 240, 180, 223, 328, 105, 163, 163, 32, 172, 32, 30, 61, 32, 10, 223, 233, 105, 243, 80, 242, 223, 88, 163, 163, 32, 55, 32, 3, 32]}, {"code": "def backward(\n    grad_input_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    x_size: tl.int32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    x_offset = tl.program_id(0) * x_block_size\n\n    grad_input_block_ptr = tl.make_block_ptr(\n        grad_input_ptr,\n        shape=(x_size,),\n        strides=(1,),\n        offsets=(x_offset,),\n        block_shape=(x_block_size,),\n        order=(0,),\n    )\n    grad_output_block_ptr = tl.make_block_ptr(\n        grad_output_ptr,\n        shape=(x_size,),\n        strides=(1,),\n        offsets=(x_offset,),\n        block_shape=(x_block_size,),\n        order=(0,),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr,\n        shape=(x_size,),\n        strides=(1,),\n        offsets=(x_offset,),\n        block_shape=(x_block_size,),\n        order=(0,),\n    )\n\n    if require_x_boundary_check:\n        grad_output = tl.load(grad_output_block_ptr, boundary_check=(0,))\n        input = tl.load(input_block_ptr, boundary_check=(0,))\n    else:\n        grad_output = tl.load(grad_output_block_ptr)\n        input = tl.load(input_block_ptr)\n\n    sigma = 1 / (1 + tl.math.fast_expf(-input.to(tl.float32)))\n    grad_input = grad_output * (sigma + input * sigma * (1 - sigma))\n\n    if require_x_boundary_check:\n        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(0,))\n    else:\n        tl.store(grad_input_block_ptr, grad_input.to(dtype))", "encoded": [29, 327, 223, 227, 62, 94, 105, 228, 62, 94, 105, 229, 62, 94, 105, 230, 62, 61, 105, 88, 62, 6, 105, 231, 62, 6, 105, 232, 62, 6, 163, 62, 32, -1, 233, 180, 159, 223, 328, 163, 224, 231, 32, 234, 180, 202, 223, 227, 105, 120, 180, 223, 230, 105, 163, 105, 235, 180, 223, 329, 105, 163, 105, 236, 180, 223, 233, 105, 163, 105, 237, 180, 223, 231, 105, 163, 105, 238, 180, 223, 328, 105, 163, 163, 32, 239, 180, 202, 223, 228, 105, 120, 180, 223, 230, 105, 163, 105, 235, 180, 223, 329, 105, 163, 105, 236, 180, 223, 233, 105, 163, 105, 237, 180, 223, 231, 105, 163, 105, 238, 180, 223, 328, 105, 163, 163, 32, 240, 180, 202, 223, 229, 105, 120, 180, 223, 230, 105, 163, 105, 235, 180, 223, 329, 105, 163, 105, 236, 180, 223, 233, 105, 163, 105, 237, 180, 223, 231, 105, 163, 105, 238, 180, 223, 328, 105, 163, 163, 32, 169, 232, 62, 32, 241, 180, 56, 223, 239, 105, 242, 180, 223, 328, 105, 163, 163, 32, 243, 180, 56, 223, 240, 105, 242, 180, 223, 328, 105, 163, 163, 32, 172, 32, 30, 62, 32, 241, 180, 56, 223, 239, 163, 32, 243, 180, 56, 223, 240, 163, 32, 55, 32, 244, 180, 329, 42, 223, 329, 73, 125, 223, 4, 243, 80, 245, 223, 140, 163, 163, 163, 32, 246, 180, 241, 224, 223, 244, 73, 243, 224, 244, 224, 223, 329, 4, 244, 163, 163, 32, 169, 232, 62, 32, 10, 223, 234, 105, 246, 80, 245, 223, 88, 163, 105, 242, 180, 223, 328, 105, 163, 163, 32, 172, 32, 30, 62, 32, 10, 223, 234, 105, 246, 80, 245, 223, 88, 163, 163, 32, 55, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n\n    max = tl.full((1, x_block_size), -float(\"inf\"), tl.float32)\n    sum = tl.zeros((1, x_block_size), tl.float32)\n\n    for x_offset in range(0, x_size, x_block_size):\n        if require_x_boundary_check:\n            input = tl.load(input_block_ptr, boundary_check=(1,))\n            condition = tl.arange(0, x_block_size) + x_offset < x_size\n            input = tl.where(condition, input, -float(\"inf\"))\n            peak = tl.where(condition, tl.maximum(max, input), 0)\n        else:\n            input = tl.load(input_block_ptr)\n            peak = tl.maximum(max, input)\n\n        sum = sum * tl.math.fast_expf(max - peak) + tl.math.fast_expf(input - peak)\n        max = peak\n        input_block_ptr = tl.advance(input_block_ptr, (0, x_block_size))\n\n    max, sum = tl.reduce((max, sum), 1, language.combine_softmax)\n\n    input_block_ptr = tl.make_block_ptr(\n        input_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n\n    for x_offset in range(0, x_size, x_block_size):\n        if require_x_boundary_check:\n            input = tl.load(input_block_ptr, boundary_check=(1,))\n        else:\n            input = tl.load(input_block_ptr)\n\n        output = tl.math.fast_expf(input - max) / sum\n\n        if require_x_boundary_check:\n            tl.store(output_block_ptr, output.to(dtype), boundary_check=(1,))\n        else:\n            tl.store(output_block_ptr, output.to(dtype))\n\n        output_block_ptr = tl.advance(output_block_ptr, (0, x_block_size))\n        input_block_ptr = tl.advance(input_block_ptr, (0, x_block_size))", "encoded": [29, 328, 224, 228, 61, 94, 105, 229, 61, 94, 105, 230, 61, 227, 105, 231, 61, 227, 105, 232, 61, 227, 105, 233, 61, 227, 105, 88, 61, 6, 105, 234, 61, 6, 105, 235, 61, 6, 163, 61, 32, -1, 236, 180, 159, 224, 329, 163, 32, 237, 180, 203, 224, 228, 105, 120, 180, 224, 230, 105, 231, 163, 105, 238, 180, 224, 232, 105, 233, 163, 105, 239, 180, 224, 236, 105, 329, 163, 105, 240, 180, 224, 330, 105, 234, 163, 105, 241, 180, 224, 330, 105, 329, 163, 163, 32, 242, 180, 203, 224, 229, 105, 120, 180, 224, 230, 105, 231, 163, 105, 238, 180, 224, 232, 105, 233, 163, 105, 239, 180, 224, 236, 105, 329, 163, 105, 240, 180, 224, 330, 105, 234, 163, 105, 241, 180, 224, 330, 105, 329, 163, 163, 32, 49, 180, 218, 224, 224, 330, 105, 234, 163, 105, 4, 243, 224, 331, 163, 105, 140, 163, 32, 101, 180, 164, 224, 224, 330, 105, 234, 163, 105, 140, 163, 32, 131, 244, 149, 5, 224, 329, 105, 231, 105, 234, 163, 61, 32, 169, 235, 61, 32, 245, 180, 54, 224, 242, 105, 246, 180, 224, 330, 105, 163, 163, 32, 247, 180, 74, 224, 329, 105, 234, 163, 73, 244, 1, 231, 32, 245, 180, 190, 224, 247, 105, 245, 105, 4, 243, 224, 331, 163, 163, 32, 248, 180, 190, 224, 247, 105, 178, 224, 49, 105, 245, 163, 105, 329, 163, 32, 172, 32, 30, 61, 32, 245, 180, 54, 224, 242, 163, 32, 248, 180, 178, 224, 49, 105, 245, 163, 32, 55, 32, 101, 180, 101, 225, 125, 224, 49, 4, 248, 163, 73, 125, 224, 245, 4, 248, 163, 32, 49, 180, 248, 32, 242, 180, 138, 224, 242, 105, 224, 329, 105, 234, 163, 163, 32, 76, 32, 49, 105, 101, 180, 185, 224, 224, 49, 105, 101, 163, 105, 330, 105, 157, 80, 249, 163, 32, 242, 180, 203, 224, 229, 105, 120, 180, 224, 230, 105, 231, 163, 105, 238, 180, 224, 232, 105, 233, 163, 105, 239, 180, 224, 236, 105, 329, 163, 105, 240, 180, 224, 330, 105, 234, 163, 105, 241, 180, 224, 330, 105, 329, 163, 163, 32, 131, 244, 149, 5, 224, 329, 105, 231, 105, 234, 163, 61, 32, 169, 235, 61, 32, 245, 180, 54, 224, 242, 105, 246, 180, 224, 330, 105, 163, 163, 32, 172, 32, 30, 61, 32, 245, 180, 54, 224, 242, 163, 32, 55, 32, 250, 180, 125, 224, 245, 4, 49, 163, 42, 101, 32, 169, 235, 61, 32, 10, 224, 237, 105, 250, 80, 251, 224, 88, 163, 105, 246, 180, 224, 330, 105, 163, 163, 32, 172, 32, 30, 61, 32, 10, 224, 237, 105, 250, 80, 251, 224, 88, 163, 163, 32, 55, 32, 237, 180, 138, 224, 237, 105, 224, 329, 105, 234, 163, 163, 32, 242, 180, 138, 224, 242, 105, 224, 329, 105, 234, 163, 163, 32, 76, 32, 3, 32]}, {"code": "def backward(\n    grad_input_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    output_ptr: tl.tensor,\n    delta_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    grad_input_block_ptr = tl.make_block_ptr(\n        grad_input_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    grad_output_block_ptr = tl.make_block_ptr(\n        grad_output_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    delta_block_ptr = tl.make_block_ptr(\n        delta_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n\n    for x_offset in range(0, x_size, x_block_size):\n        if require_x_boundary_check:\n            output = tl.load(output_block_ptr, boundary_check=(1,))\n            grad_output = tl.load(grad_output_block_ptr, boundary_check=(1,))\n        else:\n            output = tl.load(output_block_ptr)\n            grad_output = tl.load(grad_output_block_ptr)\n\n        delta = tl.load(delta_block_ptr)\n        grad_input = output * (grad_output - delta)\n\n        if require_x_boundary_check:\n            tl.store(\n                grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1,)\n            )\n        else:\n            tl.store(grad_input_block_ptr, grad_input.to(dtype))\n\n        grad_input_block_ptr = tl.advance(grad_input_block_ptr, (0, x_block_size))\n        grad_output_block_ptr = tl.advance(grad_output_block_ptr, (0, x_block_size))\n        output_block_ptr = tl.advance(output_block_ptr, (0, x_block_size))", "encoded": [29, 328, 224, 228, 62, 94, 105, 229, 62, 94, 105, 230, 62, 94, 105, 231, 62, 94, 105, 232, 62, 61, 105, 233, 62, 61, 105, 234, 62, 61, 105, 235, 62, 61, 105, 88, 62, 6, 105, 236, 62, 6, 105, 237, 62, 6, 163, 62, 32, -1, 238, 180, 159, 224, 329, 163, 32, 239, 180, 203, 224, 228, 105, 120, 180, 224, 232, 105, 233, 163, 105, 240, 180, 224, 234, 105, 235, 163, 105, 241, 180, 224, 238, 105, 329, 163, 105, 242, 180, 224, 330, 105, 236, 163, 105, 243, 180, 224, 330, 105, 329, 163, 163, 32, 244, 180, 203, 224, 229, 105, 120, 180, 224, 232, 105, 233, 163, 105, 240, 180, 224, 234, 105, 235, 163, 105, 241, 180, 224, 238, 105, 329, 163, 105, 242, 180, 224, 330, 105, 236, 163, 105, 243, 180, 224, 330, 105, 329, 163, 163, 32, 245, 180, 203, 224, 230, 105, 120, 180, 224, 232, 105, 233, 163, 105, 240, 180, 224, 234, 105, 235, 163, 105, 241, 180, 224, 238, 105, 329, 163, 105, 242, 180, 224, 330, 105, 236, 163, 105, 243, 180, 224, 330, 105, 329, 163, 163, 32, 246, 180, 203, 224, 231, 105, 120, 180, 224, 232, 105, 163, 105, 240, 180, 224, 330, 105, 163, 105, 241, 180, 224, 238, 105, 163, 105, 242, 180, 224, 330, 105, 163, 105, 243, 180, 224, 329, 105, 163, 163, 32, 131, 247, 149, 5, 224, 329, 105, 233, 105, 236, 163, 62, 32, 169, 237, 62, 32, 248, 180, 56, 224, 245, 105, 249, 180, 224, 330, 105, 163, 163, 32, 250, 180, 56, 224, 244, 105, 249, 180, 224, 330, 105, 163, 163, 32, 172, 32, 30, 62, 32, 248, 180, 56, 224, 245, 163, 32, 250, 180, 56, 224, 244, 163, 32, 55, 32, 251, 180, 56, 224, 246, 163, 32, 252, 180, 248, 225, 224, 250, 4, 251, 163, 32, 169, 237, 62, 32, 10, 224, 239, 105, 252, 80, 253, 224, 88, 163, 105, 249, 180, 224, 330, 105, 163, 163, 32, 172, 32, 30, 62, 32, 10, 224, 239, 105, 252, 80, 253, 224, 88, 163, 163, 32, 55, 32, 239, 180, 138, 224, 239, 105, 224, 329, 105, 236, 163, 163, 32, 244, 180, 138, 224, 244, 105, 224, 329, 105, 236, 163, 163, 32, 245, 180, 138, 224, 245, 105, 224, 329, 105, 236, 163, 163, 32, 76, 32, 3, 32]}, {"code": "def backward_delta(\n    delta_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    output_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    delta_block_ptr = tl.make_block_ptr(\n        delta_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n    grad_output_block_ptr = tl.make_block_ptr(\n        grad_output_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n\n    delta = tl.zeros((1, x_block_size), dtype)\n\n    for _ in range(0, x_size, x_block_size):\n        if require_x_boundary_check:\n            grad_output = tl.load(\n                grad_output_block_ptr, boundary_check=(1,), padding_option=\"zero\"\n            )\n            output = tl.load(output_block_ptr, boundary_check=(1,))\n        else:\n            grad_output = tl.load(grad_output_block_ptr)\n            output = tl.load(output_block_ptr)\n\n        delta += grad_output * output\n        output_block_ptr = tl.advance(output_block_ptr, (0, x_block_size))\n        grad_output_block_ptr = tl.advance(grad_output_block_ptr, (0, x_block_size))\n\n    delta = tl.sum(delta, 1)\n    tl.store(delta_block_ptr, delta.to(dtype))", "encoded": [29, 328, 224, 228, 61, 94, 105, 229, 61, 94, 105, 230, 61, 94, 105, 231, 61, 227, 105, 232, 61, 227, 105, 233, 61, 227, 105, 234, 61, 227, 105, 88, 61, 6, 105, 235, 61, 6, 105, 236, 61, 6, 163, 61, 32, -1, 237, 180, 159, 224, 329, 163, 32, 238, 180, 203, 224, 228, 105, 120, 180, 224, 231, 105, 163, 105, 239, 180, 224, 330, 105, 163, 105, 240, 180, 224, 237, 105, 163, 105, 241, 180, 224, 330, 105, 163, 105, 242, 180, 224, 329, 105, 163, 163, 32, 243, 180, 203, 224, 229, 105, 120, 180, 224, 231, 105, 232, 163, 105, 239, 180, 224, 233, 105, 234, 163, 105, 240, 180, 224, 237, 105, 329, 163, 105, 241, 180, 224, 330, 105, 235, 163, 105, 242, 180, 224, 330, 105, 329, 163, 163, 32, 244, 180, 203, 224, 230, 105, 120, 180, 224, 231, 105, 232, 163, 105, 239, 180, 224, 233, 105, 234, 163, 105, 240, 180, 224, 237, 105, 329, 163, 105, 241, 180, 224, 330, 105, 235, 163, 105, 242, 180, 224, 330, 105, 329, 163, 163, 32, 245, 180, 164, 224, 224, 330, 105, 235, 163, 105, 88, 163, 32, 131, 246, 149, 5, 224, 329, 105, 232, 105, 235, 163, 61, 32, 169, 236, 61, 32, 247, 180, 54, 224, 243, 105, 248, 180, 224, 330, 105, 163, 105, 249, 180, 331, 163, 32, 250, 180, 54, 224, 244, 105, 248, 180, 224, 330, 105, 163, 163, 32, 172, 32, 30, 61, 32, 247, 180, 54, 224, 243, 163, 32, 250, 180, 54, 224, 244, 163, 32, 55, 32, 245, 160, 247, 225, 250, 32, 244, 180, 138, 224, 244, 105, 224, 329, 105, 235, 163, 163, 32, 243, 180, 138, 224, 243, 105, 224, 329, 105, 235, 163, 163, 32, 76, 32, 245, 180, 205, 224, 245, 105, 330, 163, 32, 10, 224, 238, 105, 245, 80, 251, 224, 88, 163, 163, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    output = language.Sum.forward(\n        input_ptr,\n        y_size,\n        x_size,\n        y_stride,\n        x_stride,\n        y_offset,\n        dtype,\n        x_block_size,\n        require_x_boundary_check,\n    )\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n    tl.store(output_block_ptr, output)", "encoded": [29, 328, 224, 228, 62, 94, 105, 229, 62, 94, 105, 230, 62, 61, 105, 231, 62, 61, 105, 232, 62, 61, 105, 233, 62, 61, 105, 88, 62, 6, 105, 234, 62, 6, 105, 235, 62, 6, 163, 62, 32, -1, 236, 180, 159, 224, 329, 163, 32, 237, 180, 157, 80, 238, 80, 328, 224, 229, 105, 230, 105, 231, 105, 232, 105, 233, 105, 236, 105, 88, 105, 234, 105, 235, 163, 32, 239, 180, 203, 224, 228, 105, 120, 180, 224, 230, 105, 163, 105, 240, 180, 224, 330, 105, 163, 105, 241, 180, 224, 236, 105, 163, 105, 242, 180, 224, 330, 105, 163, 105, 243, 180, 224, 329, 105, 163, 163, 32, 10, 224, 239, 105, 237, 163, 32, 3, 32]}, {"code": "def backward(\n    grad_input_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n    grad_input_block_ptr = tl.make_block_ptr(\n        grad_input_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, 0),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    grad_input = language.Sum.backward(\n        grad_output_ptr, y_size, y_offset, x_block_size\n    )\n\n    for x_offset in range(0, x_size, x_block_size):\n        if require_x_boundary_check:\n            tl.store(grad_input_block_ptr, grad_input, boundary_check=(1,))\n        else:\n            tl.store(grad_input_block_ptr, grad_input)\n\n        grad_input_block_ptr = tl.advance(grad_input_block_ptr, (0, x_block_size))", "encoded": [29, 328, 224, 228, 61, 94, 105, 229, 61, 94, 105, 230, 61, 227, 105, 231, 61, 227, 105, 232, 61, 227, 105, 233, 61, 227, 105, 234, 61, 6, 105, 235, 61, 6, 163, 61, 32, -1, 236, 180, 159, 224, 329, 163, 32, 237, 180, 203, 224, 228, 105, 120, 180, 224, 230, 105, 231, 163, 105, 238, 180, 224, 232, 105, 233, 163, 105, 239, 180, 224, 236, 105, 329, 163, 105, 240, 180, 224, 330, 105, 234, 163, 105, 241, 180, 224, 330, 105, 329, 163, 163, 32, 242, 180, 157, 80, 243, 80, 328, 224, 229, 105, 230, 105, 236, 105, 234, 163, 32, 131, 244, 149, 5, 224, 329, 105, 231, 105, 234, 163, 61, 32, 169, 235, 61, 32, 10, 224, 237, 105, 242, 105, 245, 180, 224, 330, 105, 163, 163, 32, 172, 32, 30, 61, 32, 10, 224, 237, 105, 242, 163, 32, 55, 32, 237, 180, 138, 224, 237, 105, 224, 329, 105, 234, 163, 163, 32, 76, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    correction: tl.constexpr,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n\n    output, mean = language.VarMean.forward(\n        input_ptr,\n        y_size,\n        x_size,\n        y_stride,\n        x_stride,\n        y_offset,\n        correction,\n        dtype,\n        x_block_size,\n        require_x_boundary_check,\n    )\n    tl.store(output_block_ptr, output)", "encoded": [29, 328, 224, 228, 62, 94, 105, 229, 62, 94, 105, 230, 62, 61, 105, 231, 62, 61, 105, 232, 62, 61, 105, 233, 62, 61, 105, 234, 62, 6, 105, 88, 62, 6, 105, 235, 62, 6, 105, 236, 62, 6, 163, 62, 32, -1, 237, 180, 159, 224, 329, 163, 32, 238, 180, 203, 224, 228, 105, 120, 180, 224, 230, 105, 163, 105, 239, 180, 224, 330, 105, 163, 105, 240, 180, 224, 237, 105, 163, 105, 241, 180, 224, 330, 105, 163, 105, 242, 180, 224, 329, 105, 163, 163, 32, 243, 105, 244, 180, 157, 80, 245, 80, 328, 224, 229, 105, 230, 105, 231, 105, 232, 105, 233, 105, 237, 105, 234, 105, 88, 105, 235, 105, 236, 163, 32, 10, 224, 238, 105, 243, 163, 32, 3, 32]}, {"code": "def backward(\n    grad_input_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    correction: tl.constexpr,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_x_blocks = tl.cdiv(x_size, x_block_size)\n    y_offset = pid // num_x_blocks\n    x = pid % num_x_blocks\n    x_offset = x * x_block_size\n\n    grad_input_block_ptr = tl.make_block_ptr(\n        grad_input_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, x_offset),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n\n    mean = language.Mean.forward(\n        input_ptr,\n        y_size,\n        x_size,\n        y_stride,\n        x_stride,\n        y_offset,\n        dtype,\n        x_block_size,\n        require_x_boundary_check,\n    )\n    grad_input = language.Var.backward(\n        grad_output_ptr,\n        input_ptr,\n        y_size,\n        x_size,\n        y_stride,\n        x_stride,\n        y_offset,\n        x_offset,\n        mean,\n        correction,\n        dtype,\n        x_block_size,\n        require_x_boundary_check,\n    )\n\n    if require_x_boundary_check:\n        tl.store(grad_input_block_ptr, grad_input, boundary_check=(1,))\n    else:\n        tl.store(grad_input_block_ptr, grad_input)", "encoded": [29, 328, 224, 228, 61, 94, 105, 229, 61, 94, 105, 230, 61, 94, 105, 231, 61, 227, 105, 232, 61, 227, 105, 233, 61, 227, 105, 234, 61, 227, 105, 235, 61, 6, 105, 88, 61, 6, 105, 236, 61, 6, 105, 237, 61, 6, 163, 61, 32, -1, 238, 180, 159, 224, 329, 163, 32, 239, 180, 63, 224, 232, 105, 236, 163, 32, 240, 180, 238, 48, 239, 32, 241, 180, 238, 208, 239, 32, 242, 180, 241, 225, 236, 32, 243, 180, 203, 224, 228, 105, 120, 180, 224, 231, 105, 232, 163, 105, 244, 180, 224, 233, 105, 234, 163, 105, 245, 180, 224, 240, 105, 242, 163, 105, 246, 180, 224, 330, 105, 236, 163, 105, 247, 180, 224, 330, 105, 329, 163, 163, 32, 248, 180, 157, 80, 249, 80, 250, 224, 230, 105, 231, 105, 232, 105, 233, 105, 234, 105, 240, 105, 88, 105, 236, 105, 237, 163, 32, 251, 180, 157, 80, 252, 80, 328, 224, 229, 105, 230, 105, 231, 105, 232, 105, 233, 105, 234, 105, 240, 105, 242, 105, 248, 105, 235, 105, 88, 105, 236, 105, 237, 163, 32, 169, 237, 61, 32, 10, 224, 243, 105, 251, 105, 253, 180, 224, 330, 105, 163, 163, 32, 172, 32, 30, 61, 32, 10, 224, 243, 105, 251, 163, 32, 55, 32, 3, 32]}, {"code": "def forward(\n    output_ptr: tl.tensor,\n    mean_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    correction: tl.constexpr,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    y_offset = tl.program_id(0)\n\n    output_block_ptr = tl.make_block_ptr(\n        output_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n    mean_block_ptr = tl.make_block_ptr(\n        mean_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n\n    output, mean = language.VarMean.forward(\n        input_ptr,\n        y_size,\n        x_size,\n        y_stride,\n        x_stride,\n        y_offset,\n        correction,\n        dtype,\n        x_block_size,\n        require_x_boundary_check,\n    )\n    tl.store(output_block_ptr, output)\n    tl.store(mean_block_ptr, mean)", "encoded": [29, 328, 224, 228, 62, 94, 105, 229, 62, 94, 105, 230, 62, 94, 105, 231, 62, 61, 105, 232, 62, 61, 105, 233, 62, 61, 105, 234, 62, 61, 105, 235, 62, 6, 105, 88, 62, 6, 105, 236, 62, 6, 105, 237, 62, 6, 163, 62, 32, -1, 238, 180, 159, 224, 329, 163, 32, 239, 180, 203, 224, 228, 105, 120, 180, 224, 231, 105, 163, 105, 240, 180, 224, 330, 105, 163, 105, 241, 180, 224, 238, 105, 163, 105, 242, 180, 224, 330, 105, 163, 105, 243, 180, 224, 329, 105, 163, 163, 32, 244, 180, 203, 224, 229, 105, 120, 180, 224, 231, 105, 163, 105, 240, 180, 224, 330, 105, 163, 105, 241, 180, 224, 238, 105, 163, 105, 242, 180, 224, 330, 105, 163, 105, 243, 180, 224, 329, 105, 163, 163, 32, 245, 105, 246, 180, 157, 80, 247, 80, 328, 224, 230, 105, 231, 105, 232, 105, 233, 105, 234, 105, 238, 105, 235, 105, 88, 105, 236, 105, 237, 163, 32, 10, 224, 239, 105, 245, 163, 32, 10, 224, 244, 105, 246, 163, 32, 3, 32]}, {"code": "def backward(\n    grad_input_ptr: tl.tensor,\n    grad_output_ptr: tl.tensor,\n    input_ptr: tl.tensor,\n    y_size: tl.int32,\n    x_size: tl.int32,\n    y_stride: tl.int32,\n    x_stride: tl.int32,\n    mean_ptr: tl.tensor,\n    correction: tl.constexpr,\n    dtype: tl.constexpr,\n    x_block_size: tl.constexpr,\n    require_x_boundary_check: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_x_blocks = tl.cdiv(x_size, x_block_size)\n    y_offset = pid // num_x_blocks\n    x = pid % num_x_blocks\n    x_offset = x * x_block_size\n\n    grad_input_block_ptr = tl.make_block_ptr(\n        grad_input_ptr,\n        shape=(y_size, x_size),\n        strides=(y_stride, x_stride),\n        offsets=(y_offset, x_offset),\n        block_shape=(1, x_block_size),\n        order=(1, 0),\n    )\n    mean_block_ptr = tl.make_block_ptr(\n        mean_ptr,\n        shape=(y_size,),\n        strides=(1,),\n        offsets=(y_offset,),\n        block_shape=(1,),\n        order=(0,),\n    )\n\n    mean = tl.load(mean_block_ptr)\n    grad_input = language.Var.backward(\n        grad_output_ptr,\n        input_ptr,\n        y_size,\n        x_size,\n        y_stride,\n        x_stride,\n        y_offset,\n        x_offset,\n        mean,\n        correction,\n        dtype,\n        x_block_size,\n        require_x_boundary_check,\n    )\n\n    if require_x_boundary_check:\n        tl.store(grad_input_block_ptr, grad_input, boundary_check=(1,))\n    else:\n        tl.store(grad_input_block_ptr, grad_input)", "encoded": [29, 328, 224, 228, 61, 94, 105, 229, 61, 94, 105, 230, 61, 94, 105, 231, 61, 227, 105, 232, 61, 227, 105, 233, 61, 227, 105, 234, 61, 227, 105, 235, 61, 94, 105, 236, 61, 6, 105, 88, 61, 6, 105, 237, 61, 6, 105, 238, 61, 6, 163, 61, 32, -1, 239, 180, 159, 224, 329, 163, 32, 240, 180, 63, 224, 232, 105, 237, 163, 32, 241, 180, 239, 48, 240, 32, 242, 180, 239, 208, 240, 32, 243, 180, 242, 225, 237, 32, 244, 180, 203, 224, 228, 105, 120, 180, 224, 231, 105, 232, 163, 105, 245, 180, 224, 233, 105, 234, 163, 105, 246, 180, 224, 241, 105, 243, 163, 105, 247, 180, 224, 330, 105, 237, 163, 105, 248, 180, 224, 330, 105, 329, 163, 163, 32, 249, 180, 203, 224, 235, 105, 120, 180, 224, 231, 105, 163, 105, 245, 180, 224, 330, 105, 163, 105, 246, 180, 224, 241, 105, 163, 105, 247, 180, 224, 330, 105, 163, 105, 248, 180, 224, 329, 105, 163, 163, 32, 250, 180, 54, 224, 249, 163, 32, 251, 180, 157, 80, 252, 80, 328, 224, 229, 105, 230, 105, 231, 105, 232, 105, 233, 105, 234, 105, 241, 105, 243, 105, 250, 105, 236, 105, 88, 105, 237, 105, 238, 163, 32, 169, 238, 61, 32, 10, 224, 244, 105, 251, 105, 253, 180, 224, 330, 105, 163, 163, 32, 172, 32, 30, 61, 32, 10, 224, 244, 105, 251, 163, 32, 55, 32, 3, 32]}, {"code": "def _attn_fwd(\n    Q,\n    K,\n    V,\n    softmax_scale,\n    M,\n    O,\n    stride_Q_batch,\n    stride_Q_head,\n    stride_Q_seq,\n    stride_Q_dim,\n    stride_K_batch,\n    stride_K_head,\n    stride_K_seq,\n    stride_K_dim,\n    stride_V_batch,\n    stride_V_head,\n    stride_V_seq,\n    stride_V_dim,\n    stride_O_batch,\n    stride_O_head,\n    stride_O_seq,\n    stride_O_dim,\n    BATCH_SIZE,\n    NUM_HEADS: tl.constexpr,\n    SEQ_LEN: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n    BLOCK_SIZE_Q: tl.constexpr,\n    BLOCK_SIZE_KV: tl.constexpr,\n    STAGE: tl.constexpr,\n):\n    tl.static_assert(BLOCK_SIZE_KV <= HEAD_DIM)\n\n    block_index_q = tl.program_id(0)\n\n    index_batch_head = tl.program_id(1)\n\n    index_batch = index_batch_head // NUM_HEADS\n\n    index_head = index_batch_head % NUM_HEADS\n\n    qvk_offset = (\n        index_batch.to(tl.int64) * stride_Q_batch\n        + index_head.to(tl.int64) * stride_Q_head\n    )\n\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + qvk_offset,\n        shape=(SEQ_LEN, HEAD_DIM),\n        strides=(stride_Q_seq, stride_Q_dim),\n        offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n        order=(1, 0),\n    )\n\n    V_block_ptr = tl.make_block_ptr(\n        base=V + qvk_offset,\n        shape=(SEQ_LEN, HEAD_DIM),\n        strides=(stride_V_seq, stride_V_dim),\n        offsets=(0, 0),\n        block_shape=(BLOCK_SIZE_KV, HEAD_DIM),\n        order=(1, 0),\n    )\n\n    K_block_ptr = tl.make_block_ptr(\n        base=K + qvk_offset,\n        shape=(HEAD_DIM, SEQ_LEN),\n        strides=(\n            stride_K_dim,\n            stride_K_seq,\n        ),\n        offsets=(0, 0),\n        block_shape=(HEAD_DIM, BLOCK_SIZE_KV),\n        order=(0, 1),\n    )\n\n    O_block_ptr = tl.make_block_ptr(\n        base=O + qvk_offset,\n        shape=(SEQ_LEN, HEAD_DIM),\n        strides=(stride_O_seq, stride_O_dim),\n        offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n        order=(1, 0),\n    )\n\n    offs_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q)\n\n    offs_kv = tl.arange(0, BLOCK_SIZE_KV)\n\n    m_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) - float(\"inf\")\n\n    l_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) + 1.0\n\n    O_block = tl.zeros([BLOCK_SIZE_Q, HEAD_DIM], dtype=tl.float32)\n\n    Q_block = tl.load(Q_block_ptr)\n\n    if STAGE == 1 or STAGE == 3:\n\n        O_block, l_i, m_i = _attn_fwd_inner(\n            O_block,\n            l_i,\n            m_i,\n            Q_block,\n            K_block_ptr,\n            V_block_ptr,\n            block_index_q,\n            softmax_scale,\n            BLOCK_SIZE_Q,\n            BLOCK_SIZE_KV,\n            4 - STAGE,\n            offs_q,\n            offs_kv,\n            SEQ_LEN,\n        )\n\n    if STAGE == 3:\n\n        O_block, l_i, m_i = _attn_fwd_inner(\n            O_block,\n            l_i,\n            m_i,\n            Q_block,\n            K_block_ptr,\n            V_block_ptr,\n            block_index_q,\n            softmax_scale,\n            BLOCK_SIZE_Q,\n            BLOCK_SIZE_KV,\n            2,\n            offs_q,\n            offs_kv,\n            SEQ_LEN,\n        )\n\n    m_i += tl.math.log(l_i)\n    O_block = O_block / l_i[:, None]\n    m_ptrs = M + index_batch_head * SEQ_LEN + offs_q\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, O_block.to(O.type.element_ty))", "encoded": [29, 329, 225, 229, 105, 230, 105, 231, 105, 232, 105, 233, 105, 234, 105, 235, 105, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 105, 249, 105, 250, 105, 251, 105, 252, 62, 6, 105, 253, 62, 6, 105, 254, 62, 6, 105, 255, 62, 6, 105, 256, 62, 6, 105, 257, 62, 6, 163, 62, 32, -1, 81, 225, 256, 203, 254, 163, 32, 258, 180, 159, 225, 330, 163, 32, 259, 180, 159, 225, 331, 163, 32, 260, 180, 259, 48, 252, 32, 261, 180, 259, 209, 252, 32, 262, 180, 260, 80, 263, 225, 168, 163, 226, 235, 73, 261, 80, 263, 225, 168, 163, 226, 236, 32, 264, 180, 204, 225, 265, 180, 229, 73, 262, 105, 120, 180, 225, 253, 105, 254, 163, 105, 266, 180, 225, 237, 105, 238, 163, 105, 267, 180, 225, 258, 226, 255, 105, 330, 163, 105, 268, 180, 225, 255, 105, 254, 163, 105, 269, 180, 225, 331, 105, 330, 163, 163, 32, 270, 180, 204, 225, 265, 180, 231, 73, 262, 105, 120, 180, 225, 253, 105, 254, 163, 105, 266, 180, 225, 245, 105, 246, 163, 105, 267, 180, 225, 330, 105, 330, 163, 105, 268, 180, 225, 256, 105, 254, 163, 105, 269, 180, 225, 331, 105, 330, 163, 163, 32, 271, 180, 204, 225, 265, 180, 230, 73, 262, 105, 120, 180, 225, 254, 105, 253, 163, 105, 266, 180, 225, 242, 105, 241, 163, 105, 267, 180, 225, 330, 105, 330, 163, 105, 268, 180, 225, 254, 105, 256, 163, 105, 269, 180, 225, 330, 105, 331, 163, 163, 32, 272, 180, 204, 225, 265, 180, 234, 73, 262, 105, 120, 180, 225, 253, 105, 254, 163, 105, 266, 180, 225, 249, 105, 250, 163, 105, 267, 180, 225, 258, 226, 255, 105, 330, 163, 105, 268, 180, 225, 255, 105, 254, 163, 105, 269, 180, 225, 331, 105, 330, 163, 163, 32, 273, 180, 258, 226, 255, 73, 74, 225, 330, 105, 255, 163, 32, 274, 180, 74, 225, 330, 105, 256, 163, 32, 275, 180, 164, 225, 213, 255, 27, 105, 88, 180, 140, 163, 4, 276, 225, 332, 163, 32, 277, 180, 164, 225, 213, 255, 27, 105, 88, 180, 140, 163, 73, 331, 32, 278, 180, 164, 225, 213, 255, 105, 254, 27, 105, 88, 180, 140, 163, 32, 279, 180, 56, 225, 264, 163, 32, 169, 257, 75, 331, 136, 257, 75, 333, 62, 32, 278, 105, 277, 105, 275, 180, 280, 225, 278, 105, 277, 105, 275, 105, 279, 105, 271, 105, 270, 105, 258, 105, 232, 105, 255, 105, 256, 105, 334, 4, 257, 105, 273, 105, 274, 105, 253, 163, 32, 172, 32, 169, 257, 75, 333, 62, 32, 278, 105, 277, 105, 275, 180, 280, 225, 278, 105, 277, 105, 275, 105, 279, 105, 271, 105, 270, 105, 258, 105, 232, 105, 255, 105, 256, 105, 335, 105, 273, 105, 274, 105, 253, 163, 32, 172, 32, 275, 160, 194, 225, 277, 163, 32, 278, 180, 278, 42, 277, 213, 62, 105, 187, 27, 32, 281, 180, 233, 73, 259, 226, 253, 73, 273, 32, 10, 225, 281, 105, 275, 163, 32, 10, 225, 272, 105, 278, 80, 263, 225, 234, 80, 174, 80, 111, 163, 163, 32, 3, 32]}, {"code": "def _attn_fwd_base_opt(\n    Q,\n    K,\n    V,\n    sm_scale,\n    M,\n    Out,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_on,\n    Z,\n    H,\n    N_CTX,\n    profile_mem,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n    STAGE: tl.constexpr,\n    ENABLE_TMA: tl.constexpr,\n    LOOP_SCHEDULE: tl.constexpr,\n    ENABLE_WS: tl.constexpr,\n):\n    tl.static_assert(BLOCK_N <= HEAD_DIM)\n    pid = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    desc_q = None\n    desc_k = None\n    desc_v = None\n    desc_o = None\n\n    if ENABLE_TMA:\n        desc_k = tl.make_tensor_descriptor(\n            K,\n            shape=[Z * H * N_CTX, HEAD_DIM],\n            strides=[HEAD_DIM, 1],\n            block_shape=[BLOCK_N, HEAD_DIM],\n        )\n        if V.dtype == torch.float8_e5m2:\n            desc_v = tl.make_tensor_descriptor(\n                V,\n                shape=[Z * H * HEAD_DIM, N_CTX],\n                strides=[N_CTX, 1],\n                block_shape=[HEAD_DIM, BLOCK_N],\n            )\n        else:\n            desc_v = tl.make_tensor_descriptor(\n                V,\n                shape=[Z * H * N_CTX, HEAD_DIM],\n                strides=[HEAD_DIM, 1],\n                block_shape=[BLOCK_N, HEAD_DIM],\n            )\n\n        desc_q = tl.make_tensor_descriptor(\n            Q,\n            shape=[Z * H * N_CTX, HEAD_DIM],\n            strides=[HEAD_DIM, 1],\n            block_shape=[BLOCK_M, HEAD_DIM],\n        )\n        desc_o = tl.make_tensor_descriptor(\n            Out,\n            shape=[Z * H * N_CTX, HEAD_DIM],\n            strides=[HEAD_DIM, 1],\n            block_shape=[BLOCK_M, HEAD_DIM],\n        )\n\n    _attn_fwd_compute(\n        Q,\n        K,\n        V,\n        sm_scale,\n        M,\n        Out,\n        desc_q,\n        desc_k,\n        desc_v,\n        desc_o,\n        stride_qz,\n        stride_qh,\n        stride_qm,\n        stride_qk,\n        stride_kz,\n        stride_kh,\n        stride_kn,\n        stride_kk,\n        stride_vz,\n        stride_vh,\n        stride_vk,\n        stride_vn,\n        stride_oz,\n        stride_oh,\n        stride_om,\n        stride_on,\n        off_hz,\n        pid,\n        Z,\n        H,\n        N_CTX,\n        BLOCK_M,\n        BLOCK_N,\n        HEAD_DIM,\n        STAGE,\n        ENABLE_TMA,\n        LOOP_SCHEDULE,\n    )", "encoded": [29, 331, 227, 231, 106, 232, 106, 233, 106, 234, 106, 235, 106, 236, 106, 237, 106, 238, 106, 239, 106, 240, 106, 241, 106, 242, 106, 243, 106, 244, 106, 245, 106, 246, 106, 247, 106, 248, 106, 249, 106, 250, 106, 251, 106, 252, 106, 253, 106, 254, 106, 255, 106, 256, 106, 257, 62, 6, 106, 258, 62, 6, 106, 259, 62, 6, 106, 260, 62, 6, 106, 261, 62, 6, 106, 262, 62, 6, 106, 263, 62, 6, 165, 62, 32, -1, 82, 227, 258, 205, 259, 165, 32, 264, 182, 161, 227, 332, 165, 32, 265, 182, 161, 227, 333, 165, 32, 266, 182, 189, 32, 267, 182, 189, 32, 268, 182, 189, 32, 269, 182, 189, 32, 171, 261, 62, 32, 267, 182, 127, 227, 232, 106, 121, 182, 215, 253, 228, 254, 228, 255, 106, 259, 27, 106, 270, 182, 215, 259, 106, 333, 27, 106, 271, 182, 215, 258, 106, 259, 27, 165, 32, 171, 233, 81, 89, 76, 61, 62, 32, 268, 182, 127, 227, 233, 106, 121, 182, 215, 253, 228, 254, 228, 259, 106, 255, 27, 106, 270, 182, 215, 255, 106, 333, 27, 106, 271, 182, 215, 259, 106, 258, 27, 165, 32, 174, 32, 30, 62, 32, 268, 182, 127, 227, 233, 106, 121, 182, 215, 253, 228, 254, 228, 255, 106, 259, 27, 106, 270, 182, 215, 259, 106, 333, 27, 106, 271, 182, 215, 258, 106, 259, 27, 165, 32, 55, 32, 266, 182, 127, 227, 231, 106, 121, 182, 215, 253, 228, 254, 228, 255, 106, 259, 27, 106, 270, 182, 215, 259, 106, 333, 27, 106, 271, 182, 215, 257, 106, 259, 27, 165, 32, 269, 182, 127, 227, 236, 106, 121, 182, 215, 253, 228, 254, 228, 255, 106, 259, 27, 106, 270, 182, 215, 259, 106, 333, 27, 106, 271, 182, 215, 257, 106, 259, 27, 165, 32, 174, 32, 272, 227, 231, 106, 232, 106, 233, 106, 234, 106, 235, 106, 236, 106, 266, 106, 267, 106, 268, 106, 269, 106, 237, 106, 238, 106, 239, 106, 240, 106, 241, 106, 242, 106, 243, 106, 244, 106, 245, 106, 246, 106, 247, 106, 248, 106, 249, 106, 250, 106, 251, 106, 252, 106, 265, 106, 264, 106, 253, 106, 254, 106, 255, 106, 257, 106, 258, 106, 259, 106, 260, 106, 261, 106, 262, 165, 32, 3, 32]}, {"code": "def _attn_fwd_ws(\n    Q,\n    K,\n    V,\n    sm_scale,\n    M,\n    Out,\n    desc_q,\n    desc_k,\n    desc_v,\n    desc_o,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_on,\n    Z,\n    H,\n    N_CTX,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n    STAGE: tl.constexpr,\n    ENABLE_TMA: tl.constexpr,\n    LOOP_SCHEDULE: tl.constexpr,\n    ENABLE_WS: tl.constexpr,\n):\n    tl.static_assert(BLOCK_N <= HEAD_DIM)\n    pid = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    _attn_fwd_compute_ws(\n        Q,\n        K,\n        V,\n        sm_scale,\n        M,\n        Out,\n        desc_q,\n        desc_k,\n        desc_v,\n        desc_o,\n        stride_qz,\n        stride_qh,\n        stride_qm,\n        stride_qk,\n        stride_kz,\n        stride_kh,\n        stride_kn,\n        stride_kk,\n        stride_vz,\n        stride_vh,\n        stride_vk,\n        stride_vn,\n        stride_oz,\n        stride_oh,\n        stride_om,\n        stride_on,\n        off_hz,\n        pid,\n        Z,\n        H,\n        N_CTX,\n        BLOCK_M,\n        BLOCK_N,\n        HEAD_DIM,\n        STAGE,\n        ENABLE_TMA,\n        LOOP_SCHEDULE,\n    )", "encoded": [29, 331, 227, 231, 105, 232, 105, 233, 105, 234, 105, 235, 105, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 105, 249, 105, 250, 105, 251, 105, 252, 105, 253, 105, 254, 105, 255, 105, 256, 105, 257, 105, 258, 105, 259, 105, 260, 62, 6, 105, 261, 62, 6, 105, 262, 62, 6, 105, 263, 62, 6, 105, 264, 62, 6, 105, 265, 62, 6, 105, 266, 62, 6, 165, 62, 32, -1, 81, 227, 261, 205, 262, 165, 32, 267, 182, 161, 227, 332, 165, 32, 268, 182, 161, 227, 333, 165, 32, 269, 227, 231, 105, 232, 105, 233, 105, 234, 105, 235, 105, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 105, 249, 105, 250, 105, 251, 105, 252, 105, 253, 105, 254, 105, 255, 105, 256, 105, 268, 105, 267, 105, 257, 105, 258, 105, 259, 105, 260, 105, 261, 105, 262, 105, 263, 105, 264, 105, 265, 165, 32, 3, 32]}, {"code": "def _attn_fwd_base_opt(\n    Q,\n    K,\n    V,\n    sm_scale,\n    M,\n    Out,\n    desc_q,\n    desc_k,\n    desc_v,\n    desc_o,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_on,\n    Z,\n    H,\n    N_CTX,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n    STAGE: tl.constexpr,\n    ENABLE_TMA: tl.constexpr,\n    LOOP_SCHEDULE: tl.constexpr,\n    ENABLE_WS: tl.constexpr,\n):\n    tl.assume(stride_qz >= 0)\n    tl.assume(stride_qh >= 0)\n    tl.assume(stride_qm >= 0)\n    tl.assume(stride_qk >= 0)\n    tl.assume(stride_kz >= 0)\n    tl.assume(stride_kh >= 0)\n    tl.assume(stride_kn >= 0)\n    tl.assume(stride_kk >= 0)\n    tl.assume(stride_vz >= 0)\n    tl.assume(stride_vh >= 0)\n    tl.assume(stride_vk >= 0)\n    tl.assume(stride_vn >= 0)\n    tl.assume(stride_oz >= 0)\n    tl.assume(stride_oh >= 0)\n    tl.assume(stride_om >= 0)\n    tl.assume(stride_on >= 0)\n    tl.assume(Z >= 0)\n    tl.assume(H >= 0)\n\n    tl.static_assert(BLOCK_N <= HEAD_DIM)\n    pid = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    _attn_fwd_compute(\n        Q,\n        K,\n        V,\n        sm_scale,\n        M,\n        Out,\n        desc_q,\n        desc_k,\n        desc_v,\n        desc_o,\n        stride_qz,\n        stride_qh,\n        stride_qm,\n        stride_qk,\n        stride_kz,\n        stride_kh,\n        stride_kn,\n        stride_kk,\n        stride_vz,\n        stride_vh,\n        stride_vk,\n        stride_vn,\n        stride_oz,\n        stride_oh,\n        stride_om,\n        stride_on,\n        off_hz,\n        pid,\n        Z,\n        H,\n        N_CTX,\n        BLOCK_M,\n        BLOCK_N,\n        HEAD_DIM,\n        STAGE,\n        ENABLE_TMA,\n        LOOP_SCHEDULE,\n    )", "encoded": [29, 332, 228, 232, 105, 233, 105, 234, 105, 235, 105, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 105, 249, 105, 250, 105, 251, 105, 252, 105, 253, 105, 254, 105, 255, 105, 256, 105, 257, 105, 258, 105, 259, 105, 260, 105, 261, 61, 6, 105, 262, 61, 6, 105, 263, 61, 6, 105, 264, 61, 6, 105, 265, 61, 6, 105, 266, 61, 6, 105, 267, 61, 6, 166, 61, 32, -1, 155, 228, 242, 140, 333, 166, 32, 155, 228, 243, 140, 333, 166, 32, 155, 228, 244, 140, 333, 166, 32, 155, 228, 245, 140, 333, 166, 32, 155, 228, 246, 140, 333, 166, 32, 155, 228, 247, 140, 333, 166, 32, 155, 228, 248, 140, 333, 166, 32, 155, 228, 249, 140, 333, 166, 32, 155, 228, 250, 140, 333, 166, 32, 155, 228, 251, 140, 333, 166, 32, 155, 228, 252, 140, 333, 166, 32, 155, 228, 253, 140, 333, 166, 32, 155, 228, 254, 140, 333, 166, 32, 155, 228, 255, 140, 333, 166, 32, 155, 228, 256, 140, 333, 166, 32, 155, 228, 257, 140, 333, 166, 32, 155, 228, 258, 140, 333, 166, 32, 155, 228, 259, 140, 333, 166, 32, 81, 228, 262, 206, 263, 166, 32, 268, 183, 162, 228, 333, 166, 32, 269, 183, 162, 228, 334, 166, 32, 270, 228, 232, 105, 233, 105, 234, 105, 235, 105, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 105, 249, 105, 250, 105, 251, 105, 252, 105, 253, 105, 254, 105, 255, 105, 256, 105, 257, 105, 269, 105, 268, 105, 258, 105, 259, 105, 260, 105, 261, 105, 262, 105, 263, 105, 264, 105, 265, 105, 266, 166, 32, 3, 32]}, {"code": "def _attn_fwd_tma_unified(\n    Q,\n    K,\n    V,\n    sm_scale,\n    M,\n    Out,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_on,\n    Z,\n    H,\n    N_CTX,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n    STAGE: tl.constexpr,\n    ENABLE_TMA: tl.constexpr,\n    LOOP_SCHEDULE: tl.constexpr,\n    ENABLE_WS: tl.constexpr,\n):\n    tl.static_assert(BLOCK_N <= HEAD_DIM)\n    pid = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    desc_q = None\n    desc_k = None\n    desc_v = None\n    desc_o = None\n\n    if ENABLE_TMA:\n        desc_k = tl.make_tensor_descriptor(\n            K,\n            shape=[Z * H * N_CTX, HEAD_DIM],\n            strides=[HEAD_DIM, 1],\n            block_shape=[BLOCK_N, HEAD_DIM],\n        )\n        if V.dtype == torch.float8_e5m2:\n            desc_v = tl.make_tensor_descriptor(\n                V,\n                shape=[Z * H * HEAD_DIM, N_CTX],\n                strides=[N_CTX, 1],\n                block_shape=[HEAD_DIM, BLOCK_N],\n            )\n        else:\n            desc_v = tl.make_tensor_descriptor(\n                V,\n                shape=[Z * H * N_CTX, HEAD_DIM],\n                strides=[HEAD_DIM, 1],\n                block_shape=[BLOCK_N, HEAD_DIM],\n            )\n\n        desc_q = tl.make_tensor_descriptor(\n            Q,\n            shape=[Z * H * N_CTX, HEAD_DIM],\n            strides=[HEAD_DIM, 1],\n            block_shape=[BLOCK_M, HEAD_DIM],\n        )\n        desc_o = tl.make_tensor_descriptor(\n            Out,\n            shape=[Z * H * N_CTX, HEAD_DIM],\n            strides=[HEAD_DIM, 1],\n            block_shape=[BLOCK_M, HEAD_DIM],\n        )\n\n    if ENABLE_WS:\n        _attn_fwd_compute_ws(\n            Q,\n            K,\n            V,\n            sm_scale,\n            M,\n            Out,\n            desc_q,\n            desc_k,\n            desc_v,\n            desc_o,\n            stride_qz,\n            stride_qh,\n            stride_qm,\n            stride_qk,\n            stride_kz,\n            stride_kh,\n            stride_kn,\n            stride_kk,\n            stride_vz,\n            stride_vh,\n            stride_vk,\n            stride_vn,\n            stride_oz,\n            stride_oh,\n            stride_om,\n            stride_on,\n            off_hz,\n            pid,\n            Z,\n            H,\n            N_CTX,\n            BLOCK_M,\n            BLOCK_N,\n            HEAD_DIM,\n            STAGE,\n            ENABLE_TMA,\n            LOOP_SCHEDULE,\n        )\n    else:\n        _attn_fwd_compute(\n            Q,\n            K,\n            V,\n            sm_scale,\n            M,\n            Out,\n            desc_q,\n            desc_k,\n            desc_v,\n            desc_o,\n            stride_qz,\n            stride_qh,\n            stride_qm,\n            stride_qk,\n            stride_kz,\n            stride_kh,\n            stride_kn,\n            stride_kk,\n            stride_vz,\n            stride_vh,\n            stride_vk,\n            stride_vn,\n            stride_oz,\n            stride_oh,\n            stride_om,\n            stride_on,\n            off_hz,\n            pid,\n            Z,\n            H,\n            N_CTX,\n            BLOCK_M,\n            BLOCK_N,\n            HEAD_DIM,\n            STAGE,\n            ENABLE_TMA,\n            LOOP_SCHEDULE,\n        )", "encoded": [29, 332, 228, 232, 105, 233, 105, 234, 105, 235, 105, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 105, 249, 105, 250, 105, 251, 105, 252, 105, 253, 105, 254, 105, 255, 105, 256, 105, 257, 62, 6, 105, 258, 62, 6, 105, 259, 62, 6, 105, 260, 62, 6, 105, 261, 62, 6, 105, 262, 62, 6, 105, 263, 62, 6, 166, 62, 32, -1, 81, 228, 258, 206, 259, 166, 32, 264, 183, 162, 228, 333, 166, 32, 265, 183, 162, 228, 334, 166, 32, 266, 183, 190, 32, 267, 183, 190, 32, 268, 183, 190, 32, 269, 183, 190, 32, 172, 261, 62, 32, 267, 183, 126, 228, 233, 105, 120, 183, 216, 254, 229, 255, 229, 256, 105, 259, 27, 105, 270, 183, 216, 259, 105, 334, 27, 105, 271, 183, 216, 258, 105, 259, 27, 166, 32, 172, 234, 80, 88, 75, 158, 62, 32, 268, 183, 126, 228, 234, 105, 120, 183, 216, 254, 229, 255, 229, 259, 105, 256, 27, 105, 270, 183, 216, 256, 105, 334, 27, 105, 271, 183, 216, 259, 105, 258, 27, 166, 32, 175, 32, 30, 62, 32, 268, 183, 126, 228, 234, 105, 120, 183, 216, 254, 229, 255, 229, 256, 105, 259, 27, 105, 270, 183, 216, 259, 105, 334, 27, 105, 271, 183, 216, 258, 105, 259, 27, 166, 32, 55, 32, 266, 183, 126, 228, 232, 105, 120, 183, 216, 254, 229, 255, 229, 256, 105, 259, 27, 105, 270, 183, 216, 259, 105, 334, 27, 105, 271, 183, 216, 257, 105, 259, 27, 166, 32, 269, 183, 126, 228, 237, 105, 120, 183, 216, 254, 229, 255, 229, 256, 105, 259, 27, 105, 270, 183, 216, 259, 105, 334, 27, 105, 271, 183, 216, 257, 105, 259, 27, 166, 32, 175, 32, 172, 263, 62, 32, 272, 228, 232, 105, 233, 105, 234, 105, 235, 105, 236, 105, 237, 105, 266, 105, 267, 105, 268, 105, 269, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 105, 249, 105, 250, 105, 251, 105, 252, 105, 253, 105, 265, 105, 264, 105, 254, 105, 255, 105, 256, 105, 257, 105, 258, 105, 259, 105, 260, 105, 261, 105, 262, 166, 32, 175, 32, 30, 62, 32, 273, 228, 232, 105, 233, 105, 234, 105, 235, 105, 236, 105, 237, 105, 266, 105, 267, 105, 268, 105, 269, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 105, 249, 105, 250, 105, 251, 105, 252, 105, 253, 105, 265, 105, 264, 105, 254, 105, 255, 105, 256, 105, 257, 105, 258, 105, 259, 105, 260, 105, 261, 105, 262, 166, 32, 55, 32, 3, 32]}, {"code": "def _attn_fwd_tma_ws_persistent(\n    Q,\n    K,\n    V,\n    sm_scale,\n    M,\n    Out,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_on,\n    Z,\n    H,\n    N_CTX,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n    STAGE: tl.constexpr,\n    ENABLE_TMA: tl.constexpr,\n    LOOP_SCHEDULE: tl.constexpr,\n    ENABLE_WS: tl.constexpr,\n    GRID_MULTIPLE: tl.constexpr,\n):\n    tl.static_assert(BLOCK_N <= HEAD_DIM)\n\n    n_tile_num = tl.cdiv(N_CTX, BLOCK_M)\n    prog_id = tl.program_id(0)\n    num_progs = tl.num_programs(0)\n    total_tiles = n_tile_num * Z * H\n\n    tiles_per_sm = total_tiles // num_progs\n    if prog_id < total_tiles % num_progs:\n        tiles_per_sm += 1\n\n    tile_idx = prog_id\n\n    if ENABLE_TMA:\n        desc_k = tl.make_tensor_descriptor(\n            K,\n            shape=[Z * H * N_CTX, HEAD_DIM],\n            strides=[HEAD_DIM, 1],\n            block_shape=[BLOCK_N, HEAD_DIM],\n        )\n        if V.dtype == torch.float8_e5m2:\n            desc_v = tl.make_tensor_descriptor(\n                V,\n                shape=[Z * H * HEAD_DIM, N_CTX],\n                strides=[N_CTX, 1],\n                block_shape=[HEAD_DIM, BLOCK_N],\n            )\n        else:\n            desc_v = tl.make_tensor_descriptor(\n                V,\n                shape=[Z * H * N_CTX, HEAD_DIM],\n                strides=[HEAD_DIM, 1],\n                block_shape=[BLOCK_N, HEAD_DIM],\n            )\n\n        desc_q = tl.make_tensor_descriptor(\n            Q,\n            shape=[Z * H * N_CTX, HEAD_DIM],\n            strides=[HEAD_DIM, 1],\n            block_shape=[BLOCK_M, HEAD_DIM],\n        )\n        desc_o = tl.make_tensor_descriptor(\n            Out,\n            shape=[Z * H * N_CTX, HEAD_DIM],\n            strides=[HEAD_DIM, 1],\n            block_shape=[BLOCK_M, HEAD_DIM],\n        )\n\n    for _ in range(0, tiles_per_sm):\n\n        pid = tile_idx % n_tile_num\n        off_hz = tile_idx // n_tile_num\n        _attn_fwd_compute_ws(\n            Q,\n            K,\n            V,\n            sm_scale,\n            M,\n            Out,\n            desc_q,\n            desc_k,\n            desc_v,\n            desc_o,\n            stride_qz,\n            stride_qh,\n            stride_qm,\n            stride_qk,\n            stride_kz,\n            stride_kh,\n            stride_kn,\n            stride_kk,\n            stride_vz,\n            stride_vh,\n            stride_vk,\n            stride_vn,\n            stride_oz,\n            stride_oh,\n            stride_om,\n            stride_on,\n            off_hz,\n            pid,\n            Z,\n            H,\n            N_CTX,\n            BLOCK_M,\n            BLOCK_N,\n            HEAD_DIM,\n            STAGE,\n            ENABLE_TMA,\n            LOOP_SCHEDULE,\n        )\n        tile_idx += num_progs", "encoded": [29, 332, 228, 232, 105, 233, 105, 234, 105, 235, 105, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 105, 249, 105, 250, 105, 251, 105, 252, 105, 253, 105, 254, 105, 255, 105, 256, 105, 257, 61, 6, 105, 258, 61, 6, 105, 259, 61, 6, 105, 260, 61, 6, 105, 261, 61, 6, 105, 262, 61, 6, 105, 263, 61, 6, 105, 264, 61, 6, 166, 61, 32, -1, 81, 228, 258, 206, 259, 166, 32, 265, 183, 63, 228, 256, 105, 257, 166, 32, 266, 183, 162, 228, 333, 166, 32, 267, 183, 135, 228, 333, 166, 32, 268, 183, 265, 229, 254, 229, 255, 32, 269, 183, 268, 48, 267, 32, 172, 266, 1, 268, 212, 267, 61, 32, 269, 163, 334, 32, 175, 32, 270, 183, 266, 32, 172, 261, 61, 32, 271, 183, 126, 228, 233, 105, 120, 183, 216, 254, 229, 255, 229, 256, 105, 259, 27, 105, 272, 183, 216, 259, 105, 334, 27, 105, 273, 183, 216, 258, 105, 259, 27, 166, 32, 172, 234, 80, 88, 75, 158, 61, 32, 274, 183, 126, 228, 234, 105, 120, 183, 216, 254, 229, 255, 229, 259, 105, 256, 27, 105, 272, 183, 216, 256, 105, 334, 27, 105, 273, 183, 216, 259, 105, 258, 27, 166, 32, 175, 32, 30, 61, 32, 274, 183, 126, 228, 234, 105, 120, 183, 216, 254, 229, 255, 229, 256, 105, 259, 27, 105, 272, 183, 216, 259, 105, 334, 27, 105, 273, 183, 216, 258, 105, 259, 27, 166, 32, 55, 32, 275, 183, 126, 228, 232, 105, 120, 183, 216, 254, 229, 255, 229, 256, 105, 259, 27, 105, 272, 183, 216, 259, 105, 334, 27, 105, 273, 183, 216, 257, 105, 259, 27, 166, 32, 276, 183, 126, 228, 237, 105, 120, 183, 216, 254, 229, 255, 229, 256, 105, 259, 27, 105, 272, 183, 216, 259, 105, 334, 27, 105, 273, 183, 216, 257, 105, 259, 27, 166, 32, 175, 32, 132, 277, 150, 5, 228, 333, 105, 269, 166, 61, 32, 278, 183, 270, 212, 265, 32, 279, 183, 270, 48, 265, 32, 280, 228, 232, 105, 233, 105, 234, 105, 235, 105, 236, 105, 237, 105, 275, 105, 271, 105, 274, 105, 276, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 105, 249, 105, 250, 105, 251, 105, 252, 105, 253, 105, 279, 105, 278, 105, 254, 105, 255, 105, 256, 105, 257, 105, 258, 105, 259, 105, 260, 105, 261, 105, 262, 166, 32, 270, 163, 267, 32, 76, 32, 3, 32]}, {"code": "def bf16xbf16_matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n\n        accumulator = tl.dot(a, b, accumulator)\n\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.bfloat16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)", "encoded": [29, 333, 229, 233, 105, 234, 105, 235, 105, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 62, 6, 105, 246, 62, 6, 105, 247, 62, 6, 105, 248, 62, 6, 166, 62, 32, -1, 249, 183, 162, 229, 250, 183, 334, 166, 32, 251, 183, 64, 229, 236, 105, 245, 166, 32, 252, 183, 64, 229, 237, 105, 246, 166, 32, 253, 183, 248, 230, 252, 32, 254, 183, 249, 48, 253, 32, 255, 183, 254, 230, 248, 32, 256, 183, 39, 229, 251, 4, 255, 105, 248, 166, 32, 257, 183, 255, 73, 249, 212, 253, 212, 256, 32, 258, 183, 249, 212, 253, 48, 256, 32, 259, 183, 229, 257, 230, 245, 73, 74, 229, 334, 105, 245, 166, 166, 212, 236, 32, 260, 183, 229, 258, 230, 246, 73, 74, 229, 334, 105, 246, 166, 166, 212, 237, 32, 261, 183, 74, 229, 334, 105, 247, 166, 32, 262, 183, 233, 73, 229, 259, 216, 62, 105, 190, 27, 230, 239, 73, 261, 216, 190, 105, 62, 27, 230, 240, 166, 32, 263, 183, 234, 73, 229, 261, 216, 62, 105, 190, 27, 230, 241, 73, 260, 216, 190, 105, 62, 27, 230, 242, 166, 32, 264, 183, 167, 229, 229, 245, 105, 246, 166, 105, 88, 183, 141, 166, 32, 132, 265, 150, 5, 229, 334, 105, 64, 229, 238, 105, 247, 166, 166, 62, 32, 266, 183, 56, 229, 262, 105, 267, 183, 261, 216, 190, 105, 62, 27, 1, 238, 4, 265, 230, 247, 105, 268, 183, 334, 166, 32, 269, 183, 56, 229, 263, 105, 267, 183, 261, 216, 62, 105, 190, 27, 1, 238, 4, 265, 230, 247, 105, 268, 183, 334, 166, 32, 264, 183, 15, 229, 266, 105, 269, 105, 264, 166, 32, 262, 163, 247, 230, 240, 32, 263, 163, 247, 230, 241, 32, 76, 32, 270, 183, 264, 80, 271, 229, 219, 166, 32, 272, 183, 257, 230, 245, 73, 74, 229, 334, 105, 245, 166, 32, 273, 183, 258, 230, 246, 73, 74, 229, 334, 105, 246, 166, 32, 274, 183, 235, 73, 243, 230, 272, 216, 62, 105, 190, 27, 73, 244, 230, 273, 216, 190, 105, 62, 27, 32, 275, 183, 229, 272, 216, 62, 105, 190, 27, 1, 236, 166, 164, 229, 273, 216, 190, 105, 62, 27, 1, 237, 166, 32, 10, 229, 274, 105, 270, 105, 267, 183, 275, 166, 32, 3, 32]}, {"code": "def bf16xint16_matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    TRANSPOSE: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0)\n        if TRANSPOSE:\n            tl.static_assert(a.dtype == tl.int16)\n            tl.static_assert(b.dtype == tl.bfloat16)\n            a_bf16 = a.to(tl.bfloat16)\n            b_bf16 = b\n        else:\n            tl.static_assert(a.dtype == tl.bfloat16)\n            tl.static_assert(b.dtype == tl.int16)\n            a_bf16 = a\n            b_bf16 = b.to(tl.bfloat16)\n\n        accumulator = tl.dot(a_bf16, b_bf16, accumulator)\n\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.bfloat16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)", "encoded": [29, 333, 229, 233, 105, 234, 105, 235, 105, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 61, 6, 105, 246, 61, 6, 105, 247, 61, 6, 105, 248, 61, 6, 105, 249, 61, 6, 166, 61, 32, -1, 250, 183, 162, 229, 251, 183, 334, 166, 32, 252, 183, 63, 229, 236, 105, 245, 166, 32, 253, 183, 63, 229, 237, 105, 246, 166, 32, 254, 183, 248, 230, 253, 32, 255, 183, 250, 48, 254, 32, 256, 183, 255, 230, 248, 32, 257, 183, 39, 229, 252, 4, 256, 105, 248, 166, 32, 258, 183, 256, 73, 250, 212, 254, 212, 257, 32, 259, 183, 250, 212, 254, 48, 257, 32, 260, 183, 229, 258, 230, 245, 73, 74, 229, 334, 105, 245, 166, 166, 212, 236, 32, 261, 183, 229, 259, 230, 246, 73, 74, 229, 334, 105, 246, 166, 166, 212, 237, 32, 262, 183, 74, 229, 334, 105, 247, 166, 32, 263, 183, 233, 73, 229, 260, 216, 61, 105, 190, 27, 230, 239, 73, 262, 216, 190, 105, 61, 27, 230, 240, 166, 32, 264, 183, 234, 73, 229, 262, 216, 61, 105, 190, 27, 230, 241, 73, 261, 216, 190, 105, 61, 27, 230, 242, 166, 32, 265, 183, 167, 229, 229, 245, 105, 246, 166, 105, 88, 183, 141, 166, 32, 132, 266, 150, 5, 229, 334, 105, 63, 229, 238, 105, 247, 166, 166, 61, 32, 267, 183, 54, 229, 263, 105, 268, 183, 262, 216, 190, 105, 61, 27, 1, 238, 4, 266, 230, 247, 105, 269, 183, 334, 166, 32, 270, 183, 54, 229, 264, 105, 268, 183, 262, 216, 61, 105, 190, 27, 1, 238, 4, 266, 230, 247, 105, 269, 183, 334, 166, 32, 172, 249, 61, 32, 81, 229, 267, 80, 88, 75, 17, 166, 32, 81, 229, 270, 80, 88, 75, 219, 166, 32, 271, 183, 267, 80, 272, 229, 219, 166, 32, 273, 183, 270, 32, 175, 32, 30, 61, 32, 81, 229, 267, 80, 88, 75, 219, 166, 32, 81, 229, 270, 80, 88, 75, 17, 166, 32, 271, 183, 267, 32, 273, 183, 270, 80, 272, 229, 219, 166, 32, 55, 32, 265, 183, 15, 229, 271, 105, 273, 105, 265, 166, 32, 263, 163, 247, 230, 240, 32, 264, 163, 247, 230, 241, 32, 76, 32, 274, 183, 265, 80, 272, 229, 219, 166, 32, 275, 183, 258, 230, 245, 73, 74, 229, 334, 105, 245, 166, 32, 276, 183, 259, 230, 246, 73, 74, 229, 334, 105, 246, 166, 32, 277, 183, 235, 73, 243, 230, 275, 216, 61, 105, 190, 27, 73, 244, 230, 276, 216, 190, 105, 61, 27, 32, 278, 183, 229, 275, 216, 61, 105, 190, 27, 1, 236, 166, 164, 229, 276, 216, 190, 105, 61, 27, 1, 237, 166, 32, 10, 229, 277, 105, 274, 105, 268, 183, 278, 166, 32, 3, 32]}, {"code": "def matmul_kernel_persistent(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    NUM_SMS: tl.constexpr,\n):\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n\n    offs_k_for_mask = tl.arange(0, BLOCK_SIZE_K)\n\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    pid_m = 0\n    pid_n = 0\n    offs_am = tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = tl.arange(0, BLOCK_SIZE_N)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_SIZE_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n            pid_m = first_pid_m + (tile_id % group_size_m)\n            pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n            start_m = pid_m * BLOCK_SIZE_M\n            start_n = pid_n * BLOCK_SIZE_N\n            offs_am = tl.arange(0, BLOCK_SIZE_M)\n            offs_bn = tl.arange(0, BLOCK_SIZE_N)\n            offs_am = tl.where(offs_am < M - start_m, offs_am, 0)\n            offs_bn = tl.where(offs_bn < N - start_n, offs_bn, 0)\n            offs_am = tl.max_contiguous(\n                tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M\n            )\n            offs_bn = tl.max_contiguous(\n                tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N\n            )\n        offs_k = ki * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n        a = tl.load(\n            a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_SIZE_K, other=0.0\n        )\n        b = tl.load(\n            b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_SIZE_K, other=0.0\n        )\n        accumulator = tl.dot(a, b, accumulator)\n\n        if ki == k_tiles - 1:\n            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n            c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n            if c_ptr.dtype == tl.float8e4nv:\n                c = accumulator.to(tl.float8e4nv)\n            else:\n                c = accumulator.to(tl.float16)\n            tl.store(c_ptrs, c, mask=c_mask)\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)", "encoded": [29, 334, 230, 234, 105, 235, 105, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 62, 6, 105, 247, 62, 6, 105, 248, 62, 6, 105, 249, 62, 6, 105, 250, 62, 6, 166, 62, 32, -1, 251, 183, 162, 230, 252, 183, 335, 166, 32, 253, 183, 64, 230, 237, 105, 246, 166, 32, 254, 183, 64, 230, 238, 105, 247, 166, 32, 255, 183, 64, 230, 239, 105, 248, 166, 32, 256, 183, 253, 231, 254, 32, 257, 183, 256, 48, 250, 32, 172, 251, 1, 256, 213, 250, 62, 32, 257, 163, 336, 32, 175, 32, 258, 183, 251, 4, 250, 32, 259, 183, 4, 336, 32, 260, 183, 74, 230, 335, 105, 248, 166, 32, 261, 183, 249, 231, 254, 32, 262, 183, 335, 32, 263, 183, 335, 32, 264, 183, 74, 230, 335, 105, 246, 166, 32, 265, 183, 74, 230, 335, 105, 247, 166, 32, 266, 183, 167, 230, 230, 246, 105, 247, 166, 105, 88, 183, 141, 166, 32, 132, 267, 150, 5, 230, 335, 105, 255, 231, 257, 166, 62, 32, 259, 183, 193, 230, 259, 75, 255, 4, 336, 105, 335, 105, 259, 73, 336, 166, 32, 172, 259, 75, 335, 62, 32, 258, 163, 250, 32, 268, 183, 258, 48, 261, 32, 269, 183, 268, 231, 249, 32, 270, 183, 39, 230, 253, 4, 269, 105, 249, 166, 32, 262, 183, 269, 73, 258, 213, 270, 32, 263, 183, 258, 213, 261, 48, 270, 32, 271, 183, 262, 231, 246, 32, 272, 183, 263, 231, 247, 32, 264, 183, 74, 230, 335, 105, 246, 166, 32, 265, 183, 74, 230, 335, 105, 247, 166, 32, 264, 183, 193, 230, 264, 1, 237, 4, 271, 105, 264, 105, 335, 166, 32, 265, 183, 193, 230, 265, 1, 238, 4, 272, 105, 265, 105, 335, 166, 32, 264, 183, 199, 230, 54, 230, 264, 105, 246, 166, 105, 246, 166, 32, 265, 183, 199, 230, 54, 230, 265, 105, 247, 166, 105, 247, 166, 32, 175, 32, 273, 183, 259, 231, 248, 73, 74, 230, 335, 105, 248, 166, 32, 274, 183, 234, 73, 230, 264, 217, 62, 105, 190, 27, 231, 240, 73, 273, 217, 190, 105, 62, 27, 231, 241, 166, 32, 275, 183, 235, 73, 230, 273, 217, 62, 105, 190, 27, 231, 242, 73, 265, 217, 190, 105, 62, 27, 231, 243, 166, 32, 276, 183, 56, 230, 274, 105, 277, 183, 260, 217, 190, 105, 62, 27, 1, 239, 4, 259, 231, 248, 105, 278, 183, 335, 166, 32, 279, 183, 56, 230, 275, 105, 277, 183, 260, 217, 62, 105, 190, 27, 1, 239, 4, 259, 231, 248, 105, 278, 183, 335, 166, 32, 266, 183, 15, 230, 276, 105, 279, 105, 266, 166, 32, 172, 259, 75, 255, 4, 336, 62, 32, 280, 183, 262, 231, 246, 73, 74, 230, 335, 105, 246, 166, 32, 281, 183, 263, 231, 247, 73, 74, 230, 335, 105, 247, 166, 32, 282, 183, 236, 73, 244, 231, 280, 217, 62, 105, 190, 27, 73, 245, 231, 281, 217, 190, 105, 62, 27, 32, 283, 183, 230, 280, 217, 62, 105, 190, 27, 1, 237, 166, 164, 230, 281, 217, 190, 105, 62, 27, 1, 238, 166, 32, 172, 236, 80, 88, 75, 210, 62, 32, 284, 183, 266, 80, 285, 230, 210, 166, 32, 175, 32, 30, 62, 32, 284, 183, 266, 80, 285, 230, 21, 166, 32, 55, 32, 10, 230, 282, 105, 284, 105, 277, 183, 283, 166, 32, 266, 183, 167, 230, 230, 246, 105, 247, 166, 105, 88, 183, 141, 166, 32, 175, 32, 76, 32, 3, 32]}, {"code": "def matmul_kernel_tma_persistent(\n    a_desc_ptr,\n    b_desc_ptr,\n    c_desc_ptr,\n    M,\n    N,\n    K,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    FP8_OUTPUT: tl.constexpr,\n    NUM_SMS: tl.constexpr,\n):\n    dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n\n    pid_m = 0\n    pid_n = 0\n    offs_am = 0\n    offs_bn = 0\n\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_SIZE_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n            pid_m = first_pid_m + (tile_id % group_size_m)\n            pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n            offs_am = pid_m * BLOCK_SIZE_M\n            offs_bn = pid_n * BLOCK_SIZE_N\n\n        offs_k = ki * BLOCK_SIZE_K\n\n        a = tl._experimental_descriptor_load(\n            a_desc_ptr, [offs_am, offs_k], [BLOCK_SIZE_M, BLOCK_SIZE_K], dtype\n        )\n        b = tl._experimental_descriptor_load(\n            b_desc_ptr, [offs_bn, offs_k], [BLOCK_SIZE_N, BLOCK_SIZE_K], dtype\n        )\n        accumulator = tl.dot(a, b.T, accumulator)\n\n        if ki == k_tiles - 1:\n            c = accumulator.to(dtype)\n\n            tl._experimental_descriptor_store(c_desc_ptr, c, [offs_am, offs_bn])\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)", "encoded": [29, 336, 232, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 61, 6, 105, 243, 61, 6, 105, 244, 61, 6, 105, 245, 61, 6, 105, 246, 61, 6, 105, 247, 61, 6, 168, 61, 32, -1, 88, 185, 212, 174, 246, 30, 21, 32, 248, 185, 164, 232, 249, 185, 337, 168, 32, 250, 185, 63, 232, 239, 105, 242, 168, 32, 251, 185, 63, 232, 240, 105, 243, 168, 32, 252, 185, 63, 232, 241, 105, 244, 168, 32, 253, 185, 250, 233, 251, 32, 254, 185, 253, 48, 247, 32, 174, 248, 1, 253, 215, 247, 61, 32, 254, 165, 338, 32, 177, 32, 255, 185, 248, 4, 247, 32, 256, 185, 4, 338, 32, 257, 185, 337, 32, 258, 185, 337, 32, 259, 185, 337, 32, 260, 185, 337, 32, 261, 185, 245, 233, 251, 32, 262, 185, 169, 232, 232, 242, 105, 243, 168, 105, 88, 185, 141, 168, 32, 132, 263, 151, 5, 232, 337, 105, 252, 233, 254, 168, 61, 32, 256, 185, 195, 232, 256, 75, 252, 4, 338, 105, 337, 105, 256, 73, 338, 168, 32, 174, 256, 75, 337, 61, 32, 255, 165, 247, 32, 264, 185, 255, 48, 261, 32, 265, 185, 264, 233, 245, 32, 266, 185, 39, 232, 250, 4, 265, 105, 245, 168, 32, 257, 185, 265, 73, 255, 215, 266, 32, 258, 185, 255, 215, 261, 48, 266, 32, 259, 185, 257, 233, 242, 32, 260, 185, 258, 233, 243, 32, 177, 32, 267, 185, 256, 233, 244, 32, 268, 185, 144, 232, 236, 105, 219, 259, 105, 267, 27, 105, 219, 242, 105, 244, 27, 105, 88, 168, 32, 269, 185, 144, 232, 237, 105, 219, 260, 105, 267, 27, 105, 219, 243, 105, 244, 27, 105, 88, 168, 32, 262, 185, 15, 232, 268, 105, 269, 80, 270, 105, 262, 168, 32, 174, 256, 75, 252, 4, 338, 61, 32, 271, 185, 262, 80, 272, 232, 88, 168, 32, 157, 232, 238, 105, 271, 105, 219, 259, 105, 260, 27, 168, 32, 262, 185, 169, 232, 232, 242, 105, 243, 168, 105, 88, 185, 141, 168, 32, 177, 32, 76, 32, 3, 32]}, {"code": "def matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n\n        accumulator = tl.dot(a, b, accumulator)\n\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if ACTIVATION == \"leaky_relu\":\n        accumulator = leaky_relu(accumulator)\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)", "encoded": [29, 336, 232, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 62, 6, 105, 249, 62, 6, 105, 250, 62, 6, 105, 251, 62, 6, 105, 252, 62, 6, 168, 62, 32, -1, 253, 185, 164, 232, 254, 185, 337, 168, 32, 255, 185, 64, 232, 239, 105, 248, 168, 32, 256, 185, 64, 232, 240, 105, 249, 168, 32, 257, 185, 251, 233, 256, 32, 258, 185, 253, 48, 257, 32, 259, 185, 258, 233, 251, 32, 260, 185, 39, 232, 255, 4, 259, 105, 251, 168, 32, 261, 185, 259, 73, 253, 215, 257, 215, 260, 32, 262, 185, 253, 215, 257, 48, 260, 32, 263, 185, 232, 261, 233, 248, 73, 74, 232, 337, 105, 248, 168, 168, 215, 239, 32, 264, 185, 232, 262, 233, 249, 73, 74, 232, 337, 105, 249, 168, 168, 215, 240, 32, 265, 185, 74, 232, 337, 105, 250, 168, 32, 266, 185, 236, 73, 232, 263, 219, 62, 105, 192, 27, 233, 242, 73, 265, 219, 192, 105, 62, 27, 233, 243, 168, 32, 267, 185, 237, 73, 232, 265, 219, 62, 105, 192, 27, 233, 244, 73, 264, 219, 192, 105, 62, 27, 233, 245, 168, 32, 268, 185, 169, 232, 232, 248, 105, 249, 168, 105, 88, 185, 141, 168, 32, 132, 269, 151, 5, 232, 337, 105, 64, 232, 241, 105, 250, 168, 168, 62, 32, 270, 185, 56, 232, 266, 105, 271, 185, 265, 219, 192, 105, 62, 27, 1, 241, 4, 269, 233, 250, 105, 272, 185, 337, 168, 32, 273, 185, 56, 232, 267, 105, 271, 185, 265, 219, 62, 105, 192, 27, 1, 241, 4, 269, 233, 250, 105, 272, 185, 337, 168, 32, 268, 185, 15, 232, 270, 105, 273, 105, 268, 168, 32, 266, 165, 250, 233, 243, 32, 267, 165, 250, 233, 244, 32, 76, 32, 174, 252, 75, 338, 62, 32, 268, 185, 274, 232, 268, 168, 32, 177, 32, 275, 185, 268, 80, 276, 232, 21, 168, 32, 277, 185, 261, 233, 248, 73, 74, 232, 337, 105, 248, 168, 32, 278, 185, 262, 233, 249, 73, 74, 232, 337, 105, 249, 168, 32, 279, 185, 238, 73, 246, 233, 277, 219, 62, 105, 192, 27, 73, 247, 233, 278, 219, 192, 105, 62, 27, 32, 280, 185, 232, 277, 219, 62, 105, 192, 27, 1, 239, 168, 166, 232, 278, 219, 192, 105, 62, 27, 1, 240, 168, 32, 10, 232, 279, 105, 275, 105, 271, 185, 280, 168, 32, 3, 32]}, {"code": "def triton_red_fused_mv_0(\n    in_ptr0,\n    in_ptr1,\n    in_ptr2,\n    out_ptr1,\n    xnumel,\n    rnumel,\n    XBLOCK: tl.constexpr,\n    RBLOCK: tl.constexpr,\n):\n    xoffset = tl.program_id(0).to(tl.int64) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None].to(tl.int64)\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :].to(tl.int64)\n    x0 = xindex\n\n    tmp0 = tl.load(in_ptr0 + (x0 // rnumel), None, eviction_policy=\"evict_last\")\n    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp7 = tl.load(in_ptr2 + (r1), None, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp1 = tmp0 + 8\n        tmp2 = tmp0 < 0\n        tmp3 = tl.where(tmp2, tmp1, tmp0)\n\n        tmp4 = tl.load(\n            in_ptr1 + (r1 + (rnumel * (x0 % rnumel)) + (rnumel * rnumel * tmp3)),\n            None,\n            eviction_policy=\"evict_first\",\n        )\n        tmp5 = tmp4.to(tl.float32)\n        tmp6 = tmp5.to(tl.float32)\n        tmp8 = tmp7.to(tl.float32)\n        tmp9 = tmp6 * tmp8\n        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])\n        tmp12 = _tmp11 + tmp10\n        _tmp11 = tmp12\n    tmp11 = tl.sum(_tmp11, 1)[:, None]\n    tmp13 = tmp11.to(tl.float32)\n    tl.store(out_ptr1 + (x0), tmp13, None)", "encoded": [29, 336, 232, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 61, 6, 105, 243, 61, 6, 168, 61, 32, -1, 244, 185, 164, 232, 337, 168, 80, 245, 232, 173, 168, 233, 242, 32, 246, 185, 244, 73, 74, 232, 337, 105, 242, 168, 219, 61, 105, 192, 27, 80, 245, 232, 173, 168, 32, 247, 185, 246, 1, 240, 32, 248, 185, 74, 232, 337, 105, 243, 168, 219, 192, 105, 61, 27, 80, 245, 232, 173, 168, 32, 249, 185, 246, 32, 250, 185, 54, 232, 236, 73, 249, 48, 241, 105, 192, 105, 251, 185, 338, 168, 32, 252, 185, 226, 232, 219, 242, 105, 243, 27, 105, 337, 105, 141, 168, 32, 132, 253, 151, 5, 232, 337, 105, 241, 105, 243, 168, 61, 32, 254, 185, 253, 73, 248, 32, 255, 185, 254, 1, 241, 32, 256, 185, 254, 32, 257, 185, 54, 232, 238, 73, 256, 105, 192, 105, 251, 185, 338, 168, 80, 245, 232, 141, 168, 32, 258, 185, 250, 73, 339, 32, 259, 185, 250, 1, 337, 32, 260, 185, 195, 232, 259, 105, 258, 105, 250, 168, 32, 261, 185, 54, 232, 237, 73, 232, 256, 73, 241, 233, 232, 249, 215, 241, 168, 73, 241, 233, 241, 233, 260, 168, 105, 192, 105, 251, 185, 340, 168, 32, 262, 185, 261, 80, 245, 232, 141, 168, 32, 263, 185, 262, 80, 245, 232, 141, 168, 32, 264, 185, 257, 80, 245, 232, 141, 168, 32, 265, 185, 263, 233, 264, 32, 266, 185, 184, 232, 265, 105, 219, 242, 105, 243, 27, 168, 32, 267, 185, 252, 73, 266, 32, 252, 185, 267, 32, 76, 32, 268, 185, 211, 232, 252, 105, 341, 168, 219, 61, 105, 192, 27, 32, 269, 185, 268, 80, 245, 232, 141, 168, 32, 10, 232, 239, 73, 249, 105, 269, 105, 192, 168, 32, 3, 32]}, {"code": "def _matmul_partition_k(\n    a_ptr,\n    b_ptr,\n    c_buf_ptr,\n    M,\n    N,\n    K,\n    PK,\n    PK_SIZE,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cb_m,\n    stride_cb_n,\n    stride_cb_k,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_pk = PK\n    num_pid_nk = num_pid_n * num_pid_pk\n    num_pid_in_group = GROUP_SIZE_M * num_pid_nk\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_nk = (pid % num_pid_in_group) // group_size_m\n    pid_n = pid_nk // num_pid_pk\n    pid_pk = pid_nk % num_pid_pk\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = (pid_pk * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)) % K\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(PK_SIZE, BLOCK_SIZE_K)):\n\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        accumulator += tl.dot(a, b)\n        a_ptrs += PK_SIZE * stride_ak\n        b_ptrs += PK_SIZE * stride_bk\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_ck = pid_pk\n    c_buf_ptrs = (\n        c_buf_ptr\n        + stride_cb_m * offs_cm[:, None, None]\n        + stride_cb_n * offs_cn[None, :, None]\n        + stride_cb_k * offs_ck[None, None, :]\n    )\n    tl.store(c_buf_ptrs, accumulator[:, :, None])", "encoded": [29, 336, 232, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 105, 249, 105, 250, 105, 251, 62, 6, 105, 252, 62, 6, 105, 253, 62, 6, 105, 254, 62, 6, 168, 62, 32, -1, 255, 185, 164, 232, 337, 168, 32, 256, 185, 64, 232, 239, 105, 251, 168, 32, 257, 185, 64, 232, 240, 105, 252, 168, 32, 258, 185, 242, 32, 259, 185, 257, 233, 258, 32, 260, 185, 254, 233, 259, 32, 261, 185, 255, 48, 260, 32, 262, 185, 261, 233, 254, 32, 263, 185, 39, 232, 256, 4, 262, 105, 254, 168, 32, 264, 185, 262, 73, 255, 215, 263, 32, 265, 185, 255, 215, 260, 48, 263, 32, 266, 185, 265, 48, 258, 32, 267, 185, 265, 215, 258, 32, 268, 185, 232, 264, 233, 251, 73, 74, 232, 337, 105, 251, 168, 168, 215, 239, 32, 269, 185, 232, 266, 233, 252, 73, 74, 232, 337, 105, 252, 168, 168, 215, 240, 32, 270, 185, 232, 267, 233, 253, 73, 74, 232, 337, 105, 253, 168, 168, 215, 241, 32, 271, 185, 236, 73, 232, 268, 219, 62, 105, 192, 27, 233, 244, 73, 270, 219, 192, 105, 62, 27, 233, 245, 168, 32, 272, 185, 237, 73, 232, 270, 219, 62, 105, 192, 27, 233, 246, 73, 269, 219, 192, 105, 62, 27, 233, 247, 168, 32, 273, 185, 169, 232, 232, 251, 105, 252, 168, 105, 88, 185, 141, 168, 32, 132, 274, 151, 5, 232, 337, 105, 64, 232, 243, 105, 253, 168, 168, 62, 32, 275, 185, 56, 232, 271, 168, 32, 276, 185, 56, 232, 272, 168, 32, 273, 165, 15, 232, 275, 105, 276, 168, 32, 271, 165, 243, 233, 245, 32, 272, 165, 243, 233, 246, 32, 76, 32, 277, 185, 264, 233, 251, 73, 74, 232, 337, 105, 251, 168, 32, 278, 185, 266, 233, 252, 73, 74, 232, 337, 105, 252, 168, 32, 279, 185, 267, 32, 280, 185, 238, 73, 248, 233, 277, 219, 62, 105, 192, 105, 192, 27, 73, 249, 233, 278, 219, 192, 105, 62, 105, 192, 27, 73, 250, 233, 279, 219, 192, 105, 192, 105, 62, 27, 32, 10, 232, 280, 105, 273, 219, 62, 105, 62, 105, 192, 27, 168, 32, 3, 32]}, {"code": "def matmul_kernel_persistent(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    NUM_SMS: tl.constexpr,\n    ENABLE_BUFFER_OPS_ASSUMES: tl.constexpr,\n):\n    if ENABLE_BUFFER_OPS_ASSUMES:\n        tl.assume(M >= 0)\n        tl.assume(N >= 0)\n        tl.assume(K >= 0)\n        tl.assume(stride_am >= 0)\n        tl.assume(stride_ak >= 0)\n        tl.assume(stride_bn >= 0)\n        tl.assume(stride_bk >= 0)\n        tl.assume(stride_cm >= 0)\n        tl.assume(stride_cn >= 0)\n\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    k_tiles = tl.cdiv(K, BLOCK_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n\n    offs_k_for_mask = tl.arange(0, BLOCK_K)\n\n    num_pid_in_group = GROUP_M * num_pid_n\n\n    pid_m = 0\n    pid_n = 0\n    offs_am = tl.arange(0, BLOCK_M)\n    offs_bn = tl.arange(0, BLOCK_N)\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n            pid_m = first_pid_m + (tile_id % group_size_m)\n            pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n            start_m = pid_m * BLOCK_M\n            start_n = pid_n * BLOCK_N\n            offs_am = start_m + tl.arange(0, BLOCK_M)\n            offs_bn = start_n + tl.arange(0, BLOCK_N)\n            offs_am = tl.where(offs_am < M, offs_am, 0)\n            offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n            offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_M), BLOCK_M)\n            offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_N), BLOCK_N)\n        offs_k = ki * BLOCK_K + tl.arange(0, BLOCK_K)\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n        a = tl.load(a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n\n        if ki == k_tiles - 1:\n            offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n            offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n            c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n            if c_ptr.dtype == tl.float8e4nv:\n                c = accumulator.to(tl.float8e4nv)\n            else:\n                c = accumulator.to(tl.float16)\n            tl.store(c_ptrs, c, mask=c_mask)\n            accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)", "encoded": [29, 336, 232, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 61, 6, 105, 249, 61, 6, 105, 250, 61, 6, 105, 251, 61, 6, 105, 252, 61, 6, 105, 253, 61, 6, 168, 61, 32, -1, 174, 253, 61, 32, 156, 232, 239, 140, 337, 168, 32, 156, 232, 240, 140, 337, 168, 32, 156, 232, 241, 140, 337, 168, 32, 156, 232, 242, 140, 337, 168, 32, 156, 232, 243, 140, 337, 168, 32, 156, 232, 245, 140, 337, 168, 32, 156, 232, 244, 140, 337, 168, 32, 156, 232, 246, 140, 337, 168, 32, 156, 232, 247, 140, 337, 168, 32, 177, 32, 254, 185, 164, 232, 255, 185, 337, 168, 32, 256, 185, 63, 232, 239, 105, 248, 168, 32, 257, 185, 63, 232, 240, 105, 249, 168, 32, 258, 185, 63, 232, 241, 105, 250, 168, 32, 259, 185, 256, 233, 257, 32, 260, 185, 259, 48, 252, 32, 174, 254, 1, 259, 215, 252, 61, 32, 260, 165, 338, 32, 177, 32, 261, 185, 254, 4, 252, 32, 262, 185, 4, 338, 32, 263, 185, 74, 232, 337, 105, 250, 168, 32, 264, 185, 251, 233, 257, 32, 265, 185, 337, 32, 266, 185, 337, 32, 267, 185, 74, 232, 337, 105, 248, 168, 32, 268, 185, 74, 232, 337, 105, 249, 168, 32, 269, 185, 169, 232, 232, 248, 105, 249, 168, 105, 88, 185, 141, 168, 32, 132, 270, 151, 5, 232, 337, 105, 258, 233, 260, 168, 61, 32, 262, 185, 195, 232, 262, 75, 258, 4, 338, 105, 337, 105, 262, 73, 338, 168, 32, 174, 262, 75, 337, 61, 32, 261, 165, 252, 32, 271, 185, 261, 48, 264, 32, 272, 185, 271, 233, 251, 32, 273, 185, 39, 232, 256, 4, 272, 105, 251, 168, 32, 265, 185, 272, 73, 261, 215, 273, 32, 266, 185, 261, 215, 264, 48, 273, 32, 274, 185, 265, 233, 248, 32, 275, 185, 266, 233, 249, 32, 267, 185, 274, 73, 74, 232, 337, 105, 248, 168, 32, 268, 185, 275, 73, 74, 232, 337, 105, 249, 168, 32, 267, 185, 195, 232, 267, 1, 239, 105, 267, 105, 337, 168, 32, 268, 185, 195, 232, 268, 1, 240, 105, 268, 105, 337, 168, 32, 267, 185, 201, 232, 56, 232, 267, 105, 248, 168, 105, 248, 168, 32, 268, 185, 201, 232, 56, 232, 268, 105, 249, 168, 105, 249, 168, 32, 177, 32, 276, 185, 262, 233, 250, 73, 74, 232, 337, 105, 250, 168, 32, 277, 185, 236, 73, 232, 267, 219, 61, 105, 192, 27, 233, 242, 73, 276, 219, 192, 105, 61, 27, 233, 243, 168, 32, 278, 185, 237, 73, 232, 276, 219, 61, 105, 192, 27, 233, 244, 73, 268, 219, 192, 105, 61, 27, 233, 245, 168, 32, 279, 185, 54, 232, 277, 105, 280, 185, 263, 219, 192, 105, 61, 27, 1, 241, 4, 262, 233, 250, 105, 281, 185, 337, 168, 32, 282, 185, 54, 232, 278, 105, 280, 185, 263, 219, 61, 105, 192, 27, 1, 241, 4, 262, 233, 250, 105, 281, 185, 337, 168, 32, 269, 185, 15, 232, 279, 105, 282, 105, 269, 168, 32, 174, 262, 75, 258, 4, 338, 61, 32, 283, 185, 265, 233, 248, 73, 74, 232, 337, 105, 248, 168, 32, 284, 185, 266, 233, 249, 73, 74, 232, 337, 105, 249, 168, 32, 285, 185, 238, 73, 246, 233, 283, 219, 61, 105, 192, 27, 73, 247, 233, 284, 219, 192, 105, 61, 27, 32, 286, 185, 232, 283, 219, 61, 105, 192, 27, 1, 239, 168, 166, 232, 284, 219, 192, 105, 61, 27, 1, 240, 168, 32, 174, 238, 80, 88, 75, 212, 61, 32, 287, 185, 269, 80, 288, 232, 212, 168, 32, 177, 32, 30, 61, 32, 287, 185, 269, 80, 288, 232, 21, 168, 32, 55, 32, 10, 232, 285, 105, 287, 105, 280, 185, 286, 168, 32, 269, 185, 169, 232, 232, 248, 105, 249, 168, 105, 88, 185, 141, 168, 32, 177, 32, 76, 32, 3, 32]}, {"code": "def matmul_kernel_tma_persistent(\n    a_desc_ptr,\n    b_desc_ptr,\n    c_desc_ptr,\n    M,\n    N,\n    K,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    FP8_OUTPUT: tl.constexpr,\n    NUM_SMS: tl.constexpr,\n):\n    dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n\n    pid_m = 0\n    pid_n = 0\n    offs_am = 0\n    offs_bn = 0\n\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_SIZE_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n            pid_m = first_pid_m + (tile_id % group_size_m)\n            pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n            offs_am = pid_m * BLOCK_SIZE_M\n            offs_bn = pid_n * BLOCK_SIZE_N\n\n        offs_k = ki * BLOCK_SIZE_K\n\n        a = tl._experimental_descriptor_load(\n            a_desc_ptr, [offs_am, offs_k], [BLOCK_SIZE_M, BLOCK_SIZE_K], dtype\n        )\n        b = tl._experimental_descriptor_load(\n            b_desc_ptr, [offs_bn, offs_k], [BLOCK_SIZE_N, BLOCK_SIZE_K], dtype\n        )\n        accumulator = tl.dot(a, b.T, accumulator)\n\n        if ki == k_tiles - 1:\n            c = accumulator.to(dtype)\n\n            tl._experimental_descriptor_store(c_desc_ptr, c, [offs_am, offs_bn])\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)", "encoded": [29, 336, 232, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 62, 6, 105, 243, 62, 6, 105, 244, 62, 6, 105, 245, 62, 6, 105, 246, 62, 6, 105, 247, 62, 6, 168, 62, 32, -1, 88, 185, 212, 174, 246, 30, 21, 32, 248, 185, 164, 232, 249, 185, 337, 168, 32, 250, 185, 64, 232, 239, 105, 242, 168, 32, 251, 185, 64, 232, 240, 105, 243, 168, 32, 252, 185, 64, 232, 241, 105, 244, 168, 32, 253, 185, 250, 233, 251, 32, 254, 185, 253, 48, 247, 32, 174, 248, 1, 253, 215, 247, 62, 32, 254, 165, 338, 32, 177, 32, 255, 185, 248, 4, 247, 32, 256, 185, 4, 338, 32, 257, 185, 337, 32, 258, 185, 337, 32, 259, 185, 337, 32, 260, 185, 337, 32, 261, 185, 245, 233, 251, 32, 262, 185, 169, 232, 232, 242, 105, 243, 168, 105, 88, 185, 141, 168, 32, 132, 263, 151, 5, 232, 337, 105, 252, 233, 254, 168, 62, 32, 256, 185, 195, 232, 256, 75, 252, 4, 338, 105, 337, 105, 256, 73, 338, 168, 32, 174, 256, 75, 337, 62, 32, 255, 165, 247, 32, 264, 185, 255, 48, 261, 32, 265, 185, 264, 233, 245, 32, 266, 185, 39, 232, 250, 4, 265, 105, 245, 168, 32, 257, 185, 265, 73, 255, 215, 266, 32, 258, 185, 255, 215, 261, 48, 266, 32, 259, 185, 257, 233, 242, 32, 260, 185, 258, 233, 243, 32, 177, 32, 267, 185, 256, 233, 244, 32, 268, 185, 144, 232, 236, 105, 219, 259, 105, 267, 27, 105, 219, 242, 105, 244, 27, 105, 88, 168, 32, 269, 185, 144, 232, 237, 105, 219, 260, 105, 267, 27, 105, 219, 243, 105, 244, 27, 105, 88, 168, 32, 262, 185, 15, 232, 268, 105, 269, 80, 270, 105, 262, 168, 32, 174, 256, 75, 252, 4, 338, 62, 32, 271, 185, 262, 80, 272, 232, 88, 168, 32, 157, 232, 238, 105, 271, 105, 219, 259, 105, 260, 27, 168, 32, 262, 185, 169, 232, 232, 242, 105, 243, 168, 105, 88, 185, 141, 168, 32, 177, 32, 76, 32, 3, 32]}, {"code": "def streamk_gemm(\n    A,\n    B,\n    C,\n    bias_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_bias_m,\n    stride_bias_n,\n    stride_cm,\n    stride_cn,\n    HAS_BIAS: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    NUM_SMS: tl.constexpr,\n    NUM_XCDS: tl.constexpr,\n    STREAMK_TILES: tl.constexpr,\n    EVEN_M: tl.constexpr,\n    EVEN_N: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    ENABLE_BUFFER_OPS_ASSUMES: tl.constexpr,\n):\n    if ENABLE_BUFFER_OPS_ASSUMES:\n        tl.assume(M >= 0)\n        tl.assume(N >= 0)\n        tl.assume(K >= 0)\n        tl.assume(stride_am >= 0)\n        tl.assume(stride_ak >= 0)\n        tl.assume(stride_bn >= 0)\n        tl.assume(stride_bk >= 0)\n        tl.assume(stride_cm >= 0)\n        tl.assume(stride_cn >= 0)\n    if stride_bias_m:\n        tl.assume(stride_bias_m >= 0)\n    if stride_bias_n:\n        tl.assume(stride_bias_n >= 0)\n\n    pid = tl.program_id(0)\n    if NUM_XCDS != 1:\n        pid = (pid % NUM_XCDS) * (NUM_SMS // NUM_XCDS) + (pid // NUM_XCDS)\n\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    iters_per_tile = tl.cdiv(K, BLOCK_K)\n    total_tiles = num_pid_m * num_pid_n\n    total_full_tiles = total_tiles - STREAMK_TILES\n\n    acc_dtype = tl.float32 if C.type.element_ty != tl.int8 else tl.int32\n    for tile_id in range(pid, total_full_tiles, NUM_SMS):\n        num_pid_in_group = GROUP_M * num_pid_n\n        group_id = tile_id // num_pid_in_group\n        first_pid_m = group_id * GROUP_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n        pid_m = first_pid_m + ((tile_id % num_pid_in_group) % group_size_m)\n        pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n        rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n        rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n        rk = tl.arange(0, BLOCK_K)\n        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)\n        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)\n        A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak\n        B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn\n\n        loop_k = tl.cdiv(K, BLOCK_K)\n        if not EVEN_K:\n            loop_k -= 1\n\n        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n\n        if HAS_BIAS:\n            mask = (rm < M)[:, None] & (rn < N)[None, :]\n            bias_ = rn[None, :] * stride_bias_n + rm[:, None] * stride_bias_m\n            bias = tl.load(\n                bias_ptr + (tl.broadcast_to(bias_, (BLOCK_M, BLOCK_N))),\n                mask=mask,\n            ).to(acc.dtype)\n\n        for k in range(0, loop_k):\n            a = tl.load(tl.multiple_of(A_BASE, (1, 16)))\n            b = tl.load(tl.multiple_of(B_BASE, (16, 1)))\n            acc += tl.dot(a, b)\n            A_BASE += BLOCK_K * stride_ak\n            B_BASE += BLOCK_K * stride_bk\n\n        if not EVEN_K:\n            k = loop_k\n            rk = k * BLOCK_K + tl.arange(0, BLOCK_K)\n            A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak\n            B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn\n            A_BASE = tl.multiple_of(A_BASE, (1, 16))\n            B_BASE = tl.multiple_of(B_BASE, (16, 1))\n            a = tl.load(A_BASE, mask=rk[None, :] < K, other=0.0)\n            b = tl.load(B_BASE, mask=rk[:, None] < K, other=0.0)\n            acc += tl.dot(a, b)\n\n        rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n        rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)\n        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)\n        C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn\n        mask = (rm < M)[:, None] & (rn < N)[None, :]\n\n        if HAS_BIAS:\n            acc += bias\n\n        c = acc.to(C.type.element_ty)\n        tl.store(C_, c, mask=mask)\n\n    tl.assume(pid >= 0)\n    total_streamk_iters = STREAMK_TILES * iters_per_tile\n    streamk_iters_pcu = total_streamk_iters // NUM_SMS\n    streamk_remainder_iters = total_streamk_iters % NUM_SMS\n\n    start_iter = (\n        total_full_tiles * iters_per_tile\n        + pid * streamk_iters_pcu\n        + tl.minimum(pid, streamk_remainder_iters)\n    )\n\n    last_iter = (\n        total_full_tiles * iters_per_tile\n        + (pid + 1) * streamk_iters_pcu\n        + tl.minimum(pid + 1, streamk_remainder_iters)\n    )\n    while start_iter < last_iter:\n        remainder = start_iter % iters_per_tile\n        tile_iter_end = start_iter + (iters_per_tile - remainder)\n        tile_id = start_iter // iters_per_tile\n        end_iter = tl.minimum(tile_iter_end, last_iter)\n\n        num_pid_in_group = GROUP_M * num_pid_n\n        group_id = tile_id // num_pid_in_group\n        first_pid_m = group_id * GROUP_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n        pid_m = first_pid_m + ((tile_id % num_pid_in_group) % group_size_m)\n        pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n        rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M) % M\n        rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N) % N\n        rk = tl.arange(0, BLOCK_K)\n        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)\n        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)\n        A_BASE = (\n            A\n            + rm[:, None] * stride_am\n            + rk[None, :] * stride_ak\n            + BLOCK_K * stride_ak * remainder\n        )\n        B_BASE = (\n            B\n            + rk[:, None] * stride_bk\n            + rn[None, :] * stride_bn\n            + BLOCK_K * stride_bk * remainder\n        )\n        A_BASE = tl.multiple_of(A_BASE, (1, 16))\n        B_BASE = tl.multiple_of(B_BASE, (16, 1))\n\n        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n        if HAS_BIAS:\n            mask = (rm < M)[:, None] & (rn < N)[None, :]\n            bias_ = rn[None, :] * stride_bias_n + rm[:, None] * stride_bias_m\n            bias = tl.load(\n                bias_ptr + (tl.broadcast_to(bias_, (BLOCK_M, BLOCK_N))),\n                mask=mask,\n            ).to(acc.dtype)\n        for current_iter in range(start_iter, end_iter):\n            if EVEN_K:\n                if EVEN_M:\n                    a = tl.load(A_BASE)\n                else:\n                    mask_a = (rm < M)[:, None]\n                    a = tl.load(A_BASE, mask=mask_a, other=0.0)\n                if EVEN_N:\n                    b = tl.load(B_BASE)\n                else:\n                    mask_b = (rn < N)[None, :]\n                    b = tl.load(B_BASE, mask=mask_b, other=0.0)\n            else:\n                global_k_offset = (current_iter % iters_per_tile) * BLOCK_K\n                k_mask = global_k_offset + rk < K\n                if EVEN_M:\n                    a = tl.load(A_BASE, mask=k_mask[None, :], other=0.0)\n                else:\n                    mask_a = (rm < M)[:, None]\n                    a = tl.load(A_BASE, mask=k_mask[None, :] & mask_a, other=0.0)\n                if EVEN_N:\n                    b = tl.load(B_BASE, mask=k_mask[:, None], other=0.0)\n                else:\n                    mask_b = (rn < N)[None, :]\n                    b = tl.load(B_BASE, mask=k_mask[:, None] & mask_b, other=0.0)\n            acc += tl.dot(a, b)\n            A_BASE += BLOCK_K * stride_ak\n            B_BASE += BLOCK_K * stride_bk\n\n        rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n        rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)\n        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)\n        C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn\n        mask = (rm < M)[:, None] & (rn < N)[None, :]\n\n        if HAS_BIAS:\n            acc += bias\n\n        c = acc.to(C.type.element_ty)\n        tl.atomic_add(C_, c, mask=mask, sem=\"relaxed\")\n\n        start_iter = end_iter", "encoded": [29, 336, 232, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 105, 249, 105, 250, 105, 251, 61, 6, 105, 252, 61, 6, 105, 253, 61, 6, 105, 254, 61, 6, 105, 255, 61, 6, 105, 256, 61, 6, 105, 257, 61, 6, 105, 258, 61, 6, 105, 259, 61, 6, 105, 260, 61, 6, 105, 261, 61, 6, 105, 262, 61, 6, 168, 61, 32, -1, 174, 262, 61, 32, 156, 232, 240, 140, 337, 168, 32, 156, 232, 241, 140, 337, 168, 32, 156, 232, 242, 140, 337, 168, 32, 156, 232, 243, 140, 337, 168, 32, 156, 232, 244, 140, 337, 168, 32, 156, 232, 246, 140, 337, 168, 32, 156, 232, 245, 140, 337, 168, 32, 156, 232, 249, 140, 337, 168, 32, 156, 232, 250, 140, 337, 168, 32, 177, 32, 174, 247, 61, 32, 156, 232, 247, 140, 337, 168, 32, 177, 32, 174, 248, 61, 32, 156, 232, 248, 140, 337, 168, 32, 177, 32, 263, 185, 164, 232, 337, 168, 32, 174, 257, 178, 338, 61, 32, 263, 185, 263, 215, 257, 233, 232, 256, 48, 257, 168, 73, 263, 48, 257, 32, 177, 32, 264, 185, 63, 232, 240, 105, 252, 168, 32, 265, 185, 63, 232, 241, 105, 253, 168, 32, 266, 185, 63, 232, 242, 105, 254, 168, 32, 267, 185, 264, 233, 265, 32, 268, 185, 267, 4, 258, 32, 269, 185, 141, 174, 238, 80, 179, 80, 111, 178, 136, 30, 235, 32, 132, 270, 151, 5, 232, 263, 105, 268, 105, 256, 168, 61, 32, 271, 185, 255, 233, 265, 32, 272, 185, 270, 48, 271, 32, 273, 185, 272, 233, 255, 32, 274, 185, 39, 232, 264, 4, 273, 105, 255, 168, 32, 275, 185, 273, 73, 270, 215, 271, 215, 274, 32, 276, 185, 270, 215, 271, 48, 274, 32, 277, 185, 232, 275, 233, 252, 73, 74, 232, 337, 105, 252, 168, 168, 215, 240, 32, 278, 185, 232, 276, 233, 253, 73, 74, 232, 337, 105, 253, 168, 168, 215, 241, 32, 279, 185, 74, 232, 337, 105, 254, 168, 32, 277, 185, 201, 232, 56, 232, 277, 105, 252, 168, 105, 252, 168, 32, 278, 185, 201, 232, 56, 232, 278, 105, 253, 168, 105, 253, 168, 32, 280, 185, 236, 73, 277, 219, 61, 105, 192, 27, 233, 243, 73, 279, 219, 192, 105, 61, 27, 233, 244, 32, 281, 185, 237, 73, 279, 219, 61, 105, 192, 27, 233, 245, 73, 278, 219, 192, 105, 61, 27, 233, 246, 32, 282, 185, 63, 232, 242, 105, 254, 168, 32, 174, 62, 261, 61, 32, 282, 2, 338, 32, 177, 32, 283, 185, 169, 232, 232, 252, 105, 253, 168, 105, 88, 185, 269, 168, 32, 174, 251, 61, 32, 284, 185, 232, 277, 1, 240, 168, 219, 61, 105, 192, 27, 166, 232, 278, 1, 241, 168, 219, 192, 105, 61, 27, 32, 285, 185, 278, 219, 192, 105, 61, 27, 233, 248, 73, 277, 219, 61, 105, 192, 27, 233, 247, 32, 286, 185, 54, 232, 239, 73, 184, 232, 285, 105, 232, 252, 105, 253, 168, 168, 105, 284, 185, 284, 168, 80, 287, 232, 283, 80, 88, 168, 32, 177, 32, 132, 288, 151, 5, 232, 337, 105, 282, 168, 61, 32, 289, 185, 54, 232, 56, 232, 280, 105, 232, 338, 105, 338, 339, 168, 168, 168, 32, 290, 185, 54, 232, 56, 232, 281, 105, 232, 338, 339, 105, 338, 168, 168, 168, 32, 283, 165, 15, 232, 289, 105, 290, 168, 32, 280, 165, 254, 233, 244, 32, 281, 165, 254, 233, 245, 32, 76, 32, 174, 62, 261, 61, 32, 288, 185, 282, 32, 279, 185, 288, 233, 254, 73, 74, 232, 337, 105, 254, 168, 32, 280, 185, 236, 73, 277, 219, 61, 105, 192, 27, 233, 243, 73, 279, 219, 192, 105, 61, 27, 233, 244, 32, 281, 185, 237, 73, 279, 219, 61, 105, 192, 27, 233, 245, 73, 278, 219, 192, 105, 61, 27, 233, 246, 32, 280, 185, 56, 232, 280, 105, 232, 338, 105, 338, 339, 168, 168, 32, 281, 185, 56, 232, 281, 105, 232, 338, 339, 105, 338, 168, 168, 32, 289, 185, 54, 232, 280, 105, 284, 185, 279, 219, 192, 105, 61, 27, 1, 242, 105, 291, 185, 337, 168, 32, 290, 185, 54, 232, 281, 105, 284, 185, 279, 219, 61, 105, 192, 27, 1, 242, 105, 291, 185, 337, 168, 32, 283, 165, 15, 232, 289, 105, 290, 168, 32, 177, 32, 277, 185, 232, 275, 233, 252, 73, 74, 232, 337, 105, 252, 168, 168, 215, 240, 32, 278, 185, 232, 276, 233, 253, 73, 74, 232, 337, 105, 253, 168, 168, 215, 241, 32, 277, 185, 201, 232, 56, 232, 277, 105, 252, 168, 105, 252, 168, 32, 278, 185, 201, 232, 56, 232, 278, 105, 253, 168, 105, 253, 168, 32, 292, 185, 238, 73, 277, 219, 61, 105, 192, 27, 233, 249, 73, 278, 219, 192, 105, 61, 27, 233, 250, 32, 284, 185, 232, 277, 1, 240, 168, 219, 61, 105, 192, 27, 166, 232, 278, 1, 241, 168, 219, 192, 105, 61, 27, 32, 174, 251, 61, 32, 283, 165, 286, 32, 177, 32, 293, 185, 283, 80, 287, 232, 238, 80, 179, 80, 111, 168, 32, 10, 232, 292, 105, 293, 105, 284, 185, 284, 168, 32, 76, 32, 156, 232, 263, 140, 337, 168, 32, 294, 185, 258, 233, 266, 32, 295, 185, 294, 48, 256, 32, 296, 185, 294, 215, 256, 32, 297, 185, 268, 233, 266, 73, 263, 233, 295, 73, 65, 232, 263, 105, 296, 168, 32, 298, 185, 268, 233, 266, 73, 232, 263, 73, 338, 168, 233, 295, 73, 65, 232, 263, 73, 338, 105, 296, 168, 32, 57, 297, 1, 298, 61, 32, 299, 185, 297, 215, 266, 32, 300, 185, 297, 73, 232, 266, 4, 299, 168, 32, 270, 185, 297, 48, 266, 32, 301, 185, 65, 232, 300, 105, 298, 168, 32, 271, 185, 255, 233, 265, 32, 272, 185, 270, 48, 271, 32, 273, 185, 272, 233, 255, 32, 274, 185, 39, 232, 264, 4, 273, 105, 255, 168, 32, 275, 185, 273, 73, 270, 215, 271, 215, 274, 32, 276, 185, 270, 215, 271, 48, 274, 32, 277, 185, 275, 233, 252, 73, 74, 232, 337, 105, 252, 168, 215, 240, 32, 278, 185, 276, 233, 253, 73, 74, 232, 337, 105, 253, 168, 215, 241, 32, 279, 185, 74, 232, 337, 105, 254, 168, 32, 277, 185, 201, 232, 56, 232, 277, 105, 252, 168, 105, 252, 168, 32, 278, 185, 201, 232, 56, 232, 278, 105, 253, 168, 105, 253, 168, 32, 280, 185, 236, 73, 277, 219, 61, 105, 192, 27, 233, 243, 73, 279, 219, 192, 105, 61, 27, 233, 244, 73, 254, 233, 244, 233, 299, 32, 281, 185, 237, 73, 279, 219, 61, 105, 192, 27, 233, 245, 73, 278, 219, 192, 105, 61, 27, 233, 246, 73, 254, 233, 245, 233, 299, 32, 280, 185, 56, 232, 280, 105, 232, 338, 105, 338, 339, 168, 168, 32, 281, 185, 56, 232, 281, 105, 232, 338, 339, 105, 338, 168, 168, 32, 283, 185, 169, 232, 232, 252, 105, 253, 168, 105, 88, 185, 269, 168, 32, 174, 251, 61, 32, 284, 185, 232, 277, 1, 240, 168, 219, 61, 105, 192, 27, 166, 232, 278, 1, 241, 168, 219, 192, 105, 61, 27, 32, 285, 185, 278, 219, 192, 105, 61, 27, 233, 248, 73, 277, 219, 61, 105, 192, 27, 233, 247, 32, 286, 185, 54, 232, 239, 73, 184, 232, 285, 105, 232, 252, 105, 253, 168, 168, 105, 284, 185, 284, 168, 80, 287, 232, 283, 80, 88, 168, 32, 177, 32, 132, 302, 151, 5, 232, 297, 105, 301, 168, 61, 32, 174, 261, 61, 32, 174, 259, 61, 32, 289, 185, 54, 232, 280, 168, 32, 177, 32, 30, 61, 32, 303, 185, 232, 277, 1, 240, 168, 219, 61, 105, 192, 27, 32, 289, 185, 54, 232, 280, 105, 284, 185, 303, 105, 291, 185, 337, 168, 32, 55, 32, 174, 260, 61, 32, 290, 185, 54, 232, 281, 168, 32, 177, 32, 30, 61, 32, 304, 185, 232, 278, 1, 241, 168, 219, 192, 105, 61, 27, 32, 290, 185, 54, 232, 281, 105, 284, 185, 304, 105, 291, 185, 337, 168, 32, 55, 32, 177, 32, 30, 61, 32, 305, 185, 302, 215, 266, 233, 254, 32, 306, 185, 305, 73, 279, 1, 242, 32, 174, 259, 61, 32, 289, 185, 54, 232, 280, 105, 284, 185, 306, 219, 192, 105, 61, 27, 105, 291, 185, 337, 168, 32, 177, 32, 30, 61, 32, 303, 185, 232, 277, 1, 240, 168, 219, 61, 105, 192, 27, 32, 289, 185, 54, 232, 280, 105, 284, 185, 306, 219, 192, 105, 61, 27, 166, 303, 105, 291, 185, 337, 168, 32, 55, 32, 174, 260, 61, 32, 290, 185, 54, 232, 281, 105, 284, 185, 306, 219, 61, 105, 192, 27, 105, 291, 185, 337, 168, 32, 177, 32, 30, 61, 32, 304, 185, 232, 278, 1, 241, 168, 219, 192, 105, 61, 27, 32, 290, 185, 54, 232, 281, 105, 284, 185, 306, 219, 61, 105, 192, 27, 166, 304, 105, 291, 185, 337, 168, 32, 55, 32, 55, 32, 283, 165, 15, 232, 289, 105, 290, 168, 32, 280, 165, 254, 233, 244, 32, 281, 165, 254, 233, 245, 32, 76, 32, 277, 185, 232, 275, 233, 252, 73, 74, 232, 337, 105, 252, 168, 168, 215, 240, 32, 278, 185, 232, 276, 233, 253, 73, 74, 232, 337, 105, 253, 168, 168, 215, 241, 32, 277, 185, 201, 232, 56, 232, 277, 105, 252, 168, 105, 252, 168, 32, 278, 185, 201, 232, 56, 232, 278, 105, 253, 168, 105, 253, 168, 32, 292, 185, 238, 73, 277, 219, 61, 105, 192, 27, 233, 249, 73, 278, 219, 192, 105, 61, 27, 233, 250, 32, 284, 185, 232, 277, 1, 240, 168, 219, 61, 105, 192, 27, 166, 232, 278, 1, 241, 168, 219, 192, 105, 61, 27, 32, 174, 251, 61, 32, 283, 165, 286, 32, 177, 32, 293, 185, 283, 80, 287, 232, 238, 80, 179, 80, 111, 168, 32, 187, 232, 292, 105, 293, 105, 284, 185, 284, 105, 307, 185, 340, 168, 32, 297, 185, 301, 32, 172, 32, 3, 32]}, {"code": "def matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n    ENABLE_BUFFER_OPS_ASSUMES: tl.constexpr,\n):\n\n    if ENABLE_BUFFER_OPS_ASSUMES:\n        tl.assume(M >= 0)\n        tl.assume(N >= 0)\n        tl.assume(K >= 0)\n        tl.assume(stride_am >= 0)\n        tl.assume(stride_ak >= 0)\n        tl.assume(stride_bn >= 0)\n        tl.assume(stride_bk >= 0)\n        tl.assume(stride_cm >= 0)\n        tl.assume(stride_cn >= 0)\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n    tl.assume(pid_m >= 0)\n    tl.assume(pid_n >= 0)\n\n    offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n    offs_k = tl.arange(0, BLOCK_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_K, other=0.0)\n\n        accumulator += tl.dot(a, b)\n\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    if ACTIVATION == \"leaky_relu\":\n        accumulator = leaky_relu(accumulator)\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)", "encoded": [29, 336, 232, 236, 105, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 105, 244, 105, 245, 105, 246, 105, 247, 105, 248, 62, 6, 105, 249, 62, 6, 105, 250, 62, 6, 105, 251, 62, 6, 105, 252, 62, 6, 105, 253, 62, 6, 168, 62, 32, -1, 174, 253, 62, 32, 156, 232, 239, 140, 337, 168, 32, 156, 232, 240, 140, 337, 168, 32, 156, 232, 241, 140, 337, 168, 32, 156, 232, 242, 140, 337, 168, 32, 156, 232, 243, 140, 337, 168, 32, 156, 232, 245, 140, 337, 168, 32, 156, 232, 244, 140, 337, 168, 32, 156, 232, 246, 140, 337, 168, 32, 156, 232, 247, 140, 337, 168, 32, 177, 32, 254, 185, 164, 232, 255, 185, 337, 168, 32, 256, 185, 64, 232, 239, 105, 248, 168, 32, 257, 185, 64, 232, 240, 105, 249, 168, 32, 258, 185, 251, 233, 257, 32, 259, 185, 254, 48, 258, 32, 260, 185, 259, 233, 251, 32, 261, 185, 39, 232, 256, 4, 260, 105, 251, 168, 32, 262, 185, 260, 73, 254, 215, 261, 32, 263, 185, 254, 215, 258, 48, 261, 32, 156, 232, 262, 140, 337, 168, 32, 156, 232, 263, 140, 337, 168, 32, 264, 185, 232, 262, 233, 248, 73, 74, 232, 337, 105, 248, 168, 168, 215, 239, 32, 265, 185, 232, 263, 233, 249, 73, 74, 232, 337, 105, 249, 168, 168, 215, 240, 32, 266, 185, 74, 232, 337, 105, 250, 168, 32, 267, 185, 236, 73, 232, 264, 219, 62, 105, 192, 27, 233, 242, 73, 266, 219, 192, 105, 62, 27, 233, 243, 168, 32, 268, 185, 237, 73, 232, 266, 219, 62, 105, 192, 27, 233, 244, 73, 265, 219, 192, 105, 62, 27, 233, 245, 168, 32, 269, 185, 169, 232, 232, 248, 105, 249, 168, 105, 88, 185, 141, 168, 32, 132, 270, 151, 5, 232, 337, 105, 64, 232, 241, 105, 250, 168, 168, 62, 32, 271, 185, 56, 232, 267, 105, 272, 185, 266, 219, 192, 105, 62, 27, 1, 241, 4, 270, 233, 250, 105, 273, 185, 337, 168, 32, 274, 185, 56, 232, 268, 105, 272, 185, 266, 219, 62, 105, 192, 27, 1, 241, 4, 270, 233, 250, 105, 273, 185, 337, 168, 32, 269, 165, 15, 232, 271, 105, 274, 168, 32, 267, 165, 250, 233, 243, 32, 268, 165, 250, 233, 244, 32, 76, 32, 174, 252, 75, 338, 62, 32, 269, 185, 275, 232, 269, 168, 32, 177, 32, 276, 185, 269, 80, 277, 232, 21, 168, 32, 278, 185, 262, 233, 248, 73, 74, 232, 337, 105, 248, 168, 32, 279, 185, 263, 233, 249, 73, 74, 232, 337, 105, 249, 168, 32, 280, 185, 238, 73, 246, 233, 278, 219, 62, 105, 192, 27, 73, 247, 233, 279, 219, 192, 105, 62, 27, 32, 281, 185, 232, 278, 219, 62, 105, 192, 27, 1, 239, 168, 166, 232, 279, 219, 192, 105, 62, 27, 1, 240, 168, 32, 10, 232, 280, 105, 276, 105, 272, 185, 281, 168, 32, 3, 32]}, {"code": "def grouped_matmul_kernel(\n    group_a_ptrs,\n    group_b_ptrs,\n    group_c_ptrs,\n    group_gemm_sizes,\n    g_lds,\n    group_size,\n    NUM_SM: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    tile_idx = tl.program_id(0)\n    last_problem_end = 0\n    for g in range(group_size):\n\n        gm = tl.load(group_gemm_sizes + g * 3)\n        gn = tl.load(group_gemm_sizes + g * 3 + 1)\n        gk = tl.load(group_gemm_sizes + g * 3 + 2)\n        num_m_tiles = tl.cdiv(gm, BLOCK_SIZE_M)\n        num_n_tiles = tl.cdiv(gn, BLOCK_SIZE_N)\n        num_tiles = num_m_tiles * num_n_tiles\n\n        while tile_idx >= last_problem_end and tile_idx < last_problem_end + num_tiles:\n\n            k = gk\n            lda = tl.load(g_lds + g * 3)\n            ldb = tl.load(g_lds + g * 3 + 1)\n            ldc = tl.load(g_lds + g * 3 + 2)\n            a_ptr = tl.load(group_a_ptrs + g).to(tl.pointer_type(tl.float16))\n            b_ptr = tl.load(group_b_ptrs + g).to(tl.pointer_type(tl.float16))\n            c_ptr = tl.load(group_c_ptrs + g).to(tl.pointer_type(tl.float16))\n\n            tile_idx_in_gemm = tile_idx - last_problem_end\n            tile_m_idx = tile_idx_in_gemm // num_n_tiles\n            tile_n_idx = tile_idx_in_gemm % num_n_tiles\n\n            offs_am = tile_m_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            offs_bn = tile_n_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n            offs_k = tl.arange(0, BLOCK_SIZE_K)\n            a_ptrs = a_ptr + offs_am[:, None] * lda + offs_k[None, :]\n            b_ptrs = b_ptr + offs_k[:, None] * ldb + offs_bn[None, :]\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n            for kk in range(0, tl.cdiv(k, BLOCK_SIZE_K)):\n\n                tl.multiple_of(a_ptrs, [16, 16])\n                tl.multiple_of(b_ptrs, [16, 16])\n\n                a = tl.load(a_ptrs)\n                b = tl.load(b_ptrs)\n                accumulator += tl.dot(a, b)\n                a_ptrs += BLOCK_SIZE_K\n                b_ptrs += BLOCK_SIZE_K * ldb\n            c = accumulator.to(tl.float16)\n\n            offs_cm = tile_m_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            offs_cn = tile_n_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n            c_ptrs = c_ptr + ldc * offs_cm[:, None] + offs_cn[None, :]\n\n            tl.store(c_ptrs, c)\n\n            tile_idx += NUM_SM\n\n        last_problem_end = last_problem_end + num_tiles", "encoded": [29, 337, 233, 237, 105, 238, 105, 239, 105, 240, 105, 241, 105, 242, 105, 243, 61, 6, 105, 244, 61, 6, 105, 245, 61, 6, 105, 246, 61, 6, 168, 61, 32, -1, 247, 185, 164, 233, 338, 168, 32, 248, 185, 338, 32, 132, 249, 151, 5, 233, 242, 168, 61, 32, 250, 185, 54, 233, 240, 73, 249, 234, 339, 168, 32, 251, 185, 54, 233, 240, 73, 249, 234, 339, 73, 340, 168, 32, 252, 185, 54, 233, 240, 73, 249, 234, 339, 73, 341, 168, 32, 253, 185, 63, 233, 250, 105, 244, 168, 32, 254, 185, 63, 233, 251, 105, 245, 168, 32, 255, 185, 253, 234, 254, 32, 57, 247, 140, 248, 99, 247, 1, 248, 73, 255, 61, 32, 256, 185, 252, 32, 257, 185, 54, 233, 241, 73, 249, 234, 339, 168, 32, 258, 185, 54, 233, 241, 73, 249, 234, 339, 73, 340, 168, 32, 259, 185, 54, 233, 241, 73, 249, 234, 339, 73, 341, 168, 32, 260, 185, 54, 233, 237, 73, 249, 168, 80, 261, 233, 215, 233, 21, 168, 168, 32, 262, 185, 54, 233, 238, 73, 249, 168, 80, 261, 233, 215, 233, 21, 168, 168, 32, 263, 185, 54, 233, 239, 73, 249, 168, 80, 261, 233, 215, 233, 21, 168, 168, 32, 264, 185, 247, 4, 248, 32, 265, 185, 264, 48, 254, 32, 266, 185, 264, 216, 254, 32, 267, 185, 265, 234, 244, 73, 74, 233, 338, 105, 244, 168, 32, 268, 185, 266, 234, 245, 73, 74, 233, 338, 105, 245, 168, 32, 269, 185, 74, 233, 338, 105, 246, 168, 32, 270, 185, 260, 73, 267, 220, 61, 105, 192, 27, 234, 257, 73, 269, 220, 192, 105, 61, 27, 32, 271, 185, 262, 73, 269, 220, 61, 105, 192, 27, 234, 258, 73, 268, 220, 192, 105, 61, 27, 32, 272, 185, 169, 233, 233, 244, 105, 245, 168, 105, 88, 185, 141, 168, 32, 132, 273, 151, 5, 233, 338, 105, 63, 233, 256, 105, 246, 168, 168, 61, 32, 56, 233, 270, 105, 220, 340, 342, 105, 340, 342, 27, 168, 32, 56, 233, 271, 105, 220, 340, 342, 105, 340, 342, 27, 168, 32, 274, 185, 54, 233, 270, 168, 32, 275, 185, 54, 233, 271, 168, 32, 272, 165, 15, 233, 274, 105, 275, 168, 32, 270, 165, 246, 32, 271, 165, 246, 234, 258, 32, 76, 32, 276, 185, 272, 80, 261, 233, 21, 168, 32, 277, 185, 265, 234, 244, 73, 74, 233, 338, 105, 244, 168, 32, 278, 185, 266, 234, 245, 73, 74, 233, 338, 105, 245, 168, 32, 279, 185, 263, 73, 259, 234, 277, 220, 61, 105, 192, 27, 73, 278, 220, 192, 105, 61, 27, 32, 10, 233, 279, 105, 276, 168, 32, 247, 165, 243, 32, 172, 32, 248, 185, 248, 73, 255, 32, 76, 32, 3, 32]}, {"code": "def matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n\n    tl.device_assert(K % BLOCK_SIZE_K == 0)\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_ak = tl.arange(0, BLOCK_SIZE_K)\n    offs_bk = tl.arange(0, BLOCK_SIZE_K // 2)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_ak[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_bk[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_ak[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs)\n        tl.static_assert(b.dtype == tl.int8)\n\n        _4_i8 = tl.full((1,), 4, dtype=tl.int8)\n        b_lo = (b << _4_i8) >> _4_i8\n        b_hi = b >> _4_i8\n\n        b_f16 = (\n            tl.join(b_lo.to(tl.bfloat16), b_hi.to(tl.bfloat16))\n            .permute(0, 2, 1)\n            .reshape(BLOCK_SIZE_K, BLOCK_SIZE_N)\n        )\n\n        accumulator += tl.dot(a, b_f16)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk // 2\n\n    c = accumulator.to(tl.bfloat16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)", "encoded": [29, 339, 235, 239, 106, 240, 106, 241, 106, 242, 106, 243, 106, 244, 106, 245, 106, 246, 106, 247, 106, 248, 106, 249, 106, 250, 106, 251, 63, 6, 106, 252, 63, 6, 106, 253, 63, 6, 106, 254, 63, 6, 169, 63, 32, -1, 173, 235, 244, 218, 253, 76, 340, 169, 32, 255, 187, 165, 235, 256, 187, 340, 169, 32, 257, 187, 65, 235, 242, 106, 251, 169, 32, 258, 187, 65, 235, 243, 106, 252, 169, 32, 259, 187, 254, 236, 258, 32, 260, 187, 255, 48, 259, 32, 261, 187, 260, 236, 254, 32, 262, 187, 39, 235, 257, 4, 261, 106, 254, 169, 32, 263, 187, 261, 74, 255, 218, 262, 32, 264, 187, 255, 218, 259, 48, 262, 32, 265, 187, 235, 263, 236, 251, 74, 75, 235, 340, 106, 251, 169, 169, 218, 242, 32, 266, 187, 235, 264, 236, 252, 74, 75, 235, 340, 106, 252, 169, 169, 218, 243, 32, 267, 187, 75, 235, 340, 106, 253, 169, 32, 268, 187, 75, 235, 340, 106, 253, 48, 341, 169, 32, 269, 187, 239, 74, 235, 265, 222, 63, 106, 194, 27, 236, 245, 74, 267, 222, 194, 106, 63, 27, 236, 246, 169, 32, 270, 187, 240, 74, 235, 268, 222, 63, 106, 194, 27, 236, 247, 74, 266, 222, 194, 106, 63, 27, 236, 248, 169, 32, 271, 187, 170, 235, 235, 251, 106, 252, 169, 106, 89, 187, 142, 169, 32, 133, 272, 152, 5, 235, 340, 106, 65, 235, 244, 106, 253, 169, 169, 63, 32, 273, 187, 57, 235, 269, 106, 274, 187, 267, 222, 194, 106, 63, 27, 1, 244, 4, 272, 236, 253, 106, 275, 187, 340, 169, 32, 276, 187, 57, 235, 270, 169, 32, 82, 235, 276, 81, 89, 76, 137, 169, 32, 277, 187, 229, 235, 235, 342, 106, 169, 106, 343, 106, 89, 187, 137, 169, 32, 278, 187, 276, 134, 277, 125, 277, 32, 279, 187, 276, 125, 277, 32, 280, 187, 53, 235, 278, 81, 281, 235, 225, 169, 106, 279, 81, 281, 235, 225, 169, 169, 81, 282, 235, 340, 106, 341, 106, 342, 169, 81, 283, 235, 253, 106, 252, 169, 32, 271, 166, 15, 235, 273, 106, 280, 169, 32, 269, 166, 253, 236, 246, 32, 270, 166, 253, 236, 247, 48, 341, 32, 77, 32, 284, 187, 271, 81, 281, 235, 225, 169, 32, 285, 187, 263, 236, 251, 74, 75, 235, 340, 106, 251, 169, 32, 286, 187, 264, 236, 252, 74, 75, 235, 340, 106, 252, 169, 32, 287, 187, 241, 74, 249, 236, 285, 222, 63, 106, 194, 27, 74, 250, 236, 286, 222, 194, 106, 63, 27, 32, 288, 187, 235, 285, 222, 63, 106, 194, 27, 1, 242, 169, 167, 235, 286, 222, 194, 106, 63, 27, 1, 243, 169, 32, 10, 235, 287, 106, 284, 106, 274, 187, 288, 169, 32, 3, 32]}, {"code": "def triton_jagged_mean_kernel_simple_fused_sum_then_buffer(\n    input_ptr_values,\n    input_ptr_offsets,\n    output_ptr,\n    M,\n    MAX_SEQLEN,\n    BLOCK_SIZE_RAGGED: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n\n    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)\n\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n\n    ragged_start, ragged_end = (\n        tl.load(input_ptr_offsets + pid_b),\n        tl.load(input_ptr_offsets + (pid_b + 1)),\n    )\n    ragged_len = ragged_end - ragged_start\n\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)\n\n        buffer += tl.sum(input, axis=0)\n\n    buffer_view = buffer.reshape(\n        (BLOCK_SIZE_M,),\n    )\n\n    buffer_view_mean = buffer_view * (1 / ragged_len)\n\n    output_offsets = offsets_m + (pid_b * M)\n    output_mask = output_offsets < (M * (pid_b + 1))\n\n    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)", "encoded": [29, 339, 235, 239, 106, 240, 106, 241, 106, 242, 106, 243, 106, 244, 62, 6, 106, 245, 62, 6, 169, 62, 32, -1, 246, 187, 165, 235, 247, 187, 340, 169, 32, 248, 187, 246, 48, 64, 235, 242, 106, 245, 169, 32, 249, 187, 246, 218, 64, 235, 242, 106, 245, 169, 32, 250, 187, 170, 235, 235, 341, 106, 245, 169, 106, 89, 187, 142, 169, 32, 251, 187, 249, 236, 245, 32, 252, 187, 251, 74, 75, 235, 340, 106, 245, 169, 32, 253, 187, 252, 1, 242, 32, 254, 106, 255, 187, 235, 55, 235, 240, 74, 248, 169, 106, 55, 235, 240, 74, 235, 248, 74, 341, 169, 169, 169, 32, 256, 187, 255, 4, 254, 32, 133, 257, 152, 5, 235, 340, 106, 243, 106, 244, 169, 62, 32, 258, 187, 254, 74, 257, 32, 259, 187, 258, 74, 75, 235, 340, 106, 244, 169, 32, 260, 187, 259, 1, 255, 32, 261, 187, 259, 222, 62, 106, 194, 27, 236, 242, 74, 252, 32, 262, 187, 260, 222, 62, 106, 194, 27, 167, 253, 32, 263, 187, 55, 235, 239, 74, 261, 106, 262, 187, 262, 106, 264, 187, 340, 169, 32, 250, 166, 213, 235, 263, 106, 247, 187, 340, 169, 32, 77, 32, 265, 187, 250, 81, 266, 235, 235, 245, 106, 169, 169, 32, 267, 187, 265, 236, 235, 341, 42, 256, 169, 32, 268, 187, 252, 74, 248, 236, 242, 32, 269, 187, 268, 1, 242, 236, 235, 248, 74, 341, 169, 32, 10, 235, 241, 74, 268, 106, 267, 106, 262, 187, 269, 169, 32, 3, 32]}, {"code": "def triton_jagged_mean_kernel_simple_fused_buffer_then_sum(\n    input_ptr_values,\n    input_ptr_offsets,\n    output_ptr,\n    M,\n    MAX_SEQLEN,\n    BLOCK_SIZE_RAGGED: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n\n    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\n\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n\n    ragged_start, ragged_end = (\n        tl.load(input_ptr_offsets + pid_b),\n        tl.load(input_ptr_offsets + (pid_b + 1)),\n    )\n    ragged_len = ragged_end - ragged_start\n\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)\n\n    buffer_sum = tl.sum(buffer, axis=0)\n\n    buffer_view = buffer_sum.reshape(\n        (BLOCK_SIZE_M,),\n    )\n\n    buffer_view_mean = buffer_view * (1 / ragged_len)\n\n    output_offsets = offsets_m + (pid_b * M)\n    output_mask = output_offsets < (M * (pid_b + 1))\n\n    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)", "encoded": [29, 339, 235, 239, 106, 240, 106, 241, 106, 242, 106, 243, 106, 244, 63, 6, 106, 245, 63, 6, 169, 63, 32, -1, 246, 187, 165, 235, 247, 187, 340, 169, 32, 248, 187, 246, 48, 65, 235, 242, 106, 245, 169, 32, 249, 187, 246, 218, 65, 235, 242, 106, 245, 169, 32, 250, 187, 170, 235, 235, 244, 106, 245, 169, 106, 89, 187, 142, 169, 32, 251, 187, 249, 236, 245, 32, 252, 187, 251, 74, 75, 235, 340, 106, 245, 169, 32, 253, 187, 252, 1, 242, 32, 254, 106, 255, 187, 235, 57, 235, 240, 74, 248, 169, 106, 57, 235, 240, 74, 235, 248, 74, 341, 169, 169, 169, 32, 256, 187, 255, 4, 254, 32, 133, 257, 152, 5, 235, 340, 106, 243, 106, 244, 169, 63, 32, 258, 187, 254, 74, 257, 32, 259, 187, 258, 74, 75, 235, 340, 106, 244, 169, 32, 260, 187, 259, 1, 255, 32, 261, 187, 259, 222, 63, 106, 194, 27, 236, 242, 74, 252, 32, 262, 187, 260, 222, 63, 106, 194, 27, 167, 253, 32, 250, 166, 57, 235, 239, 74, 261, 106, 262, 187, 262, 106, 263, 187, 340, 169, 32, 77, 32, 264, 187, 213, 235, 250, 106, 247, 187, 340, 169, 32, 265, 187, 264, 81, 266, 235, 235, 245, 106, 169, 169, 32, 267, 187, 265, 236, 235, 341, 42, 256, 169, 32, 268, 187, 252, 74, 248, 236, 242, 32, 269, 187, 268, 1, 242, 236, 235, 248, 74, 341, 169, 32, 10, 235, 241, 74, 268, 106, 267, 106, 262, 187, 269, 169, 32, 3, 32]}, {"code": "def triton_jagged_mean_kernel_variable_length_loop_sum_then_buffer(\n    input_ptr_values,\n    input_ptr_offsets,\n    output_ptr,\n    M,\n    BLOCK_SIZE_RAGGED: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n\n    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)\n\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n\n    ragged_start, ragged_end = (\n        tl.load(input_ptr_offsets + pid_b),\n        tl.load(input_ptr_offsets + (pid_b + 1)),\n    )\n    ragged_len = ragged_end - ragged_start\n\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)\n\n        buffer += tl.sum(input, axis=0)\n\n    buffer_view = buffer.reshape(\n        (BLOCK_SIZE_M,),\n    )\n\n    buffer_view_mean = buffer_view * (1 / ragged_len)\n\n    output_offsets = offsets_m + (pid_b * M)\n    output_mask = output_offsets < (M * (pid_b + 1))\n\n    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)", "encoded": [29, 339, 235, 239, 106, 240, 106, 241, 106, 242, 106, 243, 62, 6, 106, 244, 62, 6, 169, 62, 32, -1, 245, 187, 165, 235, 246, 187, 340, 169, 32, 247, 187, 245, 48, 64, 235, 242, 106, 244, 169, 32, 248, 187, 245, 218, 64, 235, 242, 106, 244, 169, 32, 249, 187, 170, 235, 235, 341, 106, 244, 169, 106, 89, 187, 142, 169, 32, 250, 187, 248, 236, 244, 32, 251, 187, 250, 74, 75, 235, 340, 106, 244, 169, 32, 252, 187, 251, 1, 242, 32, 253, 106, 254, 187, 235, 55, 235, 240, 74, 247, 169, 106, 55, 235, 240, 74, 235, 247, 74, 341, 169, 169, 169, 32, 255, 187, 254, 4, 253, 32, 133, 256, 152, 5, 235, 253, 106, 254, 106, 243, 169, 62, 32, 257, 187, 256, 74, 75, 235, 340, 106, 243, 169, 32, 258, 187, 257, 1, 254, 32, 259, 187, 257, 222, 62, 106, 194, 27, 236, 242, 74, 251, 32, 260, 187, 258, 222, 62, 106, 194, 27, 167, 252, 32, 261, 187, 55, 235, 239, 74, 259, 106, 260, 187, 260, 106, 262, 187, 340, 169, 32, 249, 166, 213, 235, 261, 106, 246, 187, 340, 169, 32, 77, 32, 263, 187, 249, 81, 264, 235, 235, 244, 106, 169, 169, 32, 265, 187, 263, 236, 235, 341, 42, 255, 169, 32, 266, 187, 251, 74, 247, 236, 242, 32, 267, 187, 266, 1, 242, 236, 235, 247, 74, 341, 169, 32, 10, 235, 241, 74, 266, 106, 265, 106, 260, 187, 267, 169, 32, 3, 32]}, {"code": "def triton_jagged_mean_kernel_variable_length_loop_buffer_then_sum(\n    input_ptr_values,\n    input_ptr_offsets,\n    output_ptr,\n    M,\n    BLOCK_SIZE_RAGGED: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n\n    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\n\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n\n    ragged_start, ragged_end = (\n        tl.load(input_ptr_offsets + pid_ragged),\n        tl.load(input_ptr_offsets + (pid_ragged + 1)),\n    )\n    ragged_len = ragged_end - ragged_start\n\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)\n\n    buffer_sum = tl.sum(buffer, axis=0)\n\n    buffer_view = buffer_sum.reshape(\n        (BLOCK_SIZE_M,),\n    )\n\n    buffer_view_mean = buffer_view * (1 / ragged_len)\n\n    output_offsets = offsets_m + (pid_ragged * M)\n    output_mask = output_offsets < (M * (pid_ragged + 1))\n\n    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)", "encoded": [29, 339, 235, 239, 106, 240, 106, 241, 106, 242, 106, 243, 63, 6, 106, 244, 63, 6, 169, 63, 32, -1, 245, 187, 165, 235, 246, 187, 340, 169, 32, 247, 187, 245, 48, 65, 235, 242, 106, 244, 169, 32, 248, 187, 245, 218, 65, 235, 242, 106, 244, 169, 32, 249, 187, 170, 235, 235, 243, 106, 244, 169, 106, 89, 187, 142, 169, 32, 250, 187, 248, 236, 244, 32, 251, 187, 250, 74, 75, 235, 340, 106, 244, 169, 32, 252, 187, 251, 1, 242, 32, 253, 106, 254, 187, 235, 57, 235, 240, 74, 247, 169, 106, 57, 235, 240, 74, 235, 247, 74, 341, 169, 169, 169, 32, 255, 187, 254, 4, 253, 32, 133, 256, 152, 5, 235, 253, 106, 254, 106, 243, 169, 63, 32, 257, 187, 256, 74, 75, 235, 340, 106, 243, 169, 32, 258, 187, 257, 1, 254, 32, 259, 187, 257, 222, 63, 106, 194, 27, 236, 242, 74, 251, 32, 260, 187, 258, 222, 63, 106, 194, 27, 167, 252, 32, 249, 166, 57, 235, 239, 74, 259, 106, 260, 187, 260, 106, 261, 187, 340, 169, 32, 77, 32, 262, 187, 213, 235, 249, 106, 246, 187, 340, 169, 32, 263, 187, 262, 81, 264, 235, 235, 244, 106, 169, 169, 32, 265, 187, 263, 236, 235, 341, 42, 255, 169, 32, 266, 187, 251, 74, 247, 236, 242, 32, 267, 187, 266, 1, 242, 236, 235, 247, 74, 341, 169, 32, 10, 235, 241, 74, 266, 106, 265, 106, 260, 187, 267, 169, 32, 3, 32]}, {"code": "def triton_jagged_softmax_kernel_simple_fused_buffer_then_sum(\n    input_ptr_values,\n    input_ptr_offsets,\n    output_ptr,\n    M,\n    MAX_SEQLEN,\n    BLOCK_SIZE_RAGGED: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n\n    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\n\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n\n    ragged_start, ragged_end = (\n        tl.load(input_ptr_offsets + pid_b),\n        tl.load(input_ptr_offsets + (pid_b + 1)),\n    )\n\n    buffer_max_all = tl.full(\n        (BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), value=float(\"-inf\"), dtype=tl.float32\n    )\n\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))\n        buffer_max_all = tl.maximum(buffer_max_all, input)\n\n    buffer_max = tl.max(buffer_max_all, axis=0, keep_dims=True)\n\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))\n        buffer += tl.exp(input - buffer_max)\n\n    buffer_exp_sum = tl.sum(buffer, axis=0)\n\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))\n        output = tl.fdiv(tl.exp(input - buffer_max), buffer_exp_sum)\n\n        tl.store(output_ptr + idxs, output, mask=mask)", "encoded": [29, 340, 236, 240, 107, 241, 107, 242, 107, 243, 107, 244, 107, 245, 62, 6, 107, 246, 62, 6, 170, 62, 32, -1, 247, 188, 166, 236, 248, 188, 341, 170, 32, 249, 188, 247, 48, 64, 236, 243, 107, 246, 170, 32, 250, 188, 247, 219, 64, 236, 243, 107, 246, 170, 32, 251, 188, 171, 236, 236, 245, 107, 246, 170, 107, 90, 188, 143, 170, 32, 252, 188, 250, 237, 246, 32, 253, 188, 252, 74, 75, 236, 341, 107, 246, 170, 32, 254, 188, 253, 1, 243, 32, 255, 107, 256, 188, 236, 55, 236, 241, 74, 249, 170, 107, 55, 236, 241, 74, 236, 249, 74, 342, 170, 170, 170, 32, 257, 188, 230, 236, 236, 245, 107, 246, 170, 107, 258, 188, 259, 236, 343, 170, 107, 90, 188, 143, 170, 32, 134, 260, 153, 5, 236, 341, 107, 244, 107, 245, 170, 62, 32, 261, 188, 255, 74, 260, 32, 262, 188, 261, 74, 75, 236, 341, 107, 245, 170, 32, 263, 188, 262, 1, 256, 32, 264, 188, 262, 223, 62, 107, 195, 27, 237, 243, 74, 253, 32, 265, 188, 263, 223, 62, 107, 195, 27, 168, 254, 32, 266, 188, 55, 236, 240, 74, 264, 107, 265, 188, 265, 107, 267, 188, 259, 236, 343, 170, 170, 32, 257, 188, 186, 236, 257, 107, 266, 170, 32, 77, 32, 268, 188, 12, 236, 257, 107, 248, 188, 341, 107, 269, 188, 156, 170, 32, 134, 260, 153, 5, 236, 341, 107, 244, 107, 245, 170, 62, 32, 261, 188, 255, 74, 260, 32, 262, 188, 261, 74, 75, 236, 341, 107, 245, 170, 32, 263, 188, 262, 1, 256, 32, 264, 188, 262, 223, 62, 107, 195, 27, 237, 243, 74, 253, 32, 265, 188, 263, 223, 62, 107, 195, 27, 168, 254, 32, 266, 188, 55, 236, 240, 74, 264, 107, 265, 188, 265, 107, 267, 188, 259, 236, 343, 170, 170, 32, 251, 167, 106, 236, 266, 4, 268, 170, 32, 77, 32, 270, 188, 214, 236, 251, 107, 248, 188, 341, 170, 32, 134, 260, 153, 5, 236, 341, 107, 244, 107, 245, 170, 62, 32, 261, 188, 255, 74, 260, 32, 262, 188, 261, 74, 75, 236, 341, 107, 245, 170, 32, 263, 188, 262, 1, 256, 32, 264, 188, 262, 223, 62, 107, 195, 27, 237, 243, 74, 253, 32, 265, 188, 263, 223, 62, 107, 195, 27, 168, 254, 32, 266, 188, 55, 236, 240, 74, 264, 107, 265, 188, 265, 107, 267, 188, 259, 236, 343, 170, 170, 32, 271, 188, 83, 236, 106, 236, 266, 4, 268, 170, 107, 270, 170, 32, 10, 236, 242, 74, 264, 107, 271, 107, 265, 188, 265, 170, 32, 77, 32, 3, 32]}, {"code": "def triton_jagged_softmax_kernel_variable_length_loop_buffer_then_sum(\n    input_ptr_values,\n    input_ptr_offsets,\n    output_ptr,\n    M,\n    BLOCK_SIZE_RAGGED: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n\n    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\n\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n\n    ragged_start, ragged_end = (\n        tl.load(input_ptr_offsets + pid_b),\n        tl.load(input_ptr_offsets + (pid_b + 1)),\n    )\n\n    buffer_max_all = tl.full(\n        (BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), value=float(\"-inf\"), dtype=tl.float32\n    )\n\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))\n        buffer_max_all = tl.maximum(buffer_max_all, input)\n\n    buffer_max = tl.max(buffer_max_all, axis=0, keep_dims=True)\n\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))\n        buffer += tl.exp(input - buffer_max)\n\n    buffer_exp_sum = tl.sum(buffer, axis=0)\n\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))\n        output = tl.fdiv(tl.exp(input - buffer_max), buffer_exp_sum)\n\n        tl.store(output_ptr + idxs, output, mask=mask)", "encoded": [29, 340, 236, 240, 107, 241, 107, 242, 107, 243, 107, 244, 63, 6, 107, 245, 63, 6, 170, 63, 32, -1, 246, 188, 166, 236, 247, 188, 341, 170, 32, 248, 188, 246, 48, 65, 236, 243, 107, 245, 170, 32, 249, 188, 246, 219, 65, 236, 243, 107, 245, 170, 32, 250, 188, 171, 236, 236, 244, 107, 245, 170, 107, 90, 188, 143, 170, 32, 251, 188, 249, 237, 245, 32, 252, 188, 251, 74, 75, 236, 341, 107, 245, 170, 32, 253, 188, 252, 1, 243, 32, 254, 107, 255, 188, 236, 57, 236, 241, 74, 248, 170, 107, 57, 236, 241, 74, 236, 248, 74, 342, 170, 170, 170, 32, 256, 188, 230, 236, 236, 244, 107, 245, 170, 107, 257, 188, 258, 236, 343, 170, 107, 90, 188, 143, 170, 32, 134, 259, 153, 5, 236, 254, 107, 255, 107, 244, 170, 63, 32, 260, 188, 259, 74, 75, 236, 341, 107, 244, 170, 32, 261, 188, 260, 1, 255, 32, 262, 188, 260, 223, 63, 107, 195, 27, 237, 243, 74, 252, 32, 263, 188, 261, 223, 63, 107, 195, 27, 168, 253, 32, 264, 188, 57, 236, 240, 74, 262, 107, 263, 188, 263, 107, 265, 188, 258, 236, 343, 170, 170, 32, 256, 188, 186, 236, 256, 107, 264, 170, 32, 77, 32, 266, 188, 12, 236, 256, 107, 247, 188, 341, 107, 267, 188, 156, 170, 32, 134, 259, 153, 5, 236, 254, 107, 255, 107, 244, 170, 63, 32, 260, 188, 259, 74, 75, 236, 341, 107, 244, 170, 32, 261, 188, 260, 1, 255, 32, 262, 188, 260, 223, 63, 107, 195, 27, 237, 243, 74, 252, 32, 263, 188, 261, 223, 63, 107, 195, 27, 168, 253, 32, 264, 188, 57, 236, 240, 74, 262, 107, 263, 188, 263, 107, 265, 188, 258, 236, 343, 170, 170, 32, 250, 167, 106, 236, 264, 4, 266, 170, 32, 77, 32, 268, 188, 214, 236, 250, 107, 247, 188, 341, 170, 32, 134, 259, 153, 5, 236, 254, 107, 255, 107, 244, 170, 63, 32, 260, 188, 259, 74, 75, 236, 341, 107, 244, 170, 32, 261, 188, 260, 1, 255, 32, 262, 188, 260, 223, 63, 107, 195, 27, 237, 243, 74, 252, 32, 263, 188, 261, 223, 63, 107, 195, 27, 168, 253, 32, 264, 188, 57, 236, 240, 74, 262, 107, 263, 188, 263, 107, 265, 188, 258, 236, 343, 170, 170, 32, 269, 188, 83, 236, 106, 236, 264, 4, 266, 170, 107, 268, 170, 32, 10, 236, 242, 74, 262, 107, 269, 107, 263, 188, 263, 170, 32, 77, 32, 3, 32]}, {"code": "def triton_jagged_sum_kernel_simple_fused_sum_then_buffer(\n    input_ptr_values,\n    input_ptr_offsets,\n    output_ptr,\n    M,\n    MAX_SEQLEN,\n    BLOCK_SIZE_RAGGED: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n\n    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)\n\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n\n    ragged_start, ragged_end = (\n        tl.load(input_ptr_offsets + pid_ragged),\n        tl.load(input_ptr_offsets + (pid_ragged + 1)),\n    )\n\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)\n\n        buffer += tl.sum(input, axis=0)\n\n    buffer_view = buffer.reshape(\n        (BLOCK_SIZE_M,),\n    )\n\n    output_offsets = offsets_m + (pid_ragged * M)\n    output_mask = output_offsets < (M * (pid_ragged + 1))\n\n    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)", "encoded": [29, 340, 236, 240, 107, 241, 107, 242, 107, 243, 107, 244, 107, 245, 62, 6, 107, 246, 62, 6, 170, 62, 32, -1, 247, 188, 166, 236, 248, 188, 341, 170, 32, 249, 188, 247, 48, 64, 236, 243, 107, 246, 170, 32, 250, 188, 247, 219, 64, 236, 243, 107, 246, 170, 32, 251, 188, 171, 236, 236, 342, 107, 246, 170, 107, 90, 188, 143, 170, 32, 252, 188, 250, 237, 246, 32, 253, 188, 252, 74, 75, 236, 341, 107, 246, 170, 32, 254, 188, 253, 1, 243, 32, 255, 107, 256, 188, 236, 55, 236, 241, 74, 249, 170, 107, 55, 236, 241, 74, 236, 249, 74, 342, 170, 170, 170, 32, 134, 257, 153, 5, 236, 341, 107, 244, 107, 245, 170, 62, 32, 258, 188, 255, 74, 257, 32, 259, 188, 258, 74, 75, 236, 341, 107, 245, 170, 32, 260, 188, 259, 1, 256, 32, 261, 188, 259, 223, 62, 107, 195, 27, 237, 243, 74, 253, 32, 262, 188, 260, 223, 62, 107, 195, 27, 168, 254, 32, 263, 188, 55, 236, 240, 74, 261, 107, 262, 188, 262, 107, 264, 188, 341, 170, 32, 251, 167, 214, 236, 263, 107, 248, 188, 341, 170, 32, 77, 32, 265, 188, 251, 81, 266, 236, 236, 246, 107, 170, 170, 32, 267, 188, 253, 74, 249, 237, 243, 32, 268, 188, 267, 1, 243, 237, 236, 249, 74, 342, 170, 32, 10, 236, 242, 74, 267, 107, 265, 107, 262, 188, 268, 170, 32, 3, 32]}, {"code": "def triton_jagged_sum_kernel_simple_fused_buffer_then_sum(\n    input_ptr_values,\n    input_ptr_offsets,\n    output_ptr,\n    M,\n    MAX_SEQLEN,\n    BLOCK_SIZE_RAGGED: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n\n    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\n\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n\n    ragged_start, ragged_end = (\n        tl.load(input_ptr_offsets + pid_ragged),\n        tl.load(input_ptr_offsets + (pid_ragged + 1)),\n    )\n\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)\n\n    buffer_sum = tl.sum(buffer, axis=0)\n\n    buffer_view = buffer_sum.reshape(\n        (BLOCK_SIZE_M,),\n    )\n\n    output_offsets = offsets_m + (pid_ragged * M)\n    output_mask = output_offsets < (M * (pid_ragged + 1))\n\n    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)", "encoded": [29, 340, 236, 240, 107, 241, 107, 242, 107, 243, 107, 244, 107, 245, 63, 6, 107, 246, 63, 6, 170, 63, 32, -1, 247, 188, 166, 236, 248, 188, 341, 170, 32, 249, 188, 247, 48, 65, 236, 243, 107, 246, 170, 32, 250, 188, 247, 219, 65, 236, 243, 107, 246, 170, 32, 251, 188, 171, 236, 236, 245, 107, 246, 170, 107, 90, 188, 143, 170, 32, 252, 188, 250, 237, 246, 32, 253, 188, 252, 74, 75, 236, 341, 107, 246, 170, 32, 254, 188, 253, 1, 243, 32, 255, 107, 256, 188, 236, 57, 236, 241, 74, 249, 170, 107, 57, 236, 241, 74, 236, 249, 74, 342, 170, 170, 170, 32, 134, 257, 153, 5, 236, 341, 107, 244, 107, 245, 170, 63, 32, 258, 188, 255, 74, 257, 32, 259, 188, 258, 74, 75, 236, 341, 107, 245, 170, 32, 260, 188, 259, 1, 256, 32, 261, 188, 259, 223, 63, 107, 195, 27, 237, 243, 74, 253, 32, 262, 188, 260, 223, 63, 107, 195, 27, 168, 254, 32, 251, 167, 57, 236, 240, 74, 261, 107, 262, 188, 262, 107, 263, 188, 341, 170, 32, 77, 32, 264, 188, 214, 236, 251, 107, 248, 188, 341, 170, 32, 265, 188, 264, 81, 266, 236, 236, 246, 107, 170, 170, 32, 267, 188, 253, 74, 249, 237, 243, 32, 268, 188, 267, 1, 243, 237, 236, 249, 74, 342, 170, 32, 10, 236, 242, 74, 267, 107, 265, 107, 262, 188, 268, 170, 32, 3, 32]}, {"code": "def triton_jagged_sum_kernel_variable_length_loop_sum_then_buffer(\n    input_ptr_values,\n    input_ptr_offsets,\n    output_ptr,\n    M,\n    BLOCK_SIZE_RAGGED: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n\n    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)\n\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n\n    ragged_start, ragged_end = (\n        tl.load(input_ptr_offsets + pid_b),\n        tl.load(input_ptr_offsets + (pid_b + 1)),\n    )\n\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)\n\n        buffer += tl.sum(input, axis=0)\n\n    buffer_view = buffer.reshape(\n        (BLOCK_SIZE_M,),\n    )\n\n    output_offsets = offsets_m + (pid_b * M)\n    output_mask = output_offsets < (M * (pid_b + 1))\n\n    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)", "encoded": [29, 340, 236, 240, 107, 241, 107, 242, 107, 243, 107, 244, 62, 6, 107, 245, 62, 6, 170, 62, 32, -1, 246, 188, 166, 236, 247, 188, 341, 170, 32, 248, 188, 246, 48, 64, 236, 243, 107, 245, 170, 32, 249, 188, 246, 219, 64, 236, 243, 107, 245, 170, 32, 250, 188, 171, 236, 236, 342, 107, 245, 170, 107, 90, 188, 143, 170, 32, 251, 188, 249, 237, 245, 32, 252, 188, 251, 74, 75, 236, 341, 107, 245, 170, 32, 253, 188, 252, 1, 243, 32, 254, 107, 255, 188, 236, 55, 236, 241, 74, 248, 170, 107, 55, 236, 241, 74, 236, 248, 74, 342, 170, 170, 170, 32, 134, 256, 153, 5, 236, 254, 107, 255, 107, 244, 170, 62, 32, 257, 188, 256, 74, 75, 236, 341, 107, 244, 170, 32, 258, 188, 257, 1, 255, 32, 259, 188, 257, 223, 62, 107, 195, 27, 237, 243, 74, 252, 32, 260, 188, 258, 223, 62, 107, 195, 27, 168, 253, 32, 261, 188, 55, 236, 240, 74, 259, 107, 260, 188, 260, 107, 262, 188, 341, 170, 32, 250, 167, 214, 236, 261, 107, 247, 188, 341, 170, 32, 77, 32, 263, 188, 250, 81, 264, 236, 236, 245, 107, 170, 170, 32, 265, 188, 252, 74, 248, 237, 243, 32, 266, 188, 265, 1, 243, 237, 236, 248, 74, 342, 170, 32, 10, 236, 242, 74, 265, 107, 263, 107, 260, 188, 266, 170, 32, 3, 32]}, {"code": "def triton_jagged_sum_kernel_variable_length_loop_buffer_then_sum(\n    input_ptr_values,\n    input_ptr_offsets,\n    output_ptr,\n    M,\n    BLOCK_SIZE_RAGGED: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n\n    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\n\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n\n    ragged_start, ragged_end = (\n        tl.load(input_ptr_offsets + pid_ragged),\n        tl.load(input_ptr_offsets + (pid_ragged + 1)),\n    )\n\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n\n        idxs = (offsets_ragged[:, None] * M) + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n\n        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)\n\n    buffer_sum = tl.sum(buffer, axis=0)\n\n    buffer_view = buffer_sum.reshape(\n        (BLOCK_SIZE_M,),\n    )\n\n    output_offsets = offsets_m + (pid_ragged * M)\n    output_mask = output_offsets < (M * (pid_ragged + 1))\n\n    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)", "encoded": [29, 340, 236, 240, 107, 241, 107, 242, 107, 243, 107, 244, 63, 6, 107, 245, 63, 6, 170, 63, 32, -1, 246, 188, 166, 236, 247, 188, 341, 170, 32, 248, 188, 246, 48, 65, 236, 243, 107, 245, 170, 32, 249, 188, 246, 219, 65, 236, 243, 107, 245, 170, 32, 250, 188, 171, 236, 236, 244, 107, 245, 170, 107, 90, 188, 143, 170, 32, 251, 188, 249, 237, 245, 32, 252, 188, 251, 74, 75, 236, 341, 107, 245, 170, 32, 253, 188, 252, 1, 243, 32, 254, 107, 255, 188, 236, 57, 236, 241, 74, 248, 170, 107, 57, 236, 241, 74, 236, 248, 74, 342, 170, 170, 170, 32, 134, 256, 153, 5, 236, 254, 107, 255, 107, 244, 170, 63, 32, 257, 188, 256, 74, 75, 236, 341, 107, 244, 170, 32, 258, 188, 257, 1, 255, 32, 259, 188, 257, 223, 63, 107, 195, 27, 237, 243, 74, 252, 32, 260, 188, 258, 223, 63, 107, 195, 27, 168, 253, 32, 250, 167, 57, 236, 240, 74, 259, 107, 260, 188, 260, 107, 261, 188, 341, 170, 32, 77, 32, 262, 188, 214, 236, 250, 107, 247, 188, 341, 170, 32, 263, 188, 262, 81, 264, 236, 236, 245, 107, 170, 170, 32, 265, 188, 252, 74, 248, 237, 243, 32, 266, 188, 265, 1, 243, 237, 236, 248, 74, 342, 170, 32, 10, 236, 242, 74, 265, 107, 263, 107, 260, 188, 266, 170, 32, 3, 32]}, {"code": "def quantize_2d_bf16_to_int2(\n    bf16_ptr,\n    int8_ptr,\n    M,\n    N,\n    stride_bm,\n    stride_bn,\n    stride_im,\n    stride_in,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * (BLOCK_N // 4) + tl.arange(0, BLOCK_N // 4)\n\n    mask_m = offs_m < M\n    mask_n = offs_n < (N // 4)\n\n    packed_output = tl.zeros((BLOCK_M, BLOCK_N // 4), dtype=tl.int32)\n\n    for i in range(4):\n\n        column_idx = offs_n * 4 + i\n        bf16_vals = tl.load(\n            bf16_ptr + offs_m[:, None] * stride_bm + column_idx[None, :] * stride_bn,\n            mask=mask_m[:, None] & (column_idx[None, :] < N),\n            other=0.0,\n        )\n\n        int2_vals = tl.where(\n            bf16_vals == -2.0,\n            0,\n            tl.where(\n                bf16_vals == -1.0,\n                1,\n                tl.where(bf16_vals == 0.0, 2, 3),\n            ),\n        )\n\n        packed_output = packed_output | (int2_vals << (i * 2))\n\n    tl.store(\n        int8_ptr + offs_m[:, None] * stride_im + offs_n[None, :] * stride_in,\n        packed_output.to(tl.int8),\n        mask=mask_m[:, None] & mask_n[None, :],\n    )", "encoded": [29, 340, 236, 240, 107, 241, 107, 242, 107, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 62, 6, 107, 249, 62, 6, 170, 62, 32, -1, 250, 188, 166, 236, 341, 170, 32, 251, 188, 166, 236, 342, 170, 32, 252, 188, 250, 237, 248, 74, 75, 236, 341, 107, 248, 170, 32, 253, 188, 251, 237, 236, 249, 48, 343, 170, 74, 75, 236, 341, 107, 249, 48, 343, 170, 32, 254, 188, 252, 1, 242, 32, 255, 188, 253, 1, 243, 48, 343, 32, 256, 188, 171, 236, 236, 248, 107, 249, 48, 343, 170, 107, 90, 188, 239, 170, 32, 134, 257, 153, 5, 236, 343, 170, 62, 32, 258, 188, 253, 237, 343, 74, 257, 32, 259, 188, 55, 236, 240, 74, 252, 223, 62, 107, 195, 27, 237, 244, 74, 258, 223, 195, 107, 62, 27, 237, 245, 107, 260, 188, 254, 223, 62, 107, 195, 27, 168, 236, 258, 223, 195, 107, 62, 27, 1, 243, 170, 107, 261, 188, 341, 170, 32, 262, 188, 198, 236, 259, 76, 4, 344, 107, 341, 107, 198, 236, 259, 76, 4, 342, 107, 342, 107, 198, 236, 259, 76, 341, 107, 344, 107, 345, 170, 170, 170, 32, 256, 188, 256, 148, 262, 135, 257, 237, 344, 32, 77, 32, 10, 236, 241, 74, 252, 223, 62, 107, 195, 27, 237, 246, 74, 253, 223, 195, 107, 62, 27, 237, 247, 107, 256, 81, 263, 236, 138, 170, 107, 260, 188, 254, 223, 62, 107, 195, 27, 168, 255, 223, 195, 107, 62, 27, 170, 32, 3, 32]}, {"code": "def dequantize_2d_int2_to_bf16(\n    int8_ptr,\n    bf16_ptr,\n    M,\n    N,\n    stride_im,\n    stride_in,\n    stride_bm,\n    stride_bn,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * (BLOCK_N // 4) + tl.arange(0, BLOCK_N // 4)\n\n    mask_m = offs_m < M\n    mask_n = offs_n < (N // 4)\n\n    packed_vals = tl.load(\n        int8_ptr + offs_m[:, None] * stride_im + offs_n[None, :] * stride_in,\n        mask=mask_m[:, None] & mask_n[None, :],\n        other=0,\n    ).to(tl.int32)\n\n    for i in range(4):\n        shift = i * 2\n        mask = 0b11 << shift\n        int2_vals = (packed_vals & mask) >> shift\n\n        bf16_vals = tl.where(\n            int2_vals == 0b00,\n            tl.full(int2_vals.shape, -2.0, dtype=tl.float32),\n            tl.where(\n                int2_vals == 0b01,\n                tl.full(int2_vals.shape, -1.0, dtype=tl.float32),\n                tl.where(\n                    int2_vals == 0b10,\n                    tl.full(int2_vals.shape, 0.0, dtype=tl.float32),\n                    tl.full(int2_vals.shape, 1.0, dtype=tl.float32),\n                ),\n            ),\n        )\n\n        output_idx = offs_n * 4 + i\n\n        tl.store(\n            bf16_ptr + offs_m[:, None] * stride_bm + output_idx[None, :] * stride_bn,\n            bf16_vals.to(tl.bfloat16),\n            mask=mask_m[:, None] & (output_idx[None, :] < N),\n        )", "encoded": [29, 340, 236, 240, 107, 241, 107, 242, 107, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 63, 6, 107, 249, 63, 6, 170, 63, 32, -1, 250, 188, 166, 236, 341, 170, 32, 251, 188, 166, 236, 342, 170, 32, 252, 188, 250, 237, 248, 74, 75, 236, 341, 107, 248, 170, 32, 253, 188, 251, 237, 236, 249, 48, 343, 170, 74, 75, 236, 341, 107, 249, 48, 343, 170, 32, 254, 188, 252, 1, 242, 32, 255, 188, 253, 1, 243, 48, 343, 32, 256, 188, 57, 236, 240, 74, 252, 223, 63, 107, 195, 27, 237, 244, 74, 253, 223, 195, 107, 63, 27, 237, 245, 107, 257, 188, 254, 223, 63, 107, 195, 27, 168, 255, 223, 195, 107, 63, 27, 107, 258, 188, 341, 170, 81, 259, 236, 62, 170, 32, 134, 260, 153, 5, 236, 343, 170, 63, 32, 261, 188, 260, 237, 344, 32, 257, 188, 345, 135, 261, 32, 262, 188, 236, 256, 168, 257, 170, 126, 261, 32, 263, 188, 198, 236, 262, 76, 341, 107, 230, 236, 262, 81, 122, 107, 4, 344, 107, 90, 188, 143, 170, 107, 198, 236, 262, 76, 342, 107, 230, 236, 262, 81, 122, 107, 4, 342, 107, 90, 188, 143, 170, 107, 198, 236, 262, 76, 344, 107, 230, 236, 262, 81, 122, 107, 341, 107, 90, 188, 143, 170, 107, 230, 236, 262, 81, 122, 107, 342, 107, 90, 188, 143, 170, 170, 170, 170, 32, 264, 188, 253, 237, 343, 74, 260, 32, 10, 236, 241, 74, 252, 223, 63, 107, 195, 27, 237, 246, 74, 264, 223, 195, 107, 63, 27, 237, 247, 107, 263, 81, 259, 236, 226, 170, 107, 257, 188, 254, 223, 63, 107, 195, 27, 168, 236, 264, 223, 195, 107, 63, 27, 1, 243, 170, 170, 32, 77, 32, 3, 32]}, {"code": "def triton_sum_kernel_1D_result_sum_then_buffer(\n    input_ptr,\n    output_ptr,\n    M,\n    N,\n    BLOCK_SIZE_NON_REDUCE_DIM: tl.constexpr,\n    BLOCK_SIZE_REDUCE_DIM: tl.constexpr,\n    dim: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n\n    reduce_dim_len = M if dim == 0 else N\n    non_reduce_dim_len = N if dim == 0 else M\n\n    buffer = tl.zeros((1, BLOCK_SIZE_NON_REDUCE_DIM), dtype=tl.float32)\n\n    block_start_non_reduce_dim = pid * BLOCK_SIZE_NON_REDUCE_DIM\n    offsets_non_reduce_dim = block_start_non_reduce_dim + tl.arange(\n        0, BLOCK_SIZE_NON_REDUCE_DIM\n    )\n    mask_non_reduce_dim = offsets_non_reduce_dim < non_reduce_dim_len\n\n    for block_start_reduce_dim in range(0, reduce_dim_len, BLOCK_SIZE_REDUCE_DIM):\n        offsets_reduce_dim = block_start_reduce_dim + tl.arange(\n            0, BLOCK_SIZE_REDUCE_DIM\n        )\n        mask_reduce_dim = offsets_reduce_dim < reduce_dim_len\n\n        idxs, mask = None, None\n        if dim == 0:\n            idxs = (\n                offsets_reduce_dim[:, None] * non_reduce_dim_len\n            ) + offsets_non_reduce_dim\n            mask = mask_reduce_dim[:, None] & mask_non_reduce_dim\n        elif dim == 1:\n            idxs = (\n                offsets_non_reduce_dim[:, None] * reduce_dim_len\n            ) + offsets_reduce_dim\n            mask = mask_non_reduce_dim[:, None] & mask_reduce_dim\n\n        input = tl.load(input_ptr + idxs, mask=mask, other=mask)\n\n        buffer += tl.sum(input, axis=dim)\n\n    buffer_view = buffer.reshape(\n        (BLOCK_SIZE_NON_REDUCE_DIM,),\n    )\n\n    tl.store(output_ptr + offsets_non_reduce_dim, buffer_view, mask=mask_non_reduce_dim)", "encoded": [29, 340, 236, 240, 107, 241, 107, 242, 107, 243, 107, 244, 62, 6, 107, 245, 62, 6, 107, 165, 62, 6, 170, 62, 32, -1, 246, 188, 166, 236, 247, 188, 341, 170, 32, 248, 188, 242, 177, 165, 76, 341, 30, 243, 32, 249, 188, 243, 177, 165, 76, 341, 30, 242, 32, 250, 188, 171, 236, 236, 342, 107, 244, 170, 107, 90, 188, 143, 170, 32, 251, 188, 246, 237, 244, 32, 252, 188, 251, 74, 75, 236, 341, 107, 244, 170, 32, 253, 188, 252, 1, 249, 32, 134, 254, 153, 5, 236, 341, 107, 248, 107, 245, 170, 62, 32, 255, 188, 254, 74, 75, 236, 341, 107, 245, 170, 32, 256, 188, 255, 1, 248, 32, 257, 107, 258, 188, 236, 195, 107, 195, 170, 32, 177, 165, 76, 341, 62, 32, 257, 188, 255, 223, 62, 107, 195, 27, 237, 249, 74, 252, 32, 258, 188, 256, 223, 62, 107, 195, 27, 168, 253, 32, 65, 32, 37, 165, 76, 342, 62, 32, 257, 188, 252, 223, 62, 107, 195, 27, 237, 248, 74, 255, 32, 258, 188, 253, 223, 62, 107, 195, 27, 168, 256, 32, 180, 32, 259, 188, 55, 236, 240, 74, 257, 107, 258, 188, 258, 107, 260, 188, 258, 170, 32, 250, 167, 214, 236, 259, 107, 247, 188, 165, 170, 32, 77, 32, 261, 188, 250, 81, 262, 236, 236, 244, 107, 170, 170, 32, 10, 236, 241, 74, 252, 107, 261, 107, 258, 188, 253, 170, 32, 3, 32]}, {"code": "def triton_sum_kernel_1D_result_buffer_then_sum(\n    input_ptr,\n    output_ptr,\n    M,\n    N,\n    BLOCK_SIZE_NON_REDUCE_DIM: tl.constexpr,\n    BLOCK_SIZE_REDUCE_DIM: tl.constexpr,\n    dim: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n\n    reduce_dim_len = M if dim == 0 else N\n    non_reduce_dim_len = N if dim == 0 else M\n\n    buffer = tl.zeros(\n        (BLOCK_SIZE_REDUCE_DIM, BLOCK_SIZE_NON_REDUCE_DIM), dtype=tl.float32\n    )\n\n    block_start_non_reduce_dim = pid * BLOCK_SIZE_NON_REDUCE_DIM\n    offsets_non_reduce_dim = block_start_non_reduce_dim + tl.arange(\n        0, BLOCK_SIZE_NON_REDUCE_DIM\n    )\n    mask_non_reduce_dim = offsets_non_reduce_dim < non_reduce_dim_len\n\n    for block_start_reduce_dim in range(0, reduce_dim_len, BLOCK_SIZE_REDUCE_DIM):\n        offsets_reduce_dim = block_start_reduce_dim + tl.arange(\n            0, BLOCK_SIZE_REDUCE_DIM\n        )\n        mask_reduce_dim = offsets_reduce_dim < reduce_dim_len\n\n        idxs, mask = None, None\n        if dim == 0:\n            idxs = (\n                offsets_reduce_dim[:, None] * non_reduce_dim_len\n            ) + offsets_non_reduce_dim\n            mask = mask_reduce_dim[:, None] & mask_non_reduce_dim\n        elif dim == 1:\n            idxs = (\n                offsets_non_reduce_dim[:, None] * reduce_dim_len\n            ) + offsets_reduce_dim\n            mask = mask_non_reduce_dim[:, None] & mask_reduce_dim\n\n        buffer += tl.load(input_ptr + idxs, mask=mask, other=mask)\n\n    buffer_sum = tl.sum(buffer, axis=dim)\n\n    buffer_view = buffer_sum.reshape(\n        (BLOCK_SIZE_NON_REDUCE_DIM,),\n    )\n\n    tl.store(output_ptr + offsets_non_reduce_dim, buffer_view, mask=mask_non_reduce_dim)", "encoded": [29, 340, 236, 240, 107, 241, 107, 242, 107, 243, 107, 244, 63, 6, 107, 245, 63, 6, 107, 165, 63, 6, 170, 63, 32, -1, 246, 188, 166, 236, 247, 188, 341, 170, 32, 248, 188, 242, 177, 165, 76, 341, 30, 243, 32, 249, 188, 243, 177, 165, 76, 341, 30, 242, 32, 250, 188, 171, 236, 236, 245, 107, 244, 170, 107, 90, 188, 143, 170, 32, 251, 188, 246, 237, 244, 32, 252, 188, 251, 74, 75, 236, 341, 107, 244, 170, 32, 253, 188, 252, 1, 249, 32, 134, 254, 153, 5, 236, 341, 107, 248, 107, 245, 170, 63, 32, 255, 188, 254, 74, 75, 236, 341, 107, 245, 170, 32, 256, 188, 255, 1, 248, 32, 257, 107, 258, 188, 236, 195, 107, 195, 170, 32, 177, 165, 76, 341, 63, 32, 257, 188, 255, 223, 63, 107, 195, 27, 237, 249, 74, 252, 32, 258, 188, 256, 223, 63, 107, 195, 27, 168, 253, 32, 66, 32, 37, 165, 76, 342, 63, 32, 257, 188, 252, 223, 63, 107, 195, 27, 237, 248, 74, 255, 32, 258, 188, 253, 223, 63, 107, 195, 27, 168, 256, 32, 180, 32, 250, 167, 57, 236, 240, 74, 257, 107, 258, 188, 258, 107, 259, 188, 258, 170, 32, 77, 32, 260, 188, 214, 236, 250, 107, 247, 188, 165, 170, 32, 261, 188, 260, 81, 262, 236, 236, 244, 107, 170, 170, 32, 10, 236, 241, 74, 252, 107, 261, 107, 258, 188, 253, 170, 32, 3, 32]}, {"code": "def triton_sum_kernel_2D_result_dim_1(\n    input_ptr,\n    output_ptr,\n    M: tl.constexpr,\n    N: tl.constexpr,\n    K: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n\n    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\n    pid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)\n\n    block_start_n = 0\n    block_start_k = pid_k * BLOCK_SIZE_K\n\n    offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)\n    offsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)\n\n    mask_n = offsets_n < N\n    mask_k = offsets_k < K\n\n    idxs_base = (offsets_n[:, None] * K) + offsets_k\n    idxs = idxs_base + (pid_m * N * K)\n\n    mask = mask_n[:, None] & mask_k\n\n    input = tl.load(input_ptr + idxs, mask=mask, other=0)\n\n    output = tl.sum(input, axis=0)\n\n    output_offsets = (pid_m * K) + offsets_k\n\n    tl.store(output_ptr + output_offsets, output, mask=mask_k)", "encoded": [29, 340, 236, 240, 107, 241, 107, 242, 62, 6, 107, 243, 62, 6, 107, 244, 62, 6, 107, 245, 62, 6, 107, 246, 62, 6, 170, 62, 32, -1, 247, 188, 166, 236, 248, 188, 341, 170, 32, 249, 188, 247, 48, 64, 236, 244, 107, 246, 170, 32, 250, 188, 247, 219, 64, 236, 244, 107, 246, 170, 32, 251, 188, 341, 32, 252, 188, 250, 237, 246, 32, 253, 188, 251, 74, 75, 236, 341, 107, 245, 170, 32, 254, 188, 252, 74, 75, 236, 341, 107, 246, 170, 32, 255, 188, 253, 1, 243, 32, 256, 188, 254, 1, 244, 32, 257, 188, 253, 223, 62, 107, 195, 27, 237, 244, 74, 254, 32, 258, 188, 257, 74, 249, 237, 243, 237, 244, 32, 259, 188, 255, 223, 62, 107, 195, 27, 168, 256, 32, 260, 188, 55, 236, 240, 74, 258, 107, 259, 188, 259, 107, 261, 188, 341, 170, 32, 262, 188, 214, 236, 260, 107, 248, 188, 341, 170, 32, 263, 188, 249, 237, 244, 74, 254, 32, 10, 236, 241, 74, 263, 107, 262, 107, 259, 188, 256, 170, 32, 3, 32]}, {"code": "def triton_sum_kernel_2D_result_dim_1_sum_then_buffer(\n    input_ptr,\n    output_ptr,\n    M: tl.constexpr,\n    N: tl.constexpr,\n    K: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n\n    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\n    pid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)\n\n    buffer = tl.zeros((1, BLOCK_SIZE_K), dtype=tl.float32)\n\n    block_start_k = pid_k * BLOCK_SIZE_K\n    offsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)\n    mask_k = offsets_k < K\n\n    for block_start_n in range(0, N, BLOCK_SIZE_N):\n        offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)\n        mask_n = offsets_n < N\n\n        idxs_base = (offsets_n[:, None] * K) + offsets_k\n        idxs = idxs_base + (pid_m * N * K)\n\n        mask = mask_n[:, None] & mask_k\n\n        input = tl.load(input_ptr + idxs, mask=mask, other=0)\n\n        buffer += tl.sum(input, axis=0)\n\n    buffer_view = buffer.reshape(\n        (BLOCK_SIZE_K,),\n    )\n\n    output_offsets = (pid_m * K) + offsets_k\n\n    tl.store(output_ptr + output_offsets, buffer_view, mask=mask_k)", "encoded": [29, 340, 236, 240, 107, 241, 107, 242, 63, 6, 107, 243, 63, 6, 107, 244, 63, 6, 107, 245, 63, 6, 107, 246, 63, 6, 170, 63, 32, -1, 247, 188, 166, 236, 248, 188, 341, 170, 32, 249, 188, 247, 48, 65, 236, 244, 107, 246, 170, 32, 250, 188, 247, 219, 65, 236, 244, 107, 246, 170, 32, 251, 188, 171, 236, 236, 342, 107, 246, 170, 107, 90, 188, 143, 170, 32, 252, 188, 250, 237, 246, 32, 253, 188, 252, 74, 75, 236, 341, 107, 246, 170, 32, 254, 188, 253, 1, 244, 32, 134, 255, 153, 5, 236, 341, 107, 243, 107, 245, 170, 63, 32, 256, 188, 255, 74, 75, 236, 341, 107, 245, 170, 32, 257, 188, 256, 1, 243, 32, 258, 188, 256, 223, 63, 107, 195, 27, 237, 244, 74, 253, 32, 259, 188, 258, 74, 249, 237, 243, 237, 244, 32, 260, 188, 257, 223, 63, 107, 195, 27, 168, 254, 32, 261, 188, 57, 236, 240, 74, 259, 107, 260, 188, 260, 107, 262, 188, 341, 170, 32, 251, 167, 214, 236, 261, 107, 248, 188, 341, 170, 32, 77, 32, 263, 188, 251, 81, 264, 236, 236, 246, 107, 170, 170, 32, 265, 188, 249, 237, 244, 74, 253, 32, 10, 236, 241, 74, 265, 107, 263, 107, 260, 188, 254, 170, 32, 3, 32]}, {"code": "def triton_sum_kernel_2D_result_dim_1_buffer_then_sum(\n    input_ptr,\n    output_ptr,\n    M: tl.constexpr,\n    N: tl.constexpr,\n    K: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n\n    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\n    pid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)\n\n    buffer = tl.zeros((BLOCK_SIZE_N, BLOCK_SIZE_K), dtype=tl.float32)\n\n    block_start_k = pid_k * BLOCK_SIZE_K\n    offsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)\n    mask_k = offsets_k < K\n\n    for block_start_n in range(0, N, BLOCK_SIZE_N):\n        offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)\n        mask_n = offsets_n < N\n\n        idxs_base = (offsets_n[:, None] * K) + offsets_k\n        idxs = idxs_base + (pid_m * N * K)\n\n        mask = mask_n[:, None] & mask_k\n\n        input = tl.load(input_ptr + idxs, mask=mask, other=0)\n\n        buffer += input\n\n    output = tl.sum(buffer, axis=0)\n\n    output_offsets = (pid_m * K) + offsets_k\n\n    tl.store(output_ptr + output_offsets, output, mask=mask_k)", "encoded": [29, 340, 236, 240, 107, 241, 107, 242, 62, 6, 107, 243, 62, 6, 107, 244, 62, 6, 107, 245, 62, 6, 107, 246, 62, 6, 170, 62, 32, -1, 247, 188, 166, 236, 248, 188, 341, 170, 32, 249, 188, 247, 48, 64, 236, 244, 107, 246, 170, 32, 250, 188, 247, 219, 64, 236, 244, 107, 246, 170, 32, 251, 188, 171, 236, 236, 245, 107, 246, 170, 107, 90, 188, 143, 170, 32, 252, 188, 250, 237, 246, 32, 253, 188, 252, 74, 75, 236, 341, 107, 246, 170, 32, 254, 188, 253, 1, 244, 32, 134, 255, 153, 5, 236, 341, 107, 243, 107, 245, 170, 62, 32, 256, 188, 255, 74, 75, 236, 341, 107, 245, 170, 32, 257, 188, 256, 1, 243, 32, 258, 188, 256, 223, 62, 107, 195, 27, 237, 244, 74, 253, 32, 259, 188, 258, 74, 249, 237, 243, 237, 244, 32, 260, 188, 257, 223, 62, 107, 195, 27, 168, 254, 32, 261, 188, 55, 236, 240, 74, 259, 107, 260, 188, 260, 107, 262, 188, 341, 170, 32, 251, 167, 261, 32, 77, 32, 263, 188, 214, 236, 251, 107, 248, 188, 341, 170, 32, 264, 188, 249, 237, 244, 74, 253, 32, 10, 236, 241, 74, 264, 107, 263, 107, 260, 188, 254, 170, 32, 3, 32]}, {"code": "def triton_tem_fused_no_exp2(\n    arg_Q,\n    arg_K,\n    arg_V,\n    out_ptr0,\n    num_queries: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n):\n    Q = arg_Q\n    K = arg_K\n    V = arg_V\n\n    stride_qz = 4194304\n    stride_qh = 262144\n    stride_qm = 64\n    stride_qk = 1\n\n    stride_kz = 4194304\n    stride_kh = 262144\n    stride_kn = 64\n    stride_kk = 1\n\n    stride_vz = 4194304\n    stride_vh = 262144\n    stride_vk = 64\n    stride_vn = 1\n\n    Z = 16\n    H = 16\n    N_CTX = 4096\n\n    qk_scale = 1.0\n    MATMUL_PRECISION = tl.float16\n\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    qkv_offset = off_hz * stride_qh\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + qkv_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + qkv_offset,\n        shape=(BLOCK_DMODEL, N_CTX),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1),\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + qkv_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_vk, stride_vn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    q = tl.load(Q_block_ptr)\n    q = (q * qk_scale).to(MATMUL_PRECISION)\n\n    lo = 0\n    hi = N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k.to(MATMUL_PRECISION))\n\n        tmp0 = tl.full([1], 1024, tl.int64)\n        tmp1 = (offs_m[:, None]) <= tmp0\n        tmp2 = (start_n + offs_n[None, :]) <= tmp0\n        tmp3 = tmp1 & tmp2\n        tmp4 = (offs_m[:, None]) >= (start_n + offs_n[None, :])\n        tmp5 = tmp3 | tmp4\n        tmp6 = float(\"-inf\")\n        tmp7 = tmp6.to(tl.float32)\n        tmp8 = tl.where(tmp5, (qk), tmp7)\n        qk = tmp8\n\n        row_max = tl.max(qk, 1)\n        m_i_new = tl.maximum(m_i, row_max)\n        masked_out_rows = m_i_new == float(\"-inf\")\n\n        alpha = tl.math.exp(m_i - m_i_new)\n        alpha = tl.where(masked_out_rows, 0, alpha)\n        p = tl.math.exp(qk - m_i_new[:, None])\n        p = tl.where(masked_out_rows[:, None], 0, p)\n\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p.to(MATMUL_PRECISION), v.to(MATMUL_PRECISION))\n\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n\n    acc = acc / l_i[:, None]\n\n    idx_z = tl.program_id(1) // H\n    idx_h = tl.program_id(1) % H\n    idx_m = offs_m[:, None]\n    idx_d = tl.arange(0, BLOCK_DMODEL)[None, :]\n\n    mask = (idx_m != -1) & (idx_d != -1)\n    xindex = idx_d + (64 * idx_m) + (262144 * idx_h) + (4194304 * idx_z)\n    tl.store(out_ptr0 + (xindex), acc, None)", "encoded": [29, 341, 237, 241, 107, 242, 107, 243, 107, 244, 107, 245, 63, 6, 107, 246, 63, 6, 107, 247, 63, 6, 107, 248, 63, 6, 171, 63, 32, -1, 249, 189, 241, 32, 250, 189, 242, 32, 251, 189, 243, 32, 252, 189, 342, 343, 344, 342, 345, 346, 342, 32, 253, 189, 347, 348, 347, 343, 342, 342, 32, 254, 189, 348, 342, 32, 255, 189, 343, 32, 256, 189, 342, 343, 344, 342, 345, 346, 342, 32, 257, 189, 347, 348, 347, 343, 342, 342, 32, 258, 189, 348, 342, 32, 259, 189, 343, 32, 260, 189, 342, 343, 344, 342, 345, 346, 342, 32, 261, 189, 347, 348, 347, 343, 342, 342, 32, 262, 189, 348, 342, 32, 263, 189, 343, 32, 264, 189, 343, 348, 32, 265, 189, 343, 348, 32, 266, 189, 342, 346, 344, 348, 32, 267, 189, 343, 32, 268, 189, 21, 32, 269, 189, 167, 237, 346, 171, 32, 270, 189, 167, 237, 343, 171, 32, 271, 189, 270, 238, 253, 32, 272, 189, 213, 237, 273, 189, 249, 74, 271, 107, 122, 189, 237, 266, 107, 248, 171, 107, 274, 189, 237, 254, 107, 255, 171, 107, 275, 189, 237, 269, 238, 246, 107, 346, 171, 107, 276, 189, 237, 246, 107, 248, 171, 107, 277, 189, 237, 343, 107, 346, 171, 171, 32, 278, 189, 213, 237, 273, 189, 250, 74, 271, 107, 122, 189, 237, 248, 107, 266, 171, 107, 274, 189, 237, 259, 107, 258, 171, 107, 275, 189, 237, 346, 107, 346, 171, 107, 276, 189, 237, 248, 107, 247, 171, 107, 277, 189, 237, 346, 107, 343, 171, 171, 32, 279, 189, 213, 237, 273, 189, 251, 74, 271, 107, 122, 189, 237, 266, 107, 248, 171, 107, 274, 189, 237, 262, 107, 263, 171, 107, 275, 189, 237, 346, 107, 346, 171, 107, 276, 189, 237, 247, 107, 248, 171, 107, 277, 189, 237, 343, 107, 346, 171, 171, 32, 280, 189, 269, 238, 246, 74, 75, 237, 346, 107, 246, 171, 32, 281, 189, 75, 237, 346, 107, 247, 171, 32, 282, 189, 172, 237, 224, 246, 27, 107, 90, 189, 143, 171, 4, 283, 237, 349, 171, 32, 284, 189, 172, 237, 224, 246, 27, 107, 90, 189, 143, 171, 32, 285, 189, 172, 237, 224, 246, 107, 248, 27, 107, 90, 189, 143, 171, 32, 286, 189, 57, 237, 272, 171, 32, 286, 189, 237, 286, 238, 267, 171, 81, 287, 237, 268, 171, 32, 288, 189, 346, 32, 289, 189, 266, 32, 134, 290, 154, 5, 237, 288, 107, 289, 107, 247, 171, 63, 32, 290, 189, 55, 237, 290, 107, 247, 171, 32, 291, 189, 57, 237, 278, 171, 32, 292, 189, 57, 237, 279, 171, 32, 293, 189, 172, 237, 224, 246, 107, 247, 27, 107, 90, 189, 143, 171, 32, 293, 168, 15, 237, 286, 107, 291, 81, 287, 237, 268, 171, 171, 32, 294, 189, 231, 237, 224, 343, 27, 107, 343, 346, 347, 342, 107, 177, 171, 32, 295, 189, 280, 224, 63, 107, 196, 27, 212, 294, 32, 296, 189, 290, 74, 281, 224, 196, 107, 63, 27, 212, 294, 32, 297, 189, 295, 169, 296, 32, 298, 189, 280, 224, 63, 107, 196, 27, 142, 290, 74, 281, 224, 196, 107, 63, 27, 32, 299, 189, 297, 149, 298, 32, 300, 189, 283, 237, 350, 171, 32, 301, 189, 300, 81, 287, 237, 143, 171, 32, 302, 189, 199, 237, 299, 107, 293, 107, 301, 171, 32, 293, 189, 302, 32, 303, 189, 12, 237, 293, 107, 343, 171, 32, 304, 189, 187, 237, 282, 107, 303, 171, 32, 305, 189, 304, 76, 283, 237, 350, 171, 32, 306, 189, 144, 237, 282, 4, 304, 171, 32, 306, 189, 199, 237, 305, 107, 346, 107, 306, 171, 32, 307, 189, 144, 237, 293, 4, 304, 224, 63, 107, 196, 27, 171, 32, 307, 189, 199, 237, 305, 224, 63, 107, 196, 27, 107, 346, 107, 307, 171, 32, 308, 189, 284, 238, 346, 74, 306, 32, 285, 24, 308, 224, 63, 107, 196, 27, 32, 285, 168, 15, 237, 307, 81, 287, 237, 268, 171, 107, 292, 81, 287, 237, 268, 171, 171, 32, 284, 189, 284, 238, 306, 74, 215, 237, 307, 107, 343, 171, 32, 282, 189, 304, 32, 278, 189, 141, 237, 278, 107, 237, 346, 107, 247, 171, 171, 32, 279, 189, 141, 237, 279, 107, 237, 247, 107, 346, 171, 171, 32, 77, 32, 285, 189, 285, 42, 284, 224, 63, 107, 196, 27, 32, 309, 189, 167, 237, 343, 171, 48, 265, 32, 310, 189, 167, 237, 343, 171, 220, 265, 32, 311, 189, 280, 224, 63, 107, 196, 27, 32, 312, 189, 75, 237, 346, 107, 248, 171, 224, 196, 107, 63, 27, 32, 313, 189, 237, 311, 182, 4, 343, 171, 169, 237, 312, 182, 4, 343, 171, 32, 314, 189, 312, 74, 348, 342, 238, 311, 74, 347, 348, 347, 343, 342, 342, 238, 310, 74, 342, 343, 344, 342, 345, 346, 342, 238, 309, 32, 10, 237, 244, 74, 314, 107, 285, 107, 196, 171, 32, 3, 32]}, {"code": "def triton_tem_fused_with_exp2(\n    arg_Q,\n    arg_K,\n    arg_V,\n    out_ptr0,\n    num_queries: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n):\n\n    SCORE_MOD_IS_LINEAR: tl.constexpr = False\n    ROWS_GUARANTEED_SAFE: tl.constexpr = False\n    Q = arg_Q\n    K = arg_K\n    V = arg_V\n\n    stride_qz = 4194304\n    stride_qh = 262144\n    stride_qm = 64\n    stride_qk = 1\n\n    stride_kz = 4194304\n    stride_kh = 262144\n    stride_kn = 64\n    stride_kk = 1\n\n    stride_vz = 4194304\n    stride_vh = 262144\n    stride_vk = 64\n    stride_vn = 1\n\n    Z = 16\n    H = 16\n    N_CTX = 4096\n\n    qk_scale = 1.0\n    MATMUL_PRECISION = Q.dtype.element_ty\n\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    qkv_offset = off_hz * stride_qh\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + qkv_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + qkv_offset,\n        shape=(BLOCK_DMODEL, N_CTX),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1),\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + qkv_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_vk, stride_vn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    q = tl.load(Q_block_ptr)\n    if SCORE_MOD_IS_LINEAR:\n        qk_scale *= 1.44269504\n    q = (q * qk_scale).to(MATMUL_PRECISION)\n\n    lo = 0\n    hi = N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.dot(q, k.to(MATMUL_PRECISION), acc=qk)\n\n        tmp0 = tl.full([1], 1024, tl.int64)\n        tmp1 = (offs_m[:, None]) <= tmp0\n        tmp2 = (start_n + offs_n[None, :]) <= tmp0\n        tmp3 = tmp1 & tmp2\n        tmp4 = (offs_m[:, None]) >= (start_n + offs_n[None, :])\n        tmp5 = tmp3 | tmp4\n        tmp6 = float(\"-inf\")\n        tmp7 = tmp6.to(tl.float32)\n        tmp8 = tl.where(tmp5, (qk), tmp7)\n        qk = tmp8\n\n        if not SCORE_MOD_IS_LINEAR:\n            qk *= 1.44269504\n\n        row_max = tl.max(qk, 1)\n        m_i_new = tl.maximum(m_i, row_max)\n        masked_out_rows = m_i_new == float(\"-inf\")\n\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        if not ROWS_GUARANTEED_SAFE:\n            alpha = tl.where(masked_out_rows, 0, alpha)\n            p = tl.where(masked_out_rows[:, None], 0, p)\n\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc = tl.dot(p.to(MATMUL_PRECISION), v.to(MATMUL_PRECISION), acc)\n\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n\n    acc = acc / l_i[:, None]\n\n    idx_z = tl.program_id(1) // H\n    idx_h = tl.program_id(1) % H\n    idx_m = offs_m[:, None]\n    idx_d = tl.arange(0, BLOCK_DMODEL)[None, :]\n\n    mask = (idx_m != -1) & (idx_d != -1)\n    xindex = idx_d + (64 * idx_m) + (262144 * idx_h) + (4194304 * idx_z)\n    tl.store(out_ptr0 + (xindex), acc, None)", "encoded": [29, 341, 237, 241, 107, 242, 107, 243, 107, 244, 107, 245, 62, 6, 107, 246, 62, 6, 107, 247, 62, 6, 107, 248, 62, 6, 171, 62, 32, -1, 249, 62, 6, 189, 59, 32, 250, 62, 6, 189, 59, 32, 251, 189, 241, 32, 252, 189, 242, 32, 253, 189, 243, 32, 254, 189, 342, 343, 344, 342, 345, 346, 342, 32, 255, 189, 347, 348, 347, 343, 342, 342, 32, 256, 189, 348, 342, 32, 257, 189, 343, 32, 258, 189, 342, 343, 344, 342, 345, 346, 342, 32, 259, 189, 347, 348, 347, 343, 342, 342, 32, 260, 189, 348, 342, 32, 261, 189, 343, 32, 262, 189, 342, 343, 344, 342, 345, 346, 342, 32, 263, 189, 347, 348, 347, 343, 342, 342, 32, 264, 189, 348, 342, 32, 265, 189, 343, 32, 266, 189, 343, 348, 32, 267, 189, 343, 348, 32, 268, 189, 342, 346, 344, 348, 32, 269, 189, 343, 32, 270, 189, 251, 81, 90, 81, 113, 32, 271, 189, 167, 237, 346, 171, 32, 272, 189, 167, 237, 343, 171, 32, 273, 189, 272, 238, 255, 32, 274, 189, 213, 237, 275, 189, 251, 74, 273, 107, 122, 189, 237, 268, 107, 248, 171, 107, 276, 189, 237, 256, 107, 257, 171, 107, 277, 189, 237, 271, 238, 246, 107, 346, 171, 107, 278, 189, 237, 246, 107, 248, 171, 107, 279, 189, 237, 343, 107, 346, 171, 171, 32, 280, 189, 213, 237, 275, 189, 252, 74, 273, 107, 122, 189, 237, 248, 107, 268, 171, 107, 276, 189, 237, 261, 107, 260, 171, 107, 277, 189, 237, 346, 107, 346, 171, 107, 278, 189, 237, 248, 107, 247, 171, 107, 279, 189, 237, 346, 107, 343, 171, 171, 32, 281, 189, 213, 237, 275, 189, 253, 74, 273, 107, 122, 189, 237, 268, 107, 248, 171, 107, 276, 189, 237, 264, 107, 265, 171, 107, 277, 189, 237, 346, 107, 346, 171, 107, 278, 189, 237, 247, 107, 248, 171, 107, 279, 189, 237, 343, 107, 346, 171, 171, 32, 282, 189, 271, 238, 246, 74, 75, 237, 346, 107, 246, 171, 32, 283, 189, 75, 237, 346, 107, 247, 171, 32, 284, 189, 172, 237, 224, 246, 27, 107, 90, 189, 143, 171, 4, 285, 237, 349, 171, 32, 286, 189, 172, 237, 224, 246, 27, 107, 90, 189, 143, 171, 32, 287, 189, 172, 237, 224, 246, 107, 248, 27, 107, 90, 189, 143, 171, 32, 288, 189, 55, 237, 274, 171, 32, 178, 249, 62, 32, 269, 24, 350, 32, 181, 32, 288, 189, 237, 288, 238, 269, 171, 81, 289, 237, 270, 171, 32, 290, 189, 346, 32, 291, 189, 268, 32, 134, 292, 154, 5, 237, 290, 107, 291, 107, 247, 171, 62, 32, 292, 189, 57, 237, 292, 107, 247, 171, 32, 293, 189, 55, 237, 280, 171, 32, 294, 189, 55, 237, 281, 171, 32, 295, 189, 172, 237, 224, 246, 107, 247, 27, 107, 90, 189, 143, 171, 32, 295, 189, 15, 237, 288, 107, 293, 81, 289, 237, 270, 171, 107, 287, 189, 295, 171, 32, 296, 189, 231, 237, 224, 343, 27, 107, 343, 346, 347, 342, 107, 177, 171, 32, 297, 189, 282, 224, 62, 107, 196, 27, 212, 296, 32, 298, 189, 292, 74, 283, 224, 196, 107, 62, 27, 212, 296, 32, 299, 189, 297, 169, 298, 32, 300, 189, 282, 224, 62, 107, 196, 27, 142, 292, 74, 283, 224, 196, 107, 62, 27, 32, 301, 189, 299, 149, 300, 32, 302, 189, 285, 237, 351, 171, 32, 303, 189, 302, 81, 289, 237, 143, 171, 32, 304, 189, 199, 237, 301, 107, 295, 107, 303, 171, 32, 295, 189, 304, 32, 178, 63, 249, 62, 32, 295, 24, 350, 32, 181, 32, 305, 189, 12, 237, 295, 107, 343, 171, 32, 306, 189, 187, 237, 284, 107, 305, 171, 32, 307, 189, 306, 76, 285, 237, 351, 171, 32, 308, 189, 152, 237, 284, 4, 306, 171, 32, 309, 189, 152, 237, 295, 4, 306, 224, 62, 107, 196, 27, 171, 32, 178, 63, 250, 62, 32, 308, 189, 199, 237, 307, 107, 346, 107, 308, 171, 32, 309, 189, 199, 237, 307, 224, 62, 107, 196, 27, 107, 346, 107, 309, 171, 32, 181, 32, 310, 189, 286, 238, 346, 74, 308, 32, 287, 24, 310, 224, 62, 107, 196, 27, 32, 287, 189, 15, 237, 309, 81, 289, 237, 270, 171, 107, 294, 81, 289, 237, 270, 171, 107, 287, 171, 32, 286, 189, 286, 238, 308, 74, 215, 237, 309, 107, 343, 171, 32, 284, 189, 306, 32, 280, 189, 141, 237, 280, 107, 237, 346, 107, 247, 171, 171, 32, 281, 189, 141, 237, 281, 107, 237, 247, 107, 346, 171, 171, 32, 77, 32, 287, 189, 287, 42, 286, 224, 62, 107, 196, 27, 32, 311, 189, 167, 237, 343, 171, 48, 267, 32, 312, 189, 167, 237, 343, 171, 220, 267, 32, 313, 189, 282, 224, 62, 107, 196, 27, 32, 314, 189, 75, 237, 346, 107, 248, 171, 224, 196, 107, 62, 27, 32, 315, 189, 237, 313, 182, 4, 343, 171, 169, 237, 314, 182, 4, 343, 171, 32, 316, 189, 314, 74, 348, 342, 238, 313, 74, 347, 348, 347, 343, 342, 342, 238, 312, 74, 342, 343, 344, 342, 345, 346, 342, 238, 311, 32, 10, 237, 244, 74, 316, 107, 287, 107, 196, 171, 32, 3, 32]}, {"code": "def triton_red_fused_native_layer_norm_0(\n    in_out_ptr0,\n    in_ptr0,\n    in_ptr1,\n    in_ptr2,\n    out_ptr0,\n    out_ptr1,\n    xnumel,\n    rnumel,\n    XBLOCK: tl.constexpr,\n    RBLOCK: tl.constexpr,\n):\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex\n    tmp3_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    tmp3_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    tmp3_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp0 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"\n        ).to(tl.float32)\n        tmp1 = tmp0.to(tl.float32)\n        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(\n            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0\n        )\n        tmp3_mean = tl.where(rmask, tmp3_mean_next, tmp3_mean)\n        tmp3_m2 = tl.where(rmask, tmp3_m2_next, tmp3_m2)\n        tmp3_weight = tl.where(rmask, tmp3_weight_next, tmp3_weight)\n    tmp3_tmp, tmp4_tmp, tmp5_tmp = triton_helpers.welford(\n        tmp3_mean, tmp3_m2, tmp3_weight, 1\n    )\n    tmp3 = tmp3_tmp[:, None]\n    tmp4 = tmp4_tmp[:, None]\n    tmp5 = tmp5_tmp[:, None]\n    tl.store(out_ptr0 + (x0), tmp3, None)\n    tmp6 = rnumel\n    tmp7 = tmp4 / tmp6\n    tmp8 = 1e-05\n    tmp9 = tmp7 + tmp8\n    tmp10 = libdevice.rsqrt(tmp9)\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + (x0), tmp10, None)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp11 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"\n        ).to(tl.float32)\n        tmp15 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp18 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp12 = tmp11.to(tl.float32)\n        tmp13 = tmp12 - tmp3\n        tmp14 = tmp13 * tmp10\n        tmp16 = tmp15.to(tl.float32)\n        tmp17 = tmp14 * tmp16\n        tmp19 = tmp18.to(tl.float32)\n        tmp20 = tmp17 + tmp19\n        tmp21 = tmp20.to(tl.float32)\n        tl.store(out_ptr1 + (r1 + (rnumel * x0)), tmp21, rmask)", "encoded": [29, 341, 237, 241, 107, 242, 107, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 63, 6, 107, 250, 63, 6, 171, 63, 32, -1, 251, 189, 167, 237, 342, 171, 238, 249, 32, 252, 189, 251, 74, 75, 237, 342, 107, 249, 171, 224, 63, 107, 196, 27, 32, 253, 189, 252, 1, 247, 32, 254, 189, 75, 237, 342, 107, 250, 171, 224, 196, 107, 63, 27, 32, 255, 189, 252, 32, 256, 189, 172, 237, 224, 249, 107, 250, 27, 107, 143, 171, 32, 257, 189, 172, 237, 224, 249, 107, 250, 27, 107, 143, 171, 32, 258, 189, 172, 237, 224, 249, 107, 250, 27, 107, 143, 171, 32, 134, 259, 154, 5, 237, 342, 107, 248, 107, 250, 171, 63, 32, 260, 189, 259, 74, 254, 32, 261, 189, 260, 1, 248, 32, 262, 189, 260, 32, 263, 189, 57, 237, 242, 74, 237, 262, 74, 248, 238, 255, 171, 107, 261, 107, 264, 189, 343, 171, 81, 265, 237, 143, 171, 32, 266, 189, 263, 81, 265, 237, 143, 171, 32, 267, 189, 188, 237, 266, 107, 224, 249, 107, 250, 27, 171, 32, 268, 107, 269, 107, 270, 189, 125, 81, 271, 237, 267, 107, 256, 107, 257, 107, 258, 107, 259, 76, 342, 171, 32, 256, 189, 199, 237, 261, 107, 268, 107, 256, 171, 32, 257, 189, 199, 237, 261, 107, 269, 107, 257, 171, 32, 258, 189, 199, 237, 261, 107, 270, 107, 258, 171, 32, 77, 32, 272, 107, 273, 107, 274, 189, 125, 81, 275, 237, 256, 107, 257, 107, 258, 107, 344, 171, 32, 276, 189, 272, 224, 63, 107, 196, 27, 32, 277, 189, 273, 224, 63, 107, 196, 27, 32, 278, 189, 274, 224, 63, 107, 196, 27, 32, 10, 237, 245, 74, 255, 107, 276, 107, 196, 171, 32, 279, 189, 248, 32, 280, 189, 277, 42, 279, 32, 281, 189, 345, 32, 282, 189, 280, 74, 281, 32, 283, 189, 16, 81, 284, 237, 282, 171, 32, 51, 237, 171, 32, 10, 237, 241, 74, 255, 107, 283, 107, 196, 171, 32, 134, 259, 154, 5, 237, 342, 107, 248, 107, 250, 171, 63, 32, 260, 189, 259, 74, 254, 32, 261, 189, 260, 1, 248, 32, 262, 189, 260, 32, 285, 189, 57, 237, 242, 74, 237, 262, 74, 248, 238, 255, 171, 107, 261, 107, 264, 189, 346, 171, 81, 265, 237, 143, 171, 32, 286, 189, 57, 237, 243, 74, 262, 107, 261, 107, 264, 189, 343, 171, 81, 265, 237, 143, 171, 32, 287, 189, 57, 237, 244, 74, 262, 107, 261, 107, 264, 189, 343, 171, 81, 265, 237, 143, 171, 32, 288, 189, 285, 81, 265, 237, 143, 171, 32, 289, 189, 288, 4, 276, 32, 290, 189, 289, 238, 283, 32, 291, 189, 286, 81, 265, 237, 143, 171, 32, 292, 189, 290, 238, 291, 32, 293, 189, 287, 81, 265, 237, 143, 171, 32, 294, 189, 292, 74, 293, 32, 295, 189, 294, 81, 265, 237, 143, 171, 32, 10, 237, 246, 74, 237, 262, 74, 248, 238, 255, 171, 107, 295, 107, 261, 171, 32, 77, 32, 3, 32]}, {"code": "def triton_red_fused_native_layer_norm_no_welford(\n    in_out_ptr0,\n    in_out_ptr1,\n    in_ptr0,\n    in_ptr1,\n    in_ptr2,\n    out_ptr0,\n    xnumel,\n    rnumel,\n    XBLOCK: tl.constexpr,\n    RBLOCK: tl.constexpr,\n):\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex\n    _tmp3 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp0 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"\n        ).to(tl.float32)\n        tmp1 = tmp0.to(tl.float32)\n        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n        tmp4 = _tmp3 + tmp2\n        _tmp3 = tmp4\n    tmp3 = tl.sum(_tmp3, 1)[:, None]\n    tmp5 = rnumel\n    tmp6 = tmp3 / tmp5\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + (x0), tmp6, None)\n    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp7 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"\n        ).to(tl.float32)\n        tmp8 = tmp7.to(tl.float32)\n        tmp9 = tmp8 - tmp6\n        tmp10 = tmp9 * tmp9\n        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])\n        tmp13 = _tmp12 + tmp11\n        _tmp12 = tmp13\n    tmp12 = tl.sum(_tmp12, 1)[:, None]\n    tmp14 = rnumel\n    tmp15 = tmp12 / tmp14\n    tmp16 = 1e-05\n    tmp17 = tmp15 + tmp16\n    tmp18 = libdevice.rsqrt(tmp17)\n    tl.debug_barrier()\n    tl.store(in_out_ptr1 + (x0), tmp18, None)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp19 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"\n        ).to(tl.float32)\n        tmp23 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp26 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp20 = tmp19.to(tl.float32)\n        tmp21 = tmp20 - tmp6\n        tmp22 = tmp21 * tmp18\n        tmp24 = tmp23.to(tl.float32)\n        tmp25 = tmp22 * tmp24\n        tmp27 = tmp26.to(tl.float32)\n        tmp28 = tmp25 + tmp27\n        tmp29 = tmp28.to(tl.float32)\n        tl.store(out_ptr0 + (r1 + (rnumel * x0)), tmp29, rmask)", "encoded": [29, 341, 237, 241, 107, 242, 107, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 62, 6, 107, 250, 62, 6, 171, 62, 32, -1, 251, 189, 167, 237, 342, 171, 238, 249, 32, 252, 189, 251, 74, 75, 237, 342, 107, 249, 171, 224, 62, 107, 196, 27, 32, 253, 189, 252, 1, 247, 32, 254, 189, 75, 237, 342, 107, 250, 171, 224, 196, 107, 62, 27, 32, 255, 189, 252, 32, 256, 189, 231, 237, 224, 249, 107, 250, 27, 107, 342, 107, 143, 171, 32, 134, 257, 154, 5, 237, 342, 107, 248, 107, 250, 171, 62, 32, 258, 189, 257, 74, 254, 32, 259, 189, 258, 1, 248, 32, 260, 189, 258, 32, 261, 189, 55, 237, 243, 74, 237, 260, 74, 248, 238, 255, 171, 107, 259, 107, 262, 189, 343, 171, 81, 263, 237, 143, 171, 32, 264, 189, 261, 81, 263, 237, 143, 171, 32, 265, 189, 188, 237, 264, 107, 224, 249, 107, 250, 27, 171, 32, 266, 189, 256, 74, 265, 32, 256, 189, 266, 32, 77, 32, 267, 189, 215, 237, 256, 107, 344, 171, 224, 62, 107, 196, 27, 32, 268, 189, 248, 32, 269, 189, 267, 42, 268, 32, 51, 237, 171, 32, 10, 237, 241, 74, 255, 107, 269, 107, 196, 171, 32, 270, 189, 231, 237, 224, 249, 107, 250, 27, 107, 342, 107, 143, 171, 32, 134, 257, 154, 5, 237, 342, 107, 248, 107, 250, 171, 62, 32, 258, 189, 257, 74, 254, 32, 259, 189, 258, 1, 248, 32, 260, 189, 258, 32, 271, 189, 55, 237, 243, 74, 237, 260, 74, 248, 238, 255, 171, 107, 259, 107, 262, 189, 343, 171, 81, 263, 237, 143, 171, 32, 272, 189, 271, 81, 263, 237, 143, 171, 32, 273, 189, 272, 4, 269, 32, 274, 189, 273, 238, 273, 32, 275, 189, 188, 237, 274, 107, 224, 249, 107, 250, 27, 171, 32, 276, 189, 270, 74, 275, 32, 270, 189, 276, 32, 77, 32, 277, 189, 215, 237, 270, 107, 344, 171, 224, 62, 107, 196, 27, 32, 278, 189, 248, 32, 279, 189, 277, 42, 278, 32, 280, 189, 345, 32, 281, 189, 279, 74, 280, 32, 282, 189, 16, 81, 283, 237, 281, 171, 32, 51, 237, 171, 32, 10, 237, 242, 74, 255, 107, 282, 107, 196, 171, 32, 134, 257, 154, 5, 237, 342, 107, 248, 107, 250, 171, 62, 32, 258, 189, 257, 74, 254, 32, 259, 189, 258, 1, 248, 32, 260, 189, 258, 32, 284, 189, 55, 237, 243, 74, 237, 260, 74, 248, 238, 255, 171, 107, 259, 107, 262, 189, 346, 171, 81, 263, 237, 143, 171, 32, 285, 189, 55, 237, 244, 74, 260, 107, 259, 107, 262, 189, 343, 171, 81, 263, 237, 143, 171, 32, 286, 189, 55, 237, 245, 74, 260, 107, 259, 107, 262, 189, 343, 171, 81, 263, 237, 143, 171, 32, 287, 189, 284, 81, 263, 237, 143, 171, 32, 288, 189, 287, 4, 269, 32, 289, 189, 288, 238, 282, 32, 290, 189, 285, 81, 263, 237, 143, 171, 32, 291, 189, 289, 238, 290, 32, 292, 189, 286, 81, 263, 237, 143, 171, 32, 293, 189, 291, 74, 292, 32, 294, 189, 293, 81, 263, 237, 143, 171, 32, 10, 237, 246, 74, 237, 260, 74, 248, 238, 255, 171, 107, 294, 107, 259, 171, 32, 77, 32, 3, 32]}, {"code": "def _attn_fwd(\n    Q,\n    K,\n    V,\n    mask,\n    sm_scale,\n    M,\n    Out,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_on,\n    stride_mask_z,\n    stride_mask_h,\n    stride_mask_m,\n    stride_mask_n,\n    Z,\n    H,\n    N_CTX,\n    HEAD_DIM: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    STAGE: tl.constexpr,\n    USE_MASK: tl.constexpr,\n):\n    tl.static_assert(BLOCK_N <= HEAD_DIM)\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n\n    if USE_MASK:\n        mask_offset = (\n            off_z.to(tl.int64) * stride_mask_z + off_h.to(tl.int64) * stride_mask_h\n        )\n\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + qvk_offset,\n        shape=(N_CTX, HEAD_DIM),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, HEAD_DIM),\n        order=(1, 0),\n    )\n    v_order: tl.constexpr = (0, 1) if V.dtype.element_ty == tl.float8e5 else (1, 0)\n    V_block_ptr = tl.make_block_ptr(\n        base=V + qvk_offset,\n        shape=(N_CTX, HEAD_DIM),\n        strides=(stride_vk, stride_vn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, HEAD_DIM),\n        order=v_order,\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + qvk_offset,\n        shape=(HEAD_DIM, N_CTX),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(HEAD_DIM, BLOCK_N),\n        order=(0, 1),\n    )\n    O_block_ptr = tl.make_block_ptr(\n        base=Out + qvk_offset,\n        shape=(N_CTX, HEAD_DIM),\n        strides=(stride_om, stride_on),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, HEAD_DIM),\n        order=(1, 0),\n    )\n\n    mask_block_ptr = (\n        None\n        if not USE_MASK\n        else tl.make_block_ptr(\n            base=mask + mask_offset,\n            shape=(N_CTX, N_CTX),\n            strides=(stride_mask_m, stride_mask_n),\n            offsets=(start_m * BLOCK_M, 0),\n            block_shape=(BLOCK_M, BLOCK_N),\n            order=(0, 1),\n        )\n    )\n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n\n    qk_scale = sm_scale\n    qk_scale *= 1.44269504\n\n    q = tl.load(Q_block_ptr)\n\n    if USE_MASK:\n        acc, l_i, m_i = _attn_fwd_inner(\n            acc,\n            l_i,\n            m_i,\n            q,\n            K_block_ptr,\n            V_block_ptr,\n            mask_block_ptr,\n            start_m,\n            qk_scale,\n            BLOCK_M,\n            HEAD_DIM,\n            BLOCK_N,\n            4 - STAGE,\n            offs_m,\n            offs_n,\n            N_CTX,\n            V.dtype.element_ty == tl.float8e5,\n            USE_MASK,\n        )\n    else:\n        acc, l_i, m_i = _attn_fwd_inner(\n            acc,\n            l_i,\n            m_i,\n            q,\n            K_block_ptr,\n            V_block_ptr,\n            None,\n            start_m,\n            qk_scale,\n            BLOCK_M,\n            HEAD_DIM,\n            BLOCK_N,\n            2,\n            offs_m,\n            offs_n,\n            N_CTX,\n            V.dtype.element_ty == tl.float8e5,\n            USE_MASK,\n        )\n\n    m_i += tl.math.log2(l_i)\n    acc = acc / l_i[:, None]\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty))", "encoded": [29, 342, 238, 242, 107, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 107, 253, 107, 254, 107, 255, 107, 256, 107, 257, 107, 258, 107, 259, 107, 260, 107, 261, 107, 262, 107, 263, 107, 264, 107, 265, 107, 266, 107, 267, 107, 268, 107, 269, 107, 270, 107, 271, 107, 272, 63, 6, 107, 273, 63, 6, 107, 274, 63, 6, 107, 275, 63, 6, 107, 276, 63, 6, 171, 63, 32, -1, 82, 238, 274, 213, 272, 171, 32, 277, 189, 167, 238, 343, 171, 32, 278, 189, 167, 238, 344, 171, 32, 279, 189, 278, 48, 270, 32, 280, 189, 278, 221, 270, 32, 281, 189, 279, 81, 282, 238, 177, 171, 239, 249, 74, 280, 81, 282, 238, 177, 171, 239, 250, 32, 178, 276, 63, 32, 283, 189, 279, 81, 282, 238, 177, 171, 239, 265, 74, 280, 81, 282, 238, 177, 171, 239, 266, 32, 181, 32, 284, 189, 214, 238, 285, 189, 242, 74, 281, 107, 122, 189, 238, 271, 107, 272, 171, 107, 286, 189, 238, 251, 107, 252, 171, 107, 287, 189, 238, 277, 239, 273, 107, 343, 171, 107, 288, 189, 238, 273, 107, 272, 171, 107, 289, 189, 238, 344, 107, 343, 171, 171, 32, 290, 63, 6, 189, 238, 343, 107, 344, 171, 178, 244, 81, 90, 81, 113, 76, 196, 30, 238, 344, 107, 343, 171, 32, 291, 189, 214, 238, 285, 189, 244, 74, 281, 107, 122, 189, 238, 271, 107, 272, 171, 107, 286, 189, 238, 259, 107, 260, 171, 107, 287, 189, 238, 343, 107, 343, 171, 107, 288, 189, 238, 274, 107, 272, 171, 107, 289, 189, 290, 171, 32, 292, 189, 214, 238, 285, 189, 243, 74, 281, 107, 122, 189, 238, 272, 107, 271, 171, 107, 286, 189, 238, 256, 107, 255, 171, 107, 287, 189, 238, 343, 107, 343, 171, 107, 288, 189, 238, 272, 107, 274, 171, 107, 289, 189, 238, 343, 107, 344, 171, 171, 32, 293, 189, 214, 238, 285, 189, 248, 74, 281, 107, 122, 189, 238, 271, 107, 272, 171, 107, 286, 189, 238, 263, 107, 264, 171, 107, 287, 189, 238, 277, 239, 273, 107, 343, 171, 107, 288, 189, 238, 273, 107, 272, 171, 107, 289, 189, 238, 344, 107, 343, 171, 171, 32, 294, 189, 197, 178, 64, 276, 30, 214, 238, 285, 189, 245, 74, 283, 107, 122, 189, 238, 271, 107, 271, 171, 107, 286, 189, 238, 267, 107, 268, 171, 107, 287, 189, 238, 277, 239, 273, 107, 343, 171, 107, 288, 189, 238, 273, 107, 274, 171, 107, 289, 189, 238, 343, 107, 344, 171, 171, 32, 295, 189, 277, 239, 273, 74, 75, 238, 343, 107, 273, 171, 32, 296, 189, 75, 238, 343, 107, 274, 171, 32, 297, 189, 172, 238, 225, 273, 27, 107, 90, 189, 143, 171, 4, 298, 238, 345, 171, 32, 299, 189, 172, 238, 225, 273, 27, 107, 90, 189, 143, 171, 74, 344, 32, 300, 189, 172, 238, 225, 273, 107, 272, 27, 107, 90, 189, 143, 171, 32, 301, 189, 246, 32, 301, 24, 346, 32, 302, 189, 57, 238, 284, 171, 32, 178, 276, 63, 32, 300, 107, 299, 107, 297, 189, 303, 238, 300, 107, 299, 107, 297, 107, 302, 107, 292, 107, 291, 107, 294, 107, 277, 107, 301, 107, 273, 107, 272, 107, 274, 107, 347, 4, 275, 107, 295, 107, 296, 107, 271, 107, 244, 81, 90, 81, 113, 76, 196, 107, 276, 171, 32, 181, 32, 30, 63, 32, 300, 107, 299, 107, 297, 189, 303, 238, 300, 107, 299, 107, 297, 107, 302, 107, 292, 107, 291, 107, 197, 107, 277, 107, 301, 107, 273, 107, 272, 107, 274, 107, 348, 107, 295, 107, 296, 107, 271, 107, 244, 81, 90, 81, 113, 76, 196, 107, 276, 171, 32, 56, 32, 297, 168, 110, 238, 299, 171, 32, 300, 189, 300, 42, 299, 225, 63, 107, 197, 27, 32, 304, 189, 247, 74, 278, 239, 271, 74, 295, 32, 10, 238, 304, 107, 297, 171, 32, 10, 238, 293, 107, 300, 81, 282, 238, 248, 81, 183, 81, 113, 171, 171, 32, 3, 32]}, {"code": "def linear_xent_fwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    loss_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n):\n    idx = tl.program_id(axis=0)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + offsets)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V // V_BLOCK_SIZE):\n\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        local_x_block_ptr = x_block_ptr\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(local_x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == v_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        m = m_new\n        A_block_ptr = tl.advance(\n            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]\n        )\n        v_range = v_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n    loss += tl.sum(lse) / N\n    tl.atomic_add(loss_ptr, loss)\n    tl.store(lse_ptr + offsets, lse)", "encoded": [29, 342, 238, 242, 107, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 62, 6, 107, 252, 62, 6, 107, 253, 62, 6, 107, 254, 62, 6, 107, 255, 62, 6, 107, 256, 62, 6, 171, 62, 32, -1, 257, 189, 167, 238, 258, 189, 343, 171, 32, 259, 189, 214, 238, 260, 189, 242, 107, 122, 189, 238, 252, 107, 253, 171, 107, 261, 189, 238, 247, 107, 248, 171, 107, 262, 189, 238, 257, 239, 255, 107, 343, 171, 107, 263, 189, 238, 255, 107, 256, 171, 107, 264, 189, 238, 344, 107, 343, 171, 171, 32, 265, 189, 214, 238, 260, 189, 244, 107, 122, 189, 238, 253, 107, 251, 171, 107, 261, 189, 238, 249, 107, 250, 171, 107, 262, 189, 238, 343, 107, 343, 171, 107, 263, 189, 238, 256, 107, 254, 171, 107, 264, 189, 238, 344, 107, 343, 171, 171, 32, 262, 189, 257, 239, 255, 74, 75, 238, 343, 107, 255, 171, 32, 266, 189, 75, 238, 343, 107, 254, 171, 32, 267, 189, 55, 238, 243, 74, 262, 171, 32, 268, 189, 172, 238, 238, 255, 107, 171, 107, 90, 189, 143, 171, 4, 269, 238, 345, 171, 32, 270, 189, 172, 238, 238, 255, 107, 171, 107, 90, 189, 143, 171, 32, 271, 189, 343, 32, 134, 272, 154, 5, 238, 251, 48, 254, 171, 62, 32, 273, 189, 172, 238, 238, 255, 107, 254, 171, 107, 90, 189, 143, 171, 32, 274, 189, 259, 32, 134, 272, 154, 5, 238, 253, 48, 256, 171, 62, 32, 275, 189, 55, 238, 274, 171, 32, 276, 189, 55, 238, 265, 171, 32, 273, 189, 15, 238, 275, 107, 276, 107, 273, 171, 32, 274, 189, 141, 238, 274, 107, 225, 343, 107, 256, 27, 171, 32, 265, 189, 141, 238, 265, 107, 225, 256, 107, 343, 27, 171, 32, 77, 32, 277, 189, 187, 238, 268, 107, 12, 238, 273, 107, 344, 171, 171, 32, 278, 189, 216, 238, 106, 238, 273, 4, 277, 225, 62, 107, 197, 27, 171, 107, 258, 189, 344, 171, 32, 270, 189, 270, 239, 106, 238, 268, 4, 277, 171, 74, 278, 32, 279, 189, 267, 225, 62, 107, 197, 27, 76, 266, 225, 197, 107, 62, 27, 32, 271, 2, 216, 238, 200, 238, 279, 107, 273, 107, 269, 238, 343, 171, 171, 171, 42, 252, 32, 268, 189, 277, 32, 265, 189, 141, 238, 265, 107, 225, 4, 256, 239, 238, 253, 48, 256, 171, 107, 254, 27, 171, 32, 266, 189, 266, 74, 254, 32, 77, 32, 280, 189, 268, 74, 50, 238, 270, 171, 32, 271, 168, 216, 238, 280, 171, 42, 252, 32, 191, 238, 245, 107, 271, 171, 32, 10, 238, 246, 74, 262, 107, 280, 171, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_prologue(\n    sz_ptr,\n    x_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_sz_N,\n    stride_sz_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1)\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n\n    offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    lse = tl.load(lse_global_ptr + offsets)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    sz_block_ptr = tl.make_block_ptr(\n        base=sz_ptr,\n        shape=(N, V),\n        strides=(stride_sz_N, stride_sz_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    tl.store(sz_block_ptr, softmax_z.to(tl.float16))", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 107, 253, 63, 6, 107, 254, 63, 6, 107, 255, 63, 6, 107, 256, 63, 6, 190, 344, 345, 107, 257, 63, 6, 190, 344, 345, 107, 258, 63, 6, 190, 344, 345, 171, 63, 32, -1, 259, 190, 167, 239, 260, 190, 346, 171, 32, 261, 190, 167, 239, 260, 190, 344, 171, 32, 185, 239, 256, 107, 257, 107, 258, 171, 32, 262, 190, 259, 240, 257, 74, 75, 239, 346, 107, 257, 171, 32, 263, 190, 57, 239, 246, 74, 262, 171, 32, 264, 190, 215, 239, 265, 190, 244, 107, 122, 190, 239, 254, 107, 255, 171, 107, 266, 190, 239, 247, 107, 248, 171, 107, 262, 190, 239, 259, 240, 257, 107, 346, 171, 107, 267, 190, 239, 257, 107, 258, 171, 107, 268, 190, 239, 344, 107, 346, 171, 171, 32, 269, 190, 215, 239, 265, 190, 245, 107, 122, 190, 239, 255, 107, 253, 171, 107, 266, 190, 239, 249, 107, 250, 171, 107, 262, 190, 239, 346, 107, 261, 240, 256, 171, 107, 267, 190, 239, 258, 107, 256, 171, 107, 268, 190, 239, 344, 107, 346, 171, 171, 32, 270, 190, 215, 239, 265, 190, 243, 107, 122, 190, 239, 254, 107, 253, 171, 107, 266, 190, 239, 251, 107, 252, 171, 107, 262, 190, 239, 259, 240, 257, 107, 261, 240, 256, 171, 107, 267, 190, 239, 257, 107, 256, 171, 107, 268, 190, 239, 344, 107, 346, 171, 171, 32, 271, 190, 172, 239, 239, 257, 107, 256, 171, 107, 90, 190, 143, 171, 32, 134, 272, 154, 5, 239, 255, 48, 258, 171, 63, 32, 273, 190, 57, 239, 264, 171, 32, 274, 190, 57, 239, 269, 171, 32, 271, 190, 15, 239, 273, 107, 274, 107, 271, 171, 32, 264, 190, 141, 239, 264, 107, 226, 346, 107, 258, 27, 171, 32, 269, 190, 141, 239, 269, 107, 226, 258, 107, 346, 27, 171, 32, 77, 32, 275, 190, 239, 271, 4, 263, 226, 63, 107, 198, 27, 171, 81, 203, 239, 171, 32, 10, 239, 270, 107, 275, 81, 276, 239, 21, 171, 171, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(\n    sz_ptr,\n    y_ptr,\n    A_t_ptr,\n    x_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_x_grad_N,\n    stride_x_grad_H,\n    stride_A_grad_H,\n    stride_A_grad_V,\n    stride_sz_N,\n    stride_sz_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_H = tl.program_id(axis=0)\n    idx_N = tl.program_id(axis=1)\n\n    x_grad_block_ptr = tl.make_block_ptr(\n        base=x_grad_ptr,\n        shape=(N, H),\n        strides=(stride_x_grad_N, stride_x_grad_H),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_t_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    sz_block_ptr = tl.make_block_ptr(\n        base=sz_ptr,\n        shape=(N, V),\n        strides=(stride_sz_N, stride_sz_V),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_offsets)\n\n    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)\n    for idx_V in range(V // V_BLOCK_SIZE):\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n        A_v = tl.load(A_t_block_ptr).trans()\n        sz = tl.load(sz_block_ptr)\n\n        x_grad_acc = tl.dot(sz, A_v, x_grad_acc)\n        x_grad_acc -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1)\n\n        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n        sz_block_ptr = tl.advance(sz_block_ptr, [0, V_BLOCK_SIZE])\n        V_offsets += V_BLOCK_SIZE\n\n    tl.store(x_grad_block_ptr, x_grad_acc / N)", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 107, 253, 107, 254, 107, 255, 107, 256, 107, 257, 62, 6, 107, 258, 62, 6, 107, 259, 62, 6, 107, 260, 62, 6, 190, 344, 345, 107, 261, 62, 6, 190, 344, 345, 107, 262, 62, 6, 190, 344, 345, 171, 62, 32, -1, 263, 190, 167, 239, 264, 190, 346, 171, 32, 265, 190, 167, 239, 264, 190, 344, 171, 32, 266, 190, 215, 239, 267, 190, 246, 107, 122, 190, 239, 258, 107, 259, 171, 107, 268, 190, 239, 251, 107, 252, 171, 107, 269, 190, 239, 265, 240, 261, 107, 263, 240, 262, 171, 107, 270, 190, 239, 261, 107, 262, 171, 107, 271, 190, 239, 344, 107, 346, 171, 171, 32, 272, 190, 215, 239, 267, 190, 245, 107, 122, 190, 239, 259, 107, 257, 171, 107, 268, 190, 239, 249, 107, 250, 171, 107, 269, 190, 239, 263, 240, 262, 107, 346, 171, 107, 270, 190, 239, 262, 107, 260, 171, 107, 271, 190, 239, 344, 107, 346, 171, 171, 32, 273, 190, 215, 239, 267, 190, 243, 107, 122, 190, 239, 258, 107, 257, 171, 107, 268, 190, 239, 255, 107, 256, 171, 107, 269, 190, 239, 265, 240, 261, 107, 346, 171, 107, 270, 190, 239, 261, 107, 260, 171, 107, 271, 190, 239, 344, 107, 346, 171, 171, 32, 274, 190, 265, 240, 261, 74, 75, 239, 346, 107, 261, 171, 32, 275, 190, 75, 239, 346, 107, 260, 171, 32, 276, 190, 55, 239, 244, 74, 274, 171, 32, 277, 190, 172, 239, 239, 261, 107, 262, 171, 107, 143, 171, 32, 134, 278, 154, 5, 239, 257, 48, 260, 171, 62, 32, 279, 190, 239, 276, 226, 62, 107, 198, 27, 76, 275, 226, 198, 107, 62, 27, 171, 226, 62, 107, 62, 107, 198, 27, 32, 280, 190, 55, 239, 272, 171, 81, 281, 239, 171, 32, 282, 190, 55, 239, 273, 171, 32, 277, 190, 15, 239, 282, 107, 280, 107, 277, 171, 32, 277, 2, 217, 239, 201, 239, 279, 107, 280, 226, 198, 107, 62, 107, 62, 27, 107, 346, 171, 107, 264, 190, 344, 171, 32, 272, 190, 141, 239, 272, 107, 226, 346, 107, 260, 27, 171, 32, 273, 190, 141, 239, 273, 107, 226, 346, 107, 260, 27, 171, 32, 275, 168, 260, 32, 77, 32, 10, 239, 266, 107, 277, 42, 258, 171, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(\n    sz_ptr,\n    x_ptr,\n    y_ptr,\n    A_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_x_grad_N,\n    stride_x_grad_H,\n    stride_A_grad_H,\n    stride_A_grad_V,\n    stride_sz_N,\n    stride_sz_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_H = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1) - (N // N_BLOCK_SIZE)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(0, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_t_grad_block_ptr = tl.make_block_ptr(\n        base=A_grad_ptr,\n        shape=(H, V),\n        strides=(stride_A_grad_H, stride_A_grad_V),\n        offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    sz_block_ptr = tl.make_block_ptr(\n        base=sz_ptr,\n        shape=(N, V),\n        strides=(stride_sz_N, stride_sz_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_offsets = tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_grad_acc = tl.zeros((V_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)\n    for idx_N in range(N // N_BLOCK_SIZE):\n        y = tl.load(y_ptr + N_offsets)\n\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n        x_chunk = tl.load(x_block_ptr)\n        sz = tl.load(sz_block_ptr).trans()\n\n        A_grad_acc = tl.dot(sz, x_chunk, A_grad_acc)\n        A_grad_acc -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n        sz_block_ptr = tl.advance(sz_block_ptr, [N_BLOCK_SIZE, 0])\n        N_offsets += N_BLOCK_SIZE\n\n    tl.store(A_t_grad_block_ptr, A_grad_acc.trans() / N)", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 107, 253, 107, 254, 107, 255, 107, 256, 107, 257, 63, 6, 107, 258, 63, 6, 107, 259, 63, 6, 107, 260, 63, 6, 190, 344, 345, 107, 261, 63, 6, 190, 344, 345, 107, 262, 63, 6, 190, 344, 345, 171, 63, 32, -1, 263, 190, 167, 239, 264, 190, 346, 171, 32, 265, 190, 167, 239, 264, 190, 344, 171, 4, 258, 48, 261, 32, 266, 190, 215, 239, 267, 190, 244, 107, 122, 190, 239, 258, 107, 259, 171, 107, 268, 190, 239, 247, 107, 248, 171, 107, 269, 190, 239, 346, 107, 263, 240, 262, 171, 107, 270, 190, 239, 261, 107, 262, 171, 107, 271, 190, 239, 344, 107, 346, 171, 171, 32, 272, 190, 215, 239, 267, 190, 246, 107, 122, 190, 239, 259, 107, 257, 171, 107, 268, 190, 239, 253, 107, 254, 171, 107, 269, 190, 239, 263, 240, 262, 107, 265, 240, 260, 171, 107, 270, 190, 239, 262, 107, 260, 171, 107, 271, 190, 239, 344, 107, 346, 171, 171, 32, 273, 190, 215, 239, 267, 190, 243, 107, 122, 190, 239, 258, 107, 257, 171, 107, 268, 190, 239, 255, 107, 256, 171, 107, 269, 190, 239, 346, 107, 265, 240, 260, 171, 107, 270, 190, 239, 261, 107, 260, 171, 107, 271, 190, 239, 344, 107, 346, 171, 171, 32, 274, 190, 75, 239, 346, 107, 261, 171, 32, 275, 190, 265, 240, 260, 74, 75, 239, 346, 107, 260, 171, 32, 276, 190, 172, 239, 239, 260, 107, 262, 171, 107, 143, 171, 32, 134, 277, 154, 5, 239, 258, 48, 261, 171, 63, 32, 278, 190, 57, 239, 245, 74, 274, 171, 32, 279, 190, 239, 278, 226, 63, 107, 198, 27, 76, 275, 226, 198, 107, 63, 27, 171, 226, 63, 107, 63, 107, 198, 27, 32, 280, 190, 57, 239, 266, 171, 32, 281, 190, 57, 239, 273, 171, 81, 282, 239, 171, 32, 276, 190, 15, 239, 281, 107, 280, 107, 276, 171, 32, 276, 2, 217, 239, 201, 239, 279, 107, 280, 226, 63, 107, 198, 107, 63, 27, 107, 346, 171, 107, 264, 190, 346, 171, 32, 266, 190, 141, 239, 266, 107, 226, 261, 107, 346, 27, 171, 32, 273, 190, 141, 239, 273, 107, 226, 261, 107, 346, 27, 171, 32, 274, 168, 261, 32, 77, 32, 10, 239, 272, 107, 276, 81, 282, 239, 171, 42, 258, 171, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    stride_lse_N,\n    stride_lse_B,\n    stride_loss_Nb,\n    stride_loss_B,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    V_TILES: tl.constexpr = 1,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n\n    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_ptr,\n        shape=(N, V // 64),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_V_group),\n        block_shape=(N_BLOCK_SIZE, 1),\n        order=(1, 0),\n    )\n    loss_val_ptr = (\n        losses_ptr\n        + (idx_N + idx_N_group * N_group // N_BLOCK_SIZE) * stride_loss_Nb\n        + idx_V_group * stride_loss_B\n    )\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V_TILES):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == V_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n        m = m_new\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        V_range = V_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n\n    tl.store(loss_val_ptr, loss)\n    tl.store(lse_row_ptr, lse[:, None])", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 107, 253, 107, 254, 107, 255, 107, 256, 107, 257, 107, 258, 107, 259, 107, 260, 62, 6, 107, 261, 62, 6, 107, 262, 62, 6, 107, 263, 62, 6, 107, 264, 62, 6, 107, 265, 62, 6, 107, 266, 62, 6, 107, 267, 62, 6, 190, 344, 171, 62, 32, -1, 268, 190, 167, 239, 269, 190, 345, 171, 32, 270, 190, 167, 239, 269, 190, 344, 171, 32, 271, 62, 6, 190, 267, 240, 264, 32, 272, 190, 215, 239, 273, 190, 243, 107, 122, 190, 239, 262, 107, 263, 171, 107, 274, 190, 239, 249, 107, 250, 171, 107, 275, 190, 239, 259, 240, 260, 74, 268, 240, 265, 107, 345, 171, 107, 276, 190, 239, 265, 107, 266, 171, 107, 277, 190, 239, 344, 107, 345, 171, 171, 32, 278, 190, 215, 239, 273, 190, 245, 107, 122, 190, 239, 263, 107, 261, 171, 107, 274, 190, 239, 251, 107, 252, 171, 107, 275, 190, 239, 345, 107, 270, 240, 271, 171, 107, 276, 190, 239, 266, 107, 264, 171, 107, 277, 190, 239, 344, 107, 345, 171, 171, 32, 279, 190, 215, 239, 273, 190, 246, 107, 122, 190, 239, 260, 107, 261, 171, 107, 274, 190, 239, 253, 107, 254, 171, 107, 275, 190, 239, 268, 240, 265, 107, 270, 240, 271, 171, 107, 276, 190, 239, 265, 107, 264, 171, 107, 277, 190, 239, 344, 107, 345, 171, 171, 32, 280, 190, 215, 239, 273, 190, 248, 107, 122, 190, 239, 262, 107, 261, 48, 346, 347, 171, 107, 274, 190, 239, 255, 107, 256, 171, 107, 275, 190, 239, 259, 240, 260, 74, 268, 240, 265, 107, 270, 171, 107, 276, 190, 239, 265, 107, 344, 171, 107, 277, 190, 239, 344, 107, 345, 171, 171, 32, 281, 190, 247, 74, 239, 268, 74, 259, 240, 260, 48, 265, 171, 240, 257, 74, 270, 240, 258, 32, 282, 190, 259, 240, 260, 74, 268, 240, 265, 74, 75, 239, 345, 107, 265, 171, 32, 283, 190, 270, 240, 271, 74, 75, 239, 345, 107, 264, 171, 32, 284, 190, 55, 239, 244, 74, 282, 171, 32, 285, 190, 172, 239, 239, 265, 107, 171, 107, 90, 190, 143, 171, 4, 286, 239, 348, 171, 32, 287, 190, 172, 239, 239, 265, 107, 171, 107, 90, 190, 143, 171, 32, 288, 190, 345, 32, 134, 289, 154, 5, 239, 267, 171, 62, 32, 290, 190, 172, 239, 239, 265, 107, 264, 171, 107, 90, 190, 143, 171, 32, 134, 289, 154, 5, 239, 263, 48, 266, 171, 62, 32, 291, 190, 55, 239, 272, 171, 32, 292, 190, 55, 239, 278, 171, 32, 290, 190, 15, 239, 291, 107, 292, 107, 290, 171, 32, 272, 190, 141, 239, 272, 107, 226, 345, 107, 266, 27, 171, 32, 278, 190, 141, 239, 278, 107, 226, 266, 107, 345, 27, 171, 32, 77, 32, 293, 190, 188, 239, 285, 107, 12, 239, 290, 107, 344, 171, 171, 32, 294, 190, 217, 239, 106, 239, 290, 4, 293, 226, 62, 107, 198, 27, 171, 107, 269, 190, 344, 171, 32, 287, 190, 287, 240, 106, 239, 285, 4, 293, 171, 74, 294, 32, 295, 190, 284, 226, 62, 107, 198, 27, 76, 283, 226, 198, 107, 62, 27, 32, 288, 2, 217, 239, 201, 239, 295, 107, 290, 107, 286, 239, 345, 171, 171, 171, 42, 262, 32, 10, 239, 279, 107, 290, 81, 296, 239, 246, 81, 183, 81, 113, 171, 171, 32, 285, 190, 293, 32, 272, 190, 141, 239, 272, 107, 226, 345, 107, 4, 263, 27, 171, 32, 278, 190, 141, 239, 278, 107, 226, 4, 263, 107, 264, 27, 171, 32, 279, 190, 141, 239, 279, 107, 226, 345, 107, 264, 27, 171, 32, 283, 190, 283, 74, 264, 32, 77, 32, 297, 190, 285, 74, 50, 239, 287, 171, 32, 10, 239, 281, 107, 288, 171, 32, 10, 239, 280, 107, 297, 226, 62, 107, 198, 27, 171, 32, 3, 32]}, {"code": "def linear_xent_mini_bwd_prologue_kernel(\n    z_nv_ptr,\n    y_ptr,\n    lse_ptr,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1)\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n    lse = tl.load(lse_ptr + N_range)\n    z_j_to_k = tl.load(z_block_ptr)\n\n    mask = y[:, None] == v_range[None, :]\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N\n\n    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 63, 6, 107, 250, 63, 6, 107, 251, 63, 6, 107, 252, 63, 6, 107, 253, 63, 6, 171, 63, 32, -1, 254, 190, 167, 239, 255, 190, 344, 171, 32, 256, 190, 167, 239, 255, 190, 345, 171, 32, 257, 190, 215, 239, 258, 190, 243, 107, 122, 190, 239, 249, 107, 250, 171, 107, 259, 190, 239, 246, 107, 247, 171, 107, 260, 190, 239, 254, 240, 253, 107, 256, 240, 252, 171, 107, 261, 190, 239, 253, 107, 252, 171, 107, 262, 190, 239, 345, 107, 344, 171, 171, 32, 263, 190, 248, 240, 249, 74, 254, 240, 253, 74, 75, 239, 344, 107, 253, 171, 32, 264, 190, 256, 240, 252, 74, 75, 239, 344, 107, 252, 171, 32, 265, 190, 57, 239, 244, 74, 263, 171, 32, 266, 190, 57, 239, 245, 74, 263, 171, 32, 267, 190, 57, 239, 257, 171, 32, 268, 190, 265, 226, 63, 107, 198, 27, 76, 264, 226, 198, 107, 63, 27, 32, 269, 190, 239, 267, 4, 266, 226, 63, 107, 198, 27, 171, 81, 203, 239, 171, 32, 270, 190, 239, 269, 4, 201, 239, 268, 107, 345, 107, 344, 171, 171, 42, 251, 32, 10, 239, 257, 107, 270, 81, 271, 239, 243, 81, 183, 81, 113, 171, 171, 32, 3, 32]}, {"code": "def linear_xent_fwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n):\n    idx = tl.program_id(axis=0)\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + offsets)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V // V_BLOCK_SIZE):\n\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        local_x_block_ptr = x_block_ptr\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(local_x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == v_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        m = m_new\n        A_block_ptr = tl.advance(\n            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]\n        )\n        v_range = v_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n    loss += tl.sum(lse) / N\n    tl.store(losses_ptr + idx, loss)\n    tl.store(lse_ptr + offsets, lse)", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 62, 6, 107, 253, 62, 6, 107, 254, 62, 6, 107, 255, 62, 6, 107, 256, 62, 6, 107, 257, 62, 6, 171, 62, 32, -1, 258, 190, 167, 239, 259, 190, 344, 171, 32, 185, 239, 255, 107, 256, 107, 257, 171, 32, 260, 190, 215, 239, 261, 190, 243, 107, 122, 190, 239, 253, 107, 254, 171, 107, 262, 190, 239, 248, 107, 249, 171, 107, 263, 190, 239, 258, 240, 256, 107, 344, 171, 107, 264, 190, 239, 256, 107, 257, 171, 107, 265, 190, 239, 345, 107, 344, 171, 171, 32, 266, 190, 215, 239, 261, 190, 245, 107, 122, 190, 239, 254, 107, 252, 171, 107, 262, 190, 239, 250, 107, 251, 171, 107, 263, 190, 239, 344, 107, 344, 171, 107, 264, 190, 239, 257, 107, 255, 171, 107, 265, 190, 239, 345, 107, 344, 171, 171, 32, 263, 190, 258, 240, 256, 74, 75, 239, 344, 107, 256, 171, 32, 267, 190, 75, 239, 344, 107, 255, 171, 32, 268, 190, 55, 239, 244, 74, 263, 171, 32, 269, 190, 172, 239, 239, 256, 107, 171, 107, 90, 190, 143, 171, 4, 270, 239, 346, 171, 32, 271, 190, 172, 239, 239, 256, 107, 171, 107, 90, 190, 143, 171, 32, 272, 190, 344, 32, 134, 273, 154, 5, 239, 252, 48, 255, 171, 62, 32, 274, 190, 172, 239, 239, 256, 107, 255, 171, 107, 90, 190, 143, 171, 32, 275, 190, 260, 32, 134, 273, 154, 5, 239, 254, 48, 257, 171, 62, 32, 276, 190, 55, 239, 275, 171, 32, 277, 190, 55, 239, 266, 171, 32, 274, 190, 15, 239, 276, 107, 277, 107, 274, 171, 32, 275, 190, 141, 239, 275, 107, 226, 344, 107, 257, 27, 171, 32, 266, 190, 141, 239, 266, 107, 226, 257, 107, 344, 27, 171, 32, 77, 32, 278, 190, 188, 239, 269, 107, 12, 239, 274, 107, 345, 171, 171, 32, 279, 190, 217, 239, 106, 239, 274, 4, 278, 226, 62, 107, 198, 27, 171, 107, 259, 190, 345, 171, 32, 271, 190, 271, 240, 106, 239, 269, 4, 278, 171, 74, 279, 32, 280, 190, 268, 226, 62, 107, 198, 27, 76, 267, 226, 198, 107, 62, 27, 32, 272, 2, 217, 239, 201, 239, 280, 107, 274, 107, 270, 239, 344, 171, 171, 171, 42, 253, 32, 269, 190, 278, 32, 266, 190, 141, 239, 266, 107, 226, 4, 257, 240, 239, 254, 48, 257, 171, 107, 255, 27, 171, 32, 267, 190, 267, 74, 255, 32, 77, 32, 281, 190, 269, 74, 50, 239, 271, 171, 32, 272, 168, 217, 239, 281, 171, 42, 253, 32, 10, 239, 246, 74, 258, 107, 272, 171, 32, 10, 239, 247, 74, 263, 107, 281, 171, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_dA(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    A_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_V = tl.program_id(axis=0)\n\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n\n    N_offsets = tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    for idx_N in range(N // N_BLOCK_SIZE):\n        x_block_ptr = tl.make_block_ptr(\n            base=x_ptr,\n            shape=(N, H),\n            strides=(stride_x_N, stride_x_H),\n            offsets=(idx_N * N_BLOCK_SIZE, 0),\n            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n            order=(1, 0),\n        )\n\n        y = tl.load(y_ptr + N_offsets)\n        lse = tl.load(lse_global_ptr + N_offsets)\n\n        local_x_block_ptr = x_block_ptr\n        local_A_block_ptr = A_block_ptr\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(local_x_block_ptr)\n            A_v = tl.load(local_A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n\n        local_x_block_ptr = x_block_ptr\n        local_A_block_ptr = A_block_ptr\n\n        for idx_H in range(H // H_BLOCK_SIZE):\n            A_grad_block_ptr = tl.make_block_ptr(\n                base=A_grad_ptr,\n                shape=(H, V),\n                strides=(stride_A_H, stride_A_V),\n                offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n                block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n                order=(1, 0),\n            )\n\n            x_chunk = tl.load(local_x_block_ptr).to(tl.float32)\n            A_v = tl.load(local_A_block_ptr).to(tl.float32)\n\n            temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)\n            temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n            temp_AgradT = temp_Agrad.trans() / N + tl.load(A_grad_block_ptr)\n            tl.store(A_grad_block_ptr, temp_AgradT, boundary_check=(0, 1))\n\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n        N_offsets += N_BLOCK_SIZE", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 63, 6, 107, 253, 63, 6, 107, 254, 63, 6, 107, 255, 63, 6, 190, 344, 345, 107, 256, 63, 6, 190, 344, 345, 107, 257, 63, 6, 190, 344, 345, 171, 63, 32, -1, 258, 190, 167, 239, 259, 190, 346, 171, 32, 185, 239, 255, 107, 256, 107, 257, 171, 32, 260, 190, 75, 239, 346, 107, 256, 171, 32, 261, 190, 258, 240, 255, 74, 75, 239, 346, 107, 255, 171, 32, 262, 190, 215, 239, 263, 190, 245, 107, 122, 190, 239, 254, 107, 252, 171, 107, 264, 190, 239, 250, 107, 251, 171, 107, 265, 190, 239, 346, 107, 258, 240, 255, 171, 107, 266, 190, 239, 257, 107, 255, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 134, 268, 154, 5, 239, 253, 48, 256, 171, 63, 32, 269, 190, 215, 239, 263, 190, 243, 107, 122, 190, 239, 253, 107, 254, 171, 107, 264, 190, 239, 248, 107, 249, 171, 107, 265, 190, 239, 268, 240, 256, 107, 346, 171, 107, 266, 190, 239, 256, 107, 257, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 270, 190, 57, 239, 244, 74, 260, 171, 32, 271, 190, 57, 239, 246, 74, 260, 171, 32, 272, 190, 269, 32, 273, 190, 262, 32, 274, 190, 172, 239, 239, 256, 107, 255, 171, 107, 90, 190, 143, 171, 32, 134, 275, 154, 5, 239, 254, 48, 257, 171, 63, 32, 276, 190, 57, 239, 272, 171, 32, 277, 190, 57, 239, 273, 171, 32, 274, 190, 15, 239, 276, 107, 277, 107, 274, 171, 32, 272, 190, 141, 239, 272, 107, 226, 346, 107, 257, 27, 171, 32, 273, 190, 141, 239, 273, 107, 226, 257, 107, 346, 27, 171, 32, 77, 32, 278, 190, 239, 270, 226, 63, 107, 198, 27, 76, 261, 226, 198, 107, 63, 27, 171, 226, 63, 107, 63, 107, 198, 27, 32, 279, 190, 239, 274, 4, 271, 226, 63, 107, 198, 27, 171, 81, 203, 239, 171, 32, 272, 190, 269, 32, 273, 190, 262, 32, 134, 280, 154, 5, 239, 254, 48, 257, 171, 63, 32, 281, 190, 215, 239, 263, 190, 247, 107, 122, 190, 239, 254, 107, 252, 171, 107, 264, 190, 239, 250, 107, 251, 171, 107, 265, 190, 239, 280, 240, 257, 107, 258, 240, 255, 171, 107, 266, 190, 239, 257, 107, 255, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 276, 190, 57, 239, 272, 171, 81, 282, 239, 143, 171, 32, 277, 190, 57, 239, 273, 171, 81, 282, 239, 143, 171, 32, 283, 190, 15, 239, 279, 81, 284, 239, 171, 107, 276, 171, 32, 283, 2, 217, 239, 201, 239, 278, 107, 276, 226, 63, 107, 198, 107, 63, 27, 107, 346, 171, 107, 259, 190, 346, 171, 32, 285, 190, 283, 81, 284, 239, 171, 42, 253, 74, 57, 239, 281, 171, 32, 10, 239, 281, 107, 285, 107, 286, 190, 239, 346, 107, 344, 171, 171, 32, 272, 190, 141, 239, 272, 107, 226, 346, 107, 257, 27, 171, 32, 273, 190, 141, 239, 273, 107, 226, 257, 107, 346, 27, 171, 32, 77, 32, 260, 168, 256, 32, 77, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_dx(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    x_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_N = tl.program_id(axis=0)\n\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n\n    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = tl.arange(0, V_BLOCK_SIZE)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    x_grad_block_ptr = tl.make_block_ptr(\n        base=x_grad_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    y = tl.load(y_ptr + N_offsets)\n    lse = tl.load(lse_global_ptr + N_offsets)\n\n    for idx_V in range(V // V_BLOCK_SIZE):\n        A_block_ptr = tl.make_block_ptr(\n            base=A_t_ptr,\n            shape=(H, V),\n            strides=(stride_A_H, stride_A_V),\n            offsets=(0, idx_V * V_BLOCK_SIZE),\n            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n            order=(1, 0),\n        )\n\n        local_x_block_ptr = x_block_ptr\n        local_A_block_ptr = A_block_ptr\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(local_x_block_ptr)\n            A_v = tl.load(local_A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n\n        local_x_block_ptr = x_block_ptr\n        local_A_block_ptr = A_block_ptr\n        local_x_grad_block_ptr = x_grad_block_ptr\n        for idx_H in range(H // H_BLOCK_SIZE):\n\n            x_chunk = tl.load(local_x_block_ptr).to(tl.float32)\n            A_v = tl.load(local_A_block_ptr).to(tl.float32)\n\n            temp_xgrad = tl.dot(softmax_z, A_v.trans()) / N\n            temp_xgrad -= (\n                tl.sum(tl.where(mask, A_v.trans()[None, :, :], 0.0), axis=1) / N\n            )\n\n            temp_xgrad += tl.load(local_x_grad_block_ptr)\n            tl.store(local_x_grad_block_ptr, temp_xgrad, boundary_check=(0, 1))\n\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n            local_x_grad_block_ptr = tl.advance(\n                local_x_grad_block_ptr, [0, H_BLOCK_SIZE]\n            )\n            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        V_offsets += V_BLOCK_SIZE", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 62, 6, 107, 253, 62, 6, 107, 254, 62, 6, 107, 255, 62, 6, 190, 344, 345, 107, 256, 62, 6, 190, 344, 345, 107, 257, 62, 6, 190, 344, 345, 171, 62, 32, -1, 258, 190, 167, 239, 259, 190, 346, 171, 32, 185, 239, 255, 107, 256, 107, 257, 171, 32, 260, 190, 258, 240, 256, 74, 75, 239, 346, 107, 256, 171, 32, 261, 190, 75, 239, 346, 107, 255, 171, 32, 262, 190, 215, 239, 263, 190, 243, 107, 122, 190, 239, 253, 107, 254, 171, 107, 264, 190, 239, 248, 107, 249, 171, 107, 265, 190, 239, 258, 240, 256, 107, 346, 171, 107, 266, 190, 239, 256, 107, 257, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 268, 190, 215, 239, 263, 190, 247, 107, 122, 190, 239, 253, 107, 254, 171, 107, 264, 190, 239, 248, 107, 249, 171, 107, 265, 190, 239, 258, 240, 256, 107, 346, 171, 107, 266, 190, 239, 256, 107, 257, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 269, 190, 55, 239, 244, 74, 260, 171, 32, 270, 190, 55, 239, 246, 74, 260, 171, 32, 134, 271, 154, 5, 239, 252, 48, 255, 171, 62, 32, 272, 190, 215, 239, 263, 190, 245, 107, 122, 190, 239, 254, 107, 252, 171, 107, 264, 190, 239, 250, 107, 251, 171, 107, 265, 190, 239, 346, 107, 271, 240, 255, 171, 107, 266, 190, 239, 257, 107, 255, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 273, 190, 262, 32, 274, 190, 272, 32, 275, 190, 172, 239, 239, 256, 107, 255, 171, 107, 90, 190, 143, 171, 32, 134, 276, 154, 5, 239, 254, 48, 257, 171, 62, 32, 277, 190, 55, 239, 273, 171, 32, 278, 190, 55, 239, 274, 171, 32, 275, 190, 15, 239, 277, 107, 278, 107, 275, 171, 32, 273, 190, 141, 239, 273, 107, 226, 346, 107, 257, 27, 171, 32, 274, 190, 141, 239, 274, 107, 226, 257, 107, 346, 27, 171, 32, 77, 32, 279, 190, 239, 269, 226, 62, 107, 198, 27, 76, 261, 226, 198, 107, 62, 27, 171, 226, 62, 107, 62, 107, 198, 27, 32, 280, 190, 239, 275, 4, 270, 226, 62, 107, 198, 27, 171, 81, 203, 239, 171, 32, 273, 190, 262, 32, 274, 190, 272, 32, 281, 190, 268, 32, 134, 282, 154, 5, 239, 254, 48, 257, 171, 62, 32, 277, 190, 55, 239, 273, 171, 81, 283, 239, 143, 171, 32, 278, 190, 55, 239, 274, 171, 81, 283, 239, 143, 171, 32, 284, 190, 15, 239, 280, 107, 278, 81, 285, 239, 171, 171, 42, 253, 32, 284, 2, 217, 239, 201, 239, 279, 107, 278, 81, 285, 239, 171, 226, 198, 107, 62, 107, 62, 27, 107, 346, 171, 107, 259, 190, 344, 171, 42, 253, 32, 284, 168, 55, 239, 281, 171, 32, 10, 239, 281, 107, 284, 107, 286, 190, 239, 346, 107, 344, 171, 171, 32, 273, 190, 141, 239, 273, 107, 226, 346, 107, 257, 27, 171, 32, 281, 190, 141, 239, 281, 107, 226, 346, 107, 257, 27, 171, 32, 274, 190, 141, 239, 274, 107, 226, 257, 107, 346, 27, 171, 32, 77, 32, 261, 168, 255, 32, 77, 32, 3, 32]}, {"code": "def linear_xent_fwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n):\n    idx = tl.program_id(axis=0)\n\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + offsets)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V // V_BLOCK_SIZE):\n\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        local_x_block_ptr = x_block_ptr\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(local_x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == v_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        m = m_new\n        A_block_ptr = tl.advance(\n            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]\n        )\n        v_range = v_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n    loss += tl.sum(lse) / N\n    tl.store(losses_ptr + idx, loss)\n    tl.store(lse_ptr + offsets, lse)", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 63, 6, 107, 253, 63, 6, 107, 254, 63, 6, 107, 255, 63, 6, 107, 256, 63, 6, 107, 257, 63, 6, 171, 63, 32, -1, 258, 190, 167, 239, 259, 190, 344, 171, 32, 82, 239, 253, 222, 256, 76, 344, 171, 32, 82, 239, 252, 222, 255, 76, 344, 171, 32, 82, 239, 254, 222, 257, 76, 344, 171, 32, 260, 190, 215, 239, 261, 190, 243, 107, 122, 190, 239, 253, 107, 254, 171, 107, 262, 190, 239, 248, 107, 249, 171, 107, 263, 190, 239, 258, 240, 256, 107, 344, 171, 107, 264, 190, 239, 256, 107, 257, 171, 107, 265, 190, 239, 345, 107, 344, 171, 171, 32, 266, 190, 215, 239, 261, 190, 245, 107, 122, 190, 239, 254, 107, 252, 171, 107, 262, 190, 239, 250, 107, 251, 171, 107, 263, 190, 239, 344, 107, 344, 171, 107, 264, 190, 239, 257, 107, 255, 171, 107, 265, 190, 239, 345, 107, 344, 171, 171, 32, 263, 190, 258, 240, 256, 74, 75, 239, 344, 107, 256, 171, 32, 267, 190, 75, 239, 344, 107, 255, 171, 32, 268, 190, 57, 239, 244, 74, 263, 171, 32, 269, 190, 172, 239, 239, 256, 107, 171, 107, 90, 190, 143, 171, 4, 270, 239, 346, 171, 32, 271, 190, 172, 239, 239, 256, 107, 171, 107, 90, 190, 143, 171, 32, 272, 190, 344, 32, 134, 273, 154, 5, 239, 252, 48, 255, 171, 63, 32, 274, 190, 172, 239, 239, 256, 107, 255, 171, 107, 90, 190, 143, 171, 32, 275, 190, 260, 32, 134, 273, 154, 5, 239, 254, 48, 257, 171, 63, 32, 276, 190, 57, 239, 275, 171, 32, 277, 190, 57, 239, 266, 171, 32, 274, 190, 15, 239, 276, 107, 277, 107, 274, 171, 32, 275, 190, 141, 239, 275, 107, 226, 344, 107, 257, 27, 171, 32, 266, 190, 141, 239, 266, 107, 226, 257, 107, 344, 27, 171, 32, 77, 32, 278, 190, 188, 239, 269, 107, 12, 239, 274, 107, 345, 171, 171, 32, 279, 190, 217, 239, 106, 239, 274, 4, 278, 226, 63, 107, 198, 27, 171, 107, 259, 190, 345, 171, 32, 271, 190, 271, 240, 106, 239, 269, 4, 278, 171, 74, 279, 32, 280, 190, 268, 226, 63, 107, 198, 27, 76, 267, 226, 198, 107, 63, 27, 32, 272, 2, 217, 239, 201, 239, 280, 107, 274, 107, 270, 239, 344, 171, 171, 171, 42, 253, 32, 269, 190, 278, 32, 266, 190, 141, 239, 266, 107, 226, 4, 257, 240, 239, 254, 48, 257, 171, 107, 255, 27, 171, 32, 267, 190, 267, 74, 255, 32, 77, 32, 281, 190, 269, 74, 50, 239, 271, 171, 32, 272, 168, 217, 239, 281, 171, 42, 253, 32, 10, 239, 246, 74, 258, 107, 272, 171, 32, 10, 239, 247, 74, 263, 107, 281, 171, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_dA(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    A_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_V = tl.program_id(axis=0)\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n\n    N_offsets = tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_grad_block_ptr = tl.make_block_ptr(\n        base=A_grad_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(0 * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    for idx_N in range(N // N_BLOCK_SIZE):\n\n        y = tl.load(y_ptr + N_offsets)\n        lse = tl.load(lse_global_ptr + N_offsets)\n\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, 0])\n\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n\n        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n\n        for idx_H in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n\n            temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)\n            temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n            temp_AgradT = temp_Agrad.trans() / N\n            tl.store(\n                A_grad_block_ptr, temp_AgradT.to(tl.float16) + tl.load(A_grad_block_ptr)\n            )\n\n            A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, -H])\n        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [-H, 0])\n        N_offsets += N_BLOCK_SIZE", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 62, 6, 107, 253, 62, 6, 107, 254, 62, 6, 107, 255, 62, 6, 190, 344, 345, 107, 256, 62, 6, 190, 344, 345, 107, 257, 62, 6, 190, 344, 345, 171, 62, 32, -1, 258, 190, 167, 239, 259, 190, 346, 171, 32, 82, 239, 253, 222, 256, 76, 346, 171, 32, 82, 239, 252, 222, 255, 76, 346, 171, 32, 82, 239, 254, 222, 257, 76, 346, 171, 32, 260, 190, 75, 239, 346, 107, 256, 171, 32, 261, 190, 258, 240, 255, 74, 75, 239, 346, 107, 255, 171, 32, 262, 190, 215, 239, 263, 190, 245, 107, 122, 190, 239, 254, 107, 252, 171, 107, 264, 190, 239, 250, 107, 251, 171, 107, 265, 190, 239, 346, 107, 258, 240, 255, 171, 107, 266, 190, 239, 257, 107, 255, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 268, 190, 215, 239, 263, 190, 247, 107, 122, 190, 239, 254, 107, 252, 171, 107, 264, 190, 239, 250, 107, 251, 171, 107, 265, 190, 239, 346, 240, 257, 107, 258, 240, 255, 171, 107, 266, 190, 239, 257, 107, 255, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 269, 190, 215, 239, 263, 190, 243, 107, 122, 190, 239, 253, 107, 254, 171, 107, 264, 190, 239, 248, 107, 249, 171, 107, 265, 190, 239, 346, 240, 256, 107, 346, 171, 107, 266, 190, 239, 256, 107, 257, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 134, 270, 154, 5, 239, 253, 48, 256, 171, 62, 32, 271, 190, 55, 239, 244, 74, 260, 171, 32, 272, 190, 55, 239, 246, 74, 260, 171, 32, 273, 190, 172, 239, 239, 256, 107, 255, 171, 107, 90, 190, 143, 171, 32, 134, 274, 154, 5, 239, 254, 48, 257, 171, 62, 32, 275, 190, 55, 239, 269, 171, 32, 276, 190, 55, 239, 262, 171, 32, 273, 190, 15, 239, 275, 107, 276, 107, 273, 171, 32, 269, 190, 141, 239, 269, 107, 226, 346, 107, 257, 27, 171, 32, 262, 190, 141, 239, 262, 107, 226, 257, 107, 346, 27, 171, 32, 77, 32, 269, 190, 141, 239, 269, 107, 226, 346, 107, 4, 254, 27, 171, 32, 262, 190, 141, 239, 262, 107, 226, 4, 254, 107, 346, 27, 171, 32, 277, 190, 239, 271, 226, 62, 107, 198, 27, 76, 261, 226, 198, 107, 62, 27, 171, 226, 62, 107, 62, 107, 198, 27, 32, 278, 190, 239, 273, 4, 272, 226, 62, 107, 198, 27, 171, 81, 203, 239, 171, 81, 279, 239, 21, 171, 32, 134, 280, 154, 5, 239, 254, 48, 257, 171, 62, 32, 275, 190, 55, 239, 269, 171, 32, 281, 190, 15, 239, 278, 81, 282, 239, 171, 107, 275, 171, 32, 281, 2, 217, 239, 201, 239, 277, 107, 275, 226, 62, 107, 198, 107, 62, 27, 107, 346, 171, 107, 259, 190, 346, 171, 32, 283, 190, 281, 81, 282, 239, 171, 42, 253, 32, 10, 239, 268, 107, 283, 81, 279, 239, 21, 171, 74, 55, 239, 268, 171, 171, 32, 268, 190, 141, 239, 268, 107, 226, 257, 107, 346, 27, 171, 32, 269, 190, 141, 239, 269, 107, 226, 346, 107, 257, 27, 171, 32, 77, 32, 269, 190, 141, 239, 269, 107, 226, 256, 107, 4, 254, 27, 171, 32, 268, 190, 141, 239, 268, 107, 226, 4, 254, 107, 346, 27, 171, 32, 260, 168, 256, 32, 77, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_dx(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    x_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_N = tl.program_id(axis=0)\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n\n    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = tl.arange(0, V_BLOCK_SIZE)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    x_grad_block_ptr = tl.make_block_ptr(\n        base=x_grad_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N * N_BLOCK_SIZE, 0 * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    y = tl.load(y_ptr + N_offsets)\n    lse = tl.load(lse_global_ptr + N_offsets)\n\n    for idx_V in range(V // V_BLOCK_SIZE):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for idx_H_1 in range(H // H_BLOCK_SIZE):\n            x_block_ptr = tl.make_block_ptr(\n                base=x_ptr,\n                shape=(N, H),\n                strides=(stride_x_N, stride_x_H),\n                offsets=(idx_N * N_BLOCK_SIZE, idx_H_1 * H_BLOCK_SIZE),\n                block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n                order=(1, 0),\n            )\n            A_block_ptr = tl.make_block_ptr(\n                base=A_t_ptr,\n                shape=(H, V),\n                strides=(stride_A_H, stride_A_V),\n                offsets=(idx_H_1 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n                block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n                order=(1, 0),\n            )\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n\n        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n\n        for idx_H in range(H // H_BLOCK_SIZE):\n            x_grad_block_ptr = tl.make_block_ptr(\n                base=x_grad_ptr,\n                shape=(N, H),\n                strides=(stride_x_N, stride_x_H),\n                offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),\n                block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n                order=(1, 0),\n            )\n            A_block_ptr = tl.make_block_ptr(\n                base=A_t_ptr,\n                shape=(H, V),\n                strides=(stride_A_H, stride_A_V),\n                offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n                block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n                order=(1, 0),\n            )\n            A_v = tl.load(A_block_ptr).trans()\n\n            temp_xgrad = tl.dot(softmax_z, A_v) / N\n            temp_xgrad -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1) / N\n            tl.store(\n                x_grad_block_ptr, tl.load(x_grad_block_ptr) + temp_xgrad.to(tl.float16)\n            )\n\n        V_offsets += V_BLOCK_SIZE", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 63, 6, 107, 253, 63, 6, 107, 254, 63, 6, 107, 255, 63, 6, 190, 344, 345, 107, 256, 63, 6, 190, 344, 345, 107, 257, 63, 6, 190, 344, 345, 171, 63, 32, -1, 258, 190, 167, 239, 259, 190, 346, 171, 32, 82, 239, 253, 222, 256, 76, 346, 171, 32, 82, 239, 252, 222, 255, 76, 346, 171, 32, 82, 239, 254, 222, 257, 76, 346, 171, 32, 260, 190, 258, 240, 256, 74, 75, 239, 346, 107, 256, 171, 32, 261, 190, 75, 239, 346, 107, 255, 171, 32, 262, 190, 215, 239, 263, 190, 243, 107, 122, 190, 239, 253, 107, 254, 171, 107, 264, 190, 239, 248, 107, 249, 171, 107, 265, 190, 239, 258, 240, 256, 107, 346, 171, 107, 266, 190, 239, 256, 107, 257, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 268, 190, 215, 239, 263, 190, 247, 107, 122, 190, 239, 253, 107, 254, 171, 107, 264, 190, 239, 248, 107, 249, 171, 107, 265, 190, 239, 258, 240, 256, 107, 346, 240, 257, 171, 107, 266, 190, 239, 256, 107, 257, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 269, 190, 215, 239, 263, 190, 245, 107, 122, 190, 239, 254, 107, 252, 171, 107, 264, 190, 239, 250, 107, 251, 171, 107, 265, 190, 239, 346, 107, 346, 171, 107, 266, 190, 239, 257, 107, 255, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 270, 190, 57, 239, 244, 74, 260, 171, 32, 271, 190, 57, 239, 246, 74, 260, 171, 32, 134, 272, 154, 5, 239, 252, 48, 255, 171, 63, 32, 273, 190, 172, 239, 239, 256, 107, 255, 171, 107, 90, 190, 143, 171, 32, 134, 274, 154, 5, 239, 254, 48, 257, 171, 63, 32, 262, 190, 215, 239, 263, 190, 243, 107, 122, 190, 239, 253, 107, 254, 171, 107, 264, 190, 239, 248, 107, 249, 171, 107, 265, 190, 239, 258, 240, 256, 107, 274, 240, 257, 171, 107, 266, 190, 239, 256, 107, 257, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 269, 190, 215, 239, 263, 190, 245, 107, 122, 190, 239, 254, 107, 252, 171, 107, 264, 190, 239, 250, 107, 251, 171, 107, 265, 190, 239, 274, 240, 257, 107, 272, 240, 255, 171, 107, 266, 190, 239, 257, 107, 255, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 275, 190, 57, 239, 262, 171, 32, 276, 190, 57, 239, 269, 171, 32, 273, 190, 15, 239, 275, 107, 276, 107, 273, 171, 32, 77, 32, 277, 190, 239, 270, 226, 63, 107, 198, 27, 76, 261, 226, 198, 107, 63, 27, 171, 226, 63, 107, 63, 107, 198, 27, 32, 278, 190, 239, 273, 4, 271, 226, 63, 107, 198, 27, 171, 81, 203, 239, 171, 81, 279, 239, 21, 171, 32, 134, 280, 154, 5, 239, 254, 48, 257, 171, 63, 32, 268, 190, 215, 239, 263, 190, 247, 107, 122, 190, 239, 253, 107, 254, 171, 107, 264, 190, 239, 248, 107, 249, 171, 107, 265, 190, 239, 258, 240, 256, 107, 280, 240, 257, 171, 107, 266, 190, 239, 256, 107, 257, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 269, 190, 215, 239, 263, 190, 245, 107, 122, 190, 239, 254, 107, 252, 171, 107, 264, 190, 239, 250, 107, 251, 171, 107, 265, 190, 239, 280, 240, 257, 107, 272, 240, 255, 171, 107, 266, 190, 239, 257, 107, 255, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 276, 190, 57, 239, 269, 171, 81, 281, 239, 171, 32, 282, 190, 15, 239, 278, 107, 276, 171, 42, 253, 32, 282, 2, 217, 239, 201, 239, 277, 107, 276, 226, 198, 107, 63, 107, 63, 27, 107, 346, 171, 107, 259, 190, 344, 171, 42, 253, 32, 10, 239, 268, 107, 57, 239, 268, 171, 74, 282, 81, 279, 239, 21, 171, 171, 32, 77, 32, 261, 168, 255, 32, 77, 32, 3, 32]}, {"code": "def linear_xent_fwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n):\n    idx = tl.program_id(axis=0)\n\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + offsets)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V // V_BLOCK_SIZE):\n\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        local_x_block_ptr = x_block_ptr\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(local_x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == v_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        m = m_new\n        A_block_ptr = tl.advance(\n            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]\n        )\n        v_range = v_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n    loss += tl.sum(lse) / N\n    tl.store(losses_ptr + idx, loss)\n    tl.store(lse_ptr + offsets, lse)", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 62, 6, 107, 253, 62, 6, 107, 254, 62, 6, 107, 255, 62, 6, 107, 256, 62, 6, 107, 257, 62, 6, 171, 62, 32, -1, 258, 190, 167, 239, 259, 190, 344, 171, 32, 82, 239, 253, 222, 256, 76, 344, 171, 32, 82, 239, 252, 222, 255, 76, 344, 171, 32, 82, 239, 254, 222, 257, 76, 344, 171, 32, 260, 190, 215, 239, 261, 190, 243, 107, 122, 190, 239, 253, 107, 254, 171, 107, 262, 190, 239, 248, 107, 249, 171, 107, 263, 190, 239, 258, 240, 256, 107, 344, 171, 107, 264, 190, 239, 256, 107, 257, 171, 107, 265, 190, 239, 345, 107, 344, 171, 171, 32, 266, 190, 215, 239, 261, 190, 245, 107, 122, 190, 239, 254, 107, 252, 171, 107, 262, 190, 239, 250, 107, 251, 171, 107, 263, 190, 239, 344, 107, 344, 171, 107, 264, 190, 239, 257, 107, 255, 171, 107, 265, 190, 239, 345, 107, 344, 171, 171, 32, 263, 190, 258, 240, 256, 74, 75, 239, 344, 107, 256, 171, 32, 267, 190, 75, 239, 344, 107, 255, 171, 32, 268, 190, 55, 239, 244, 74, 263, 171, 32, 269, 190, 172, 239, 239, 256, 107, 171, 107, 90, 190, 143, 171, 4, 270, 239, 346, 171, 32, 271, 190, 172, 239, 239, 256, 107, 171, 107, 90, 190, 143, 171, 32, 272, 190, 344, 32, 134, 273, 154, 5, 239, 252, 48, 255, 171, 62, 32, 274, 190, 172, 239, 239, 256, 107, 255, 171, 107, 90, 190, 143, 171, 32, 275, 190, 260, 32, 134, 273, 154, 5, 239, 254, 48, 257, 171, 62, 32, 276, 190, 55, 239, 275, 171, 32, 277, 190, 55, 239, 266, 171, 32, 274, 190, 15, 239, 276, 107, 277, 107, 274, 171, 32, 275, 190, 141, 239, 275, 107, 226, 344, 107, 257, 27, 171, 32, 266, 190, 141, 239, 266, 107, 226, 257, 107, 344, 27, 171, 32, 77, 32, 278, 190, 188, 239, 269, 107, 12, 239, 274, 107, 345, 171, 171, 32, 279, 190, 217, 239, 106, 239, 274, 4, 278, 226, 62, 107, 198, 27, 171, 107, 259, 190, 345, 171, 32, 271, 190, 271, 240, 106, 239, 269, 4, 278, 171, 74, 279, 32, 280, 190, 268, 226, 62, 107, 198, 27, 76, 267, 226, 198, 107, 62, 27, 32, 272, 2, 217, 239, 201, 239, 280, 107, 274, 107, 270, 239, 344, 171, 171, 171, 42, 253, 32, 269, 190, 278, 32, 266, 190, 141, 239, 266, 107, 226, 4, 257, 240, 239, 254, 48, 257, 171, 107, 255, 27, 171, 32, 267, 190, 267, 74, 255, 32, 77, 32, 281, 190, 269, 74, 50, 239, 271, 171, 32, 272, 168, 217, 239, 281, 171, 42, 253, 32, 10, 239, 246, 74, 258, 107, 272, 171, 32, 10, 239, 247, 74, 263, 107, 281, 171, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_dA(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    A_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_V = tl.program_id(axis=0)\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n\n    N_offsets = tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_grad_block_ptr = tl.make_block_ptr(\n        base=A_grad_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(0 * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    for idx_N in range(N // N_BLOCK_SIZE):\n\n        y = tl.load(y_ptr + N_offsets)\n        lse = tl.load(lse_global_ptr + N_offsets)\n\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, 0])\n\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n\n        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n\n        for idx_H in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n\n            temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)\n            temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n            temp_AgradT = temp_Agrad.trans() / N\n            tl.store(\n                A_grad_block_ptr, temp_AgradT.to(tl.float16) + tl.load(A_grad_block_ptr)\n            )\n\n            A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, -H])\n        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [-H, 0])\n        N_offsets += N_BLOCK_SIZE", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 63, 6, 107, 253, 63, 6, 107, 254, 63, 6, 107, 255, 63, 6, 190, 344, 345, 107, 256, 63, 6, 190, 344, 345, 107, 257, 63, 6, 190, 344, 345, 171, 63, 32, -1, 258, 190, 167, 239, 259, 190, 346, 171, 32, 82, 239, 253, 222, 256, 76, 346, 171, 32, 82, 239, 252, 222, 255, 76, 346, 171, 32, 82, 239, 254, 222, 257, 76, 346, 171, 32, 260, 190, 75, 239, 346, 107, 256, 171, 32, 261, 190, 258, 240, 255, 74, 75, 239, 346, 107, 255, 171, 32, 262, 190, 215, 239, 263, 190, 245, 107, 122, 190, 239, 254, 107, 252, 171, 107, 264, 190, 239, 250, 107, 251, 171, 107, 265, 190, 239, 346, 107, 258, 240, 255, 171, 107, 266, 190, 239, 257, 107, 255, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 268, 190, 215, 239, 263, 190, 247, 107, 122, 190, 239, 254, 107, 252, 171, 107, 264, 190, 239, 250, 107, 251, 171, 107, 265, 190, 239, 346, 240, 257, 107, 258, 240, 255, 171, 107, 266, 190, 239, 257, 107, 255, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 269, 190, 215, 239, 263, 190, 243, 107, 122, 190, 239, 253, 107, 254, 171, 107, 264, 190, 239, 248, 107, 249, 171, 107, 265, 190, 239, 346, 240, 256, 107, 346, 171, 107, 266, 190, 239, 256, 107, 257, 171, 107, 267, 190, 239, 344, 107, 346, 171, 171, 32, 134, 270, 154, 5, 239, 253, 48, 256, 171, 63, 32, 271, 190, 57, 239, 244, 74, 260, 171, 32, 272, 190, 57, 239, 246, 74, 260, 171, 32, 273, 190, 172, 239, 239, 256, 107, 255, 171, 107, 90, 190, 143, 171, 32, 134, 274, 154, 5, 239, 254, 48, 257, 171, 63, 32, 275, 190, 57, 239, 269, 171, 32, 276, 190, 57, 239, 262, 171, 32, 273, 190, 15, 239, 275, 107, 276, 107, 273, 171, 32, 269, 190, 141, 239, 269, 107, 226, 346, 107, 257, 27, 171, 32, 262, 190, 141, 239, 262, 107, 226, 257, 107, 346, 27, 171, 32, 77, 32, 269, 190, 141, 239, 269, 107, 226, 346, 107, 4, 254, 27, 171, 32, 262, 190, 141, 239, 262, 107, 226, 4, 254, 107, 346, 27, 171, 32, 277, 190, 239, 271, 226, 63, 107, 198, 27, 76, 261, 226, 198, 107, 63, 27, 171, 226, 63, 107, 63, 107, 198, 27, 32, 278, 190, 239, 273, 4, 272, 226, 63, 107, 198, 27, 171, 81, 203, 239, 171, 81, 279, 239, 21, 171, 32, 134, 280, 154, 5, 239, 254, 48, 257, 171, 63, 32, 275, 190, 57, 239, 269, 171, 32, 281, 190, 15, 239, 278, 81, 282, 239, 171, 107, 275, 171, 32, 281, 2, 217, 239, 201, 239, 277, 107, 275, 226, 63, 107, 198, 107, 63, 27, 107, 346, 171, 107, 259, 190, 346, 171, 32, 283, 190, 281, 81, 282, 239, 171, 42, 253, 32, 10, 239, 268, 107, 283, 81, 279, 239, 21, 171, 74, 57, 239, 268, 171, 171, 32, 268, 190, 141, 239, 268, 107, 226, 257, 107, 346, 27, 171, 32, 269, 190, 141, 239, 269, 107, 226, 346, 107, 257, 27, 171, 32, 77, 32, 269, 190, 141, 239, 269, 107, 226, 256, 107, 4, 254, 27, 171, 32, 268, 190, 141, 239, 268, 107, 226, 4, 254, 107, 346, 27, 171, 32, 260, 168, 256, 32, 77, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_dx(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    x_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n    H_GROUP_SIZE: tl.constexpr = 4,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_H_group = tl.program_id(axis=1)\n\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n\n    H_GROUPS: tl.constexpr = H // (H_GROUP_SIZE * H_BLOCK_SIZE)\n    tl.static_print(H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, H_GROUP_SIZE, H_GROUPS)\n\n    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = tl.arange(0, V_BLOCK_SIZE)\n    H_group_offsets = tl.arange(0, H_GROUP_SIZE)\n\n    y = tl.load(y_ptr + N_offsets)\n    lse = tl.load(lse_global_ptr + N_offsets)\n\n    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE, H_GROUP_SIZE), dtype=tl.float16)\n\n    for idx_V in range(V // V_BLOCK_SIZE):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for idx_H_1 in range(H // H_BLOCK_SIZE):\n            x_block_ptr = tl.make_block_ptr(\n                base=x_ptr,\n                shape=(N, H),\n                strides=(stride_x_N, stride_x_H),\n                offsets=(idx_N * N_BLOCK_SIZE, idx_H_1 * H_BLOCK_SIZE),\n                block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n                order=(1, 0),\n            )\n            A_block_ptr = tl.make_block_ptr(\n                base=A_t_ptr,\n                shape=(H, V),\n                strides=(stride_A_H, stride_A_V),\n                offsets=(idx_H_1 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n                block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n                order=(1, 0),\n            )\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n\n        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n\n        A_block_ptr = tl.make_block_ptr(\n            base=A_t_ptr,\n            shape=(H, V),\n            strides=(stride_A_H, stride_A_V),\n            offsets=((H_GROUP_SIZE * H_BLOCK_SIZE) * idx_H_group, idx_V * V_BLOCK_SIZE),\n            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n            order=(1, 0),\n        )\n        for idx_H_in_group in range(H_GROUP_SIZE):\n\n            A_v = tl.load(A_block_ptr).trans()\n\n            x_grad_block = tl.dot(softmax_z, A_v) / N\n            x_grad_block -= tl.sum(tl.where(mask, A_v[None, :, :], 0), axis=1) / N\n            x_grad_slice = x_grad_block[:, :, None].to(tl.float16)\n\n            accum_mask = (idx_H_in_group == H_group_offsets)[None, None, :]\n            x_grad_acc += tl.where(accum_mask, x_grad_slice, 0)\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n        V_offsets += V_BLOCK_SIZE\n\n    x_grad_block_ptr = tl.make_block_ptr(\n        base=x_grad_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_H_group * H_GROUP_SIZE * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_GROUP_SIZE * H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    tl.store(\n        x_grad_block_ptr, x_grad_acc.reshape(N_BLOCK_SIZE, H_GROUP_SIZE * H_BLOCK_SIZE)\n    )", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 62, 6, 107, 253, 62, 6, 107, 254, 62, 6, 107, 255, 62, 6, 190, 344, 345, 107, 256, 62, 6, 190, 344, 345, 107, 257, 62, 6, 190, 344, 345, 107, 258, 62, 6, 190, 346, 171, 62, 32, -1, 259, 190, 167, 239, 260, 190, 347, 171, 32, 261, 190, 167, 239, 260, 190, 344, 171, 32, 82, 239, 253, 222, 256, 76, 347, 171, 32, 82, 239, 252, 222, 255, 76, 347, 171, 32, 82, 239, 254, 222, 257, 76, 347, 171, 32, 262, 62, 6, 190, 254, 48, 239, 258, 240, 257, 171, 32, 185, 239, 254, 107, 255, 107, 256, 107, 257, 107, 258, 107, 262, 171, 32, 263, 190, 259, 240, 256, 74, 75, 239, 347, 107, 256, 171, 32, 264, 190, 75, 239, 347, 107, 255, 171, 32, 265, 190, 75, 239, 347, 107, 258, 171, 32, 266, 190, 55, 239, 244, 74, 263, 171, 32, 267, 190, 55, 239, 246, 74, 263, 171, 32, 268, 190, 172, 239, 239, 256, 107, 257, 107, 258, 171, 107, 90, 190, 21, 171, 32, 134, 269, 154, 5, 239, 252, 48, 255, 171, 62, 32, 270, 190, 172, 239, 239, 256, 107, 255, 171, 107, 90, 190, 143, 171, 32, 134, 271, 154, 5, 239, 254, 48, 257, 171, 62, 32, 272, 190, 215, 239, 273, 190, 243, 107, 122, 190, 239, 253, 107, 254, 171, 107, 274, 190, 239, 248, 107, 249, 171, 107, 275, 190, 239, 259, 240, 256, 107, 271, 240, 257, 171, 107, 276, 190, 239, 256, 107, 257, 171, 107, 277, 190, 239, 344, 107, 347, 171, 171, 32, 278, 190, 215, 239, 273, 190, 245, 107, 122, 190, 239, 254, 107, 252, 171, 107, 274, 190, 239, 250, 107, 251, 171, 107, 275, 190, 239, 271, 240, 257, 107, 269, 240, 255, 171, 107, 276, 190, 239, 257, 107, 255, 171, 107, 277, 190, 239, 344, 107, 347, 171, 171, 32, 279, 190, 55, 239, 272, 171, 32, 280, 190, 55, 239, 278, 171, 32, 270, 190, 15, 239, 279, 107, 280, 107, 270, 171, 32, 77, 32, 281, 190, 239, 266, 226, 62, 107, 198, 27, 76, 264, 226, 198, 107, 62, 27, 171, 226, 62, 107, 62, 107, 198, 27, 32, 282, 190, 239, 270, 4, 267, 226, 62, 107, 198, 27, 171, 81, 203, 239, 171, 81, 283, 239, 21, 171, 32, 278, 190, 215, 239, 273, 190, 245, 107, 122, 190, 239, 254, 107, 252, 171, 107, 274, 190, 239, 250, 107, 251, 171, 107, 275, 190, 239, 258, 240, 257, 240, 261, 107, 269, 240, 255, 171, 107, 276, 190, 239, 257, 107, 255, 171, 107, 277, 190, 239, 344, 107, 347, 171, 171, 32, 134, 284, 154, 5, 239, 258, 171, 62, 32, 280, 190, 55, 239, 278, 171, 81, 285, 239, 171, 32, 286, 190, 15, 239, 282, 107, 280, 171, 42, 253, 32, 286, 2, 217, 239, 201, 239, 281, 107, 280, 226, 198, 107, 62, 107, 62, 27, 107, 347, 171, 107, 260, 190, 344, 171, 42, 253, 32, 287, 190, 286, 226, 62, 107, 62, 107, 198, 27, 81, 283, 239, 21, 171, 32, 288, 190, 239, 284, 76, 265, 171, 226, 198, 107, 198, 107, 62, 27, 32, 268, 168, 201, 239, 288, 107, 287, 107, 347, 171, 32, 278, 190, 141, 239, 278, 107, 226, 257, 107, 347, 27, 171, 32, 77, 32, 264, 168, 255, 32, 77, 32, 289, 190, 215, 239, 273, 190, 247, 107, 122, 190, 239, 253, 107, 254, 171, 107, 274, 190, 239, 248, 107, 249, 171, 107, 275, 190, 239, 259, 240, 256, 107, 261, 240, 258, 240, 257, 171, 107, 276, 190, 239, 256, 107, 258, 240, 257, 171, 107, 277, 190, 239, 344, 107, 347, 171, 171, 32, 10, 239, 289, 107, 268, 81, 290, 239, 256, 107, 258, 240, 257, 171, 171, 32, 3, 32]}, {"code": "def linear_xent_fwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n):\n    idx = tl.program_id(axis=0)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + offsets)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V // V_BLOCK_SIZE):\n\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        local_x_block_ptr = x_block_ptr\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(local_x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == v_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        m = m_new\n        A_block_ptr = tl.advance(\n            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]\n        )\n        v_range = v_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n    loss += tl.sum(lse) / N\n    tl.store(losses_ptr + idx, loss)\n    tl.store(lse_ptr + offsets, lse)", "encoded": [29, 343, 239, 243, 107, 244, 107, 245, 107, 246, 107, 247, 107, 248, 107, 249, 107, 250, 107, 251, 107, 252, 63, 6, 107, 253, 63, 6, 107, 254, 63, 6, 107, 255, 63, 6, 107, 256, 63, 6, 107, 257, 63, 6, 171, 63, 32, -1, 258, 190, 167, 239, 259, 190, 344, 171, 32, 260, 190, 215, 239, 261, 190, 243, 107, 122, 190, 239, 253, 107, 254, 171, 107, 262, 190, 239, 248, 107, 249, 171, 107, 263, 190, 239, 258, 240, 256, 107, 344, 171, 107, 264, 190, 239, 256, 107, 257, 171, 107, 265, 190, 239, 345, 107, 344, 171, 171, 32, 266, 190, 215, 239, 261, 190, 245, 107, 122, 190, 239, 254, 107, 252, 171, 107, 262, 190, 239, 250, 107, 251, 171, 107, 263, 190, 239, 344, 107, 344, 171, 107, 264, 190, 239, 257, 107, 255, 171, 107, 265, 190, 239, 345, 107, 344, 171, 171, 32, 263, 190, 258, 240, 256, 74, 75, 239, 344, 107, 256, 171, 32, 267, 190, 75, 239, 344, 107, 255, 171, 32, 268, 190, 57, 239, 244, 74, 263, 171, 32, 269, 190, 172, 239, 239, 256, 107, 171, 107, 90, 190, 143, 171, 4, 270, 239, 346, 171, 32, 271, 190, 172, 239, 239, 256, 107, 171, 107, 90, 190, 143, 171, 32, 272, 190, 344, 32, 134, 273, 154, 5, 239, 252, 48, 255, 171, 63, 32, 274, 190, 172, 239, 239, 256, 107, 255, 171, 107, 90, 190, 143, 171, 32, 275, 190, 260, 32, 134, 273, 154, 5, 239, 254, 48, 257, 171, 63, 32, 276, 190, 57, 239, 275, 171, 32, 277, 190, 57, 239, 266, 171, 32, 274, 190, 15, 239, 276, 107, 277, 107, 274, 171, 32, 275, 190, 141, 239, 275, 107, 226, 344, 107, 257, 27, 171, 32, 266, 190, 141, 239, 266, 107, 226, 257, 107, 344, 27, 171, 32, 77, 32, 278, 190, 188, 239, 269, 107, 12, 239, 274, 107, 345, 171, 171, 32, 279, 190, 217, 239, 106, 239, 274, 4, 278, 226, 63, 107, 198, 27, 171, 107, 259, 190, 345, 171, 32, 271, 190, 271, 240, 106, 239, 269, 4, 278, 171, 74, 279, 32, 280, 190, 268, 226, 63, 107, 198, 27, 76, 267, 226, 198, 107, 63, 27, 32, 272, 2, 217, 239, 201, 239, 280, 107, 274, 107, 270, 239, 344, 171, 171, 171, 42, 253, 32, 269, 190, 278, 32, 266, 190, 141, 239, 266, 107, 226, 4, 257, 240, 239, 254, 48, 257, 171, 107, 255, 27, 171, 32, 267, 190, 267, 74, 255, 32, 77, 32, 281, 190, 269, 74, 50, 239, 271, 171, 32, 272, 168, 217, 239, 281, 171, 42, 253, 32, 10, 239, 246, 74, 258, 107, 272, 171, 32, 10, 239, 247, 74, 263, 107, 281, 171, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_dA(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    A_grad_ptr,\n    locks_N_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_V, idx_N = tl.program_id(axis=0), tl.program_id(axis=1)\n\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n\n    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_grad_block_ptr = tl.make_block_ptr(\n        base=A_grad_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    y = tl.load(y_ptr + N_offsets)\n    lse = tl.load(lse_global_ptr + N_offsets)\n\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n    x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n\n    mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n\n    softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n\n    for idx_H in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n\n        temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)\n        temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n        temp_AgradT = (temp_Agrad.trans() / N).to(tl.float16)\n\n        while tl.atomic_cas(locks_N_ptr + idx_V, 0, 1) == 1:\n            pass\n        tl.store(A_grad_block_ptr, temp_AgradT + tl.load(A_grad_block_ptr))\n        tl.atomic_xchg(locks_N_ptr + idx_V, 0)\n\n        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])\n\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 62, 6, 108, 256, 62, 6, 108, 257, 62, 6, 108, 258, 62, 6, 192, 346, 347, 108, 259, 62, 6, 192, 346, 347, 108, 260, 62, 6, 192, 346, 347, 173, 62, 32, -1, 261, 108, 262, 192, 241, 169, 241, 263, 192, 348, 173, 108, 169, 241, 263, 192, 346, 173, 173, 32, 83, 241, 256, 224, 259, 77, 348, 173, 32, 83, 241, 255, 224, 258, 77, 348, 173, 32, 83, 241, 257, 224, 260, 77, 348, 173, 32, 264, 192, 262, 242, 259, 74, 76, 241, 348, 108, 259, 173, 32, 265, 192, 261, 242, 258, 74, 76, 241, 348, 108, 258, 173, 32, 266, 192, 217, 241, 267, 192, 247, 108, 123, 192, 241, 257, 108, 255, 173, 108, 268, 192, 241, 253, 108, 254, 173, 108, 269, 192, 241, 348, 108, 261, 242, 258, 173, 108, 270, 192, 241, 260, 108, 258, 173, 108, 271, 192, 241, 346, 108, 348, 173, 173, 32, 272, 192, 217, 241, 267, 192, 249, 108, 123, 192, 241, 257, 108, 255, 173, 108, 268, 192, 241, 253, 108, 254, 173, 108, 269, 192, 241, 348, 242, 260, 108, 261, 242, 258, 173, 108, 270, 192, 241, 260, 108, 258, 173, 108, 271, 192, 241, 346, 108, 348, 173, 173, 32, 273, 192, 217, 241, 267, 192, 245, 108, 123, 192, 241, 256, 108, 257, 173, 108, 268, 192, 241, 251, 108, 252, 173, 108, 269, 192, 241, 262, 242, 259, 108, 348, 173, 108, 270, 192, 241, 259, 108, 260, 173, 108, 271, 192, 241, 346, 108, 348, 173, 173, 32, 274, 192, 55, 241, 246, 74, 264, 173, 32, 275, 192, 55, 241, 248, 74, 264, 173, 32, 276, 192, 174, 241, 241, 259, 108, 258, 173, 108, 91, 192, 144, 173, 32, 135, 277, 156, 5, 241, 257, 48, 260, 173, 62, 32, 278, 192, 55, 241, 273, 173, 32, 279, 192, 55, 241, 266, 173, 32, 276, 192, 15, 241, 278, 108, 279, 108, 276, 173, 32, 273, 192, 142, 241, 273, 108, 228, 348, 108, 260, 27, 173, 32, 266, 192, 142, 241, 266, 108, 228, 260, 108, 348, 27, 173, 32, 78, 32, 273, 192, 142, 241, 273, 108, 228, 348, 108, 4, 257, 27, 173, 32, 280, 192, 241, 274, 228, 62, 108, 200, 27, 77, 265, 228, 200, 108, 62, 27, 173, 228, 62, 108, 62, 108, 200, 27, 32, 281, 192, 241, 276, 4, 275, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 82, 282, 241, 21, 173, 32, 135, 283, 156, 5, 241, 257, 48, 260, 173, 62, 32, 278, 192, 55, 241, 273, 173, 32, 284, 192, 15, 241, 281, 82, 285, 241, 173, 108, 278, 173, 32, 284, 2, 219, 241, 203, 241, 280, 108, 278, 228, 62, 108, 200, 108, 62, 27, 108, 348, 173, 108, 263, 192, 348, 173, 32, 286, 192, 241, 284, 82, 285, 241, 173, 42, 256, 173, 82, 282, 241, 21, 173, 32, 58, 149, 241, 250, 74, 261, 108, 348, 108, 346, 173, 77, 346, 62, 32, 237, 32, 178, 32, 10, 241, 272, 108, 286, 74, 55, 241, 272, 173, 173, 32, 75, 241, 250, 74, 261, 108, 348, 173, 32, 272, 192, 142, 241, 272, 108, 228, 260, 108, 348, 27, 173, 32, 273, 192, 142, 241, 273, 108, 228, 348, 108, 260, 27, 173, 32, 78, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_dx(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    x_grad_ptr,\n    locks_V_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_V, idx_N = tl.program_id(axis=0), tl.program_id(axis=1)\n\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n\n    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_offsets)\n    lse = tl.load(lse_global_ptr + N_offsets)\n\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for idx_H_1 in range(H // H_BLOCK_SIZE):\n        x_block_ptr = tl.make_block_ptr(\n            base=x_ptr,\n            shape=(N, H),\n            strides=(stride_x_N, stride_x_H),\n            offsets=(idx_N * N_BLOCK_SIZE, idx_H_1 * H_BLOCK_SIZE),\n            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n            order=(1, 0),\n        )\n        A_block_ptr = tl.make_block_ptr(\n            base=A_t_ptr,\n            shape=(H, V),\n            strides=(stride_A_H, stride_A_V),\n            offsets=(idx_H_1 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n            order=(1, 0),\n        )\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n    mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n\n    softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n\n    for idx_H in range(H // H_BLOCK_SIZE):\n        x_grad_block_ptr = tl.make_block_ptr(\n            base=x_grad_ptr,\n            shape=(N, H),\n            strides=(stride_x_N, stride_x_H),\n            offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),\n            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n            order=(1, 0),\n        )\n        A_block_ptr = tl.make_block_ptr(\n            base=A_t_ptr,\n            shape=(H, V),\n            strides=(stride_A_H, stride_A_V),\n            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n            order=(1, 0),\n        )\n        A_v = tl.load(A_block_ptr).trans()\n\n        temp_xgrad = tl.dot(softmax_z, A_v) / N\n        temp_xgrad -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1) / N\n\n        while tl.atomic_cas(locks_V_ptr + idx_N, 0, 1) == 1:\n            pass\n        tl.store(\n            x_grad_block_ptr, tl.load(x_grad_block_ptr) + temp_xgrad.to(tl.float16)\n        )\n        tl.atomic_xchg(locks_V_ptr + idx_N, 0)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 63, 6, 108, 256, 63, 6, 108, 257, 63, 6, 108, 258, 63, 6, 192, 346, 347, 108, 259, 63, 6, 192, 346, 347, 108, 260, 63, 6, 192, 346, 347, 173, 63, 32, -1, 261, 108, 262, 192, 241, 169, 241, 263, 192, 348, 173, 108, 169, 241, 263, 192, 346, 173, 173, 32, 83, 241, 256, 224, 259, 77, 348, 173, 32, 83, 241, 255, 224, 258, 77, 348, 173, 32, 83, 241, 257, 224, 260, 77, 348, 173, 32, 264, 192, 262, 242, 259, 74, 76, 241, 348, 108, 259, 173, 32, 265, 192, 261, 242, 258, 74, 76, 241, 348, 108, 258, 173, 32, 266, 192, 57, 241, 246, 74, 264, 173, 32, 267, 192, 57, 241, 248, 74, 264, 173, 32, 268, 192, 174, 241, 241, 259, 108, 258, 173, 108, 91, 192, 144, 173, 32, 135, 269, 156, 5, 241, 257, 48, 260, 173, 63, 32, 270, 192, 217, 241, 271, 192, 245, 108, 123, 192, 241, 256, 108, 257, 173, 108, 272, 192, 241, 251, 108, 252, 173, 108, 273, 192, 241, 262, 242, 259, 108, 269, 242, 260, 173, 108, 274, 192, 241, 259, 108, 260, 173, 108, 275, 192, 241, 346, 108, 348, 173, 173, 32, 276, 192, 217, 241, 271, 192, 247, 108, 123, 192, 241, 257, 108, 255, 173, 108, 272, 192, 241, 253, 108, 254, 173, 108, 273, 192, 241, 269, 242, 260, 108, 261, 242, 258, 173, 108, 274, 192, 241, 260, 108, 258, 173, 108, 275, 192, 241, 346, 108, 348, 173, 173, 32, 277, 192, 57, 241, 270, 173, 32, 278, 192, 57, 241, 276, 173, 32, 268, 192, 15, 241, 277, 108, 278, 108, 268, 173, 32, 78, 32, 279, 192, 241, 266, 228, 63, 108, 200, 27, 77, 265, 228, 200, 108, 63, 27, 173, 228, 63, 108, 63, 108, 200, 27, 32, 280, 192, 241, 268, 4, 267, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 82, 281, 241, 21, 173, 32, 135, 282, 156, 5, 241, 257, 48, 260, 173, 63, 32, 283, 192, 217, 241, 271, 192, 249, 108, 123, 192, 241, 256, 108, 257, 173, 108, 272, 192, 241, 251, 108, 252, 173, 108, 273, 192, 241, 262, 242, 259, 108, 282, 242, 260, 173, 108, 274, 192, 241, 259, 108, 260, 173, 108, 275, 192, 241, 346, 108, 348, 173, 173, 32, 276, 192, 217, 241, 271, 192, 247, 108, 123, 192, 241, 257, 108, 255, 173, 108, 272, 192, 241, 253, 108, 254, 173, 108, 273, 192, 241, 282, 242, 260, 108, 261, 242, 258, 173, 108, 274, 192, 241, 260, 108, 258, 173, 108, 275, 192, 241, 346, 108, 348, 173, 173, 32, 278, 192, 57, 241, 276, 173, 82, 284, 241, 173, 32, 285, 192, 15, 241, 280, 108, 278, 173, 42, 256, 32, 285, 2, 219, 241, 203, 241, 279, 108, 278, 228, 200, 108, 63, 108, 63, 27, 108, 348, 173, 108, 263, 192, 346, 173, 42, 256, 32, 58, 149, 241, 250, 74, 262, 108, 348, 108, 346, 173, 77, 346, 63, 32, 237, 32, 178, 32, 10, 241, 283, 108, 57, 241, 283, 173, 74, 285, 82, 281, 241, 21, 173, 173, 32, 75, 241, 250, 74, 262, 108, 348, 173, 32, 78, 32, 3, 32]}, {"code": "def linear_xent_fwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n):\n    idx = tl.program_id(axis=0)\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + offsets)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V // V_BLOCK_SIZE):\n\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        local_x_block_ptr = x_block_ptr\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(local_x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == v_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        m = m_new\n        A_block_ptr = tl.advance(\n            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]\n        )\n        v_range = v_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n    loss += tl.sum(lse) / N\n    tl.store(losses_ptr + idx, loss)\n    tl.store(lse_ptr + offsets, lse)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 62, 6, 108, 255, 62, 6, 108, 256, 62, 6, 108, 257, 62, 6, 108, 258, 62, 6, 108, 259, 62, 6, 173, 62, 32, -1, 260, 192, 169, 241, 261, 192, 346, 173, 32, 187, 241, 257, 108, 258, 108, 259, 173, 32, 262, 192, 217, 241, 263, 192, 245, 108, 123, 192, 241, 255, 108, 256, 173, 108, 264, 192, 241, 250, 108, 251, 173, 108, 265, 192, 241, 260, 242, 258, 108, 346, 173, 108, 266, 192, 241, 258, 108, 259, 173, 108, 267, 192, 241, 347, 108, 346, 173, 173, 32, 268, 192, 217, 241, 263, 192, 247, 108, 123, 192, 241, 256, 108, 254, 173, 108, 264, 192, 241, 252, 108, 253, 173, 108, 265, 192, 241, 346, 108, 346, 173, 108, 266, 192, 241, 259, 108, 257, 173, 108, 267, 192, 241, 347, 108, 346, 173, 173, 32, 265, 192, 260, 242, 258, 74, 76, 241, 346, 108, 258, 173, 32, 269, 192, 76, 241, 346, 108, 257, 173, 32, 270, 192, 55, 241, 246, 74, 265, 173, 32, 271, 192, 174, 241, 241, 258, 108, 173, 108, 91, 192, 144, 173, 4, 272, 241, 348, 173, 32, 273, 192, 174, 241, 241, 258, 108, 173, 108, 91, 192, 144, 173, 32, 274, 192, 346, 32, 135, 275, 156, 5, 241, 254, 48, 257, 173, 62, 32, 276, 192, 174, 241, 241, 258, 108, 257, 173, 108, 91, 192, 144, 173, 32, 277, 192, 262, 32, 135, 275, 156, 5, 241, 256, 48, 259, 173, 62, 32, 278, 192, 55, 241, 277, 173, 32, 279, 192, 55, 241, 268, 173, 32, 276, 192, 15, 241, 278, 108, 279, 108, 276, 173, 32, 277, 192, 142, 241, 277, 108, 228, 346, 108, 259, 27, 173, 32, 268, 192, 142, 241, 268, 108, 228, 259, 108, 346, 27, 173, 32, 78, 32, 280, 192, 190, 241, 271, 108, 12, 241, 276, 108, 347, 173, 173, 32, 281, 192, 219, 241, 107, 241, 276, 4, 280, 228, 62, 108, 200, 27, 173, 108, 261, 192, 347, 173, 32, 273, 192, 273, 242, 107, 241, 271, 4, 280, 173, 74, 281, 32, 282, 192, 270, 228, 62, 108, 200, 27, 77, 269, 228, 200, 108, 62, 27, 32, 274, 2, 219, 241, 203, 241, 282, 108, 276, 108, 272, 241, 346, 173, 173, 173, 42, 255, 32, 271, 192, 280, 32, 268, 192, 142, 241, 268, 108, 228, 4, 259, 242, 241, 256, 48, 259, 173, 108, 257, 27, 173, 32, 269, 192, 269, 74, 257, 32, 78, 32, 283, 192, 271, 74, 50, 241, 273, 173, 32, 274, 170, 219, 241, 283, 173, 42, 255, 32, 10, 241, 248, 74, 260, 108, 274, 173, 32, 10, 241, 249, 74, 265, 108, 283, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_dA(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    A_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_V = tl.program_id(axis=0)\n\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n\n    N_offsets = tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_grad_block_ptr = tl.make_block_ptr(\n        base=A_grad_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(0 * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    for idx_N in range(N // N_BLOCK_SIZE):\n\n        y = tl.load(y_ptr + N_offsets)\n        lse = tl.load(lse_global_ptr + N_offsets)\n\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, 0])\n\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n\n        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n\n        for idx_H in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n\n            temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)\n            temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n            temp_AgradT = temp_Agrad.trans() / N\n            tl.store(\n                A_grad_block_ptr, temp_AgradT.to(tl.float16) + tl.load(A_grad_block_ptr)\n            )\n\n            A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, -H])\n        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [-H, 0])\n        N_offsets += N_BLOCK_SIZE", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 63, 6, 108, 255, 63, 6, 108, 256, 63, 6, 108, 257, 63, 6, 192, 346, 347, 108, 258, 63, 6, 192, 346, 347, 108, 259, 63, 6, 192, 346, 347, 173, 63, 32, -1, 260, 192, 169, 241, 261, 192, 348, 173, 32, 187, 241, 257, 108, 258, 108, 259, 173, 32, 262, 192, 76, 241, 348, 108, 258, 173, 32, 263, 192, 260, 242, 257, 74, 76, 241, 348, 108, 257, 173, 32, 264, 192, 217, 241, 265, 192, 247, 108, 123, 192, 241, 256, 108, 254, 173, 108, 266, 192, 241, 252, 108, 253, 173, 108, 267, 192, 241, 348, 108, 260, 242, 257, 173, 108, 268, 192, 241, 259, 108, 257, 173, 108, 269, 192, 241, 346, 108, 348, 173, 173, 32, 270, 192, 217, 241, 265, 192, 249, 108, 123, 192, 241, 256, 108, 254, 173, 108, 266, 192, 241, 252, 108, 253, 173, 108, 267, 192, 241, 348, 242, 259, 108, 260, 242, 257, 173, 108, 268, 192, 241, 259, 108, 257, 173, 108, 269, 192, 241, 346, 108, 348, 173, 173, 32, 271, 192, 217, 241, 265, 192, 245, 108, 123, 192, 241, 255, 108, 256, 173, 108, 266, 192, 241, 250, 108, 251, 173, 108, 267, 192, 241, 348, 242, 258, 108, 348, 173, 108, 268, 192, 241, 258, 108, 259, 173, 108, 269, 192, 241, 346, 108, 348, 173, 173, 32, 135, 272, 156, 5, 241, 255, 48, 258, 173, 63, 32, 273, 192, 57, 241, 246, 74, 262, 173, 32, 274, 192, 57, 241, 248, 74, 262, 173, 32, 275, 192, 174, 241, 241, 258, 108, 257, 173, 108, 91, 192, 144, 173, 32, 135, 276, 156, 5, 241, 256, 48, 259, 173, 63, 32, 277, 192, 57, 241, 271, 173, 32, 278, 192, 57, 241, 264, 173, 32, 275, 192, 15, 241, 277, 108, 278, 108, 275, 173, 32, 271, 192, 142, 241, 271, 108, 228, 348, 108, 259, 27, 173, 32, 264, 192, 142, 241, 264, 108, 228, 259, 108, 348, 27, 173, 32, 78, 32, 271, 192, 142, 241, 271, 108, 228, 348, 108, 4, 256, 27, 173, 32, 264, 192, 142, 241, 264, 108, 228, 4, 256, 108, 348, 27, 173, 32, 279, 192, 241, 273, 228, 63, 108, 200, 27, 77, 263, 228, 200, 108, 63, 27, 173, 228, 63, 108, 63, 108, 200, 27, 32, 280, 192, 241, 275, 4, 274, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 82, 281, 241, 21, 173, 32, 135, 282, 156, 5, 241, 256, 48, 259, 173, 63, 32, 277, 192, 57, 241, 271, 173, 32, 283, 192, 15, 241, 280, 82, 284, 241, 173, 108, 277, 173, 32, 283, 2, 219, 241, 203, 241, 279, 108, 277, 228, 63, 108, 200, 108, 63, 27, 108, 348, 173, 108, 261, 192, 348, 173, 32, 285, 192, 283, 82, 284, 241, 173, 42, 255, 32, 10, 241, 270, 108, 285, 82, 281, 241, 21, 173, 74, 57, 241, 270, 173, 173, 32, 270, 192, 142, 241, 270, 108, 228, 259, 108, 348, 27, 173, 32, 271, 192, 142, 241, 271, 108, 228, 348, 108, 259, 27, 173, 32, 78, 32, 271, 192, 142, 241, 271, 108, 228, 258, 108, 4, 256, 27, 173, 32, 270, 192, 142, 241, 270, 108, 228, 4, 256, 108, 348, 27, 173, 32, 262, 170, 258, 32, 78, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_dx(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    x_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 1,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_N = tl.program_id(axis=0)\n\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE)\n\n    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_offsets)\n    lse = tl.load(lse_global_ptr + N_offsets)\n    x_grad_slice = tl.zeros((N_BLOCK_SIZE, H), tl.float16)\n\n    for idx_V in range(V // V_BLOCK_SIZE):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n\n        x_block_ptr = tl.make_block_ptr(\n            base=x_ptr,\n            shape=(N, H),\n            strides=(stride_x_N, stride_x_H),\n            offsets=(idx_N * N_BLOCK_SIZE, 0),\n            block_shape=(N_BLOCK_SIZE, H),\n            order=(1, 0),\n        )\n        A_slice_ptr = tl.make_block_ptr(\n            base=A_t_ptr,\n            shape=(H, V),\n            strides=(stride_A_H, stride_A_V),\n            offsets=(0, idx_V * V_BLOCK_SIZE),\n            block_shape=(H, V_BLOCK_SIZE),\n            order=(1, 0),\n        )\n        x_chunk = tl.load(x_block_ptr)\n        A_v_full = tl.load(A_slice_ptr)\n\n        z_j_to_k = tl.sum(x_chunk[:, :, None] * A_v_full[None, :, :], axis=1).to(\n            tl.float32\n        )\n\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n\n        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n\n        temp_xgrad = (\n            tl.sum(softmax_z[:, :, None] * A_v_full.trans()[None, :, :], axis=1) / N\n        )\n        temp_xgrad -= (\n            tl.sum(tl.where(mask, A_v_full.trans()[None, :, :], 0.0), axis=1) / N\n        )\n        temp_xgrad = temp_xgrad.to(tl.float16)\n\n        x_grad_slice += temp_xgrad\n        V_offsets += V_BLOCK_SIZE\n\n    x_grad_block_ptr = tl.make_block_ptr(\n        base=x_grad_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H),\n        order=(1, 0),\n    )\n    tl.store(x_grad_block_ptr, x_grad_slice)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 62, 6, 108, 255, 62, 6, 108, 256, 62, 6, 108, 257, 62, 6, 192, 346, 347, 108, 258, 62, 6, 192, 346, 108, 259, 62, 6, 192, 346, 347, 173, 62, 32, -1, 260, 192, 169, 241, 261, 192, 348, 173, 32, 187, 241, 257, 108, 258, 173, 32, 262, 192, 260, 242, 258, 74, 76, 241, 348, 108, 258, 173, 32, 263, 192, 76, 241, 348, 108, 257, 173, 32, 264, 192, 55, 241, 246, 74, 262, 173, 32, 265, 192, 55, 241, 248, 74, 262, 173, 32, 266, 192, 174, 241, 241, 258, 108, 256, 173, 108, 21, 173, 32, 135, 267, 156, 5, 241, 254, 48, 257, 173, 62, 32, 268, 192, 174, 241, 241, 258, 108, 257, 173, 108, 91, 192, 144, 173, 32, 269, 192, 217, 241, 270, 192, 245, 108, 123, 192, 241, 255, 108, 256, 173, 108, 271, 192, 241, 250, 108, 251, 173, 108, 272, 192, 241, 260, 242, 258, 108, 348, 173, 108, 273, 192, 241, 258, 108, 256, 173, 108, 274, 192, 241, 346, 108, 348, 173, 173, 32, 275, 192, 217, 241, 270, 192, 247, 108, 123, 192, 241, 256, 108, 254, 173, 108, 271, 192, 241, 252, 108, 253, 173, 108, 272, 192, 241, 348, 108, 267, 242, 257, 173, 108, 273, 192, 241, 256, 108, 257, 173, 108, 274, 192, 241, 346, 108, 348, 173, 173, 32, 276, 192, 55, 241, 269, 173, 32, 277, 192, 55, 241, 275, 173, 32, 268, 192, 219, 241, 276, 228, 62, 108, 62, 108, 200, 27, 242, 277, 228, 200, 108, 62, 108, 62, 27, 108, 261, 192, 346, 173, 82, 278, 241, 144, 173, 32, 279, 192, 241, 264, 228, 62, 108, 200, 27, 77, 263, 228, 200, 108, 62, 27, 173, 228, 62, 108, 62, 108, 200, 27, 32, 280, 192, 241, 268, 4, 265, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 82, 278, 241, 21, 173, 32, 281, 192, 219, 241, 280, 228, 62, 108, 62, 108, 200, 27, 242, 277, 82, 282, 241, 173, 228, 200, 108, 62, 108, 62, 27, 108, 261, 192, 346, 173, 42, 255, 32, 281, 2, 219, 241, 203, 241, 279, 108, 277, 82, 282, 241, 173, 228, 200, 108, 62, 108, 62, 27, 108, 348, 173, 108, 261, 192, 346, 173, 42, 255, 32, 281, 192, 281, 82, 278, 241, 21, 173, 32, 266, 170, 281, 32, 263, 170, 257, 32, 78, 32, 283, 192, 217, 241, 270, 192, 249, 108, 123, 192, 241, 255, 108, 256, 173, 108, 271, 192, 241, 250, 108, 251, 173, 108, 272, 192, 241, 260, 242, 258, 108, 348, 173, 108, 273, 192, 241, 258, 108, 256, 173, 108, 274, 192, 241, 346, 108, 348, 173, 173, 32, 10, 241, 283, 108, 266, 173, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    stride_lse_N,\n    stride_lse_B,\n    stride_loss_Nb,\n    stride_loss_B,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n    GROUP_SIZE: tl.constexpr = 32,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)\n    idx_N, idx_V_group = tl.swizzle2d(\n        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE\n    )\n\n    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n    m = tl.max(z_j_to_k, 1)\n    s = tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n\n    mask = y[:, None] == V_range[None, :]\n    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n    tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n    lse = m + tl.log(s)\n\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_ptr,\n        shape=(N_group, V // 128),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),\n        block_shape=(N_BLOCK_SIZE, 1),\n        order=(1, 0),\n    )\n    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\n\n    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)\n    tl.store(lse_row_ptr, lse[:, None])", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 108, 265, 63, 6, 108, 266, 63, 6, 192, 346, 347, 108, 267, 63, 6, 192, 346, 347, 108, 268, 63, 6, 192, 346, 347, 108, 269, 63, 6, 192, 348, 349, 173, 63, 32, -1, 270, 192, 169, 241, 271, 192, 350, 173, 32, 272, 192, 169, 241, 271, 192, 346, 173, 32, 273, 108, 274, 192, 241, 138, 241, 350, 173, 108, 138, 241, 346, 173, 173, 32, 270, 108, 272, 192, 244, 241, 270, 108, 272, 108, 273, 108, 274, 108, 269, 173, 32, 275, 63, 6, 192, 266, 32, 276, 192, 217, 241, 277, 192, 245, 108, 123, 192, 241, 264, 108, 265, 173, 108, 278, 192, 241, 251, 108, 252, 173, 108, 279, 192, 241, 261, 242, 262, 74, 270, 242, 267, 108, 350, 173, 108, 280, 192, 241, 267, 108, 268, 173, 108, 281, 192, 241, 346, 108, 350, 173, 173, 32, 282, 192, 217, 241, 277, 192, 247, 108, 123, 192, 241, 265, 108, 263, 173, 108, 278, 192, 241, 253, 108, 254, 173, 108, 279, 192, 241, 350, 108, 272, 242, 275, 173, 108, 280, 192, 241, 268, 108, 266, 173, 108, 281, 192, 241, 346, 108, 350, 173, 173, 32, 283, 192, 217, 241, 277, 192, 248, 108, 123, 192, 241, 262, 108, 263, 173, 108, 278, 192, 241, 255, 108, 256, 173, 108, 279, 192, 241, 270, 242, 267, 108, 272, 242, 275, 173, 108, 280, 192, 241, 267, 108, 266, 173, 108, 281, 192, 241, 346, 108, 350, 173, 173, 32, 284, 192, 174, 241, 241, 267, 108, 266, 173, 108, 91, 192, 144, 173, 32, 135, 285, 156, 5, 241, 265, 48, 268, 173, 63, 32, 286, 192, 57, 241, 276, 173, 32, 287, 192, 57, 241, 282, 173, 32, 284, 192, 15, 241, 286, 108, 287, 108, 284, 173, 32, 276, 192, 142, 241, 276, 108, 228, 350, 108, 268, 27, 173, 32, 282, 192, 142, 241, 282, 108, 228, 268, 108, 350, 27, 173, 32, 78, 32, 288, 192, 12, 241, 284, 108, 346, 173, 32, 289, 192, 219, 241, 107, 241, 284, 4, 288, 228, 63, 108, 200, 27, 173, 108, 271, 192, 346, 173, 32, 290, 192, 261, 242, 262, 74, 270, 242, 267, 74, 76, 241, 350, 108, 267, 173, 32, 291, 192, 272, 242, 275, 74, 76, 241, 350, 108, 266, 173, 32, 292, 192, 57, 241, 246, 74, 290, 173, 32, 293, 192, 292, 228, 63, 108, 200, 27, 77, 291, 228, 200, 108, 63, 27, 32, 294, 192, 4, 219, 241, 203, 241, 293, 108, 284, 108, 295, 241, 350, 173, 173, 173, 42, 264, 32, 10, 241, 283, 108, 284, 82, 296, 241, 248, 82, 185, 82, 114, 173, 173, 32, 297, 192, 288, 74, 50, 241, 289, 173, 32, 298, 192, 217, 241, 277, 192, 250, 108, 123, 192, 241, 262, 108, 263, 48, 346, 349, 351, 173, 108, 278, 192, 241, 257, 108, 258, 173, 108, 279, 192, 241, 270, 242, 267, 108, 272, 173, 108, 280, 192, 241, 267, 108, 346, 173, 108, 281, 192, 241, 346, 108, 350, 173, 173, 32, 299, 192, 249, 74, 270, 242, 259, 74, 272, 242, 260, 32, 10, 241, 299, 108, 57, 241, 299, 173, 74, 294, 173, 32, 10, 241, 298, 108, 297, 228, 63, 108, 200, 27, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(\n    z_nv_ptr,\n    y_ptr,\n    A_t_ptr,\n    x_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n    GROUP_SIZE: tl.constexpr = 1,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_H = tl.program_id(axis=1)\n    idx_V = 0\n\n    num_idx_N, num_idx_H = tl.num_programs(0), tl.num_programs(1)\n    idx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)\n\n    A_t_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(0, 1),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = 0 + tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_range)\n    lse = tl.load(lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE))\n\n    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)\n    for _ in range(V // V_BLOCK_SIZE):\n        mask = y[:, None] == v_range[None, :]\n        A_v = tl.load(A_t_block_ptr)\n        z_j_to_k = tl.load(z_block_ptr)\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)\n\n        x_grad_acc = tl.dot(\n            z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty\n        )\n\n        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        v_range += V_BLOCK_SIZE\n\n    x_grad_block_ptr = tl.make_block_ptr(\n        base=x_grad_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 62, 6, 108, 258, 62, 6, 108, 259, 62, 6, 108, 260, 62, 6, 108, 261, 62, 6, 192, 346, 347, 108, 262, 62, 6, 192, 346, 347, 108, 263, 62, 6, 192, 346, 347, 108, 264, 62, 6, 192, 346, 173, 62, 32, -1, 265, 192, 169, 241, 266, 192, 348, 173, 32, 267, 192, 169, 241, 266, 192, 346, 173, 32, 268, 192, 348, 32, 269, 108, 270, 192, 241, 138, 241, 348, 173, 108, 138, 241, 346, 173, 173, 32, 265, 108, 267, 192, 69, 241, 265, 108, 267, 108, 269, 108, 270, 108, 264, 173, 32, 271, 192, 217, 241, 272, 192, 247, 108, 123, 192, 241, 260, 108, 258, 173, 108, 273, 192, 241, 252, 108, 253, 173, 108, 274, 192, 241, 267, 242, 263, 108, 348, 173, 108, 275, 192, 241, 263, 108, 261, 173, 108, 276, 192, 241, 348, 108, 346, 173, 173, 32, 277, 192, 217, 241, 272, 192, 245, 108, 123, 192, 241, 257, 108, 258, 173, 108, 273, 192, 241, 254, 108, 255, 173, 108, 274, 192, 241, 265, 242, 262, 108, 268, 242, 261, 173, 108, 275, 192, 241, 262, 108, 261, 173, 108, 276, 192, 241, 346, 108, 348, 173, 173, 32, 278, 192, 256, 242, 257, 74, 265, 242, 262, 74, 76, 241, 348, 108, 262, 173, 32, 279, 192, 348, 74, 76, 241, 348, 108, 261, 173, 32, 280, 192, 55, 241, 246, 74, 278, 173, 32, 281, 192, 55, 241, 249, 74, 265, 242, 262, 74, 76, 241, 348, 108, 262, 173, 173, 32, 282, 192, 174, 241, 241, 262, 108, 263, 173, 108, 248, 82, 185, 82, 114, 173, 32, 135, 283, 156, 5, 241, 258, 48, 261, 173, 62, 32, 284, 192, 280, 228, 62, 108, 200, 27, 77, 279, 228, 200, 108, 62, 27, 32, 285, 192, 55, 241, 271, 173, 32, 286, 192, 55, 241, 277, 173, 32, 287, 192, 241, 286, 4, 281, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 32, 288, 192, 241, 287, 4, 203, 241, 284, 108, 346, 108, 348, 173, 173, 82, 289, 241, 247, 82, 185, 82, 114, 173, 32, 282, 192, 15, 241, 288, 108, 285, 82, 290, 241, 173, 108, 282, 108, 291, 192, 248, 82, 185, 82, 114, 173, 32, 271, 192, 142, 241, 271, 108, 228, 348, 108, 261, 27, 173, 32, 277, 192, 142, 241, 277, 108, 228, 348, 108, 261, 27, 173, 32, 279, 170, 261, 32, 78, 32, 292, 192, 217, 241, 272, 192, 248, 108, 123, 192, 241, 259, 108, 260, 173, 108, 273, 192, 241, 250, 108, 251, 173, 108, 274, 192, 241, 256, 242, 257, 74, 265, 242, 262, 108, 267, 242, 263, 173, 108, 275, 192, 241, 262, 108, 263, 173, 108, 276, 192, 241, 346, 108, 348, 173, 173, 32, 10, 241, 292, 108, 241, 282, 42, 259, 173, 82, 289, 241, 248, 82, 185, 82, 114, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(\n    z_nv_ptr,\n    y_ptr,\n    x_ptr,\n    A_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n    GROUP_SIZE: tl.constexpr = 16,\n):\n    idx_V = tl.program_id(axis=0)\n    idx_H = tl.program_id(axis=1)\n\n    num_idx_V, num_idx_H = tl.num_programs(0), tl.num_programs(1)\n    idx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    N_range = tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)\n    for _ in range(N_group // N_BLOCK_SIZE):\n        y = tl.load(y_ptr + idx_N_group * N_group + N_range)\n        lse = tl.load(lse_ptr + N_range)\n        mask = y[:, None] == V_range[None, :]\n\n        x_chunk = tl.load(x_block_ptr)\n        z_j_to_k = tl.load(z_block_ptr)\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)\n\n        A_grad_acc = tl.dot(\n            x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty\n        )\n\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n        N_range += N_BLOCK_SIZE\n\n    A_grad_T_block_ptr = tl.make_block_ptr(\n        base=A_grad_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(0, 1),\n    )\n    if idx_N_group > 0:\n        tl.store(\n            A_grad_T_block_ptr,\n            tl.load(A_grad_T_block_ptr)\n            + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),\n        )\n    else:\n        tl.store(A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 63, 6, 108, 258, 63, 6, 108, 259, 63, 6, 108, 260, 63, 6, 108, 261, 63, 6, 192, 346, 347, 108, 262, 63, 6, 192, 346, 347, 108, 263, 63, 6, 192, 346, 347, 108, 264, 63, 6, 192, 346, 347, 173, 63, 32, -1, 265, 192, 169, 241, 266, 192, 348, 173, 32, 267, 192, 169, 241, 266, 192, 346, 173, 32, 268, 108, 269, 192, 241, 138, 241, 348, 173, 108, 138, 241, 346, 173, 173, 32, 265, 108, 267, 192, 244, 241, 265, 108, 267, 108, 268, 108, 269, 108, 264, 173, 32, 270, 192, 217, 241, 271, 192, 247, 108, 123, 192, 241, 259, 108, 260, 173, 108, 272, 192, 241, 250, 108, 251, 173, 108, 273, 192, 241, 256, 242, 257, 108, 267, 242, 263, 173, 108, 274, 192, 241, 262, 108, 263, 173, 108, 275, 192, 241, 346, 108, 348, 173, 173, 32, 276, 192, 217, 241, 271, 192, 245, 108, 123, 192, 241, 257, 108, 258, 173, 108, 272, 192, 241, 254, 108, 255, 173, 108, 273, 192, 241, 348, 108, 265, 242, 261, 173, 108, 274, 192, 241, 262, 108, 261, 173, 108, 275, 192, 241, 346, 108, 348, 173, 173, 32, 277, 192, 76, 241, 348, 108, 262, 173, 32, 278, 192, 265, 242, 261, 74, 76, 241, 348, 108, 261, 173, 32, 279, 192, 174, 241, 241, 263, 108, 261, 173, 108, 248, 82, 185, 82, 114, 173, 32, 135, 280, 156, 5, 241, 257, 48, 262, 173, 63, 32, 281, 192, 57, 241, 246, 74, 256, 242, 257, 74, 277, 173, 32, 282, 192, 57, 241, 249, 74, 277, 173, 32, 283, 192, 281, 228, 63, 108, 200, 27, 77, 278, 228, 200, 108, 63, 27, 32, 284, 192, 57, 241, 270, 173, 32, 285, 192, 57, 241, 276, 173, 32, 286, 192, 241, 285, 4, 282, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 287, 192, 241, 286, 4, 203, 241, 283, 108, 346, 108, 348, 173, 173, 82, 288, 241, 247, 82, 185, 82, 114, 173, 32, 279, 192, 15, 241, 284, 82, 289, 241, 173, 108, 287, 108, 279, 108, 290, 192, 248, 82, 185, 82, 114, 173, 32, 270, 192, 142, 241, 270, 108, 228, 262, 108, 348, 27, 173, 32, 276, 192, 142, 241, 276, 108, 228, 262, 108, 348, 27, 173, 32, 277, 170, 262, 32, 78, 32, 291, 192, 217, 241, 271, 192, 248, 108, 123, 192, 241, 260, 108, 258, 173, 108, 272, 192, 241, 252, 108, 253, 173, 108, 273, 192, 241, 267, 242, 263, 108, 265, 242, 261, 173, 108, 274, 192, 241, 263, 108, 261, 173, 108, 275, 192, 241, 348, 108, 346, 173, 173, 32, 180, 256, 124, 348, 63, 32, 10, 241, 291, 108, 57, 241, 291, 173, 74, 241, 279, 42, 259, 173, 82, 288, 241, 248, 82, 185, 82, 114, 173, 173, 32, 183, 32, 30, 63, 32, 10, 241, 291, 108, 241, 279, 42, 259, 173, 82, 288, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 3, 32]}, {"code": "def linear_xent_fwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    stride_lse_N,\n    stride_lse_B,\n    stride_loss_Nb,\n    stride_loss_B,\n    reduction_ptr,\n    ignore_index: tl.constexpr,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)\n    idx_N, idx_V_group = tl.swizzle2d(\n        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE\n    )\n\n    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n        A_v = tl.load(A_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_range, mask=N_range < N, other=ignore_index)\n\n    reduction = tl.load(reduction_ptr)\n    mask = y[:, None] == tl.where(V_range != ignore_index, V_range, -1)[None, :]\n    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / reduction\n\n    tl.store(\n        z_block_ptr,\n        (z_j_to_k + tl.log(1 / reduction)).to(z_nv_ptr.type.element_ty),\n        boundary_check=(0, 1),\n    )\n\n    m = tl.max(z_j_to_k, 1)\n    zero_lse_constant: tl.constexpr = tl.log(1 / tl.cdiv(V, V_BLOCK_SIZE))\n    lse = tl.where(\n        y != ignore_index,\n        tl.log(tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)) + m,\n        zero_lse_constant,\n    )\n\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_ptr,\n        shape=(N_group, V // 128),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),\n        block_shape=(N_BLOCK_SIZE, 1),\n        order=(1, 0),\n    )\n    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\n\n    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)\n    tl.store(lse_row_ptr, lse[:, None], boundary_check=(0,))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 62, 6, 108, 263, 108, 264, 62, 6, 108, 265, 62, 6, 108, 266, 62, 6, 108, 267, 62, 6, 108, 268, 62, 6, 108, 269, 62, 6, 108, 270, 62, 6, 108, 271, 62, 6, 173, 62, 32, -1, 272, 192, 169, 241, 273, 192, 346, 173, 32, 274, 192, 169, 241, 273, 192, 347, 173, 32, 275, 108, 276, 192, 241, 138, 241, 346, 173, 108, 138, 241, 347, 173, 173, 32, 272, 108, 274, 192, 69, 241, 272, 108, 274, 108, 275, 108, 276, 108, 271, 173, 32, 277, 62, 6, 192, 268, 32, 278, 192, 217, 241, 279, 192, 245, 108, 123, 192, 241, 266, 108, 267, 173, 108, 280, 192, 241, 251, 108, 252, 173, 108, 281, 192, 241, 263, 242, 264, 74, 272, 242, 269, 108, 346, 173, 108, 282, 192, 241, 269, 108, 270, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 284, 192, 217, 241, 279, 192, 247, 108, 123, 192, 241, 267, 108, 265, 173, 108, 280, 192, 241, 253, 108, 254, 173, 108, 281, 192, 241, 346, 108, 274, 242, 277, 173, 108, 282, 192, 241, 270, 108, 268, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 285, 192, 217, 241, 279, 192, 248, 108, 123, 192, 241, 264, 108, 265, 173, 108, 280, 192, 241, 255, 108, 256, 173, 108, 281, 192, 241, 272, 242, 269, 108, 274, 242, 277, 173, 108, 282, 192, 241, 269, 108, 268, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 286, 192, 174, 241, 241, 269, 108, 268, 173, 108, 91, 192, 144, 173, 32, 135, 287, 156, 5, 241, 267, 48, 270, 173, 62, 32, 288, 192, 55, 241, 278, 108, 289, 192, 241, 346, 108, 347, 173, 108, 290, 192, 348, 173, 32, 291, 192, 55, 241, 284, 108, 289, 192, 241, 346, 108, 347, 173, 108, 290, 192, 348, 173, 32, 286, 192, 15, 241, 288, 108, 291, 108, 286, 173, 32, 278, 192, 142, 241, 278, 108, 228, 346, 108, 270, 27, 173, 32, 284, 192, 142, 241, 284, 108, 228, 270, 108, 346, 27, 173, 32, 78, 32, 292, 192, 274, 242, 277, 74, 76, 241, 346, 108, 268, 173, 32, 293, 192, 263, 242, 264, 74, 272, 242, 269, 74, 76, 241, 346, 108, 269, 173, 32, 294, 192, 55, 241, 246, 74, 293, 108, 295, 192, 293, 1, 266, 108, 296, 192, 262, 173, 32, 297, 192, 55, 241, 261, 173, 32, 295, 192, 294, 228, 62, 108, 200, 27, 77, 203, 241, 292, 184, 262, 108, 292, 108, 4, 347, 173, 228, 200, 108, 62, 27, 32, 298, 192, 4, 219, 241, 203, 241, 295, 108, 286, 108, 299, 241, 346, 173, 173, 173, 42, 297, 32, 10, 241, 285, 108, 241, 286, 74, 50, 241, 347, 42, 297, 173, 173, 82, 300, 241, 248, 82, 185, 82, 114, 173, 108, 289, 192, 241, 346, 108, 347, 173, 173, 32, 301, 192, 12, 241, 286, 108, 347, 173, 32, 302, 62, 6, 192, 50, 241, 347, 42, 64, 241, 265, 108, 268, 173, 173, 32, 303, 192, 203, 241, 294, 184, 262, 108, 50, 241, 219, 241, 107, 241, 286, 4, 301, 228, 62, 108, 200, 27, 173, 108, 273, 192, 347, 173, 173, 74, 301, 108, 302, 173, 32, 304, 192, 217, 241, 279, 192, 250, 108, 123, 192, 241, 264, 108, 265, 48, 347, 349, 350, 173, 108, 280, 192, 241, 257, 108, 258, 173, 108, 281, 192, 241, 272, 242, 269, 108, 274, 173, 108, 282, 192, 241, 269, 108, 347, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 305, 192, 249, 74, 272, 242, 259, 74, 274, 242, 260, 32, 10, 241, 305, 108, 55, 241, 305, 173, 74, 298, 173, 32, 10, 241, 304, 108, 303, 228, 62, 108, 200, 27, 108, 289, 192, 241, 346, 108, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(\n    z_nv_ptr,\n    y_ptr,\n    A_t_ptr,\n    x_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    z_regularization: tl.constexpr,\n    fp32_grad_accumulators: tl.constexpr,\n    reduction_ptr,\n    ignore_index: tl.constexpr,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n    SPLIT_N: tl.constexpr,\n    SPLIT_V: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0) // SPLIT_V\n    idx_H = tl.program_id(axis=1)\n    idx_V_tile = tl.program_id(axis=0) % SPLIT_V\n\n    num_idx_N, num_idx_H = tl.num_programs(0) - (\n        triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N\n    ), tl.num_programs(1)\n    idx_N, idx_H = tl.swizzle2d(\n        idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE\n    )\n\n    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)\n\n    A_t_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(0, 1),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")\n    lse = tl.load(\n        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),\n        eviction_policy=\"evict_last\",\n    )\n    reduction = tl.load(reduction_ptr)\n\n    acc_dtype = tl.float32 if fp32_grad_accumulators else x_grad_ptr.type.element_ty\n    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), acc_dtype)\n    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):\n        mask = y[:, None] == V_range[None, :]\n        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")\n        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        if z_regularization > 0:\n            softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z\n        z_grad = softmax_z - tl.where(mask, 1 / reduction, 0.0)\n        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(\n            A_v.type.element_ty\n        )\n\n        x_grad_acc = tl.dot(valid_z_grad, A_v.trans(), x_grad_acc, out_dtype=acc_dtype)\n\n        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        V_range += V_BLOCK_SIZE\n\n    if SPLIT_V == 1:\n        x_grad_block_ptr = tl.make_block_ptr(\n            base=x_grad_ptr,\n            shape=(N, H),\n            strides=(stride_x_N, stride_x_H),\n            offsets=(\n                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,\n                idx_H * H_BLOCK_SIZE,\n            ),\n            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n            order=(1, 0),\n        )\n        tl.store(x_grad_block_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))\n    else:\n        row_n = (\n            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n        )\n        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n        x_grad_simple_ptr = (\n            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H\n        )\n        tl.atomic_add(x_grad_simple_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 63, 6, 108, 257, 63, 6, 108, 258, 108, 259, 63, 6, 108, 260, 108, 261, 63, 6, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 108, 265, 63, 6, 108, 266, 63, 6, 108, 267, 63, 6, 108, 268, 63, 6, 108, 269, 63, 6, 108, 270, 63, 6, 173, 63, 32, -1, 271, 192, 169, 241, 272, 192, 346, 173, 48, 270, 32, 273, 192, 169, 241, 272, 192, 347, 173, 32, 274, 192, 169, 241, 272, 192, 346, 173, 224, 270, 32, 275, 108, 276, 192, 241, 138, 241, 346, 173, 4, 20, 82, 277, 241, 262, 108, 265, 173, 242, 269, 108, 138, 241, 347, 173, 173, 32, 271, 108, 273, 192, 244, 241, 271, 108, 273, 108, 275, 48, 270, 108, 276, 108, 268, 173, 32, 278, 192, 274, 242, 65, 241, 262, 108, 270, 173, 32, 279, 192, 217, 241, 280, 192, 247, 108, 123, 192, 241, 264, 108, 262, 173, 108, 281, 192, 241, 252, 108, 253, 173, 108, 282, 192, 241, 273, 242, 267, 108, 278, 173, 108, 283, 192, 241, 267, 108, 265, 173, 108, 284, 192, 241, 346, 108, 347, 173, 173, 32, 285, 192, 217, 241, 280, 192, 245, 108, 123, 192, 241, 261, 108, 262, 173, 108, 281, 192, 241, 254, 108, 255, 173, 108, 282, 192, 241, 271, 242, 266, 108, 278, 173, 108, 283, 192, 241, 266, 108, 265, 173, 108, 284, 192, 241, 347, 108, 346, 173, 173, 32, 286, 192, 260, 242, 261, 74, 271, 242, 266, 74, 76, 241, 346, 108, 266, 173, 32, 287, 192, 278, 74, 76, 241, 346, 108, 265, 173, 32, 288, 192, 57, 241, 246, 74, 286, 108, 289, 192, 348, 173, 32, 290, 192, 57, 241, 249, 74, 271, 242, 266, 74, 76, 241, 346, 108, 266, 173, 108, 289, 192, 348, 173, 32, 291, 192, 57, 241, 258, 173, 32, 292, 192, 144, 180, 257, 30, 248, 82, 185, 82, 114, 32, 293, 192, 174, 241, 241, 266, 108, 267, 173, 108, 292, 173, 32, 135, 294, 156, 5, 241, 346, 108, 65, 241, 262, 108, 265, 242, 270, 173, 173, 63, 32, 295, 192, 288, 228, 63, 108, 200, 27, 77, 287, 228, 200, 108, 63, 27, 32, 296, 192, 57, 241, 279, 108, 289, 192, 349, 173, 32, 297, 192, 57, 241, 285, 108, 289, 192, 348, 173, 32, 298, 192, 241, 297, 4, 290, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 180, 256, 124, 346, 63, 32, 298, 170, 350, 242, 256, 242, 290, 228, 63, 108, 200, 27, 242, 298, 32, 183, 32, 299, 192, 298, 4, 203, 241, 295, 108, 347, 42, 291, 108, 346, 173, 32, 300, 192, 203, 241, 241, 288, 77, 259, 173, 228, 63, 108, 200, 27, 108, 346, 108, 299, 173, 82, 301, 241, 296, 82, 185, 82, 114, 173, 32, 293, 192, 15, 241, 300, 108, 296, 82, 302, 241, 173, 108, 293, 108, 303, 192, 292, 173, 32, 279, 192, 142, 241, 279, 108, 228, 346, 108, 265, 27, 173, 32, 285, 192, 142, 241, 285, 108, 228, 346, 108, 265, 27, 173, 32, 287, 170, 265, 32, 78, 32, 180, 270, 77, 347, 63, 32, 304, 192, 217, 241, 280, 192, 248, 108, 123, 192, 241, 263, 108, 264, 173, 108, 281, 192, 241, 250, 108, 251, 173, 108, 282, 192, 241, 260, 242, 261, 74, 271, 242, 266, 108, 273, 242, 267, 173, 108, 283, 192, 241, 266, 108, 267, 173, 108, 284, 192, 241, 347, 108, 346, 173, 173, 32, 10, 241, 304, 108, 293, 82, 301, 241, 248, 82, 185, 82, 114, 173, 173, 32, 183, 32, 30, 63, 32, 305, 192, 260, 242, 261, 74, 271, 242, 266, 74, 76, 241, 346, 108, 266, 173, 32, 306, 192, 273, 242, 267, 74, 76, 241, 346, 108, 267, 173, 32, 307, 192, 248, 74, 305, 228, 63, 108, 200, 27, 242, 250, 74, 306, 228, 200, 108, 63, 27, 242, 251, 32, 194, 241, 307, 108, 293, 82, 301, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(\n    z_nv_ptr,\n    y_ptr,\n    x_ptr,\n    A_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    z_regularization: tl.constexpr,\n    fp32_grad_accumulators: tl.constexpr,\n    reduction_ptr,\n    ignore_index: tl.constexpr,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n    SPLIT_N: tl.constexpr,\n    SPLIT_V: tl.constexpr,\n):\n    idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N\n    idx_H = tl.program_id(axis=1)\n    idx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N\n\n    num_idx_V, num_idx_H = tl.num_programs(0) - (\n        N_group // N_BLOCK_SIZE * SPLIT_V\n    ), tl.num_programs(1)\n    idx_V, idx_H = tl.swizzle2d(\n        idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE\n    )\n\n    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    reduction = tl.load(reduction_ptr)\n\n    acc_dtype = tl.float32 if fp32_grad_accumulators else A_grad_ptr.type.element_ty\n    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), acc_dtype)\n    for _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):\n        y = tl.load(\n            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"\n        )\n        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")\n        mask = y[:, None] == V_range[None, :]\n\n        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")\n        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        if z_regularization > 0:\n            softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z\n        z_grad = softmax_z - tl.where(mask, 1 / reduction, 0)\n        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(\n            x_ptr.type.element_ty\n        )\n\n        A_grad_acc = tl.dot(\n            x_chunk.trans(), valid_z_grad, A_grad_acc, out_dtype=acc_dtype\n        )\n\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n        N_range += N_BLOCK_SIZE\n\n    if SPLIT_N == 1:\n        A_grad_T_block_ptr = tl.make_block_ptr(\n            base=A_grad_ptr,\n            shape=(H, V),\n            strides=(stride_A_H, stride_A_V),\n            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n            order=(0, 1),\n        )\n        if idx_N_group > 0:\n            tl.store(\n                A_grad_T_block_ptr,\n                tl.load(A_grad_T_block_ptr) + A_grad_acc.to(A_grad_ptr.type.element_ty),\n            )\n        else:\n            tl.store(A_grad_T_block_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))\n    else:\n        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n        A_grad_T_simple_ptr = (\n            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V\n        )\n        tl.atomic_add(A_grad_T_simple_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 62, 6, 108, 257, 62, 6, 108, 258, 108, 259, 62, 6, 108, 260, 108, 261, 62, 6, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 108, 266, 62, 6, 108, 267, 62, 6, 108, 268, 62, 6, 108, 269, 62, 6, 108, 270, 62, 6, 173, 62, 32, -1, 271, 192, 241, 169, 241, 272, 192, 346, 173, 4, 261, 48, 266, 242, 270, 173, 48, 269, 32, 273, 192, 169, 241, 272, 192, 347, 173, 32, 274, 192, 241, 169, 241, 272, 192, 346, 173, 4, 261, 48, 266, 242, 270, 173, 224, 269, 32, 275, 108, 276, 192, 241, 138, 241, 346, 173, 4, 261, 48, 266, 242, 270, 108, 138, 241, 347, 173, 173, 32, 271, 108, 273, 192, 69, 241, 271, 108, 273, 108, 275, 48, 269, 108, 276, 108, 268, 173, 32, 277, 192, 274, 242, 64, 241, 261, 108, 269, 173, 32, 278, 192, 217, 241, 279, 192, 247, 108, 123, 192, 241, 263, 108, 264, 173, 108, 280, 192, 241, 250, 108, 251, 173, 108, 281, 192, 241, 260, 242, 261, 74, 277, 108, 273, 242, 267, 173, 108, 282, 192, 241, 266, 108, 267, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 284, 192, 217, 241, 279, 192, 245, 108, 123, 192, 241, 261, 108, 262, 173, 108, 280, 192, 241, 254, 108, 255, 173, 108, 281, 192, 241, 277, 108, 271, 242, 265, 173, 108, 282, 192, 241, 266, 108, 265, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 285, 192, 277, 74, 76, 241, 346, 108, 266, 173, 32, 286, 192, 271, 242, 265, 74, 76, 241, 346, 108, 265, 173, 32, 287, 192, 55, 241, 258, 173, 32, 288, 192, 144, 180, 257, 30, 248, 82, 185, 82, 114, 32, 289, 192, 174, 241, 241, 267, 108, 265, 173, 108, 288, 173, 32, 135, 290, 156, 5, 241, 346, 108, 64, 241, 261, 108, 266, 242, 269, 173, 173, 62, 32, 291, 192, 55, 241, 246, 74, 260, 242, 261, 74, 285, 108, 292, 192, 348, 173, 32, 293, 192, 55, 241, 249, 74, 285, 108, 292, 192, 348, 173, 32, 294, 192, 291, 228, 62, 108, 200, 27, 77, 286, 228, 200, 108, 62, 27, 32, 295, 192, 55, 241, 278, 108, 292, 192, 349, 173, 32, 296, 192, 55, 241, 284, 108, 292, 192, 348, 173, 32, 297, 192, 241, 296, 4, 293, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 32, 180, 256, 124, 346, 62, 32, 297, 170, 350, 242, 256, 242, 293, 228, 62, 108, 200, 27, 242, 297, 32, 183, 32, 298, 192, 297, 4, 203, 241, 294, 108, 347, 42, 287, 108, 346, 173, 32, 299, 192, 203, 241, 241, 291, 77, 259, 173, 228, 62, 108, 200, 27, 108, 346, 108, 298, 173, 82, 300, 241, 247, 82, 185, 82, 114, 173, 32, 289, 192, 15, 241, 295, 82, 301, 241, 173, 108, 299, 108, 289, 108, 302, 192, 288, 173, 32, 278, 192, 142, 241, 278, 108, 228, 266, 108, 346, 27, 173, 32, 284, 192, 142, 241, 284, 108, 228, 266, 108, 346, 27, 173, 32, 285, 170, 266, 32, 78, 32, 180, 269, 77, 347, 62, 32, 303, 192, 217, 241, 279, 192, 248, 108, 123, 192, 241, 264, 108, 262, 173, 108, 280, 192, 241, 252, 108, 253, 173, 108, 281, 192, 241, 273, 242, 267, 108, 271, 242, 265, 173, 108, 282, 192, 241, 267, 108, 265, 173, 108, 283, 192, 241, 346, 108, 347, 173, 173, 32, 180, 260, 124, 346, 62, 32, 10, 241, 303, 108, 55, 241, 303, 173, 74, 289, 82, 300, 241, 248, 82, 185, 82, 114, 173, 173, 32, 183, 32, 30, 62, 32, 10, 241, 303, 108, 289, 82, 300, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 183, 32, 30, 62, 32, 304, 192, 273, 242, 267, 74, 76, 241, 346, 108, 267, 173, 32, 305, 192, 271, 242, 265, 74, 76, 241, 346, 108, 265, 173, 32, 306, 192, 248, 74, 304, 228, 62, 108, 200, 27, 242, 252, 74, 305, 228, 200, 108, 62, 27, 242, 253, 32, 194, 241, 306, 108, 289, 82, 300, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 3, 32]}, {"code": "def logsumexp_reduction_kernel(\n    lse_local_ptr,\n    lse_global_ptr,\n    lse_sum_ptr,\n    reduction_ptr,\n    z_regularization: tl.constexpr,\n    stride_lse_N,\n    stride_lse_B,\n    N_group,\n    V: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr = 32,\n):\n    idx_N = tl.program_id(axis=0)\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_local_ptr,\n        shape=(N_group, V // 128),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, V // V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    lse_local = tl.load(lse_row_ptr)\n    m = tl.max(lse_local, 1)\n    lse = tl.log(tl.sum(tl.exp((lse_local - m[:, None])), axis=1)) + m\n\n    lse_reduction = (tl.sum(lse) + z_regularization * tl.sum(lse * lse)) / tl.load(\n        reduction_ptr\n    )\n\n    tl.atomic_add(lse_sum_ptr, lse_reduction)\n    tl.store(lse_global_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE), lse)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 63, 6, 108, 250, 108, 251, 108, 252, 108, 253, 63, 6, 108, 254, 63, 6, 108, 255, 63, 6, 192, 346, 347, 173, 63, 32, -1, 256, 192, 169, 241, 257, 192, 348, 173, 32, 258, 192, 217, 241, 259, 192, 245, 108, 123, 192, 241, 252, 108, 253, 48, 349, 347, 350, 173, 108, 260, 192, 241, 250, 108, 251, 173, 108, 261, 192, 241, 256, 242, 255, 108, 348, 173, 108, 262, 192, 241, 255, 108, 253, 48, 254, 173, 108, 263, 192, 241, 349, 108, 348, 173, 173, 32, 264, 192, 57, 241, 258, 173, 32, 265, 192, 12, 241, 264, 108, 349, 173, 32, 266, 192, 50, 241, 219, 241, 107, 241, 264, 4, 265, 228, 63, 108, 200, 27, 173, 108, 257, 192, 349, 173, 173, 74, 265, 32, 267, 192, 241, 219, 241, 266, 173, 74, 249, 242, 219, 241, 266, 242, 266, 173, 173, 42, 57, 241, 248, 173, 32, 194, 241, 247, 108, 267, 173, 32, 10, 241, 246, 74, 256, 242, 255, 74, 76, 241, 348, 108, 255, 173, 108, 266, 173, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    stride_lse_N,\n    stride_lse_B,\n    stride_loss_Nb,\n    stride_loss_B,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)\n    idx_N, idx_V_group = tl.swizzle2d(\n        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE\n    )\n\n    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n    m = tl.max(z_j_to_k, 1)\n    s = tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n\n    mask = y[:, None] == V_range[None, :]\n    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n    tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n    lse = m + tl.log(s)\n\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_ptr,\n        shape=(N_group, V // 128),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),\n        block_shape=(N_BLOCK_SIZE, 1),\n        order=(1, 0),\n    )\n    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\n\n    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)\n    tl.store(lse_row_ptr, lse[:, None])", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 108, 266, 62, 6, 108, 267, 62, 6, 108, 268, 62, 6, 108, 269, 62, 6, 173, 62, 32, -1, 270, 192, 169, 241, 271, 192, 346, 173, 32, 272, 192, 169, 241, 271, 192, 347, 173, 32, 273, 108, 274, 192, 241, 138, 241, 346, 173, 108, 138, 241, 347, 173, 173, 32, 270, 108, 272, 192, 69, 241, 270, 108, 272, 108, 273, 108, 274, 108, 269, 173, 32, 275, 62, 6, 192, 266, 32, 276, 192, 217, 241, 277, 192, 245, 108, 123, 192, 241, 264, 108, 265, 173, 108, 278, 192, 241, 251, 108, 252, 173, 108, 279, 192, 241, 261, 242, 262, 74, 270, 242, 267, 108, 346, 173, 108, 280, 192, 241, 267, 108, 268, 173, 108, 281, 192, 241, 347, 108, 346, 173, 173, 32, 282, 192, 217, 241, 277, 192, 247, 108, 123, 192, 241, 265, 108, 263, 173, 108, 278, 192, 241, 253, 108, 254, 173, 108, 279, 192, 241, 346, 108, 272, 242, 275, 173, 108, 280, 192, 241, 268, 108, 266, 173, 108, 281, 192, 241, 347, 108, 346, 173, 173, 32, 283, 192, 217, 241, 277, 192, 248, 108, 123, 192, 241, 262, 108, 263, 173, 108, 278, 192, 241, 255, 108, 256, 173, 108, 279, 192, 241, 270, 242, 267, 108, 272, 242, 275, 173, 108, 280, 192, 241, 267, 108, 266, 173, 108, 281, 192, 241, 347, 108, 346, 173, 173, 32, 284, 192, 174, 241, 241, 267, 108, 266, 173, 108, 91, 192, 144, 173, 32, 135, 285, 156, 5, 241, 265, 48, 268, 173, 62, 32, 286, 192, 55, 241, 276, 173, 32, 287, 192, 55, 241, 282, 173, 32, 284, 192, 15, 241, 286, 108, 287, 108, 284, 173, 32, 276, 192, 142, 241, 276, 108, 228, 346, 108, 268, 27, 173, 32, 282, 192, 142, 241, 282, 108, 228, 268, 108, 346, 27, 173, 32, 78, 32, 288, 192, 12, 241, 284, 108, 347, 173, 32, 289, 192, 219, 241, 107, 241, 284, 4, 288, 228, 62, 108, 200, 27, 173, 108, 271, 192, 347, 173, 32, 290, 192, 261, 242, 262, 74, 270, 242, 267, 74, 76, 241, 346, 108, 267, 173, 32, 291, 192, 272, 242, 275, 74, 76, 241, 346, 108, 266, 173, 32, 292, 192, 55, 241, 246, 74, 290, 173, 32, 293, 192, 292, 228, 62, 108, 200, 27, 77, 291, 228, 200, 108, 62, 27, 32, 294, 192, 4, 219, 241, 203, 241, 293, 108, 284, 108, 295, 241, 346, 173, 173, 173, 42, 264, 32, 10, 241, 283, 108, 284, 82, 296, 241, 248, 82, 185, 82, 114, 173, 173, 32, 297, 192, 288, 74, 50, 241, 289, 173, 32, 298, 192, 217, 241, 277, 192, 250, 108, 123, 192, 241, 262, 108, 263, 48, 347, 348, 349, 173, 108, 278, 192, 241, 257, 108, 258, 173, 108, 279, 192, 241, 270, 242, 267, 108, 272, 173, 108, 280, 192, 241, 267, 108, 347, 173, 108, 281, 192, 241, 347, 108, 346, 173, 173, 32, 299, 192, 249, 74, 270, 242, 259, 74, 272, 242, 260, 32, 10, 241, 299, 108, 55, 241, 299, 173, 74, 294, 173, 32, 10, 241, 298, 108, 297, 228, 62, 108, 200, 27, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(\n    z_nv_ptr,\n    y_ptr,\n    A_t_ptr,\n    x_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_H = tl.program_id(axis=1)\n    idx_V = 0\n\n    num_idx_N, num_idx_H = tl.num_programs(0) - (V // V_BLOCK_SIZE), tl.num_programs(1)\n    idx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)\n\n    A_t_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(0, 1),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = 0 + tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")\n    lse = tl.load(\n        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),\n        eviction_policy=\"evict_last\",\n    )\n\n    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)\n    for _ in range(V // V_BLOCK_SIZE):\n        mask = y[:, None] == v_range[None, :]\n        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")\n        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)\n\n        x_grad_acc = tl.dot(\n            z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty\n        )\n\n        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        v_range += V_BLOCK_SIZE\n\n    x_grad_block_ptr = tl.make_block_ptr(\n        base=x_grad_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 63, 6, 108, 258, 63, 6, 108, 259, 63, 6, 108, 260, 63, 6, 108, 261, 63, 6, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 173, 63, 32, -1, 265, 192, 169, 241, 266, 192, 346, 173, 32, 267, 192, 169, 241, 266, 192, 347, 173, 32, 268, 192, 346, 32, 269, 108, 270, 192, 241, 138, 241, 346, 173, 4, 258, 48, 261, 108, 138, 241, 347, 173, 173, 32, 265, 108, 267, 192, 244, 241, 265, 108, 267, 108, 269, 108, 270, 108, 264, 173, 32, 271, 192, 217, 241, 272, 192, 247, 108, 123, 192, 241, 260, 108, 258, 173, 108, 273, 192, 241, 252, 108, 253, 173, 108, 274, 192, 241, 267, 242, 263, 108, 346, 173, 108, 275, 192, 241, 263, 108, 261, 173, 108, 276, 192, 241, 346, 108, 347, 173, 173, 32, 277, 192, 217, 241, 272, 192, 245, 108, 123, 192, 241, 257, 108, 258, 173, 108, 273, 192, 241, 254, 108, 255, 173, 108, 274, 192, 241, 265, 242, 262, 108, 268, 242, 261, 173, 108, 275, 192, 241, 262, 108, 261, 173, 108, 276, 192, 241, 347, 108, 346, 173, 173, 32, 278, 192, 256, 242, 257, 74, 265, 242, 262, 74, 76, 241, 346, 108, 262, 173, 32, 279, 192, 346, 74, 76, 241, 346, 108, 261, 173, 32, 280, 192, 57, 241, 246, 74, 278, 108, 281, 192, 348, 173, 32, 282, 192, 57, 241, 249, 74, 265, 242, 262, 74, 76, 241, 346, 108, 262, 173, 108, 281, 192, 348, 173, 32, 283, 192, 174, 241, 241, 262, 108, 263, 173, 108, 248, 82, 185, 82, 114, 173, 32, 135, 284, 156, 5, 241, 258, 48, 261, 173, 63, 32, 285, 192, 280, 228, 63, 108, 200, 27, 77, 279, 228, 200, 108, 63, 27, 32, 286, 192, 57, 241, 271, 108, 281, 192, 349, 173, 32, 287, 192, 57, 241, 277, 108, 281, 192, 348, 173, 32, 288, 192, 241, 287, 4, 282, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 289, 192, 241, 288, 4, 203, 241, 285, 108, 347, 108, 346, 173, 173, 82, 290, 241, 247, 82, 185, 82, 114, 173, 32, 283, 192, 15, 241, 289, 108, 286, 82, 291, 241, 173, 108, 283, 108, 292, 192, 248, 82, 185, 82, 114, 173, 32, 271, 192, 142, 241, 271, 108, 228, 346, 108, 261, 27, 173, 32, 277, 192, 142, 241, 277, 108, 228, 346, 108, 261, 27, 173, 32, 279, 170, 261, 32, 78, 32, 293, 192, 217, 241, 272, 192, 248, 108, 123, 192, 241, 259, 108, 260, 173, 108, 273, 192, 241, 250, 108, 251, 173, 108, 274, 192, 241, 256, 242, 257, 74, 265, 242, 262, 108, 267, 242, 263, 173, 108, 275, 192, 241, 262, 108, 263, 173, 108, 276, 192, 241, 347, 108, 346, 173, 173, 32, 10, 241, 293, 108, 241, 283, 42, 259, 173, 82, 290, 241, 248, 82, 185, 82, 114, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(\n    z_nv_ptr,\n    y_ptr,\n    x_ptr,\n    A_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    idx_V = tl.program_id(axis=0) - N_group // N_BLOCK_SIZE\n    idx_H = tl.program_id(axis=1)\n\n    num_idx_V, num_idx_H = tl.num_programs(0) - (\n        N_group // N_BLOCK_SIZE\n    ), tl.num_programs(1)\n    idx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    N_range = tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)\n    for _ in range(N_group // N_BLOCK_SIZE):\n        y = tl.load(\n            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"\n        )\n        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")\n        mask = y[:, None] == V_range[None, :]\n\n        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")\n        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)\n\n        A_grad_acc = tl.dot(\n            x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty\n        )\n\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n        N_range += N_BLOCK_SIZE\n\n    A_grad_T_block_ptr = tl.make_block_ptr(\n        base=A_grad_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(0, 1),\n    )\n    if idx_N_group > 0:\n        tl.store(\n            A_grad_T_block_ptr,\n            tl.load(A_grad_T_block_ptr)\n            + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),\n        )\n    else:\n        tl.store(A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 62, 6, 108, 258, 62, 6, 108, 259, 62, 6, 108, 260, 62, 6, 108, 261, 62, 6, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 173, 62, 32, -1, 265, 192, 169, 241, 266, 192, 346, 173, 4, 257, 48, 262, 32, 267, 192, 169, 241, 266, 192, 347, 173, 32, 268, 108, 269, 192, 241, 138, 241, 346, 173, 4, 257, 48, 262, 108, 138, 241, 347, 173, 173, 32, 265, 108, 267, 192, 69, 241, 265, 108, 267, 108, 268, 108, 269, 108, 264, 173, 32, 270, 192, 217, 241, 271, 192, 247, 108, 123, 192, 241, 259, 108, 260, 173, 108, 272, 192, 241, 250, 108, 251, 173, 108, 273, 192, 241, 256, 242, 257, 108, 267, 242, 263, 173, 108, 274, 192, 241, 262, 108, 263, 173, 108, 275, 192, 241, 347, 108, 346, 173, 173, 32, 276, 192, 217, 241, 271, 192, 245, 108, 123, 192, 241, 257, 108, 258, 173, 108, 272, 192, 241, 254, 108, 255, 173, 108, 273, 192, 241, 346, 108, 265, 242, 261, 173, 108, 274, 192, 241, 262, 108, 261, 173, 108, 275, 192, 241, 347, 108, 346, 173, 173, 32, 277, 192, 76, 241, 346, 108, 262, 173, 32, 278, 192, 265, 242, 261, 74, 76, 241, 346, 108, 261, 173, 32, 279, 192, 174, 241, 241, 263, 108, 261, 173, 108, 248, 82, 185, 82, 114, 173, 32, 135, 280, 156, 5, 241, 257, 48, 262, 173, 62, 32, 281, 192, 55, 241, 246, 74, 256, 242, 257, 74, 277, 108, 282, 192, 348, 173, 32, 283, 192, 55, 241, 249, 74, 277, 108, 282, 192, 348, 173, 32, 284, 192, 281, 228, 62, 108, 200, 27, 77, 278, 228, 200, 108, 62, 27, 32, 285, 192, 55, 241, 270, 108, 282, 192, 349, 173, 32, 286, 192, 55, 241, 276, 108, 282, 192, 348, 173, 32, 287, 192, 241, 286, 4, 283, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 32, 288, 192, 241, 287, 4, 203, 241, 284, 108, 347, 108, 346, 173, 173, 82, 289, 241, 247, 82, 185, 82, 114, 173, 32, 279, 192, 15, 241, 285, 82, 290, 241, 173, 108, 288, 108, 279, 108, 291, 192, 248, 82, 185, 82, 114, 173, 32, 270, 192, 142, 241, 270, 108, 228, 262, 108, 346, 27, 173, 32, 276, 192, 142, 241, 276, 108, 228, 262, 108, 346, 27, 173, 32, 277, 170, 262, 32, 78, 32, 292, 192, 217, 241, 271, 192, 248, 108, 123, 192, 241, 260, 108, 258, 173, 108, 272, 192, 241, 252, 108, 253, 173, 108, 273, 192, 241, 267, 242, 263, 108, 265, 242, 261, 173, 108, 274, 192, 241, 263, 108, 261, 173, 108, 275, 192, 241, 346, 108, 347, 173, 173, 32, 180, 256, 124, 346, 62, 32, 10, 241, 292, 108, 55, 241, 292, 173, 74, 241, 279, 42, 259, 173, 82, 289, 241, 248, 82, 185, 82, 114, 173, 173, 32, 183, 32, 30, 62, 32, 10, 241, 292, 108, 241, 279, 42, 259, 173, 82, 289, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    stride_lse_N,\n    stride_lse_B,\n    stride_loss_Nb,\n    stride_loss_B,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)\n    idx_N, idx_V_group = tl.swizzle2d(\n        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE\n    )\n\n    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n    m = tl.max(z_j_to_k, 1)\n    s = tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n\n    mask = y[:, None] == V_range[None, :]\n    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n    tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n    lse = m + tl.log(s)\n\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_ptr,\n        shape=(N_group, V // 128),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),\n        block_shape=(N_BLOCK_SIZE, 1),\n        order=(1, 0),\n    )\n    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\n\n    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)\n    tl.store(lse_row_ptr, lse[:, None])", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 108, 265, 63, 6, 108, 266, 63, 6, 108, 267, 63, 6, 108, 268, 63, 6, 108, 269, 63, 6, 173, 63, 32, -1, 270, 192, 169, 241, 271, 192, 346, 173, 32, 272, 192, 169, 241, 271, 192, 347, 173, 32, 273, 108, 274, 192, 241, 138, 241, 346, 173, 108, 138, 241, 347, 173, 173, 32, 270, 108, 272, 192, 244, 241, 270, 108, 272, 108, 273, 108, 274, 108, 269, 173, 32, 275, 63, 6, 192, 266, 32, 276, 192, 217, 241, 277, 192, 245, 108, 123, 192, 241, 264, 108, 265, 173, 108, 278, 192, 241, 251, 108, 252, 173, 108, 279, 192, 241, 261, 242, 262, 74, 270, 242, 267, 108, 346, 173, 108, 280, 192, 241, 267, 108, 268, 173, 108, 281, 192, 241, 347, 108, 346, 173, 173, 32, 282, 192, 217, 241, 277, 192, 247, 108, 123, 192, 241, 265, 108, 263, 173, 108, 278, 192, 241, 253, 108, 254, 173, 108, 279, 192, 241, 346, 108, 272, 242, 275, 173, 108, 280, 192, 241, 268, 108, 266, 173, 108, 281, 192, 241, 347, 108, 346, 173, 173, 32, 283, 192, 217, 241, 277, 192, 248, 108, 123, 192, 241, 262, 108, 263, 173, 108, 278, 192, 241, 255, 108, 256, 173, 108, 279, 192, 241, 270, 242, 267, 108, 272, 242, 275, 173, 108, 280, 192, 241, 267, 108, 266, 173, 108, 281, 192, 241, 347, 108, 346, 173, 173, 32, 284, 192, 174, 241, 241, 267, 108, 266, 173, 108, 91, 192, 144, 173, 32, 135, 285, 156, 5, 241, 265, 48, 268, 173, 63, 32, 286, 192, 57, 241, 276, 173, 32, 287, 192, 57, 241, 282, 173, 32, 284, 192, 15, 241, 286, 108, 287, 108, 284, 173, 32, 276, 192, 142, 241, 276, 108, 228, 346, 108, 268, 27, 173, 32, 282, 192, 142, 241, 282, 108, 228, 268, 108, 346, 27, 173, 32, 78, 32, 288, 192, 12, 241, 284, 108, 347, 173, 32, 289, 192, 219, 241, 107, 241, 284, 4, 288, 228, 63, 108, 200, 27, 173, 108, 271, 192, 347, 173, 32, 290, 192, 261, 242, 262, 74, 270, 242, 267, 74, 76, 241, 346, 108, 267, 173, 32, 291, 192, 272, 242, 275, 74, 76, 241, 346, 108, 266, 173, 32, 292, 192, 57, 241, 246, 74, 290, 173, 32, 293, 192, 292, 228, 63, 108, 200, 27, 77, 291, 228, 200, 108, 63, 27, 32, 294, 192, 4, 219, 241, 203, 241, 293, 108, 284, 108, 295, 241, 346, 173, 173, 173, 42, 264, 32, 10, 241, 283, 108, 284, 82, 296, 241, 248, 82, 185, 82, 114, 173, 173, 32, 297, 192, 288, 74, 50, 241, 289, 173, 32, 298, 192, 217, 241, 277, 192, 250, 108, 123, 192, 241, 262, 108, 263, 48, 347, 348, 349, 173, 108, 278, 192, 241, 257, 108, 258, 173, 108, 279, 192, 241, 270, 242, 267, 108, 272, 173, 108, 280, 192, 241, 267, 108, 347, 173, 108, 281, 192, 241, 347, 108, 346, 173, 173, 32, 299, 192, 249, 74, 270, 242, 259, 74, 272, 242, 260, 32, 10, 241, 299, 108, 57, 241, 299, 173, 74, 294, 173, 32, 10, 241, 298, 108, 297, 228, 63, 108, 200, 27, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(\n    z_nv_ptr,\n    y_ptr,\n    A_t_ptr,\n    x_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n    SPLIT_V: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_H = tl.program_id(axis=1)\n    idx_V_tile = tl.program_id(axis=2)\n\n    num_idx_N, num_idx_H = tl.num_programs(0) - (V // V_BLOCK_SIZE), tl.num_programs(1)\n    idx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)\n\n    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)\n\n    A_t_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(0, 1),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")\n    lse = tl.load(\n        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),\n        eviction_policy=\"evict_last\",\n    )\n\n    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)\n    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):\n        mask = y[:, None] == v_range[None, :]\n        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")\n        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)\n\n        x_grad_acc = tl.dot(\n            z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty\n        )\n\n        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        v_range += V_BLOCK_SIZE\n\n    if SPLIT_V == 1:\n        x_grad_block_ptr = tl.make_block_ptr(\n            base=x_grad_ptr,\n            shape=(N, H),\n            strides=(stride_x_N, stride_x_H),\n            offsets=(\n                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,\n                idx_H * H_BLOCK_SIZE,\n            ),\n            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n            order=(1, 0),\n        )\n        tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))\n    else:\n        row_n = (\n            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n        )\n        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n        x_grad_simple_ptr = (\n            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H\n        )\n        tl.atomic_add(\n            x_grad_simple_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty)\n        )", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 62, 6, 108, 258, 62, 6, 108, 259, 62, 6, 108, 260, 62, 6, 108, 261, 62, 6, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 173, 62, 32, -1, 266, 192, 169, 241, 267, 192, 346, 173, 32, 268, 192, 169, 241, 267, 192, 347, 173, 32, 269, 192, 169, 241, 267, 192, 348, 173, 32, 270, 108, 271, 192, 241, 138, 241, 346, 173, 4, 258, 48, 261, 108, 138, 241, 347, 173, 173, 32, 266, 108, 268, 192, 69, 241, 266, 108, 268, 108, 270, 108, 271, 108, 264, 173, 32, 272, 192, 269, 242, 64, 241, 258, 108, 265, 173, 32, 273, 192, 217, 241, 274, 192, 247, 108, 123, 192, 241, 260, 108, 258, 173, 108, 275, 192, 241, 252, 108, 253, 173, 108, 276, 192, 241, 268, 242, 263, 108, 272, 173, 108, 277, 192, 241, 263, 108, 261, 173, 108, 278, 192, 241, 346, 108, 347, 173, 173, 32, 279, 192, 217, 241, 274, 192, 245, 108, 123, 192, 241, 257, 108, 258, 173, 108, 275, 192, 241, 254, 108, 255, 173, 108, 276, 192, 241, 266, 242, 262, 108, 272, 173, 108, 277, 192, 241, 262, 108, 261, 173, 108, 278, 192, 241, 347, 108, 346, 173, 173, 32, 280, 192, 256, 242, 257, 74, 266, 242, 262, 74, 76, 241, 346, 108, 262, 173, 32, 281, 192, 272, 74, 76, 241, 346, 108, 261, 173, 32, 282, 192, 55, 241, 246, 74, 280, 108, 283, 192, 349, 173, 32, 284, 192, 55, 241, 249, 74, 266, 242, 262, 74, 76, 241, 346, 108, 262, 173, 108, 283, 192, 349, 173, 32, 285, 192, 174, 241, 241, 262, 108, 263, 173, 108, 248, 82, 185, 82, 114, 173, 32, 135, 286, 156, 5, 241, 346, 108, 64, 241, 258, 108, 261, 242, 265, 173, 173, 62, 32, 287, 192, 282, 228, 62, 108, 200, 27, 77, 281, 228, 200, 108, 62, 27, 32, 288, 192, 55, 241, 273, 108, 283, 192, 350, 173, 32, 289, 192, 55, 241, 279, 108, 283, 192, 349, 173, 32, 290, 192, 241, 289, 4, 284, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 32, 291, 192, 241, 290, 4, 203, 241, 287, 108, 347, 108, 346, 173, 173, 82, 292, 241, 247, 82, 185, 82, 114, 173, 32, 285, 192, 15, 241, 291, 108, 288, 82, 293, 241, 173, 108, 285, 108, 294, 192, 248, 82, 185, 82, 114, 173, 32, 273, 192, 142, 241, 273, 108, 228, 346, 108, 261, 27, 173, 32, 279, 192, 142, 241, 279, 108, 228, 346, 108, 261, 27, 173, 32, 281, 170, 261, 32, 78, 32, 180, 265, 77, 347, 62, 32, 295, 192, 217, 241, 274, 192, 248, 108, 123, 192, 241, 259, 108, 260, 173, 108, 275, 192, 241, 250, 108, 251, 173, 108, 276, 192, 241, 256, 242, 257, 74, 266, 242, 262, 108, 268, 242, 263, 173, 108, 277, 192, 241, 262, 108, 263, 173, 108, 278, 192, 241, 347, 108, 346, 173, 173, 32, 10, 241, 295, 108, 241, 285, 42, 259, 173, 82, 292, 241, 248, 82, 185, 82, 114, 173, 173, 32, 183, 32, 30, 62, 32, 296, 192, 256, 242, 257, 74, 266, 242, 262, 74, 76, 241, 346, 108, 262, 173, 32, 297, 192, 268, 242, 263, 74, 76, 241, 346, 108, 263, 173, 32, 298, 192, 248, 74, 296, 228, 62, 108, 200, 27, 242, 250, 74, 297, 228, 200, 108, 62, 27, 242, 251, 32, 194, 241, 298, 108, 241, 285, 42, 259, 173, 82, 292, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(\n    z_nv_ptr,\n    y_ptr,\n    x_ptr,\n    A_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n    SPLIT_N: tl.constexpr,\n):\n    idx_V = tl.program_id(axis=0) - N_group // N_BLOCK_SIZE\n    idx_H = tl.program_id(axis=1)\n    idx_N_tile = tl.program_id(axis=2)\n\n    num_idx_V, num_idx_H = tl.num_programs(0) - (\n        N_group // N_BLOCK_SIZE\n    ), tl.num_programs(1)\n    idx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)\n\n    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)\n    for _ in range(0, tl.cdiv(N, N_BLOCK_SIZE * SPLIT_N)):\n        y = tl.load(\n            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"\n        )\n        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")\n        mask = y[:, None] == V_range[None, :]\n\n        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")\n        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)\n\n        A_grad_acc = tl.dot(\n            x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty\n        )\n\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n        N_range += N_BLOCK_SIZE\n\n    if SPLIT_N == 1:\n        A_grad_T_block_ptr = tl.make_block_ptr(\n            base=A_grad_ptr,\n            shape=(H, V),\n            strides=(stride_A_H, stride_A_V),\n            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n            order=(0, 1),\n        )\n        if idx_N_group > 0:\n            tl.store(\n                A_grad_T_block_ptr,\n                tl.load(A_grad_T_block_ptr)\n                + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),\n            )\n        else:\n            tl.store(\n                A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty)\n            )\n    else:\n        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n        A_grad_T_simple_ptr = (\n            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V\n        )\n        tl.atomic_add(\n            A_grad_T_simple_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty)\n        )", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 63, 6, 108, 258, 63, 6, 108, 259, 63, 6, 108, 260, 63, 6, 108, 261, 63, 6, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 108, 265, 63, 6, 173, 63, 32, -1, 266, 192, 169, 241, 267, 192, 346, 173, 4, 257, 48, 262, 32, 268, 192, 169, 241, 267, 192, 347, 173, 32, 269, 192, 169, 241, 267, 192, 348, 173, 32, 270, 108, 271, 192, 241, 138, 241, 346, 173, 4, 257, 48, 262, 108, 138, 241, 347, 173, 173, 32, 266, 108, 268, 192, 244, 241, 266, 108, 268, 108, 270, 108, 271, 108, 264, 173, 32, 272, 192, 269, 242, 65, 241, 257, 108, 265, 173, 32, 273, 192, 217, 241, 274, 192, 247, 108, 123, 192, 241, 259, 108, 260, 173, 108, 275, 192, 241, 250, 108, 251, 173, 108, 276, 192, 241, 256, 242, 257, 74, 272, 108, 268, 242, 263, 173, 108, 277, 192, 241, 262, 108, 263, 173, 108, 278, 192, 241, 347, 108, 346, 173, 173, 32, 279, 192, 217, 241, 274, 192, 245, 108, 123, 192, 241, 257, 108, 258, 173, 108, 275, 192, 241, 254, 108, 255, 173, 108, 276, 192, 241, 272, 108, 266, 242, 261, 173, 108, 277, 192, 241, 262, 108, 261, 173, 108, 278, 192, 241, 347, 108, 346, 173, 173, 32, 280, 192, 272, 74, 76, 241, 346, 108, 262, 173, 32, 281, 192, 266, 242, 261, 74, 76, 241, 346, 108, 261, 173, 32, 282, 192, 174, 241, 241, 263, 108, 261, 173, 108, 248, 82, 185, 82, 114, 173, 32, 135, 283, 156, 5, 241, 346, 108, 65, 241, 259, 108, 262, 242, 265, 173, 173, 63, 32, 284, 192, 57, 241, 246, 74, 256, 242, 257, 74, 280, 108, 285, 192, 349, 173, 32, 286, 192, 57, 241, 249, 74, 280, 108, 285, 192, 349, 173, 32, 287, 192, 284, 228, 63, 108, 200, 27, 77, 281, 228, 200, 108, 63, 27, 32, 288, 192, 57, 241, 273, 108, 285, 192, 350, 173, 32, 289, 192, 57, 241, 279, 108, 285, 192, 349, 173, 32, 290, 192, 241, 289, 4, 286, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 291, 192, 241, 290, 4, 203, 241, 287, 108, 347, 108, 346, 173, 173, 82, 292, 241, 247, 82, 185, 82, 114, 173, 32, 282, 192, 15, 241, 288, 82, 293, 241, 173, 108, 291, 108, 282, 108, 294, 192, 248, 82, 185, 82, 114, 173, 32, 273, 192, 142, 241, 273, 108, 228, 262, 108, 346, 27, 173, 32, 279, 192, 142, 241, 279, 108, 228, 262, 108, 346, 27, 173, 32, 280, 170, 262, 32, 78, 32, 180, 265, 77, 347, 63, 32, 295, 192, 217, 241, 274, 192, 248, 108, 123, 192, 241, 260, 108, 258, 173, 108, 275, 192, 241, 252, 108, 253, 173, 108, 276, 192, 241, 268, 242, 263, 108, 266, 242, 261, 173, 108, 277, 192, 241, 263, 108, 261, 173, 108, 278, 192, 241, 346, 108, 347, 173, 173, 32, 180, 256, 124, 346, 63, 32, 10, 241, 295, 108, 57, 241, 295, 173, 74, 241, 282, 42, 259, 173, 82, 292, 241, 248, 82, 185, 82, 114, 173, 173, 32, 183, 32, 30, 63, 32, 10, 241, 295, 108, 241, 282, 42, 259, 173, 82, 292, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 183, 32, 30, 63, 32, 296, 192, 268, 242, 263, 74, 76, 241, 346, 108, 263, 173, 32, 297, 192, 266, 242, 261, 74, 76, 241, 346, 108, 261, 173, 32, 298, 192, 248, 74, 296, 228, 63, 108, 200, 27, 242, 252, 74, 297, 228, 200, 108, 63, 27, 242, 253, 32, 194, 241, 298, 108, 241, 282, 42, 259, 173, 82, 292, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    stride_lse_N,\n    stride_lse_B,\n    stride_loss_Nb,\n    stride_loss_B,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)\n    idx_N, idx_V_group = tl.swizzle2d(\n        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE\n    )\n\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE)\n    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n    m = tl.max(z_j_to_k, 1)\n    s = tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n\n    mask = y[:, None] == V_range[None, :]\n    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n    tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n    lse = m + tl.log(s)\n\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_ptr,\n        shape=(N_group, V // 128),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),\n        block_shape=(N_BLOCK_SIZE, 1),\n        order=(1, 0),\n    )\n    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\n\n    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)\n    tl.store(lse_row_ptr, lse[:, None])", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 108, 266, 62, 6, 108, 267, 62, 6, 108, 268, 62, 6, 108, 269, 62, 6, 173, 62, 32, -1, 270, 192, 169, 241, 271, 192, 346, 173, 32, 272, 192, 169, 241, 271, 192, 347, 173, 32, 273, 108, 274, 192, 241, 138, 241, 346, 173, 108, 138, 241, 347, 173, 173, 32, 270, 108, 272, 192, 69, 241, 270, 108, 272, 108, 273, 108, 274, 108, 269, 173, 32, 187, 241, 266, 108, 267, 108, 268, 108, 269, 173, 32, 275, 62, 6, 192, 266, 32, 276, 192, 217, 241, 277, 192, 245, 108, 123, 192, 241, 264, 108, 265, 173, 108, 278, 192, 241, 251, 108, 252, 173, 108, 279, 192, 241, 261, 242, 262, 74, 270, 242, 267, 108, 346, 173, 108, 280, 192, 241, 267, 108, 268, 173, 108, 281, 192, 241, 347, 108, 346, 173, 173, 32, 282, 192, 217, 241, 277, 192, 247, 108, 123, 192, 241, 265, 108, 263, 173, 108, 278, 192, 241, 253, 108, 254, 173, 108, 279, 192, 241, 346, 108, 272, 242, 275, 173, 108, 280, 192, 241, 268, 108, 266, 173, 108, 281, 192, 241, 347, 108, 346, 173, 173, 32, 283, 192, 217, 241, 277, 192, 248, 108, 123, 192, 241, 262, 108, 263, 173, 108, 278, 192, 241, 255, 108, 256, 173, 108, 279, 192, 241, 270, 242, 267, 108, 272, 242, 275, 173, 108, 280, 192, 241, 267, 108, 266, 173, 108, 281, 192, 241, 347, 108, 346, 173, 173, 32, 284, 192, 174, 241, 241, 267, 108, 266, 173, 108, 91, 192, 144, 173, 32, 135, 285, 156, 5, 241, 265, 48, 268, 173, 62, 32, 286, 192, 55, 241, 276, 173, 32, 287, 192, 55, 241, 282, 173, 32, 284, 192, 15, 241, 286, 108, 287, 108, 284, 173, 32, 276, 192, 142, 241, 276, 108, 228, 346, 108, 268, 27, 173, 32, 282, 192, 142, 241, 282, 108, 228, 268, 108, 346, 27, 173, 32, 78, 32, 288, 192, 12, 241, 284, 108, 347, 173, 32, 289, 192, 219, 241, 107, 241, 284, 4, 288, 228, 62, 108, 200, 27, 173, 108, 271, 192, 347, 173, 32, 290, 192, 261, 242, 262, 74, 270, 242, 267, 74, 76, 241, 346, 108, 267, 173, 32, 291, 192, 272, 242, 275, 74, 76, 241, 346, 108, 266, 173, 32, 292, 192, 55, 241, 246, 74, 290, 173, 32, 293, 192, 292, 228, 62, 108, 200, 27, 77, 291, 228, 200, 108, 62, 27, 32, 294, 192, 4, 219, 241, 203, 241, 293, 108, 284, 108, 295, 241, 346, 173, 173, 173, 42, 264, 32, 10, 241, 283, 108, 284, 82, 296, 241, 248, 82, 185, 82, 114, 173, 173, 32, 297, 192, 288, 74, 50, 241, 289, 173, 32, 298, 192, 217, 241, 277, 192, 250, 108, 123, 192, 241, 262, 108, 263, 48, 347, 348, 349, 173, 108, 278, 192, 241, 257, 108, 258, 173, 108, 279, 192, 241, 270, 242, 267, 108, 272, 173, 108, 280, 192, 241, 267, 108, 347, 173, 108, 281, 192, 241, 347, 108, 346, 173, 173, 32, 299, 192, 249, 74, 270, 242, 259, 74, 272, 242, 260, 32, 10, 241, 299, 108, 55, 241, 299, 173, 74, 294, 173, 32, 10, 241, 298, 108, 297, 228, 62, 108, 200, 27, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(\n    z_nv_ptr,\n    y_ptr,\n    A_t_ptr,\n    x_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n    SPLIT_N: tl.constexpr,\n    SPLIT_V: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0) // SPLIT_V\n    idx_H = tl.program_id(axis=1)\n    idx_V_tile = tl.program_id(axis=0) % SPLIT_V\n\n    num_idx_N, num_idx_H = tl.num_programs(0) - (\n        triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N\n    ), tl.num_programs(1)\n    idx_N, idx_H = tl.swizzle2d(\n        idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE\n    )\n\n    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)\n\n    A_t_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(0, 1),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")\n    lse = tl.load(\n        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),\n        eviction_policy=\"evict_last\",\n    )\n\n    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)\n    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):\n        mask = y[:, None] == v_range[None, :]\n        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")\n        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)\n\n        x_grad_acc = tl.dot(\n            z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty\n        )\n\n        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        v_range += V_BLOCK_SIZE\n\n    if SPLIT_V == 1:\n        x_grad_block_ptr = tl.make_block_ptr(\n            base=x_grad_ptr,\n            shape=(N, H),\n            strides=(stride_x_N, stride_x_H),\n            offsets=(\n                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,\n                idx_H * H_BLOCK_SIZE,\n            ),\n            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n            order=(1, 0),\n        )\n        tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))\n    else:\n        row_n = (\n            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n        )\n        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n        x_grad_simple_ptr = (\n            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H\n        )\n        tl.atomic_add(\n            x_grad_simple_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty)\n        )", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 63, 6, 108, 258, 63, 6, 108, 259, 63, 6, 108, 260, 63, 6, 108, 261, 63, 6, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 108, 265, 63, 6, 108, 266, 63, 6, 173, 63, 32, -1, 267, 192, 169, 241, 268, 192, 346, 173, 48, 266, 32, 269, 192, 169, 241, 268, 192, 347, 173, 32, 270, 192, 169, 241, 268, 192, 346, 173, 224, 266, 32, 271, 108, 272, 192, 241, 138, 241, 346, 173, 4, 20, 82, 273, 241, 258, 108, 261, 173, 242, 265, 108, 138, 241, 347, 173, 173, 32, 267, 108, 269, 192, 244, 241, 267, 108, 269, 108, 271, 48, 266, 108, 272, 108, 264, 173, 32, 274, 192, 270, 242, 65, 241, 258, 108, 266, 173, 32, 275, 192, 217, 241, 276, 192, 247, 108, 123, 192, 241, 260, 108, 258, 173, 108, 277, 192, 241, 252, 108, 253, 173, 108, 278, 192, 241, 269, 242, 263, 108, 274, 173, 108, 279, 192, 241, 263, 108, 261, 173, 108, 280, 192, 241, 346, 108, 347, 173, 173, 32, 281, 192, 217, 241, 276, 192, 245, 108, 123, 192, 241, 257, 108, 258, 173, 108, 277, 192, 241, 254, 108, 255, 173, 108, 278, 192, 241, 267, 242, 262, 108, 274, 173, 108, 279, 192, 241, 262, 108, 261, 173, 108, 280, 192, 241, 347, 108, 346, 173, 173, 32, 282, 192, 256, 242, 257, 74, 267, 242, 262, 74, 76, 241, 346, 108, 262, 173, 32, 283, 192, 274, 74, 76, 241, 346, 108, 261, 173, 32, 284, 192, 57, 241, 246, 74, 282, 108, 285, 192, 348, 173, 32, 286, 192, 57, 241, 249, 74, 267, 242, 262, 74, 76, 241, 346, 108, 262, 173, 108, 285, 192, 348, 173, 32, 287, 192, 174, 241, 241, 262, 108, 263, 173, 108, 248, 82, 185, 82, 114, 173, 32, 135, 288, 156, 5, 241, 346, 108, 65, 241, 258, 108, 261, 242, 266, 173, 173, 63, 32, 289, 192, 284, 228, 63, 108, 200, 27, 77, 283, 228, 200, 108, 63, 27, 32, 290, 192, 57, 241, 275, 108, 285, 192, 349, 173, 32, 291, 192, 57, 241, 281, 108, 285, 192, 348, 173, 32, 292, 192, 241, 291, 4, 286, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 293, 192, 241, 292, 4, 203, 241, 289, 108, 347, 108, 346, 173, 173, 82, 294, 241, 247, 82, 185, 82, 114, 173, 32, 287, 192, 15, 241, 293, 108, 290, 82, 295, 241, 173, 108, 287, 108, 296, 192, 248, 82, 185, 82, 114, 173, 32, 275, 192, 142, 241, 275, 108, 228, 346, 108, 261, 27, 173, 32, 281, 192, 142, 241, 281, 108, 228, 346, 108, 261, 27, 173, 32, 283, 170, 261, 32, 78, 32, 180, 266, 77, 347, 63, 32, 297, 192, 217, 241, 276, 192, 248, 108, 123, 192, 241, 259, 108, 260, 173, 108, 277, 192, 241, 250, 108, 251, 173, 108, 278, 192, 241, 256, 242, 257, 74, 267, 242, 262, 108, 269, 242, 263, 173, 108, 279, 192, 241, 262, 108, 263, 173, 108, 280, 192, 241, 347, 108, 346, 173, 173, 32, 10, 241, 297, 108, 241, 287, 42, 259, 173, 82, 294, 241, 248, 82, 185, 82, 114, 173, 173, 32, 183, 32, 30, 63, 32, 298, 192, 256, 242, 257, 74, 267, 242, 262, 74, 76, 241, 346, 108, 262, 173, 32, 299, 192, 269, 242, 263, 74, 76, 241, 346, 108, 263, 173, 32, 300, 192, 248, 74, 298, 228, 63, 108, 200, 27, 242, 250, 74, 299, 228, 200, 108, 63, 27, 242, 251, 32, 194, 241, 300, 108, 241, 287, 42, 259, 173, 82, 294, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(\n    z_nv_ptr,\n    y_ptr,\n    x_ptr,\n    A_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n    SPLIT_N: tl.constexpr,\n    SPLIT_V: tl.constexpr,\n):\n    idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N\n    idx_H = tl.program_id(axis=1)\n    idx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N\n\n    num_idx_V, num_idx_H = tl.num_programs(0) - (\n        N_group // N_BLOCK_SIZE * SPLIT_V\n    ), tl.num_programs(1)\n    idx_V, idx_H = tl.swizzle2d(\n        idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE\n    )\n\n    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)\n    for _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):\n        y = tl.load(\n            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"\n        )\n        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")\n        mask = y[:, None] == V_range[None, :]\n\n        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")\n        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)\n\n        A_grad_acc = tl.dot(\n            x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty\n        )\n\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n        N_range += N_BLOCK_SIZE\n\n    if SPLIT_N == 1:\n        A_grad_T_block_ptr = tl.make_block_ptr(\n            base=A_grad_ptr,\n            shape=(H, V),\n            strides=(stride_A_H, stride_A_V),\n            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n            order=(0, 1),\n        )\n        if idx_N_group > 0:\n            tl.store(\n                A_grad_T_block_ptr,\n                tl.load(A_grad_T_block_ptr)\n                + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),\n            )\n        else:\n            tl.store(\n                A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty)\n            )\n    else:\n        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n        A_grad_T_simple_ptr = (\n            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V\n        )\n        tl.atomic_add(\n            A_grad_T_simple_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty)\n        )", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 62, 6, 108, 258, 62, 6, 108, 259, 62, 6, 108, 260, 62, 6, 108, 261, 62, 6, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 108, 266, 62, 6, 173, 62, 32, -1, 267, 192, 241, 169, 241, 268, 192, 346, 173, 4, 257, 48, 262, 242, 266, 173, 48, 265, 32, 269, 192, 169, 241, 268, 192, 347, 173, 32, 270, 192, 241, 169, 241, 268, 192, 346, 173, 4, 257, 48, 262, 242, 266, 173, 224, 265, 32, 271, 108, 272, 192, 241, 138, 241, 346, 173, 4, 257, 48, 262, 242, 266, 108, 138, 241, 347, 173, 173, 32, 267, 108, 269, 192, 69, 241, 267, 108, 269, 108, 271, 48, 265, 108, 272, 108, 264, 173, 32, 273, 192, 270, 242, 64, 241, 257, 108, 265, 173, 32, 274, 192, 217, 241, 275, 192, 247, 108, 123, 192, 241, 259, 108, 260, 173, 108, 276, 192, 241, 250, 108, 251, 173, 108, 277, 192, 241, 256, 242, 257, 74, 273, 108, 269, 242, 263, 173, 108, 278, 192, 241, 262, 108, 263, 173, 108, 279, 192, 241, 347, 108, 346, 173, 173, 32, 280, 192, 217, 241, 275, 192, 245, 108, 123, 192, 241, 257, 108, 258, 173, 108, 276, 192, 241, 254, 108, 255, 173, 108, 277, 192, 241, 273, 108, 267, 242, 261, 173, 108, 278, 192, 241, 262, 108, 261, 173, 108, 279, 192, 241, 347, 108, 346, 173, 173, 32, 281, 192, 273, 74, 76, 241, 346, 108, 262, 173, 32, 282, 192, 267, 242, 261, 74, 76, 241, 346, 108, 261, 173, 32, 283, 192, 174, 241, 241, 263, 108, 261, 173, 108, 248, 82, 185, 82, 114, 173, 32, 135, 284, 156, 5, 241, 346, 108, 64, 241, 257, 108, 262, 242, 265, 173, 173, 62, 32, 285, 192, 55, 241, 246, 74, 256, 242, 257, 74, 281, 108, 286, 192, 348, 173, 32, 287, 192, 55, 241, 249, 74, 281, 108, 286, 192, 348, 173, 32, 288, 192, 285, 228, 62, 108, 200, 27, 77, 282, 228, 200, 108, 62, 27, 32, 289, 192, 55, 241, 274, 108, 286, 192, 349, 173, 32, 290, 192, 55, 241, 280, 108, 286, 192, 348, 173, 32, 291, 192, 241, 290, 4, 287, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 32, 292, 192, 241, 291, 4, 203, 241, 288, 108, 347, 108, 346, 173, 173, 82, 293, 241, 247, 82, 185, 82, 114, 173, 32, 283, 192, 15, 241, 289, 82, 294, 241, 173, 108, 292, 108, 283, 108, 295, 192, 248, 82, 185, 82, 114, 173, 32, 274, 192, 142, 241, 274, 108, 228, 262, 108, 346, 27, 173, 32, 280, 192, 142, 241, 280, 108, 228, 262, 108, 346, 27, 173, 32, 281, 170, 262, 32, 78, 32, 180, 265, 77, 347, 62, 32, 296, 192, 217, 241, 275, 192, 248, 108, 123, 192, 241, 260, 108, 258, 173, 108, 276, 192, 241, 252, 108, 253, 173, 108, 277, 192, 241, 269, 242, 263, 108, 267, 242, 261, 173, 108, 278, 192, 241, 263, 108, 261, 173, 108, 279, 192, 241, 346, 108, 347, 173, 173, 32, 180, 256, 124, 346, 62, 32, 10, 241, 296, 108, 55, 241, 296, 173, 74, 241, 283, 42, 259, 173, 82, 293, 241, 248, 82, 185, 82, 114, 173, 173, 32, 183, 32, 30, 62, 32, 10, 241, 296, 108, 241, 283, 42, 259, 173, 82, 293, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 183, 32, 30, 62, 32, 297, 192, 269, 242, 263, 74, 76, 241, 346, 108, 263, 173, 32, 298, 192, 267, 242, 261, 74, 76, 241, 346, 108, 261, 173, 32, 299, 192, 248, 74, 297, 228, 62, 108, 200, 27, 242, 252, 74, 298, 228, 200, 108, 62, 27, 242, 253, 32, 194, 241, 299, 108, 241, 283, 42, 259, 173, 82, 293, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 3, 32]}, {"code": "def linear_xent_fwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    stride_lse_N,\n    stride_lse_B,\n    stride_loss_Nb,\n    stride_loss_B,\n    reduction_ptr,\n    ignore_index: tl.constexpr,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)\n    idx_N, idx_V_group = tl.swizzle2d(\n        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE\n    )\n\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE)\n    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_range)\n\n    reduction = tl.load(reduction_ptr)\n    mask = y[:, None] == tl.where(V_range != ignore_index, V_range, -1)[None, :]\n    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / reduction\n\n    tl.store(\n        z_block_ptr, (z_j_to_k + tl.log(1 / reduction)).to(z_nv_ptr.type.element_ty)\n    )\n\n    m = tl.max(z_j_to_k, 1)\n    zero_lse_constant: tl.constexpr = tl.log(1 / tl.cdiv(V, V_BLOCK_SIZE))\n    lse = tl.where(\n        y != ignore_index,\n        tl.log(tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)) + m,\n        zero_lse_constant,\n    )\n\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_ptr,\n        shape=(N_group, V // 128),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),\n        block_shape=(N_BLOCK_SIZE, 1),\n        order=(1, 0),\n    )\n    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\n\n    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)\n    tl.store(lse_row_ptr, lse[:, None])", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 63, 6, 108, 263, 108, 264, 63, 6, 108, 265, 63, 6, 108, 266, 63, 6, 108, 267, 63, 6, 108, 268, 63, 6, 108, 269, 63, 6, 108, 270, 63, 6, 108, 271, 63, 6, 173, 63, 32, -1, 272, 192, 169, 241, 273, 192, 346, 173, 32, 274, 192, 169, 241, 273, 192, 347, 173, 32, 275, 108, 276, 192, 241, 138, 241, 346, 173, 108, 138, 241, 347, 173, 173, 32, 272, 108, 274, 192, 244, 241, 272, 108, 274, 108, 275, 108, 276, 108, 271, 173, 32, 187, 241, 268, 108, 269, 108, 270, 108, 271, 173, 32, 277, 63, 6, 192, 268, 32, 278, 192, 217, 241, 279, 192, 245, 108, 123, 192, 241, 266, 108, 267, 173, 108, 280, 192, 241, 251, 108, 252, 173, 108, 281, 192, 241, 263, 242, 264, 74, 272, 242, 269, 108, 346, 173, 108, 282, 192, 241, 269, 108, 270, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 284, 192, 217, 241, 279, 192, 247, 108, 123, 192, 241, 267, 108, 265, 173, 108, 280, 192, 241, 253, 108, 254, 173, 108, 281, 192, 241, 346, 108, 274, 242, 277, 173, 108, 282, 192, 241, 270, 108, 268, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 285, 192, 217, 241, 279, 192, 248, 108, 123, 192, 241, 264, 108, 265, 173, 108, 280, 192, 241, 255, 108, 256, 173, 108, 281, 192, 241, 272, 242, 269, 108, 274, 242, 277, 173, 108, 282, 192, 241, 269, 108, 268, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 286, 192, 174, 241, 241, 269, 108, 268, 173, 108, 91, 192, 144, 173, 32, 135, 287, 156, 5, 241, 267, 48, 270, 173, 63, 32, 288, 192, 57, 241, 278, 173, 32, 289, 192, 57, 241, 284, 173, 32, 286, 192, 15, 241, 288, 108, 289, 108, 286, 173, 32, 278, 192, 142, 241, 278, 108, 228, 346, 108, 270, 27, 173, 32, 284, 192, 142, 241, 284, 108, 228, 270, 108, 346, 27, 173, 32, 78, 32, 290, 192, 274, 242, 277, 74, 76, 241, 346, 108, 268, 173, 32, 291, 192, 263, 242, 264, 74, 272, 242, 269, 74, 76, 241, 346, 108, 269, 173, 32, 292, 192, 57, 241, 246, 74, 291, 173, 32, 293, 192, 57, 241, 261, 173, 32, 294, 192, 292, 228, 63, 108, 200, 27, 77, 203, 241, 290, 184, 262, 108, 290, 108, 4, 347, 173, 228, 200, 108, 63, 27, 32, 295, 192, 4, 219, 241, 203, 241, 294, 108, 286, 108, 296, 241, 346, 173, 173, 173, 42, 293, 32, 10, 241, 285, 108, 241, 286, 74, 50, 241, 347, 42, 293, 173, 173, 82, 297, 241, 248, 82, 185, 82, 114, 173, 173, 32, 298, 192, 12, 241, 286, 108, 347, 173, 32, 299, 63, 6, 192, 50, 241, 347, 42, 65, 241, 265, 108, 268, 173, 173, 32, 300, 192, 203, 241, 292, 184, 262, 108, 50, 241, 219, 241, 107, 241, 286, 4, 298, 228, 63, 108, 200, 27, 173, 108, 273, 192, 347, 173, 173, 74, 298, 108, 299, 173, 32, 301, 192, 217, 241, 279, 192, 250, 108, 123, 192, 241, 264, 108, 265, 48, 347, 348, 349, 173, 108, 280, 192, 241, 257, 108, 258, 173, 108, 281, 192, 241, 272, 242, 269, 108, 274, 173, 108, 282, 192, 241, 269, 108, 347, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 302, 192, 249, 74, 272, 242, 259, 74, 274, 242, 260, 32, 10, 241, 302, 108, 57, 241, 302, 173, 74, 295, 173, 32, 10, 241, 301, 108, 300, 228, 63, 108, 200, 27, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(\n    z_nv_ptr,\n    y_ptr,\n    A_t_ptr,\n    x_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    z_regularization: tl.constexpr,\n    fp32_grad_accumulators: tl.constexpr,\n    reduction_ptr,\n    ignore_index: tl.constexpr,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n    SPLIT_N: tl.constexpr,\n    SPLIT_V: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0) // SPLIT_V\n    idx_H = tl.program_id(axis=1)\n    idx_V_tile = tl.program_id(axis=0) % SPLIT_V\n\n    num_idx_N, num_idx_H = tl.num_programs(0) - (\n        triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N\n    ), tl.num_programs(1)\n    idx_N, idx_H = tl.swizzle2d(\n        idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE\n    )\n\n    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)\n\n    A_t_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(0, 1),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")\n    lse = tl.load(\n        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),\n        eviction_policy=\"evict_last\",\n    )\n    reduction = tl.load(reduction_ptr)\n\n    acc_dtype = tl.float32 if fp32_grad_accumulators else x_grad_ptr.type.element_ty\n    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), acc_dtype)\n    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):\n        mask = y[:, None] == V_range[None, :]\n        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")\n        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        if z_regularization > 0:\n            softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z\n        z_grad = softmax_z - tl.where(mask, 1 / reduction, 0.0)\n        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(\n            A_v.type.element_ty\n        )\n\n        x_grad_acc = tl.dot(valid_z_grad, A_v.trans(), x_grad_acc, out_dtype=acc_dtype)\n\n        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        V_range += V_BLOCK_SIZE\n\n    if SPLIT_V == 1:\n        x_grad_block_ptr = tl.make_block_ptr(\n            base=x_grad_ptr,\n            shape=(N, H),\n            strides=(stride_x_N, stride_x_H),\n            offsets=(\n                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,\n                idx_H * H_BLOCK_SIZE,\n            ),\n            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n            order=(1, 0),\n        )\n        tl.store(x_grad_block_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))\n    else:\n        row_n = (\n            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n        )\n        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n        x_grad_simple_ptr = (\n            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H\n        )\n        tl.atomic_add(x_grad_simple_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 62, 6, 108, 257, 62, 6, 108, 258, 108, 259, 62, 6, 108, 260, 108, 261, 62, 6, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 108, 266, 62, 6, 108, 267, 62, 6, 108, 268, 62, 6, 108, 269, 62, 6, 108, 270, 62, 6, 173, 62, 32, -1, 271, 192, 169, 241, 272, 192, 346, 173, 48, 270, 32, 273, 192, 169, 241, 272, 192, 347, 173, 32, 274, 192, 169, 241, 272, 192, 346, 173, 224, 270, 32, 275, 108, 276, 192, 241, 138, 241, 346, 173, 4, 20, 82, 277, 241, 262, 108, 265, 173, 242, 269, 108, 138, 241, 347, 173, 173, 32, 271, 108, 273, 192, 69, 241, 271, 108, 273, 108, 275, 48, 270, 108, 276, 108, 268, 173, 32, 278, 192, 274, 242, 64, 241, 262, 108, 270, 173, 32, 279, 192, 217, 241, 280, 192, 247, 108, 123, 192, 241, 264, 108, 262, 173, 108, 281, 192, 241, 252, 108, 253, 173, 108, 282, 192, 241, 273, 242, 267, 108, 278, 173, 108, 283, 192, 241, 267, 108, 265, 173, 108, 284, 192, 241, 346, 108, 347, 173, 173, 32, 285, 192, 217, 241, 280, 192, 245, 108, 123, 192, 241, 261, 108, 262, 173, 108, 281, 192, 241, 254, 108, 255, 173, 108, 282, 192, 241, 271, 242, 266, 108, 278, 173, 108, 283, 192, 241, 266, 108, 265, 173, 108, 284, 192, 241, 347, 108, 346, 173, 173, 32, 286, 192, 260, 242, 261, 74, 271, 242, 266, 74, 76, 241, 346, 108, 266, 173, 32, 287, 192, 278, 74, 76, 241, 346, 108, 265, 173, 32, 288, 192, 55, 241, 246, 74, 286, 108, 289, 192, 348, 173, 32, 290, 192, 55, 241, 249, 74, 271, 242, 266, 74, 76, 241, 346, 108, 266, 173, 108, 289, 192, 348, 173, 32, 291, 192, 55, 241, 258, 173, 32, 292, 192, 144, 180, 257, 30, 248, 82, 185, 82, 114, 32, 293, 192, 174, 241, 241, 266, 108, 267, 173, 108, 292, 173, 32, 135, 294, 156, 5, 241, 346, 108, 64, 241, 262, 108, 265, 242, 270, 173, 173, 62, 32, 295, 192, 288, 228, 62, 108, 200, 27, 77, 287, 228, 200, 108, 62, 27, 32, 296, 192, 55, 241, 279, 108, 289, 192, 349, 173, 32, 297, 192, 55, 241, 285, 108, 289, 192, 348, 173, 32, 298, 192, 241, 297, 4, 290, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 32, 180, 256, 124, 346, 62, 32, 298, 170, 350, 242, 256, 242, 290, 228, 62, 108, 200, 27, 242, 298, 32, 183, 32, 299, 192, 298, 4, 203, 241, 295, 108, 347, 42, 291, 108, 346, 173, 32, 300, 192, 203, 241, 241, 288, 77, 259, 173, 228, 62, 108, 200, 27, 108, 346, 108, 299, 173, 82, 301, 241, 296, 82, 185, 82, 114, 173, 32, 293, 192, 15, 241, 300, 108, 296, 82, 302, 241, 173, 108, 293, 108, 303, 192, 292, 173, 32, 279, 192, 142, 241, 279, 108, 228, 346, 108, 265, 27, 173, 32, 285, 192, 142, 241, 285, 108, 228, 346, 108, 265, 27, 173, 32, 287, 170, 265, 32, 78, 32, 180, 270, 77, 347, 62, 32, 304, 192, 217, 241, 280, 192, 248, 108, 123, 192, 241, 263, 108, 264, 173, 108, 281, 192, 241, 250, 108, 251, 173, 108, 282, 192, 241, 260, 242, 261, 74, 271, 242, 266, 108, 273, 242, 267, 173, 108, 283, 192, 241, 266, 108, 267, 173, 108, 284, 192, 241, 347, 108, 346, 173, 173, 32, 10, 241, 304, 108, 293, 82, 301, 241, 248, 82, 185, 82, 114, 173, 173, 32, 183, 32, 30, 62, 32, 305, 192, 260, 242, 261, 74, 271, 242, 266, 74, 76, 241, 346, 108, 266, 173, 32, 306, 192, 273, 242, 267, 74, 76, 241, 346, 108, 267, 173, 32, 307, 192, 248, 74, 305, 228, 62, 108, 200, 27, 242, 250, 74, 306, 228, 200, 108, 62, 27, 242, 251, 32, 194, 241, 307, 108, 293, 82, 301, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(\n    z_nv_ptr,\n    y_ptr,\n    x_ptr,\n    A_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    z_regularization: tl.constexpr,\n    fp32_grad_accumulators: tl.constexpr,\n    reduction_ptr,\n    ignore_index: tl.constexpr,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n    SPLIT_N: tl.constexpr,\n    SPLIT_V: tl.constexpr,\n):\n    idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N\n    idx_H = tl.program_id(axis=1)\n    idx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N\n\n    num_idx_V, num_idx_H = tl.num_programs(0) - (\n        N_group // N_BLOCK_SIZE * SPLIT_V\n    ), tl.num_programs(1)\n    idx_V, idx_H = tl.swizzle2d(\n        idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE\n    )\n\n    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    reduction = tl.load(reduction_ptr)\n\n    acc_dtype = tl.float32 if fp32_grad_accumulators else A_grad_ptr.type.element_ty\n    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), acc_dtype)\n    for _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):\n        y = tl.load(\n            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"\n        )\n        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")\n        mask = y[:, None] == V_range[None, :]\n\n        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")\n        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        if z_regularization > 0:\n            softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z\n        z_grad = softmax_z - tl.where(mask, 1 / reduction, 0)\n        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(\n            x_ptr.type.element_ty\n        )\n\n        A_grad_acc = tl.dot(\n            x_chunk.trans(), valid_z_grad, A_grad_acc, out_dtype=acc_dtype\n        )\n\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n        N_range += N_BLOCK_SIZE\n\n    if SPLIT_N == 1:\n        A_grad_T_block_ptr = tl.make_block_ptr(\n            base=A_grad_ptr,\n            shape=(H, V),\n            strides=(stride_A_H, stride_A_V),\n            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n            order=(0, 1),\n        )\n        if idx_N_group > 0:\n            tl.store(\n                A_grad_T_block_ptr,\n                tl.load(A_grad_T_block_ptr) + A_grad_acc.to(A_grad_ptr.type.element_ty),\n            )\n        else:\n            tl.store(A_grad_T_block_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))\n    else:\n        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n        A_grad_T_simple_ptr = (\n            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V\n        )\n        tl.atomic_add(A_grad_T_simple_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 63, 6, 108, 257, 63, 6, 108, 258, 108, 259, 63, 6, 108, 260, 108, 261, 63, 6, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 108, 265, 63, 6, 108, 266, 63, 6, 108, 267, 63, 6, 108, 268, 63, 6, 108, 269, 63, 6, 108, 270, 63, 6, 173, 63, 32, -1, 271, 192, 241, 169, 241, 272, 192, 346, 173, 4, 261, 48, 266, 242, 270, 173, 48, 269, 32, 273, 192, 169, 241, 272, 192, 347, 173, 32, 274, 192, 241, 169, 241, 272, 192, 346, 173, 4, 261, 48, 266, 242, 270, 173, 224, 269, 32, 275, 108, 276, 192, 241, 138, 241, 346, 173, 4, 261, 48, 266, 242, 270, 108, 138, 241, 347, 173, 173, 32, 271, 108, 273, 192, 244, 241, 271, 108, 273, 108, 275, 48, 269, 108, 276, 108, 268, 173, 32, 277, 192, 274, 242, 65, 241, 261, 108, 269, 173, 32, 278, 192, 217, 241, 279, 192, 247, 108, 123, 192, 241, 263, 108, 264, 173, 108, 280, 192, 241, 250, 108, 251, 173, 108, 281, 192, 241, 260, 242, 261, 74, 277, 108, 273, 242, 267, 173, 108, 282, 192, 241, 266, 108, 267, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 284, 192, 217, 241, 279, 192, 245, 108, 123, 192, 241, 261, 108, 262, 173, 108, 280, 192, 241, 254, 108, 255, 173, 108, 281, 192, 241, 277, 108, 271, 242, 265, 173, 108, 282, 192, 241, 266, 108, 265, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 285, 192, 277, 74, 76, 241, 346, 108, 266, 173, 32, 286, 192, 271, 242, 265, 74, 76, 241, 346, 108, 265, 173, 32, 287, 192, 57, 241, 258, 173, 32, 288, 192, 144, 180, 257, 30, 248, 82, 185, 82, 114, 32, 289, 192, 174, 241, 241, 267, 108, 265, 173, 108, 288, 173, 32, 135, 290, 156, 5, 241, 346, 108, 65, 241, 261, 108, 266, 242, 269, 173, 173, 63, 32, 291, 192, 57, 241, 246, 74, 260, 242, 261, 74, 285, 108, 292, 192, 348, 173, 32, 293, 192, 57, 241, 249, 74, 285, 108, 292, 192, 348, 173, 32, 294, 192, 291, 228, 63, 108, 200, 27, 77, 286, 228, 200, 108, 63, 27, 32, 295, 192, 57, 241, 278, 108, 292, 192, 349, 173, 32, 296, 192, 57, 241, 284, 108, 292, 192, 348, 173, 32, 297, 192, 241, 296, 4, 293, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 180, 256, 124, 346, 63, 32, 297, 170, 350, 242, 256, 242, 293, 228, 63, 108, 200, 27, 242, 297, 32, 183, 32, 298, 192, 297, 4, 203, 241, 294, 108, 347, 42, 287, 108, 346, 173, 32, 299, 192, 203, 241, 241, 291, 77, 259, 173, 228, 63, 108, 200, 27, 108, 346, 108, 298, 173, 82, 300, 241, 247, 82, 185, 82, 114, 173, 32, 289, 192, 15, 241, 295, 82, 301, 241, 173, 108, 299, 108, 289, 108, 302, 192, 288, 173, 32, 278, 192, 142, 241, 278, 108, 228, 266, 108, 346, 27, 173, 32, 284, 192, 142, 241, 284, 108, 228, 266, 108, 346, 27, 173, 32, 285, 170, 266, 32, 78, 32, 180, 269, 77, 347, 63, 32, 303, 192, 217, 241, 279, 192, 248, 108, 123, 192, 241, 264, 108, 262, 173, 108, 280, 192, 241, 252, 108, 253, 173, 108, 281, 192, 241, 273, 242, 267, 108, 271, 242, 265, 173, 108, 282, 192, 241, 267, 108, 265, 173, 108, 283, 192, 241, 346, 108, 347, 173, 173, 32, 180, 260, 124, 346, 63, 32, 10, 241, 303, 108, 57, 241, 303, 173, 74, 289, 82, 300, 241, 248, 82, 185, 82, 114, 173, 173, 32, 183, 32, 30, 63, 32, 10, 241, 303, 108, 289, 82, 300, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 183, 32, 30, 63, 32, 304, 192, 273, 242, 267, 74, 76, 241, 346, 108, 267, 173, 32, 305, 192, 271, 242, 265, 74, 76, 241, 346, 108, 265, 173, 32, 306, 192, 248, 74, 304, 228, 63, 108, 200, 27, 242, 252, 74, 305, 228, 200, 108, 63, 27, 242, 253, 32, 194, 241, 306, 108, 289, 82, 300, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 3, 32]}, {"code": "def logsumexp_reduction_kernel(\n    lse_local_ptr,\n    lse_global_ptr,\n    lse_sum_ptr,\n    reduction_ptr,\n    z_regularization: tl.constexpr,\n    stride_lse_N,\n    stride_lse_B,\n    N_group,\n    V: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr = 32,\n):\n    idx_N = tl.program_id(axis=0)\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE)\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_local_ptr,\n        shape=(N_group, V // 128),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, V // V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    lse_local = tl.load(lse_row_ptr)\n    m = tl.max(lse_local, 1)\n    lse = tl.log(tl.sum(tl.exp((lse_local - m[:, None])), axis=1)) + m\n\n    lse_reduction = (tl.sum(lse) + z_regularization * tl.sum(lse * lse)) / tl.load(\n        reduction_ptr\n    )\n\n    tl.atomic_add(lse_sum_ptr, lse_reduction)\n    tl.store(lse_global_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE), lse)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 62, 6, 108, 250, 108, 251, 108, 252, 108, 253, 62, 6, 108, 254, 62, 6, 108, 255, 62, 6, 192, 346, 347, 173, 62, 32, -1, 256, 192, 169, 241, 257, 192, 348, 173, 32, 187, 241, 254, 108, 255, 173, 32, 258, 192, 217, 241, 259, 192, 245, 108, 123, 192, 241, 252, 108, 253, 48, 349, 347, 350, 173, 108, 260, 192, 241, 250, 108, 251, 173, 108, 261, 192, 241, 256, 242, 255, 108, 348, 173, 108, 262, 192, 241, 255, 108, 253, 48, 254, 173, 108, 263, 192, 241, 349, 108, 348, 173, 173, 32, 264, 192, 55, 241, 258, 173, 32, 265, 192, 12, 241, 264, 108, 349, 173, 32, 266, 192, 50, 241, 219, 241, 107, 241, 264, 4, 265, 228, 62, 108, 200, 27, 173, 108, 257, 192, 349, 173, 173, 74, 265, 32, 267, 192, 241, 219, 241, 266, 173, 74, 249, 242, 219, 241, 266, 242, 266, 173, 173, 42, 55, 241, 248, 173, 32, 194, 241, 247, 108, 267, 173, 32, 10, 241, 246, 74, 256, 242, 255, 74, 76, 241, 348, 108, 255, 173, 108, 266, 173, 32, 3, 32]}, {"code": "def linear_xent_fwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    stride_lse_N,\n    stride_lse_B,\n    stride_loss_Nb,\n    stride_loss_B,\n    reduction_ptr,\n    ignore_index: tl.constexpr,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)\n    idx_N, idx_V_group = tl.swizzle2d(\n        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE\n    )\n\n    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n    y = tl.load(\n        y_ptr\n        + idx_N_group * N_group\n        + idx_N * N_BLOCK_SIZE\n        + tl.arange(0, N_BLOCK_SIZE)\n    )\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    mask = y[:, None] == tl.where(V_range != ignore_index, V_range, -1)[None, :]\n    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n    tl.store(z_block_ptr, (z_j_to_k + tl.log(1 / N)).to(z_nv_ptr.type.element_ty))\n\n    m = tl.max(z_j_to_k, 1)\n    zero_lse_constant: tl.constexpr = tl.log(1 / tl.cdiv(V, V_BLOCK_SIZE))\n    lse = tl.where(\n        y != ignore_index,\n        tl.log(tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)) + m,\n        zero_lse_constant,\n    )\n\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_ptr,\n        shape=(N_group, V // 128),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),\n        block_shape=(N_BLOCK_SIZE, 1),\n        order=(1, 0),\n    )\n    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\n\n    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)\n    tl.store(lse_row_ptr, lse[:, None])", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 63, 6, 108, 263, 108, 264, 63, 6, 108, 265, 63, 6, 108, 266, 63, 6, 108, 267, 63, 6, 108, 268, 63, 6, 108, 269, 63, 6, 108, 270, 63, 6, 108, 271, 63, 6, 173, 63, 32, -1, 272, 192, 169, 241, 273, 192, 346, 173, 32, 274, 192, 169, 241, 273, 192, 347, 173, 32, 275, 108, 276, 192, 241, 138, 241, 346, 173, 108, 138, 241, 347, 173, 173, 32, 272, 108, 274, 192, 244, 241, 272, 108, 274, 108, 275, 108, 276, 108, 271, 173, 32, 277, 63, 6, 192, 268, 32, 278, 192, 217, 241, 279, 192, 245, 108, 123, 192, 241, 266, 108, 267, 173, 108, 280, 192, 241, 251, 108, 252, 173, 108, 281, 192, 241, 263, 242, 264, 74, 272, 242, 269, 108, 346, 173, 108, 282, 192, 241, 269, 108, 270, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 284, 192, 217, 241, 279, 192, 247, 108, 123, 192, 241, 267, 108, 265, 173, 108, 280, 192, 241, 253, 108, 254, 173, 108, 281, 192, 241, 346, 108, 274, 242, 277, 173, 108, 282, 192, 241, 270, 108, 268, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 285, 192, 217, 241, 279, 192, 248, 108, 123, 192, 241, 264, 108, 265, 173, 108, 280, 192, 241, 255, 108, 256, 173, 108, 281, 192, 241, 272, 242, 269, 108, 274, 242, 277, 173, 108, 282, 192, 241, 269, 108, 268, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 286, 192, 174, 241, 241, 269, 108, 268, 173, 108, 91, 192, 144, 173, 32, 135, 287, 156, 5, 241, 267, 48, 270, 173, 63, 32, 288, 192, 57, 241, 278, 173, 32, 289, 192, 57, 241, 284, 173, 32, 286, 192, 15, 241, 288, 108, 289, 108, 286, 173, 32, 278, 192, 142, 241, 278, 108, 228, 346, 108, 270, 27, 173, 32, 284, 192, 142, 241, 284, 108, 228, 270, 108, 346, 27, 173, 32, 78, 32, 290, 192, 57, 241, 246, 74, 263, 242, 264, 74, 272, 242, 269, 74, 76, 241, 346, 108, 269, 173, 173, 32, 291, 192, 274, 242, 277, 74, 76, 241, 346, 108, 268, 173, 32, 292, 192, 290, 228, 63, 108, 200, 27, 77, 203, 241, 291, 184, 262, 108, 291, 108, 4, 347, 173, 228, 200, 108, 63, 27, 32, 293, 192, 4, 219, 241, 203, 241, 292, 108, 286, 108, 294, 241, 346, 173, 173, 173, 42, 266, 32, 10, 241, 285, 108, 241, 286, 74, 50, 241, 347, 42, 266, 173, 173, 82, 295, 241, 248, 82, 185, 82, 114, 173, 173, 32, 296, 192, 12, 241, 286, 108, 347, 173, 32, 297, 63, 6, 192, 50, 241, 347, 42, 65, 241, 265, 108, 268, 173, 173, 32, 298, 192, 203, 241, 290, 184, 262, 108, 50, 241, 219, 241, 107, 241, 286, 4, 296, 228, 63, 108, 200, 27, 173, 108, 273, 192, 347, 173, 173, 74, 296, 108, 297, 173, 32, 299, 192, 217, 241, 279, 192, 250, 108, 123, 192, 241, 264, 108, 265, 48, 347, 348, 349, 173, 108, 280, 192, 241, 257, 108, 258, 173, 108, 281, 192, 241, 272, 242, 269, 108, 274, 173, 108, 282, 192, 241, 269, 108, 347, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 300, 192, 249, 74, 272, 242, 259, 74, 274, 242, 260, 32, 10, 241, 300, 108, 57, 241, 300, 173, 74, 293, 173, 32, 10, 241, 299, 108, 298, 228, 63, 108, 200, 27, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(\n    z_nv_ptr,\n    y_ptr,\n    A_t_ptr,\n    x_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    z_regularization: tl.constexpr,\n    fp32_grad_accumulators: tl.constexpr,\n    reduction_ptr,\n    ignore_index: tl.constexpr,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n    SPLIT_N: tl.constexpr,\n    SPLIT_V: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0) // SPLIT_V\n    idx_H = tl.program_id(axis=1)\n    idx_V_tile = tl.program_id(axis=0) % SPLIT_V\n\n    num_idx_N, num_idx_H = tl.num_programs(0) - (\n        triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N\n    ), tl.num_programs(1)\n    idx_N, idx_H = tl.swizzle2d(\n        idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE\n    )\n\n    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)\n\n    A_t_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(0, 1),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")\n    lse = tl.load(\n        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),\n        eviction_policy=\"evict_last\",\n    )\n\n    acc_dtype = tl.float32 if fp32_grad_accumulators else x_grad_ptr.type.element_ty\n    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), acc_dtype)\n    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):\n        mask = y[:, None] == V_range[None, :]\n        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")\n        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n\n        z_grad = softmax_z - tl.where(mask, 1 / N, 0.0)\n        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(\n            A_v.type.element_ty\n        )\n\n        x_grad_acc = tl.dot(valid_z_grad, A_v.trans(), x_grad_acc, out_dtype=acc_dtype)\n\n        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        V_range += V_BLOCK_SIZE\n\n    if SPLIT_V == 1:\n        x_grad_block_ptr = tl.make_block_ptr(\n            base=x_grad_ptr,\n            shape=(N, H),\n            strides=(stride_x_N, stride_x_H),\n            offsets=(\n                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,\n                idx_H * H_BLOCK_SIZE,\n            ),\n            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n            order=(1, 0),\n        )\n        tl.store(x_grad_block_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))\n    else:\n        row_n = (\n            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n        )\n        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n        x_grad_simple_ptr = (\n            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H\n        )\n        tl.atomic_add(x_grad_simple_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 62, 6, 108, 257, 62, 6, 108, 258, 108, 259, 62, 6, 108, 260, 108, 261, 62, 6, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 108, 266, 62, 6, 108, 267, 62, 6, 108, 268, 62, 6, 108, 269, 62, 6, 108, 270, 62, 6, 173, 62, 32, -1, 271, 192, 169, 241, 272, 192, 346, 173, 48, 270, 32, 273, 192, 169, 241, 272, 192, 347, 173, 32, 274, 192, 169, 241, 272, 192, 346, 173, 224, 270, 32, 275, 108, 276, 192, 241, 138, 241, 346, 173, 4, 20, 82, 277, 241, 262, 108, 265, 173, 242, 269, 108, 138, 241, 347, 173, 173, 32, 271, 108, 273, 192, 69, 241, 271, 108, 273, 108, 275, 48, 270, 108, 276, 108, 268, 173, 32, 278, 192, 274, 242, 64, 241, 262, 108, 270, 173, 32, 279, 192, 217, 241, 280, 192, 247, 108, 123, 192, 241, 264, 108, 262, 173, 108, 281, 192, 241, 252, 108, 253, 173, 108, 282, 192, 241, 273, 242, 267, 108, 278, 173, 108, 283, 192, 241, 267, 108, 265, 173, 108, 284, 192, 241, 346, 108, 347, 173, 173, 32, 285, 192, 217, 241, 280, 192, 245, 108, 123, 192, 241, 261, 108, 262, 173, 108, 281, 192, 241, 254, 108, 255, 173, 108, 282, 192, 241, 271, 242, 266, 108, 278, 173, 108, 283, 192, 241, 266, 108, 265, 173, 108, 284, 192, 241, 347, 108, 346, 173, 173, 32, 286, 192, 260, 242, 261, 74, 271, 242, 266, 74, 76, 241, 346, 108, 266, 173, 32, 287, 192, 278, 74, 76, 241, 346, 108, 265, 173, 32, 288, 192, 55, 241, 246, 74, 286, 108, 289, 192, 348, 173, 32, 290, 192, 55, 241, 249, 74, 271, 242, 266, 74, 76, 241, 346, 108, 266, 173, 108, 289, 192, 348, 173, 32, 291, 192, 144, 180, 257, 30, 248, 82, 185, 82, 114, 32, 292, 192, 174, 241, 241, 266, 108, 267, 173, 108, 291, 173, 32, 135, 293, 156, 5, 241, 346, 108, 64, 241, 262, 108, 265, 242, 270, 173, 173, 62, 32, 294, 192, 288, 228, 62, 108, 200, 27, 77, 287, 228, 200, 108, 62, 27, 32, 295, 192, 55, 241, 279, 108, 289, 192, 349, 173, 32, 296, 192, 55, 241, 285, 108, 289, 192, 348, 173, 32, 297, 192, 241, 296, 4, 290, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 32, 298, 192, 297, 4, 203, 241, 294, 108, 347, 42, 263, 108, 346, 173, 32, 299, 192, 203, 241, 241, 288, 77, 259, 173, 228, 62, 108, 200, 27, 108, 346, 108, 298, 173, 82, 300, 241, 295, 82, 185, 82, 114, 173, 32, 292, 192, 15, 241, 299, 108, 295, 82, 301, 241, 173, 108, 292, 108, 302, 192, 291, 173, 32, 279, 192, 142, 241, 279, 108, 228, 346, 108, 265, 27, 173, 32, 285, 192, 142, 241, 285, 108, 228, 346, 108, 265, 27, 173, 32, 287, 170, 265, 32, 78, 32, 180, 270, 77, 347, 62, 32, 303, 192, 217, 241, 280, 192, 248, 108, 123, 192, 241, 263, 108, 264, 173, 108, 281, 192, 241, 250, 108, 251, 173, 108, 282, 192, 241, 260, 242, 261, 74, 271, 242, 266, 108, 273, 242, 267, 173, 108, 283, 192, 241, 266, 108, 267, 173, 108, 284, 192, 241, 347, 108, 346, 173, 173, 32, 10, 241, 303, 108, 292, 82, 300, 241, 248, 82, 185, 82, 114, 173, 173, 32, 183, 32, 30, 62, 32, 304, 192, 260, 242, 261, 74, 271, 242, 266, 74, 76, 241, 346, 108, 266, 173, 32, 305, 192, 273, 242, 267, 74, 76, 241, 346, 108, 267, 173, 32, 306, 192, 248, 74, 304, 228, 62, 108, 200, 27, 242, 250, 74, 305, 228, 200, 108, 62, 27, 242, 251, 32, 194, 241, 306, 108, 292, 82, 300, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(\n    z_nv_ptr,\n    y_ptr,\n    x_ptr,\n    A_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    z_regularization: tl.constexpr,\n    fp32_grad_accumulators: tl.constexpr,\n    reduction_ptr,\n    ignore_index: tl.constexpr,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    GROUP_SIZE: tl.constexpr,\n    SPLIT_N: tl.constexpr,\n    SPLIT_V: tl.constexpr,\n):\n    idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N\n    idx_H = tl.program_id(axis=1)\n    idx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N\n\n    num_idx_V, num_idx_H = tl.num_programs(0) - (\n        N_group // N_BLOCK_SIZE * SPLIT_V\n    ), tl.num_programs(1)\n    idx_V, idx_H = tl.swizzle2d(\n        idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE\n    )\n\n    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    acc_dtype = tl.float32 if fp32_grad_accumulators else A_grad_ptr.type.element_ty\n    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), acc_dtype)\n    for _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):\n        y = tl.load(\n            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"\n        )\n        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")\n        mask = y[:, None] == V_range[None, :]\n\n        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")\n        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n\n        z_grad = softmax_z - tl.where(mask, 1 / N, 0)\n        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(\n            x_ptr.type.element_ty\n        )\n\n        A_grad_acc = tl.dot(\n            x_chunk.trans(), valid_z_grad, A_grad_acc, out_dtype=acc_dtype\n        )\n\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n        N_range += N_BLOCK_SIZE\n\n    if SPLIT_N == 1:\n        A_grad_T_block_ptr = tl.make_block_ptr(\n            base=A_grad_ptr,\n            shape=(H, V),\n            strides=(stride_A_H, stride_A_V),\n            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n            order=(0, 1),\n        )\n        if idx_N_group > 0:\n            tl.store(\n                A_grad_T_block_ptr,\n                tl.load(A_grad_T_block_ptr) + A_grad_acc.to(A_grad_ptr.type.element_ty),\n            )\n        else:\n            tl.store(A_grad_T_block_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))\n    else:\n        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n        A_grad_T_simple_ptr = (\n            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V\n        )\n        tl.atomic_add(A_grad_T_simple_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 63, 6, 108, 257, 63, 6, 108, 258, 108, 259, 63, 6, 108, 260, 108, 261, 63, 6, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 108, 265, 63, 6, 108, 266, 63, 6, 108, 267, 63, 6, 108, 268, 63, 6, 108, 269, 63, 6, 108, 270, 63, 6, 173, 63, 32, -1, 271, 192, 241, 169, 241, 272, 192, 346, 173, 4, 261, 48, 266, 242, 270, 173, 48, 269, 32, 273, 192, 169, 241, 272, 192, 347, 173, 32, 274, 192, 241, 169, 241, 272, 192, 346, 173, 4, 261, 48, 266, 242, 270, 173, 224, 269, 32, 275, 108, 276, 192, 241, 138, 241, 346, 173, 4, 261, 48, 266, 242, 270, 108, 138, 241, 347, 173, 173, 32, 271, 108, 273, 192, 244, 241, 271, 108, 273, 108, 275, 48, 269, 108, 276, 108, 268, 173, 32, 277, 192, 274, 242, 65, 241, 261, 108, 269, 173, 32, 278, 192, 217, 241, 279, 192, 247, 108, 123, 192, 241, 263, 108, 264, 173, 108, 280, 192, 241, 250, 108, 251, 173, 108, 281, 192, 241, 260, 242, 261, 74, 277, 108, 273, 242, 267, 173, 108, 282, 192, 241, 266, 108, 267, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 284, 192, 217, 241, 279, 192, 245, 108, 123, 192, 241, 261, 108, 262, 173, 108, 280, 192, 241, 254, 108, 255, 173, 108, 281, 192, 241, 277, 108, 271, 242, 265, 173, 108, 282, 192, 241, 266, 108, 265, 173, 108, 283, 192, 241, 347, 108, 346, 173, 173, 32, 285, 192, 277, 74, 76, 241, 346, 108, 266, 173, 32, 286, 192, 271, 242, 265, 74, 76, 241, 346, 108, 265, 173, 32, 287, 192, 144, 180, 257, 30, 248, 82, 185, 82, 114, 32, 288, 192, 174, 241, 241, 267, 108, 265, 173, 108, 287, 173, 32, 135, 289, 156, 5, 241, 346, 108, 65, 241, 261, 108, 266, 242, 269, 173, 173, 63, 32, 290, 192, 57, 241, 246, 74, 260, 242, 261, 74, 285, 108, 291, 192, 348, 173, 32, 292, 192, 57, 241, 249, 74, 285, 108, 291, 192, 348, 173, 32, 293, 192, 290, 228, 63, 108, 200, 27, 77, 286, 228, 200, 108, 63, 27, 32, 294, 192, 57, 241, 278, 108, 291, 192, 349, 173, 32, 295, 192, 57, 241, 284, 108, 291, 192, 348, 173, 32, 296, 192, 241, 295, 4, 292, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 297, 192, 296, 4, 203, 241, 293, 108, 347, 42, 263, 108, 346, 173, 32, 298, 192, 203, 241, 241, 290, 77, 259, 173, 228, 63, 108, 200, 27, 108, 346, 108, 297, 173, 82, 299, 241, 247, 82, 185, 82, 114, 173, 32, 288, 192, 15, 241, 294, 82, 300, 241, 173, 108, 298, 108, 288, 108, 301, 192, 287, 173, 32, 278, 192, 142, 241, 278, 108, 228, 266, 108, 346, 27, 173, 32, 284, 192, 142, 241, 284, 108, 228, 266, 108, 346, 27, 173, 32, 285, 170, 266, 32, 78, 32, 180, 269, 77, 347, 63, 32, 302, 192, 217, 241, 279, 192, 248, 108, 123, 192, 241, 264, 108, 262, 173, 108, 280, 192, 241, 252, 108, 253, 173, 108, 281, 192, 241, 273, 242, 267, 108, 271, 242, 265, 173, 108, 282, 192, 241, 267, 108, 265, 173, 108, 283, 192, 241, 346, 108, 347, 173, 173, 32, 180, 260, 124, 346, 63, 32, 10, 241, 302, 108, 57, 241, 302, 173, 74, 288, 82, 299, 241, 248, 82, 185, 82, 114, 173, 173, 32, 183, 32, 30, 63, 32, 10, 241, 302, 108, 288, 82, 299, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 183, 32, 30, 63, 32, 303, 192, 273, 242, 267, 74, 76, 241, 346, 108, 267, 173, 32, 304, 192, 271, 242, 265, 74, 76, 241, 346, 108, 265, 173, 32, 305, 192, 248, 74, 303, 228, 63, 108, 200, 27, 242, 252, 74, 304, 228, 200, 108, 63, 27, 242, 253, 32, 194, 241, 305, 108, 288, 82, 299, 241, 248, 82, 185, 82, 114, 173, 173, 32, 56, 32, 3, 32]}, {"code": "def linear_xent_fwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    loss_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n):\n    idx = tl.program_id(axis=0)\n\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + offsets)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V // V_BLOCK_SIZE):\n\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        local_x_block_ptr = x_block_ptr\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(local_x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == v_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        m = m_new\n        A_block_ptr = tl.advance(\n            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]\n        )\n        v_range = v_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n    loss += tl.sum(lse) / N\n    tl.atomic_add(loss_ptr, loss)\n    tl.store(lse_ptr + offsets, lse)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 62, 6, 108, 255, 62, 6, 108, 256, 62, 6, 108, 257, 62, 6, 108, 258, 62, 6, 108, 259, 62, 6, 173, 62, 32, -1, 260, 192, 169, 241, 261, 192, 346, 173, 32, 83, 241, 255, 224, 258, 77, 346, 173, 32, 83, 241, 254, 224, 257, 77, 346, 173, 32, 83, 241, 256, 224, 259, 77, 346, 173, 32, 262, 192, 217, 241, 263, 192, 245, 108, 123, 192, 241, 255, 108, 256, 173, 108, 264, 192, 241, 250, 108, 251, 173, 108, 265, 192, 241, 260, 242, 258, 108, 346, 173, 108, 266, 192, 241, 258, 108, 259, 173, 108, 267, 192, 241, 347, 108, 346, 173, 173, 32, 268, 192, 217, 241, 263, 192, 247, 108, 123, 192, 241, 256, 108, 254, 173, 108, 264, 192, 241, 252, 108, 253, 173, 108, 265, 192, 241, 346, 108, 346, 173, 108, 266, 192, 241, 259, 108, 257, 173, 108, 267, 192, 241, 347, 108, 346, 173, 173, 32, 265, 192, 260, 242, 258, 74, 76, 241, 346, 108, 258, 173, 32, 269, 192, 76, 241, 346, 108, 257, 173, 32, 270, 192, 55, 241, 246, 74, 265, 173, 32, 271, 192, 174, 241, 241, 258, 108, 173, 108, 91, 192, 144, 173, 4, 272, 241, 348, 173, 32, 273, 192, 174, 241, 241, 258, 108, 173, 108, 91, 192, 144, 173, 32, 274, 192, 346, 32, 135, 275, 156, 5, 241, 254, 48, 257, 173, 62, 32, 276, 192, 174, 241, 241, 258, 108, 257, 173, 108, 91, 192, 144, 173, 32, 277, 192, 262, 32, 135, 275, 156, 5, 241, 256, 48, 259, 173, 62, 32, 278, 192, 55, 241, 277, 173, 32, 279, 192, 55, 241, 268, 173, 32, 276, 192, 15, 241, 278, 108, 279, 108, 276, 173, 32, 277, 192, 142, 241, 277, 108, 228, 346, 108, 259, 27, 173, 32, 268, 192, 142, 241, 268, 108, 228, 259, 108, 346, 27, 173, 32, 78, 32, 280, 192, 190, 241, 271, 108, 12, 241, 276, 108, 347, 173, 173, 32, 281, 192, 219, 241, 107, 241, 276, 4, 280, 228, 62, 108, 200, 27, 173, 108, 261, 192, 347, 173, 32, 273, 192, 273, 242, 107, 241, 271, 4, 280, 173, 74, 281, 32, 282, 192, 270, 228, 62, 108, 200, 27, 77, 269, 228, 200, 108, 62, 27, 32, 274, 2, 219, 241, 203, 241, 282, 108, 276, 108, 272, 241, 346, 173, 173, 173, 42, 255, 32, 271, 192, 280, 32, 268, 192, 142, 241, 268, 108, 228, 4, 259, 242, 241, 256, 48, 259, 173, 108, 257, 27, 173, 32, 269, 192, 269, 74, 257, 32, 78, 32, 283, 192, 271, 74, 50, 241, 273, 173, 32, 274, 170, 219, 241, 283, 173, 42, 255, 32, 194, 241, 248, 108, 274, 173, 32, 10, 241, 249, 74, 265, 108, 283, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_prologue(\n    sz_ptr,\n    x_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_sz_N,\n    stride_sz_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1)\n\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n\n    offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    lse = tl.load(lse_global_ptr + offsets)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    sz_block_ptr = tl.make_block_ptr(\n        base=sz_ptr,\n        shape=(N, V),\n        strides=(stride_sz_N, stride_sz_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    tl.store(sz_block_ptr, softmax_z.to(tl.float16))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 63, 6, 108, 256, 63, 6, 108, 257, 63, 6, 108, 258, 63, 6, 192, 346, 347, 108, 259, 63, 6, 192, 346, 347, 108, 260, 63, 6, 192, 346, 347, 173, 63, 32, -1, 261, 192, 169, 241, 262, 192, 348, 173, 32, 263, 192, 169, 241, 262, 192, 346, 173, 32, 83, 241, 256, 224, 259, 77, 348, 173, 32, 83, 241, 255, 224, 258, 77, 348, 173, 32, 83, 241, 257, 224, 260, 77, 348, 173, 32, 264, 192, 261, 242, 259, 74, 76, 241, 348, 108, 259, 173, 32, 265, 192, 57, 241, 248, 74, 264, 173, 32, 266, 192, 217, 241, 267, 192, 246, 108, 123, 192, 241, 256, 108, 257, 173, 108, 268, 192, 241, 249, 108, 250, 173, 108, 264, 192, 241, 261, 242, 259, 108, 348, 173, 108, 269, 192, 241, 259, 108, 260, 173, 108, 270, 192, 241, 346, 108, 348, 173, 173, 32, 271, 192, 217, 241, 267, 192, 247, 108, 123, 192, 241, 257, 108, 255, 173, 108, 268, 192, 241, 251, 108, 252, 173, 108, 264, 192, 241, 348, 108, 263, 242, 258, 173, 108, 269, 192, 241, 260, 108, 258, 173, 108, 270, 192, 241, 346, 108, 348, 173, 173, 32, 272, 192, 217, 241, 267, 192, 245, 108, 123, 192, 241, 256, 108, 255, 173, 108, 268, 192, 241, 253, 108, 254, 173, 108, 264, 192, 241, 261, 242, 259, 108, 263, 242, 258, 173, 108, 269, 192, 241, 259, 108, 258, 173, 108, 270, 192, 241, 346, 108, 348, 173, 173, 32, 273, 192, 174, 241, 241, 259, 108, 258, 173, 108, 91, 192, 144, 173, 32, 135, 274, 156, 5, 241, 257, 48, 260, 173, 63, 32, 275, 192, 57, 241, 266, 173, 32, 276, 192, 57, 241, 271, 173, 32, 273, 192, 15, 241, 275, 108, 276, 108, 273, 173, 32, 266, 192, 142, 241, 266, 108, 228, 348, 108, 260, 27, 173, 32, 271, 192, 142, 241, 271, 108, 228, 260, 108, 348, 27, 173, 32, 78, 32, 277, 192, 241, 273, 4, 265, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 10, 241, 272, 108, 277, 82, 278, 241, 21, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(\n    sz_ptr,\n    y_ptr,\n    A_t_ptr,\n    x_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_sz_N,\n    stride_sz_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_H = tl.program_id(axis=0)\n    idx_N = tl.program_id(axis=1)\n\n    x_grad_block_ptr = tl.make_block_ptr(\n        base=x_grad_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_t_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    sz_block_ptr = tl.make_block_ptr(\n        base=sz_ptr,\n        shape=(N, V),\n        strides=(stride_sz_N, stride_sz_V),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_offsets)\n\n    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)\n    for idx_V in range(V // V_BLOCK_SIZE):\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n        A_v = tl.load(A_t_block_ptr).trans()\n        sz = tl.load(sz_block_ptr)\n\n        x_grad_acc = tl.dot(sz, A_v, x_grad_acc)\n        x_grad_acc -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1)\n\n        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n        sz_block_ptr = tl.advance(sz_block_ptr, [0, V_BLOCK_SIZE])\n        V_offsets += V_BLOCK_SIZE\n\n    tl.store(x_grad_block_ptr, x_grad_acc / N)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 62, 6, 108, 256, 62, 6, 108, 257, 62, 6, 108, 258, 62, 6, 192, 346, 347, 108, 259, 62, 6, 192, 346, 347, 108, 260, 62, 6, 192, 346, 347, 173, 62, 32, -1, 261, 192, 169, 241, 262, 192, 348, 173, 32, 263, 192, 169, 241, 262, 192, 346, 173, 32, 264, 192, 217, 241, 265, 192, 248, 108, 123, 192, 241, 256, 108, 257, 173, 108, 266, 192, 241, 249, 108, 250, 173, 108, 267, 192, 241, 263, 242, 259, 108, 261, 242, 260, 173, 108, 268, 192, 241, 259, 108, 260, 173, 108, 269, 192, 241, 346, 108, 348, 173, 173, 32, 270, 192, 217, 241, 265, 192, 247, 108, 123, 192, 241, 257, 108, 255, 173, 108, 266, 192, 241, 251, 108, 252, 173, 108, 267, 192, 241, 261, 242, 260, 108, 348, 173, 108, 268, 192, 241, 260, 108, 258, 173, 108, 269, 192, 241, 346, 108, 348, 173, 173, 32, 271, 192, 217, 241, 265, 192, 245, 108, 123, 192, 241, 256, 108, 255, 173, 108, 266, 192, 241, 253, 108, 254, 173, 108, 267, 192, 241, 263, 242, 259, 108, 348, 173, 108, 268, 192, 241, 259, 108, 258, 173, 108, 269, 192, 241, 346, 108, 348, 173, 173, 32, 272, 192, 263, 242, 259, 74, 76, 241, 348, 108, 259, 173, 32, 273, 192, 76, 241, 348, 108, 258, 173, 32, 274, 192, 55, 241, 246, 74, 272, 173, 32, 275, 192, 174, 241, 241, 259, 108, 260, 173, 108, 144, 173, 32, 135, 276, 156, 5, 241, 255, 48, 258, 173, 62, 32, 277, 192, 241, 274, 228, 62, 108, 200, 27, 77, 273, 228, 200, 108, 62, 27, 173, 228, 62, 108, 62, 108, 200, 27, 32, 278, 192, 55, 241, 270, 173, 82, 279, 241, 173, 32, 280, 192, 55, 241, 271, 173, 32, 275, 192, 15, 241, 280, 108, 278, 108, 275, 173, 32, 275, 2, 219, 241, 203, 241, 277, 108, 278, 228, 200, 108, 62, 108, 62, 27, 108, 348, 173, 108, 262, 192, 346, 173, 32, 270, 192, 142, 241, 270, 108, 228, 348, 108, 258, 27, 173, 32, 271, 192, 142, 241, 271, 108, 228, 348, 108, 258, 27, 173, 32, 273, 170, 258, 32, 78, 32, 10, 241, 264, 108, 275, 42, 256, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(\n    sz_ptr,\n    x_ptr,\n    y_ptr,\n    A_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_sz_N,\n    stride_sz_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_H = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1) - (N // N_BLOCK_SIZE)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(0, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_t_grad_block_ptr = tl.make_block_ptr(\n        base=A_grad_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    sz_block_ptr = tl.make_block_ptr(\n        base=sz_ptr,\n        shape=(N, V),\n        strides=(stride_sz_N, stride_sz_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_offsets = tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_grad_acc = tl.zeros((V_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)\n    for idx_N in range(N // N_BLOCK_SIZE):\n        y = tl.load(y_ptr + N_offsets)\n\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n        x_chunk = tl.load(x_block_ptr)\n        sz = tl.load(sz_block_ptr).trans()\n\n        A_grad_acc = tl.dot(sz, x_chunk, A_grad_acc)\n        A_grad_acc -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n        sz_block_ptr = tl.advance(sz_block_ptr, [N_BLOCK_SIZE, 0])\n        N_offsets += N_BLOCK_SIZE\n\n    tl.store(A_t_grad_block_ptr, A_grad_acc.trans() / N)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 63, 6, 108, 256, 63, 6, 108, 257, 63, 6, 108, 258, 63, 6, 192, 346, 347, 108, 259, 63, 6, 192, 346, 347, 108, 260, 63, 6, 192, 346, 347, 173, 63, 32, -1, 261, 192, 169, 241, 262, 192, 348, 173, 32, 263, 192, 169, 241, 262, 192, 346, 173, 4, 256, 48, 259, 32, 264, 192, 217, 241, 265, 192, 246, 108, 123, 192, 241, 256, 108, 257, 173, 108, 266, 192, 241, 249, 108, 250, 173, 108, 267, 192, 241, 348, 108, 261, 242, 260, 173, 108, 268, 192, 241, 259, 108, 260, 173, 108, 269, 192, 241, 346, 108, 348, 173, 173, 32, 270, 192, 217, 241, 265, 192, 248, 108, 123, 192, 241, 257, 108, 255, 173, 108, 266, 192, 241, 251, 108, 252, 173, 108, 267, 192, 241, 261, 242, 260, 108, 263, 242, 258, 173, 108, 268, 192, 241, 260, 108, 258, 173, 108, 269, 192, 241, 346, 108, 348, 173, 173, 32, 271, 192, 217, 241, 265, 192, 245, 108, 123, 192, 241, 256, 108, 255, 173, 108, 266, 192, 241, 253, 108, 254, 173, 108, 267, 192, 241, 348, 108, 263, 242, 258, 173, 108, 268, 192, 241, 259, 108, 258, 173, 108, 269, 192, 241, 346, 108, 348, 173, 173, 32, 272, 192, 76, 241, 348, 108, 259, 173, 32, 273, 192, 263, 242, 258, 74, 76, 241, 348, 108, 258, 173, 32, 274, 192, 174, 241, 241, 258, 108, 260, 173, 108, 144, 173, 32, 135, 275, 156, 5, 241, 256, 48, 259, 173, 63, 32, 276, 192, 57, 241, 247, 74, 272, 173, 32, 277, 192, 241, 276, 228, 63, 108, 200, 27, 77, 273, 228, 200, 108, 63, 27, 173, 228, 63, 108, 63, 108, 200, 27, 32, 278, 192, 57, 241, 264, 173, 32, 279, 192, 57, 241, 271, 173, 82, 280, 241, 173, 32, 274, 192, 15, 241, 279, 108, 278, 108, 274, 173, 32, 274, 2, 219, 241, 203, 241, 277, 108, 278, 228, 63, 108, 200, 108, 63, 27, 108, 348, 173, 108, 262, 192, 348, 173, 32, 264, 192, 142, 241, 264, 108, 228, 259, 108, 348, 27, 173, 32, 271, 192, 142, 241, 271, 108, 228, 259, 108, 348, 27, 173, 32, 272, 170, 259, 32, 78, 32, 10, 241, 270, 108, 274, 82, 280, 241, 173, 42, 256, 173, 32, 3, 32]}, {"code": "def linear_xent_fwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    loss_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n):\n    idx = tl.program_id(axis=0)\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + offsets)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V // V_BLOCK_SIZE):\n\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        local_x_block_ptr = x_block_ptr\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(local_x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == v_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        m = m_new\n        A_block_ptr = tl.advance(\n            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]\n        )\n        v_range = v_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n    loss += tl.sum(lse) / N\n    tl.atomic_add(loss_ptr, loss)\n    tl.store(lse_ptr + offsets, lse)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 62, 6, 108, 255, 62, 6, 108, 256, 62, 6, 108, 257, 62, 6, 108, 258, 62, 6, 108, 259, 62, 6, 173, 62, 32, -1, 260, 192, 169, 241, 261, 192, 346, 173, 32, 83, 241, 255, 224, 258, 77, 346, 173, 32, 83, 241, 254, 224, 257, 77, 346, 173, 32, 83, 241, 256, 224, 259, 77, 346, 173, 32, 187, 241, 257, 108, 258, 108, 259, 173, 32, 262, 192, 217, 241, 263, 192, 245, 108, 123, 192, 241, 255, 108, 256, 173, 108, 264, 192, 241, 250, 108, 251, 173, 108, 265, 192, 241, 260, 242, 258, 108, 346, 173, 108, 266, 192, 241, 258, 108, 259, 173, 108, 267, 192, 241, 347, 108, 346, 173, 173, 32, 268, 192, 217, 241, 263, 192, 247, 108, 123, 192, 241, 256, 108, 254, 173, 108, 264, 192, 241, 252, 108, 253, 173, 108, 265, 192, 241, 346, 108, 346, 173, 108, 266, 192, 241, 259, 108, 257, 173, 108, 267, 192, 241, 347, 108, 346, 173, 173, 32, 265, 192, 260, 242, 258, 74, 76, 241, 346, 108, 258, 173, 32, 269, 192, 76, 241, 346, 108, 257, 173, 32, 270, 192, 55, 241, 246, 74, 265, 173, 32, 271, 192, 174, 241, 241, 258, 108, 173, 108, 91, 192, 144, 173, 4, 272, 241, 348, 173, 32, 273, 192, 174, 241, 241, 258, 108, 173, 108, 91, 192, 144, 173, 32, 274, 192, 346, 32, 135, 275, 156, 5, 241, 254, 48, 257, 173, 62, 32, 276, 192, 174, 241, 241, 258, 108, 257, 173, 108, 91, 192, 144, 173, 32, 277, 192, 262, 32, 135, 275, 156, 5, 241, 256, 48, 259, 173, 62, 32, 278, 192, 55, 241, 277, 173, 32, 279, 192, 55, 241, 268, 173, 32, 276, 192, 15, 241, 278, 108, 279, 108, 276, 173, 32, 277, 192, 142, 241, 277, 108, 228, 346, 108, 259, 27, 173, 32, 268, 192, 142, 241, 268, 108, 228, 259, 108, 346, 27, 173, 32, 78, 32, 280, 192, 190, 241, 271, 108, 12, 241, 276, 108, 347, 173, 173, 32, 281, 192, 219, 241, 107, 241, 276, 4, 280, 228, 62, 108, 200, 27, 173, 108, 261, 192, 347, 173, 32, 273, 192, 273, 242, 107, 241, 271, 4, 280, 173, 74, 281, 32, 282, 192, 270, 228, 62, 108, 200, 27, 77, 269, 228, 200, 108, 62, 27, 32, 274, 2, 219, 241, 203, 241, 282, 108, 276, 108, 272, 241, 346, 173, 173, 173, 42, 255, 32, 271, 192, 280, 32, 268, 192, 142, 241, 268, 108, 228, 4, 259, 242, 241, 256, 48, 259, 173, 108, 257, 27, 173, 32, 269, 192, 269, 74, 257, 32, 78, 32, 283, 192, 271, 74, 50, 241, 273, 173, 32, 274, 170, 219, 241, 283, 173, 42, 255, 32, 194, 241, 248, 108, 274, 173, 32, 10, 241, 249, 74, 265, 108, 283, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    A_grad_ptr,\n    x_grad_ptr,\n    locks_N_ptr,\n    locks_V_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_x_grad_N,\n    stride_x_grad_H,\n    stride_A_grad_H,\n    stride_A_grad_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1)\n\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n\n    offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + offsets)\n    lse = tl.load(lse_global_ptr + offsets)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    x_grad_block_ptr = tl.make_block_ptr(\n        base=x_grad_ptr,\n        shape=(N, H),\n        strides=(stride_x_grad_N, stride_x_grad_H),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    A_grad_block_ptr = tl.make_block_ptr(\n        base=A_grad_ptr,\n        shape=(H, V),\n        strides=(stride_A_grad_H, stride_A_grad_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n\n    local_x_block_ptr = x_block_ptr\n    local_A_block_ptr = A_block_ptr\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(local_x_block_ptr)\n        A_v = tl.load(local_A_block_ptr)\n\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n        local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n        local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n\n    mask = (y[:, None] == v_range[None, :])[:, :, None]\n\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr).to(tl.float32)\n        A_v = tl.load(A_block_ptr).to(tl.float32)\n\n        temp_xgrad = tl.dot(softmax_z, A_v.trans())\n        temp_xgrad -= tl.sum(tl.where(mask, A_v.trans()[None, :, :], 0.0), axis=1)\n\n        while tl.atomic_cas(locks_V_ptr + idx_N, 0, 1) == 1:\n            pass\n        temp_xgrad = temp_xgrad / N + tl.load(x_grad_block_ptr)\n        tl.store(x_grad_block_ptr, temp_xgrad)\n        tl.atomic_xchg(locks_V_ptr + idx_N, 0)\n\n        temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)\n        temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n        temp_Agrad = temp_Agrad.trans()\n\n        while tl.atomic_cas(locks_N_ptr + idx_V, 0, 1) == 1:\n            pass\n        temp_Agrad = temp_Agrad / N + tl.load(A_grad_block_ptr)\n\n        tl.store(A_grad_block_ptr, temp_Agrad)\n        tl.atomic_xchg(locks_N_ptr + idx_V, 0)\n\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        x_grad_block_ptr = tl.advance(x_grad_block_ptr, [0, H_BLOCK_SIZE])\n\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 63, 6, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 192, 346, 347, 108, 265, 63, 6, 192, 346, 347, 108, 266, 63, 6, 192, 346, 347, 173, 63, 32, -1, 267, 192, 169, 241, 268, 192, 348, 173, 32, 269, 192, 169, 241, 268, 192, 346, 173, 32, 187, 241, 264, 108, 265, 108, 266, 173, 32, 83, 241, 262, 224, 265, 77, 348, 173, 32, 83, 241, 261, 224, 264, 77, 348, 173, 32, 83, 241, 263, 224, 266, 77, 348, 173, 32, 270, 192, 267, 242, 265, 74, 76, 241, 348, 108, 265, 173, 32, 271, 192, 269, 242, 264, 74, 76, 241, 348, 108, 264, 173, 32, 272, 192, 57, 241, 246, 74, 270, 173, 32, 273, 192, 57, 241, 248, 74, 270, 173, 32, 274, 192, 217, 241, 275, 192, 245, 108, 123, 192, 241, 262, 108, 263, 173, 108, 276, 192, 241, 253, 108, 254, 173, 108, 270, 192, 241, 267, 242, 265, 108, 348, 173, 108, 277, 192, 241, 265, 108, 266, 173, 108, 278, 192, 241, 346, 108, 348, 173, 173, 32, 279, 192, 217, 241, 275, 192, 250, 108, 123, 192, 241, 262, 108, 263, 173, 108, 276, 192, 241, 257, 108, 258, 173, 108, 270, 192, 241, 267, 242, 265, 108, 348, 173, 108, 277, 192, 241, 265, 108, 266, 173, 108, 278, 192, 241, 346, 108, 348, 173, 173, 32, 280, 192, 217, 241, 275, 192, 247, 108, 123, 192, 241, 263, 108, 261, 173, 108, 276, 192, 241, 255, 108, 256, 173, 108, 270, 192, 241, 348, 108, 269, 242, 264, 173, 108, 277, 192, 241, 266, 108, 264, 173, 108, 278, 192, 241, 346, 108, 348, 173, 173, 32, 281, 192, 217, 241, 275, 192, 249, 108, 123, 192, 241, 263, 108, 261, 173, 108, 276, 192, 241, 259, 108, 260, 173, 108, 270, 192, 241, 348, 108, 269, 242, 264, 173, 108, 277, 192, 241, 266, 108, 264, 173, 108, 278, 192, 241, 346, 108, 348, 173, 173, 32, 282, 192, 174, 241, 241, 265, 108, 264, 173, 108, 91, 192, 144, 173, 32, 283, 192, 274, 32, 284, 192, 280, 32, 135, 285, 156, 5, 241, 263, 48, 266, 173, 63, 32, 286, 192, 57, 241, 283, 173, 32, 287, 192, 57, 241, 284, 173, 32, 282, 192, 15, 241, 286, 108, 287, 108, 282, 173, 32, 283, 192, 142, 241, 283, 108, 228, 348, 108, 266, 27, 173, 32, 284, 192, 142, 241, 284, 108, 228, 266, 108, 348, 27, 173, 32, 78, 32, 288, 192, 241, 272, 228, 63, 108, 200, 27, 77, 271, 228, 200, 108, 63, 27, 173, 228, 63, 108, 63, 108, 200, 27, 32, 289, 192, 241, 282, 4, 273, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 135, 285, 156, 5, 241, 263, 48, 266, 173, 63, 32, 286, 192, 57, 241, 274, 173, 82, 290, 241, 144, 173, 32, 287, 192, 57, 241, 280, 173, 82, 290, 241, 144, 173, 32, 291, 192, 15, 241, 289, 108, 287, 82, 292, 241, 173, 173, 32, 291, 2, 219, 241, 203, 241, 288, 108, 287, 82, 292, 241, 173, 228, 200, 108, 63, 108, 63, 27, 108, 348, 173, 108, 268, 192, 346, 173, 32, 58, 149, 241, 252, 74, 267, 108, 348, 108, 346, 173, 77, 346, 63, 32, 237, 32, 178, 32, 291, 192, 291, 42, 262, 74, 57, 241, 279, 173, 32, 10, 241, 279, 108, 291, 173, 32, 75, 241, 252, 74, 267, 108, 348, 173, 32, 293, 192, 15, 241, 289, 82, 292, 241, 173, 108, 286, 173, 32, 293, 2, 219, 241, 203, 241, 288, 108, 286, 228, 63, 108, 200, 108, 63, 27, 108, 348, 173, 108, 268, 192, 348, 173, 32, 293, 192, 293, 82, 292, 241, 173, 32, 58, 149, 241, 251, 74, 269, 108, 348, 108, 346, 173, 77, 346, 63, 32, 237, 32, 178, 32, 293, 192, 293, 42, 262, 74, 57, 241, 281, 173, 32, 10, 241, 281, 108, 293, 173, 32, 75, 241, 251, 74, 269, 108, 348, 173, 32, 274, 192, 142, 241, 274, 108, 228, 348, 108, 266, 27, 173, 32, 279, 192, 142, 241, 279, 108, 228, 348, 108, 266, 27, 173, 32, 280, 192, 142, 241, 280, 108, 228, 266, 108, 348, 27, 173, 32, 281, 192, 142, 241, 281, 108, 228, 266, 108, 348, 27, 173, 32, 78, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V // V_BLOCK_SIZE):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == V_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n        m = m_new\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        V_range = V_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n    loss += tl.sum(lse) / N\n    tl.store(losses_ptr + idx_N + idx_N_group * N_group // N_BLOCK_SIZE, loss)\n    tl.store(lse_ptr + N_range, lse)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 62, 6, 108, 259, 62, 6, 108, 260, 62, 6, 108, 261, 62, 6, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 173, 62, 32, -1, 265, 192, 169, 241, 266, 192, 346, 173, 32, 187, 241, 262, 108, 263, 108, 264, 173, 32, 267, 192, 217, 241, 268, 192, 245, 108, 123, 192, 241, 260, 108, 261, 173, 108, 269, 192, 241, 251, 108, 252, 173, 108, 270, 192, 241, 257, 242, 258, 74, 265, 242, 263, 108, 346, 173, 108, 271, 192, 241, 263, 108, 264, 173, 108, 272, 192, 241, 347, 108, 346, 173, 173, 32, 273, 192, 217, 241, 268, 192, 247, 108, 123, 192, 241, 261, 108, 259, 173, 108, 269, 192, 241, 253, 108, 254, 173, 108, 270, 192, 241, 346, 108, 346, 173, 108, 271, 192, 241, 264, 108, 262, 173, 108, 272, 192, 241, 347, 108, 346, 173, 173, 32, 274, 192, 217, 241, 268, 192, 248, 108, 123, 192, 241, 258, 108, 259, 173, 108, 269, 192, 241, 255, 108, 256, 173, 108, 270, 192, 241, 265, 242, 263, 108, 346, 173, 108, 271, 192, 241, 263, 108, 262, 173, 108, 272, 192, 241, 347, 108, 346, 173, 173, 32, 275, 192, 257, 242, 258, 74, 265, 242, 263, 74, 76, 241, 346, 108, 263, 173, 32, 276, 192, 76, 241, 346, 108, 262, 173, 32, 277, 192, 55, 241, 246, 74, 275, 173, 32, 278, 192, 174, 241, 241, 263, 108, 173, 108, 91, 192, 144, 173, 4, 279, 241, 348, 173, 32, 280, 192, 174, 241, 241, 263, 108, 173, 108, 91, 192, 144, 173, 32, 281, 192, 346, 32, 135, 282, 156, 5, 241, 259, 48, 262, 173, 62, 32, 283, 192, 174, 241, 241, 263, 108, 262, 173, 108, 91, 192, 144, 173, 32, 135, 282, 156, 5, 241, 261, 48, 264, 173, 62, 32, 284, 192, 55, 241, 267, 173, 32, 285, 192, 55, 241, 273, 173, 32, 283, 192, 15, 241, 284, 108, 285, 108, 283, 173, 32, 267, 192, 142, 241, 267, 108, 228, 346, 108, 264, 27, 173, 32, 273, 192, 142, 241, 273, 108, 228, 264, 108, 346, 27, 173, 32, 78, 32, 286, 192, 190, 241, 278, 108, 12, 241, 283, 108, 347, 173, 173, 32, 287, 192, 219, 241, 107, 241, 283, 4, 286, 228, 62, 108, 200, 27, 173, 108, 266, 192, 347, 173, 32, 280, 192, 280, 242, 107, 241, 278, 4, 286, 173, 74, 287, 32, 288, 192, 277, 228, 62, 108, 200, 27, 77, 276, 228, 200, 108, 62, 27, 32, 281, 2, 219, 241, 203, 241, 288, 108, 283, 108, 279, 241, 346, 173, 173, 173, 42, 260, 32, 10, 241, 274, 108, 283, 82, 289, 241, 248, 82, 185, 82, 114, 173, 173, 32, 278, 192, 286, 32, 267, 192, 142, 241, 267, 108, 228, 346, 108, 4, 261, 27, 173, 32, 273, 192, 142, 241, 273, 108, 228, 4, 261, 108, 262, 27, 173, 32, 274, 192, 142, 241, 274, 108, 228, 346, 108, 262, 27, 173, 32, 276, 192, 276, 74, 262, 32, 78, 32, 290, 192, 278, 74, 50, 241, 280, 173, 32, 281, 170, 219, 241, 290, 173, 42, 260, 32, 10, 241, 249, 74, 265, 74, 257, 242, 258, 48, 263, 108, 281, 173, 32, 10, 241, 250, 74, 275, 108, 290, 173, 32, 3, 32]}, {"code": "def linear_xent_mini_bwd_prologue_kernel(\n    z_nv_ptr,\n    y_ptr,\n    lse_ptr,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1)\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE)\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n    lse = tl.load(lse_ptr + N_range)\n    z_j_to_k = tl.load(z_block_ptr)\n\n    mask = y[:, None] == v_range[None, :]\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N\n\n    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 63, 6, 108, 252, 63, 6, 108, 253, 63, 6, 108, 254, 63, 6, 108, 255, 63, 6, 173, 63, 32, -1, 256, 192, 169, 241, 257, 192, 346, 173, 32, 258, 192, 169, 241, 257, 192, 347, 173, 32, 187, 241, 254, 108, 255, 173, 32, 259, 192, 217, 241, 260, 192, 245, 108, 123, 192, 241, 251, 108, 252, 173, 108, 261, 192, 241, 248, 108, 249, 173, 108, 262, 192, 241, 256, 242, 255, 108, 258, 242, 254, 173, 108, 263, 192, 241, 255, 108, 254, 173, 108, 264, 192, 241, 347, 108, 346, 173, 173, 32, 265, 192, 250, 242, 251, 74, 256, 242, 255, 74, 76, 241, 346, 108, 255, 173, 32, 266, 192, 258, 242, 254, 74, 76, 241, 346, 108, 254, 173, 32, 267, 192, 57, 241, 246, 74, 265, 173, 32, 268, 192, 57, 241, 247, 74, 265, 173, 32, 269, 192, 57, 241, 259, 173, 32, 270, 192, 267, 228, 63, 108, 200, 27, 77, 266, 228, 200, 108, 63, 27, 32, 271, 192, 241, 269, 4, 268, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 272, 192, 241, 271, 4, 203, 241, 270, 108, 347, 108, 346, 173, 173, 42, 253, 32, 10, 241, 259, 108, 272, 82, 273, 241, 245, 82, 185, 82, 114, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    N_offset,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n):\n    idx = tl.program_id(axis=0)\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    offsets = N_offset + idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + offsets)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V // V_BLOCK_SIZE):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == v_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n        m = m_new\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        v_range = v_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n    loss += tl.sum(lse) / N\n    tl.store(losses_ptr + idx, loss)\n    tl.store(lse_ptr + offsets, lse)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 62, 6, 108, 259, 62, 6, 108, 260, 62, 6, 108, 261, 62, 6, 108, 262, 62, 6, 108, 263, 62, 6, 173, 62, 32, -1, 264, 192, 169, 241, 265, 192, 346, 173, 32, 187, 241, 261, 108, 262, 108, 263, 173, 32, 83, 241, 259, 224, 262, 77, 346, 173, 32, 83, 241, 258, 224, 261, 77, 346, 173, 32, 83, 241, 260, 224, 263, 77, 346, 173, 32, 266, 192, 217, 241, 267, 192, 245, 108, 123, 192, 241, 259, 108, 260, 173, 108, 268, 192, 241, 251, 108, 252, 173, 108, 269, 192, 241, 264, 242, 262, 108, 346, 173, 108, 270, 192, 241, 262, 108, 263, 173, 108, 271, 192, 241, 347, 108, 346, 173, 173, 32, 272, 192, 217, 241, 267, 192, 247, 108, 123, 192, 241, 260, 108, 258, 173, 108, 268, 192, 241, 253, 108, 254, 173, 108, 269, 192, 241, 346, 108, 346, 173, 108, 270, 192, 241, 263, 108, 261, 173, 108, 271, 192, 241, 347, 108, 346, 173, 173, 32, 273, 192, 217, 241, 267, 192, 248, 108, 123, 192, 241, 259, 108, 258, 173, 108, 268, 192, 241, 255, 108, 256, 173, 108, 269, 192, 241, 264, 242, 262, 108, 346, 173, 108, 270, 192, 241, 262, 108, 261, 173, 108, 271, 192, 241, 347, 108, 346, 173, 173, 32, 269, 192, 257, 74, 264, 242, 262, 74, 76, 241, 346, 108, 262, 173, 32, 274, 192, 76, 241, 346, 108, 261, 173, 32, 275, 192, 55, 241, 246, 74, 269, 173, 32, 276, 192, 174, 241, 241, 262, 108, 173, 108, 91, 192, 144, 173, 4, 277, 241, 348, 173, 32, 278, 192, 174, 241, 241, 262, 108, 173, 108, 91, 192, 144, 173, 32, 279, 192, 346, 32, 135, 280, 156, 5, 241, 258, 48, 261, 173, 62, 32, 281, 192, 174, 241, 241, 262, 108, 261, 173, 108, 91, 192, 144, 173, 32, 135, 280, 156, 5, 241, 260, 48, 263, 173, 62, 32, 282, 192, 55, 241, 266, 173, 32, 283, 192, 55, 241, 272, 173, 32, 281, 192, 15, 241, 282, 108, 283, 108, 281, 173, 32, 266, 192, 142, 241, 266, 108, 228, 346, 108, 263, 27, 173, 32, 272, 192, 142, 241, 272, 108, 228, 263, 108, 346, 27, 173, 32, 78, 32, 284, 192, 190, 241, 276, 108, 12, 241, 281, 108, 347, 173, 173, 32, 285, 192, 219, 241, 107, 241, 281, 4, 284, 228, 62, 108, 200, 27, 173, 108, 265, 192, 347, 173, 32, 278, 192, 278, 242, 107, 241, 276, 4, 284, 173, 74, 285, 32, 286, 192, 275, 228, 62, 108, 200, 27, 77, 274, 228, 200, 108, 62, 27, 32, 279, 2, 219, 241, 203, 241, 286, 108, 281, 108, 277, 241, 346, 173, 173, 173, 42, 259, 32, 10, 241, 273, 108, 281, 82, 287, 241, 248, 82, 185, 82, 114, 173, 173, 32, 276, 192, 284, 32, 266, 192, 142, 241, 266, 108, 228, 346, 108, 4, 260, 27, 173, 32, 272, 192, 142, 241, 272, 108, 228, 4, 260, 108, 261, 27, 173, 32, 273, 192, 142, 241, 273, 108, 228, 346, 108, 261, 27, 173, 32, 274, 192, 274, 74, 261, 32, 78, 32, 288, 192, 276, 74, 50, 241, 278, 173, 32, 279, 170, 219, 241, 288, 173, 42, 259, 32, 10, 241, 249, 74, 264, 108, 279, 173, 32, 10, 241, 250, 74, 269, 108, 288, 173, 32, 3, 32]}, {"code": "def linear_xent_mini_bwd_prologue_kernel(\n    z_nv_ptr,\n    z_grad_ptr,\n    y_ptr,\n    lse_ptr,\n    stride_z_N,\n    stride_z_V,\n    N_offset,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1)\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE)\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_grad_block_ptr = tl.make_block_ptr(\n        base=z_grad_ptr,\n        shape=(N, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = N_offset + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n    lse = tl.load(lse_ptr + N_range)\n    z_j_to_k = tl.load(z_block_ptr)\n\n    mask = y[:, None] == v_range[None, :]\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N\n\n    tl.store(z_grad_block_ptr, z_grad.to(tl.float16))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 63, 6, 108, 253, 63, 6, 108, 254, 63, 6, 108, 255, 63, 6, 173, 63, 32, -1, 256, 192, 169, 241, 257, 192, 346, 173, 32, 258, 192, 169, 241, 257, 192, 347, 173, 32, 187, 241, 254, 108, 255, 173, 32, 83, 241, 253, 224, 255, 77, 346, 173, 32, 83, 241, 252, 224, 254, 77, 346, 173, 32, 259, 192, 217, 241, 260, 192, 245, 108, 123, 192, 241, 253, 108, 252, 173, 108, 261, 192, 241, 249, 108, 250, 173, 108, 262, 192, 241, 256, 242, 255, 108, 258, 242, 254, 173, 108, 263, 192, 241, 255, 108, 254, 173, 108, 264, 192, 241, 347, 108, 346, 173, 173, 32, 265, 192, 217, 241, 260, 192, 246, 108, 123, 192, 241, 253, 108, 252, 173, 108, 261, 192, 241, 249, 108, 250, 173, 108, 262, 192, 241, 256, 242, 255, 108, 258, 242, 254, 173, 108, 263, 192, 241, 255, 108, 254, 173, 108, 264, 192, 241, 347, 108, 346, 173, 173, 32, 266, 192, 251, 74, 256, 242, 255, 74, 76, 241, 346, 108, 255, 173, 32, 267, 192, 258, 242, 254, 74, 76, 241, 346, 108, 254, 173, 32, 268, 192, 57, 241, 247, 74, 266, 173, 32, 269, 192, 57, 241, 248, 74, 266, 173, 32, 270, 192, 57, 241, 259, 173, 32, 271, 192, 268, 228, 63, 108, 200, 27, 77, 267, 228, 200, 108, 63, 27, 32, 272, 192, 241, 270, 4, 269, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 273, 192, 241, 272, 4, 203, 241, 271, 108, 347, 108, 346, 173, 173, 42, 253, 32, 10, 241, 265, 108, 273, 82, 274, 241, 21, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(\n    z_nv_ptr,\n    y_ptr,\n    A_t_ptr,\n    x_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    N_offset,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_H = tl.program_id(axis=1)\n    idx_V = 0\n\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n\n    x_grad_block_ptr = tl.make_block_ptr(\n        base=x_grad_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(N_offset + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_t_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = N_offset + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = 0 + tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_range)\n    lse = tl.load(lse_ptr + N_range)\n\n    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)\n    for idx_V in range(V // V_BLOCK_SIZE):\n        mask = (y[:, None] == v_range[None, :])[:, :, None]\n        A_v = tl.load(A_t_block_ptr).trans()\n        z_j_to_k = tl.load(z_block_ptr)\n        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n\n        x_grad_acc = tl.dot(softmax_z, A_v, x_grad_acc)\n        x_grad_acc -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1)\n\n        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        v_range += V_BLOCK_SIZE\n\n    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(tl.float16))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 62, 6, 108, 258, 62, 6, 108, 259, 62, 6, 108, 260, 62, 6, 192, 346, 347, 108, 261, 62, 6, 192, 346, 347, 108, 262, 62, 6, 192, 346, 347, 173, 62, 32, -1, 263, 192, 169, 241, 264, 192, 348, 173, 32, 265, 192, 169, 241, 264, 192, 346, 173, 32, 266, 192, 348, 32, 187, 241, 260, 108, 261, 108, 262, 173, 32, 83, 241, 258, 224, 261, 77, 348, 173, 32, 83, 241, 257, 224, 260, 77, 348, 173, 32, 267, 192, 217, 241, 268, 192, 248, 108, 123, 192, 241, 258, 108, 259, 173, 108, 269, 192, 241, 250, 108, 251, 173, 108, 270, 192, 241, 256, 74, 263, 242, 261, 108, 265, 242, 262, 173, 108, 271, 192, 241, 261, 108, 262, 173, 108, 272, 192, 241, 346, 108, 348, 173, 173, 32, 273, 192, 217, 241, 268, 192, 247, 108, 123, 192, 241, 259, 108, 257, 173, 108, 269, 192, 241, 252, 108, 253, 173, 108, 270, 192, 241, 265, 242, 262, 108, 348, 173, 108, 271, 192, 241, 262, 108, 260, 173, 108, 272, 192, 241, 346, 108, 348, 173, 173, 32, 274, 192, 217, 241, 268, 192, 245, 108, 123, 192, 241, 258, 108, 257, 173, 108, 269, 192, 241, 254, 108, 255, 173, 108, 270, 192, 241, 263, 242, 261, 108, 348, 173, 108, 271, 192, 241, 261, 108, 260, 173, 108, 272, 192, 241, 346, 108, 348, 173, 173, 32, 275, 192, 256, 74, 263, 242, 261, 74, 76, 241, 348, 108, 261, 173, 32, 276, 192, 348, 74, 76, 241, 348, 108, 260, 173, 32, 277, 192, 55, 241, 246, 74, 275, 173, 32, 278, 192, 55, 241, 249, 74, 275, 173, 32, 279, 192, 174, 241, 241, 261, 108, 262, 173, 108, 144, 173, 32, 135, 266, 156, 5, 241, 257, 48, 260, 173, 62, 32, 280, 192, 241, 277, 228, 62, 108, 200, 27, 77, 276, 228, 200, 108, 62, 27, 173, 228, 62, 108, 62, 108, 200, 27, 32, 281, 192, 55, 241, 273, 173, 82, 282, 241, 173, 32, 283, 192, 55, 241, 274, 173, 32, 284, 192, 241, 283, 4, 278, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 82, 285, 241, 21, 173, 32, 279, 192, 15, 241, 284, 108, 281, 108, 279, 173, 32, 279, 2, 219, 241, 203, 241, 280, 108, 281, 228, 200, 108, 62, 108, 62, 27, 108, 348, 173, 108, 264, 192, 346, 173, 32, 273, 192, 142, 241, 273, 108, 228, 348, 108, 260, 27, 173, 32, 274, 192, 142, 241, 274, 108, 228, 348, 108, 260, 27, 173, 32, 276, 170, 260, 32, 78, 32, 10, 241, 267, 108, 241, 279, 42, 258, 173, 82, 285, 241, 21, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V // V_BLOCK_SIZE):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == V_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n        m = m_new\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        V_range = V_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n    loss += tl.sum(lse) / N\n    tl.store(losses_ptr + idx_N + idx_N_group * N_group // N_BLOCK_SIZE, loss)\n    tl.store(lse_ptr + N_range, lse)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 63, 6, 108, 259, 63, 6, 108, 260, 63, 6, 108, 261, 63, 6, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 173, 63, 32, -1, 265, 192, 169, 241, 266, 192, 346, 173, 32, 187, 241, 262, 108, 263, 108, 264, 173, 32, 83, 241, 260, 224, 263, 77, 346, 173, 32, 83, 241, 259, 224, 262, 77, 346, 173, 32, 83, 241, 261, 224, 264, 77, 346, 173, 32, 267, 192, 217, 241, 268, 192, 245, 108, 123, 192, 241, 260, 108, 261, 173, 108, 269, 192, 241, 251, 108, 252, 173, 108, 270, 192, 241, 257, 242, 258, 74, 265, 242, 263, 108, 346, 173, 108, 271, 192, 241, 263, 108, 264, 173, 108, 272, 192, 241, 347, 108, 346, 173, 173, 32, 273, 192, 217, 241, 268, 192, 247, 108, 123, 192, 241, 261, 108, 259, 173, 108, 269, 192, 241, 253, 108, 254, 173, 108, 270, 192, 241, 346, 108, 346, 173, 108, 271, 192, 241, 264, 108, 262, 173, 108, 272, 192, 241, 347, 108, 346, 173, 173, 32, 274, 192, 217, 241, 268, 192, 248, 108, 123, 192, 241, 258, 108, 259, 173, 108, 269, 192, 241, 255, 108, 256, 173, 108, 270, 192, 241, 265, 242, 263, 108, 346, 173, 108, 271, 192, 241, 263, 108, 262, 173, 108, 272, 192, 241, 347, 108, 346, 173, 173, 32, 275, 192, 257, 242, 258, 74, 265, 242, 263, 74, 76, 241, 346, 108, 263, 173, 32, 276, 192, 76, 241, 346, 108, 262, 173, 32, 277, 192, 57, 241, 246, 74, 275, 173, 32, 278, 192, 174, 241, 241, 263, 108, 173, 108, 91, 192, 144, 173, 4, 279, 241, 348, 173, 32, 280, 192, 174, 241, 241, 263, 108, 173, 108, 91, 192, 144, 173, 32, 281, 192, 346, 32, 135, 282, 156, 5, 241, 259, 48, 262, 173, 63, 32, 283, 192, 174, 241, 241, 263, 108, 262, 173, 108, 91, 192, 144, 173, 32, 135, 282, 156, 5, 241, 261, 48, 264, 173, 63, 32, 284, 192, 57, 241, 267, 173, 32, 285, 192, 57, 241, 273, 173, 32, 283, 192, 15, 241, 284, 108, 285, 108, 283, 173, 32, 267, 192, 142, 241, 267, 108, 228, 346, 108, 264, 27, 173, 32, 273, 192, 142, 241, 273, 108, 228, 264, 108, 346, 27, 173, 32, 78, 32, 286, 192, 190, 241, 278, 108, 12, 241, 283, 108, 347, 173, 173, 32, 287, 192, 219, 241, 107, 241, 283, 4, 286, 228, 63, 108, 200, 27, 173, 108, 266, 192, 347, 173, 32, 280, 192, 280, 242, 107, 241, 278, 4, 286, 173, 74, 287, 32, 288, 192, 277, 228, 63, 108, 200, 27, 77, 276, 228, 200, 108, 63, 27, 32, 281, 2, 219, 241, 203, 241, 288, 108, 283, 108, 279, 241, 346, 173, 173, 173, 42, 260, 32, 10, 241, 274, 108, 283, 82, 289, 241, 248, 82, 185, 82, 114, 173, 173, 32, 278, 192, 286, 32, 267, 192, 142, 241, 267, 108, 228, 346, 108, 4, 261, 27, 173, 32, 273, 192, 142, 241, 273, 108, 228, 4, 261, 108, 262, 27, 173, 32, 274, 192, 142, 241, 274, 108, 228, 346, 108, 262, 27, 173, 32, 276, 192, 276, 74, 262, 32, 78, 32, 290, 192, 278, 74, 50, 241, 280, 173, 32, 281, 170, 219, 241, 290, 173, 42, 260, 32, 10, 241, 249, 74, 265, 74, 257, 242, 258, 48, 263, 108, 281, 173, 32, 10, 241, 250, 74, 275, 108, 290, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(\n    z_nv_ptr,\n    y_ptr,\n    A_t_ptr,\n    x_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_H = tl.program_id(axis=1)\n    idx_V = 0\n\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n\n    x_grad_block_ptr = tl.make_block_ptr(\n        base=x_grad_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_t_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = 0 + tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_range)\n    lse = tl.load(lse_ptr + N_range)\n\n    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)\n    for idx_V in range(V // V_BLOCK_SIZE):\n        mask = (y[:, None] == v_range[None, :])[:, :, None]\n        A_v = tl.load(A_t_block_ptr).trans()\n        z_j_to_k = tl.load(z_block_ptr)\n        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n\n        x_grad_acc = tl.dot(softmax_z, A_v, x_grad_acc)\n        x_grad_acc -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1)\n\n        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        v_range += V_BLOCK_SIZE\n\n    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 62, 6, 108, 258, 62, 6, 108, 259, 62, 6, 108, 260, 62, 6, 108, 261, 62, 6, 192, 346, 347, 108, 262, 62, 6, 192, 346, 347, 108, 263, 62, 6, 192, 346, 347, 173, 62, 32, -1, 264, 192, 169, 241, 265, 192, 348, 173, 32, 266, 192, 169, 241, 265, 192, 346, 173, 32, 267, 192, 348, 32, 187, 241, 261, 108, 262, 108, 263, 173, 32, 83, 241, 259, 224, 262, 77, 348, 173, 32, 83, 241, 258, 224, 261, 77, 348, 173, 32, 268, 192, 217, 241, 269, 192, 248, 108, 123, 192, 241, 259, 108, 260, 173, 108, 270, 192, 241, 250, 108, 251, 173, 108, 271, 192, 241, 256, 242, 257, 74, 264, 242, 262, 108, 266, 242, 263, 173, 108, 272, 192, 241, 262, 108, 263, 173, 108, 273, 192, 241, 346, 108, 348, 173, 173, 32, 274, 192, 217, 241, 269, 192, 247, 108, 123, 192, 241, 260, 108, 258, 173, 108, 270, 192, 241, 252, 108, 253, 173, 108, 271, 192, 241, 266, 242, 263, 108, 348, 173, 108, 272, 192, 241, 263, 108, 261, 173, 108, 273, 192, 241, 346, 108, 348, 173, 173, 32, 275, 192, 217, 241, 269, 192, 245, 108, 123, 192, 241, 259, 108, 258, 173, 108, 270, 192, 241, 254, 108, 255, 173, 108, 271, 192, 241, 264, 242, 262, 108, 348, 173, 108, 272, 192, 241, 262, 108, 261, 173, 108, 273, 192, 241, 346, 108, 348, 173, 173, 32, 276, 192, 256, 242, 257, 74, 264, 242, 262, 74, 76, 241, 348, 108, 262, 173, 32, 277, 192, 348, 74, 76, 241, 348, 108, 261, 173, 32, 278, 192, 55, 241, 246, 74, 276, 173, 32, 279, 192, 55, 241, 249, 74, 276, 173, 32, 280, 192, 174, 241, 241, 262, 108, 263, 173, 108, 144, 173, 32, 135, 267, 156, 5, 241, 258, 48, 261, 173, 62, 32, 281, 192, 241, 278, 228, 62, 108, 200, 27, 77, 277, 228, 200, 108, 62, 27, 173, 228, 62, 108, 62, 108, 200, 27, 32, 282, 192, 55, 241, 274, 173, 82, 283, 241, 173, 32, 284, 192, 55, 241, 275, 173, 32, 285, 192, 241, 284, 4, 279, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 82, 286, 241, 21, 173, 32, 280, 192, 15, 241, 285, 108, 282, 108, 280, 173, 32, 280, 2, 219, 241, 203, 241, 281, 108, 282, 228, 200, 108, 62, 108, 62, 27, 108, 348, 173, 108, 265, 192, 346, 173, 32, 274, 192, 142, 241, 274, 108, 228, 348, 108, 261, 27, 173, 32, 275, 192, 142, 241, 275, 108, 228, 348, 108, 261, 27, 173, 32, 277, 170, 261, 32, 78, 32, 10, 241, 268, 108, 241, 280, 42, 259, 173, 82, 286, 241, 248, 82, 185, 82, 114, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(\n    z_nv_ptr,\n    y_ptr,\n    x_ptr,\n    A_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_V = tl.program_id(axis=0)\n    idx_H = tl.program_id(axis=1)\n    idx_N = 0\n\n    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_grad_block_ptr = tl.make_block_ptr(\n        base=A_grad_ptr,\n        shape=(V, H),\n        strides=(stride_A_V, stride_A_H),\n        offsets=(idx_V * V_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),\n        block_shape=(V_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    N_range = idx_N_group * N_group + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_grad_acc = tl.zeros((V_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)\n    for idx_N in range(N // N_BLOCK_SIZE):\n        y = tl.load(y_ptr + N_range)\n        lse = tl.load(lse_ptr + N_range)\n        mask = (y[:, None] == V_range[None, :])[:, :, None]\n\n        x_chunk = tl.load(x_block_ptr)\n        z_j_to_k = tl.load(z_block_ptr)\n        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n\n        A_grad_acc = tl.dot(softmax_z.trans(), x_chunk, A_grad_acc)\n        A_grad_acc -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n        N_range += N_BLOCK_SIZE\n    tl.store(A_grad_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 63, 6, 108, 258, 63, 6, 108, 259, 63, 6, 108, 260, 63, 6, 108, 261, 63, 6, 192, 346, 347, 108, 262, 63, 6, 192, 346, 347, 108, 263, 63, 6, 192, 346, 347, 173, 63, 32, -1, 264, 192, 169, 241, 265, 192, 348, 173, 32, 266, 192, 169, 241, 265, 192, 346, 173, 32, 267, 192, 348, 32, 187, 241, 261, 108, 262, 108, 263, 173, 32, 83, 241, 259, 224, 262, 77, 348, 173, 32, 83, 241, 258, 224, 261, 77, 348, 173, 32, 268, 192, 217, 241, 269, 192, 247, 108, 123, 192, 241, 259, 108, 260, 173, 108, 270, 192, 241, 250, 108, 251, 173, 108, 271, 192, 241, 256, 242, 257, 108, 266, 242, 263, 173, 108, 272, 192, 241, 262, 108, 263, 173, 108, 273, 192, 241, 346, 108, 348, 173, 173, 32, 274, 192, 217, 241, 269, 192, 248, 108, 123, 192, 241, 258, 108, 260, 173, 108, 270, 192, 241, 253, 108, 252, 173, 108, 271, 192, 241, 264, 242, 261, 108, 266, 242, 263, 173, 108, 272, 192, 241, 261, 108, 263, 173, 108, 273, 192, 241, 346, 108, 348, 173, 173, 32, 275, 192, 217, 241, 269, 192, 245, 108, 123, 192, 241, 257, 108, 258, 173, 108, 270, 192, 241, 254, 108, 255, 173, 108, 271, 192, 241, 348, 108, 264, 242, 261, 173, 108, 272, 192, 241, 262, 108, 261, 173, 108, 273, 192, 241, 346, 108, 348, 173, 173, 32, 276, 192, 256, 242, 257, 74, 76, 241, 348, 108, 262, 173, 32, 277, 192, 264, 242, 261, 74, 76, 241, 348, 108, 261, 173, 32, 278, 192, 174, 241, 241, 261, 108, 263, 173, 108, 144, 173, 32, 135, 267, 156, 5, 241, 259, 48, 262, 173, 63, 32, 279, 192, 57, 241, 246, 74, 276, 173, 32, 280, 192, 57, 241, 249, 74, 276, 173, 32, 281, 192, 241, 279, 228, 63, 108, 200, 27, 77, 277, 228, 200, 108, 63, 27, 173, 228, 63, 108, 63, 108, 200, 27, 32, 282, 192, 57, 241, 268, 173, 32, 283, 192, 57, 241, 275, 173, 32, 284, 192, 241, 283, 4, 280, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 82, 285, 241, 21, 173, 32, 278, 192, 15, 241, 284, 82, 286, 241, 173, 108, 282, 108, 278, 173, 32, 278, 2, 219, 241, 203, 241, 281, 108, 282, 228, 63, 108, 200, 108, 63, 27, 108, 348, 173, 108, 265, 192, 348, 173, 32, 268, 192, 142, 241, 268, 108, 228, 262, 108, 348, 27, 173, 32, 275, 192, 142, 241, 275, 108, 228, 262, 108, 348, 27, 173, 32, 276, 170, 262, 32, 78, 32, 10, 241, 274, 108, 241, 278, 42, 259, 173, 82, 285, 241, 248, 82, 185, 82, 114, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    stride_lse_N,\n    stride_lse_B,\n    stride_loss_Nb,\n    stride_loss_B,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    V_TILES: tl.constexpr = 4,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n\n    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_ptr,\n        shape=(N, V // 16),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_V_group),\n        block_shape=(N_BLOCK_SIZE, 1),\n        order=(1, 0),\n    )\n    loss_val_ptr = (\n        losses_ptr\n        + (idx_N + idx_N_group * N_group // N_BLOCK_SIZE) * stride_loss_Nb\n        + idx_V_group * stride_loss_B\n    )\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V_TILES):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == V_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n        m = m_new\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        V_range = V_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n\n    tl.store(loss_val_ptr, loss)\n    tl.store(lse_row_ptr, lse[:, None])", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 108, 266, 62, 6, 108, 267, 62, 6, 108, 268, 62, 6, 108, 269, 62, 6, 192, 346, 173, 62, 32, -1, 270, 192, 169, 241, 271, 192, 347, 173, 32, 272, 192, 169, 241, 271, 192, 348, 173, 32, 273, 62, 6, 192, 269, 242, 266, 32, 274, 192, 217, 241, 275, 192, 245, 108, 123, 192, 241, 264, 108, 265, 173, 108, 276, 192, 241, 251, 108, 252, 173, 108, 277, 192, 241, 261, 242, 262, 74, 270, 242, 267, 108, 347, 173, 108, 278, 192, 241, 267, 108, 268, 173, 108, 279, 192, 241, 348, 108, 347, 173, 173, 32, 280, 192, 217, 241, 275, 192, 247, 108, 123, 192, 241, 265, 108, 263, 173, 108, 276, 192, 241, 253, 108, 254, 173, 108, 277, 192, 241, 347, 108, 272, 242, 273, 173, 108, 278, 192, 241, 268, 108, 266, 173, 108, 279, 192, 241, 348, 108, 347, 173, 173, 32, 281, 192, 217, 241, 275, 192, 248, 108, 123, 192, 241, 262, 108, 263, 173, 108, 276, 192, 241, 255, 108, 256, 173, 108, 277, 192, 241, 270, 242, 267, 108, 272, 242, 273, 173, 108, 278, 192, 241, 267, 108, 266, 173, 108, 279, 192, 241, 348, 108, 347, 173, 173, 32, 282, 192, 217, 241, 275, 192, 250, 108, 123, 192, 241, 264, 108, 263, 48, 348, 349, 173, 108, 276, 192, 241, 257, 108, 258, 173, 108, 277, 192, 241, 261, 242, 262, 74, 270, 242, 267, 108, 272, 173, 108, 278, 192, 241, 267, 108, 348, 173, 108, 279, 192, 241, 348, 108, 347, 173, 173, 32, 283, 192, 249, 74, 241, 270, 74, 261, 242, 262, 48, 267, 173, 242, 259, 74, 272, 242, 260, 32, 284, 192, 261, 242, 262, 74, 270, 242, 267, 74, 76, 241, 347, 108, 267, 173, 32, 285, 192, 272, 242, 273, 74, 76, 241, 347, 108, 266, 173, 32, 286, 192, 55, 241, 246, 74, 284, 173, 32, 287, 192, 174, 241, 241, 267, 108, 173, 108, 91, 192, 144, 173, 4, 288, 241, 350, 173, 32, 289, 192, 174, 241, 241, 267, 108, 173, 108, 91, 192, 144, 173, 32, 290, 192, 347, 32, 135, 291, 156, 5, 241, 269, 173, 62, 32, 292, 192, 174, 241, 241, 267, 108, 266, 173, 108, 91, 192, 144, 173, 32, 135, 291, 156, 5, 241, 265, 48, 268, 173, 62, 32, 293, 192, 55, 241, 274, 173, 32, 294, 192, 55, 241, 280, 173, 32, 292, 192, 15, 241, 293, 108, 294, 108, 292, 173, 32, 274, 192, 142, 241, 274, 108, 228, 347, 108, 268, 27, 173, 32, 280, 192, 142, 241, 280, 108, 228, 268, 108, 347, 27, 173, 32, 78, 32, 295, 192, 190, 241, 287, 108, 12, 241, 292, 108, 348, 173, 173, 32, 296, 192, 219, 241, 107, 241, 292, 4, 295, 228, 62, 108, 200, 27, 173, 108, 271, 192, 348, 173, 32, 289, 192, 289, 242, 107, 241, 287, 4, 295, 173, 74, 296, 32, 297, 192, 286, 228, 62, 108, 200, 27, 77, 285, 228, 200, 108, 62, 27, 32, 290, 2, 219, 241, 203, 241, 297, 108, 292, 108, 288, 241, 347, 173, 173, 173, 42, 264, 32, 10, 241, 281, 108, 292, 82, 298, 241, 248, 82, 185, 82, 114, 173, 173, 32, 287, 192, 295, 32, 274, 192, 142, 241, 274, 108, 228, 347, 108, 4, 265, 27, 173, 32, 280, 192, 142, 241, 280, 108, 228, 4, 265, 108, 266, 27, 173, 32, 281, 192, 142, 241, 281, 108, 228, 347, 108, 266, 27, 173, 32, 285, 192, 285, 74, 266, 32, 78, 32, 299, 192, 287, 74, 50, 241, 289, 173, 32, 10, 241, 283, 108, 290, 173, 32, 10, 241, 282, 108, 299, 228, 62, 108, 200, 27, 173, 32, 3, 32]}, {"code": "def linear_xent_mini_bwd_prologue_kernel(\n    z_nv_ptr,\n    y_ptr,\n    lse_ptr,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1)\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n    lse = tl.load(lse_ptr + N_range)\n    z_j_to_k = tl.load(z_block_ptr)\n\n    mask = y[:, None] == v_range[None, :]\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N\n\n    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 63, 6, 108, 252, 63, 6, 108, 253, 63, 6, 108, 254, 63, 6, 108, 255, 63, 6, 173, 63, 32, -1, 256, 192, 169, 241, 257, 192, 346, 173, 32, 258, 192, 169, 241, 257, 192, 347, 173, 32, 259, 192, 217, 241, 260, 192, 245, 108, 123, 192, 241, 251, 108, 252, 173, 108, 261, 192, 241, 248, 108, 249, 173, 108, 262, 192, 241, 256, 242, 255, 108, 258, 242, 254, 173, 108, 263, 192, 241, 255, 108, 254, 173, 108, 264, 192, 241, 347, 108, 346, 173, 173, 32, 265, 192, 250, 242, 251, 74, 256, 242, 255, 74, 76, 241, 346, 108, 255, 173, 32, 266, 192, 258, 242, 254, 74, 76, 241, 346, 108, 254, 173, 32, 267, 192, 57, 241, 246, 74, 265, 173, 32, 268, 192, 57, 241, 247, 74, 265, 173, 32, 269, 192, 57, 241, 259, 173, 32, 270, 192, 267, 228, 63, 108, 200, 27, 77, 266, 228, 200, 108, 63, 27, 32, 271, 192, 241, 269, 4, 268, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 272, 192, 241, 271, 4, 203, 241, 270, 108, 347, 108, 346, 173, 173, 42, 253, 32, 10, 241, 259, 108, 272, 82, 273, 241, 245, 82, 185, 82, 114, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    stride_lse_N,\n    stride_lse_B,\n    stride_loss_Nb,\n    stride_loss_B,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    V_TILES: tl.constexpr = 1,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)\n\n    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_ptr,\n        shape=(N_group, V // 16),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),\n        block_shape=(N_BLOCK_SIZE, 1),\n        order=(1, 0),\n    )\n    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V_TILES):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == V_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n        m = m_new\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        V_range = V_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n\n    tl.store(loss_val_ptr, loss)\n    tl.store(lse_row_ptr, lse[:, None])", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 108, 266, 62, 6, 108, 267, 62, 6, 108, 268, 62, 6, 108, 269, 62, 6, 192, 346, 173, 62, 32, -1, 270, 192, 169, 241, 271, 192, 347, 173, 32, 272, 192, 169, 241, 271, 192, 346, 173, 32, 273, 108, 274, 192, 241, 138, 241, 347, 173, 108, 138, 241, 346, 173, 173, 32, 275, 62, 6, 192, 269, 242, 266, 32, 276, 192, 217, 241, 277, 192, 245, 108, 123, 192, 241, 264, 108, 265, 173, 108, 278, 192, 241, 251, 108, 252, 173, 108, 279, 192, 241, 261, 242, 262, 74, 270, 242, 267, 108, 347, 173, 108, 280, 192, 241, 267, 108, 268, 173, 108, 281, 192, 241, 346, 108, 347, 173, 173, 32, 282, 192, 217, 241, 277, 192, 247, 108, 123, 192, 241, 265, 108, 263, 173, 108, 278, 192, 241, 253, 108, 254, 173, 108, 279, 192, 241, 347, 108, 272, 242, 275, 173, 108, 280, 192, 241, 268, 108, 266, 173, 108, 281, 192, 241, 346, 108, 347, 173, 173, 32, 283, 192, 217, 241, 277, 192, 248, 108, 123, 192, 241, 262, 108, 263, 173, 108, 278, 192, 241, 255, 108, 256, 173, 108, 279, 192, 241, 270, 242, 267, 108, 272, 242, 275, 173, 108, 280, 192, 241, 267, 108, 266, 173, 108, 281, 192, 241, 346, 108, 347, 173, 173, 32, 284, 192, 217, 241, 277, 192, 250, 108, 123, 192, 241, 262, 108, 263, 48, 346, 348, 173, 108, 278, 192, 241, 257, 108, 258, 173, 108, 279, 192, 241, 270, 242, 267, 108, 272, 173, 108, 280, 192, 241, 267, 108, 346, 173, 108, 281, 192, 241, 346, 108, 347, 173, 173, 32, 285, 192, 249, 74, 270, 242, 259, 74, 272, 242, 260, 32, 286, 192, 261, 242, 262, 74, 270, 242, 267, 74, 76, 241, 347, 108, 267, 173, 32, 287, 192, 272, 242, 275, 74, 76, 241, 347, 108, 266, 173, 32, 288, 192, 55, 241, 246, 74, 286, 173, 32, 289, 192, 174, 241, 241, 267, 108, 173, 108, 91, 192, 144, 173, 4, 290, 241, 349, 173, 32, 291, 192, 174, 241, 241, 267, 108, 173, 108, 91, 192, 144, 173, 32, 292, 192, 347, 32, 135, 293, 156, 5, 241, 269, 173, 62, 32, 294, 192, 174, 241, 241, 267, 108, 266, 173, 108, 91, 192, 144, 173, 32, 135, 293, 156, 5, 241, 265, 48, 268, 173, 62, 32, 295, 192, 55, 241, 276, 173, 32, 296, 192, 55, 241, 282, 173, 32, 294, 192, 15, 241, 295, 108, 296, 108, 294, 173, 32, 276, 192, 142, 241, 276, 108, 228, 347, 108, 268, 27, 173, 32, 282, 192, 142, 241, 282, 108, 228, 268, 108, 347, 27, 173, 32, 78, 32, 297, 192, 190, 241, 289, 108, 12, 241, 294, 108, 346, 173, 173, 32, 298, 192, 219, 241, 107, 241, 294, 4, 297, 228, 62, 108, 200, 27, 173, 108, 271, 192, 346, 173, 32, 291, 192, 291, 242, 107, 241, 289, 4, 297, 173, 74, 298, 32, 299, 192, 288, 228, 62, 108, 200, 27, 77, 287, 228, 200, 108, 62, 27, 32, 292, 2, 219, 241, 203, 241, 299, 108, 294, 108, 290, 241, 347, 173, 173, 173, 42, 264, 32, 10, 241, 283, 108, 294, 82, 300, 241, 248, 82, 185, 82, 114, 173, 173, 32, 289, 192, 297, 32, 276, 192, 142, 241, 276, 108, 228, 347, 108, 4, 265, 27, 173, 32, 282, 192, 142, 241, 282, 108, 228, 4, 265, 108, 266, 27, 173, 32, 283, 192, 142, 241, 283, 108, 228, 347, 108, 266, 27, 173, 32, 287, 192, 287, 74, 266, 32, 78, 32, 301, 192, 289, 74, 50, 241, 291, 173, 32, 10, 241, 285, 108, 292, 173, 32, 10, 241, 284, 108, 301, 228, 62, 108, 200, 27, 173, 32, 3, 32]}, {"code": "def linear_xent_mini_bwd_prologue_kernel(\n    z_nv_ptr,\n    y_ptr,\n    lse_ptr,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1)\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n    lse = tl.load(lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE))\n    z_j_to_k = tl.load(z_block_ptr)\n\n    mask = y[:, None] == v_range[None, :]\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N\n\n    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 63, 6, 108, 252, 63, 6, 108, 253, 63, 6, 108, 254, 63, 6, 108, 255, 63, 6, 173, 63, 32, -1, 256, 192, 169, 241, 257, 192, 346, 173, 32, 258, 192, 169, 241, 257, 192, 347, 173, 32, 259, 192, 217, 241, 260, 192, 245, 108, 123, 192, 241, 251, 108, 252, 173, 108, 261, 192, 241, 248, 108, 249, 173, 108, 262, 192, 241, 256, 242, 255, 108, 258, 242, 254, 173, 108, 263, 192, 241, 255, 108, 254, 173, 108, 264, 192, 241, 347, 108, 346, 173, 173, 32, 265, 192, 250, 242, 251, 74, 256, 242, 255, 74, 76, 241, 346, 108, 255, 173, 32, 266, 192, 258, 242, 254, 74, 76, 241, 346, 108, 254, 173, 32, 267, 192, 57, 241, 246, 74, 265, 173, 32, 268, 192, 57, 241, 247, 74, 256, 242, 255, 74, 76, 241, 346, 108, 255, 173, 173, 32, 269, 192, 57, 241, 259, 173, 32, 270, 192, 267, 228, 63, 108, 200, 27, 77, 266, 228, 200, 108, 63, 27, 32, 271, 192, 241, 269, 4, 268, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 272, 192, 241, 271, 4, 203, 241, 270, 108, 347, 108, 346, 173, 173, 42, 253, 32, 10, 241, 259, 108, 272, 82, 273, 241, 245, 82, 185, 82, 114, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    stride_lse_N,\n    stride_lse_B,\n    stride_loss_Nb,\n    stride_loss_B,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n    V_TILES: tl.constexpr = 1,\n    GROUP_SIZE: tl.constexpr = 1,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)\n    idx_N, idx_V_group = tl.swizzle2d(\n        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE\n    )\n\n    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_ptr,\n        shape=(N_group, V // 128),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),\n        block_shape=(N_BLOCK_SIZE, 1),\n        order=(1, 0),\n    )\n    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V_TILES):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == V_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n        m = m_new\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        V_range = V_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n\n    tl.store(loss_val_ptr, loss)\n    tl.store(lse_row_ptr, lse[:, None])", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 108, 266, 62, 6, 192, 346, 347, 108, 267, 62, 6, 192, 346, 347, 108, 268, 62, 6, 192, 346, 347, 108, 269, 62, 6, 192, 346, 108, 270, 62, 6, 192, 346, 173, 62, 32, -1, 271, 192, 169, 241, 272, 192, 348, 173, 32, 273, 192, 169, 241, 272, 192, 346, 173, 32, 274, 108, 275, 192, 241, 138, 241, 348, 173, 108, 138, 241, 346, 173, 173, 32, 271, 108, 273, 192, 69, 241, 271, 108, 273, 108, 274, 108, 275, 108, 270, 173, 32, 276, 62, 6, 192, 269, 242, 266, 32, 277, 192, 217, 241, 278, 192, 245, 108, 123, 192, 241, 264, 108, 265, 173, 108, 279, 192, 241, 251, 108, 252, 173, 108, 280, 192, 241, 261, 242, 262, 74, 271, 242, 267, 108, 348, 173, 108, 281, 192, 241, 267, 108, 268, 173, 108, 282, 192, 241, 346, 108, 348, 173, 173, 32, 283, 192, 217, 241, 278, 192, 247, 108, 123, 192, 241, 265, 108, 263, 173, 108, 279, 192, 241, 253, 108, 254, 173, 108, 280, 192, 241, 348, 108, 273, 242, 276, 173, 108, 281, 192, 241, 268, 108, 266, 173, 108, 282, 192, 241, 346, 108, 348, 173, 173, 32, 284, 192, 217, 241, 278, 192, 248, 108, 123, 192, 241, 262, 108, 263, 173, 108, 279, 192, 241, 255, 108, 256, 173, 108, 280, 192, 241, 271, 242, 267, 108, 273, 242, 276, 173, 108, 281, 192, 241, 267, 108, 266, 173, 108, 282, 192, 241, 346, 108, 348, 173, 173, 32, 285, 192, 217, 241, 278, 192, 250, 108, 123, 192, 241, 262, 108, 263, 48, 346, 349, 350, 173, 108, 279, 192, 241, 257, 108, 258, 173, 108, 280, 192, 241, 271, 242, 267, 108, 273, 173, 108, 281, 192, 241, 267, 108, 346, 173, 108, 282, 192, 241, 346, 108, 348, 173, 173, 32, 286, 192, 249, 74, 271, 242, 259, 74, 273, 242, 260, 32, 287, 192, 261, 242, 262, 74, 271, 242, 267, 74, 76, 241, 348, 108, 267, 173, 32, 288, 192, 273, 242, 276, 74, 76, 241, 348, 108, 266, 173, 32, 289, 192, 55, 241, 246, 74, 287, 173, 32, 290, 192, 174, 241, 241, 267, 108, 173, 108, 91, 192, 144, 173, 4, 291, 241, 351, 173, 32, 292, 192, 174, 241, 241, 267, 108, 173, 108, 91, 192, 144, 173, 32, 293, 192, 348, 32, 135, 294, 156, 5, 241, 269, 173, 62, 32, 295, 192, 174, 241, 241, 267, 108, 266, 173, 108, 91, 192, 144, 173, 32, 135, 294, 156, 5, 241, 265, 48, 268, 173, 62, 32, 296, 192, 55, 241, 277, 173, 32, 297, 192, 55, 241, 283, 173, 32, 295, 192, 15, 241, 296, 108, 297, 108, 295, 173, 32, 277, 192, 142, 241, 277, 108, 228, 348, 108, 268, 27, 173, 32, 283, 192, 142, 241, 283, 108, 228, 268, 108, 348, 27, 173, 32, 78, 32, 298, 192, 190, 241, 290, 108, 12, 241, 295, 108, 346, 173, 173, 32, 299, 192, 219, 241, 107, 241, 295, 4, 298, 228, 62, 108, 200, 27, 173, 108, 272, 192, 346, 173, 32, 292, 192, 292, 242, 107, 241, 290, 4, 298, 173, 74, 299, 32, 300, 192, 289, 228, 62, 108, 200, 27, 77, 288, 228, 200, 108, 62, 27, 32, 293, 2, 219, 241, 203, 241, 300, 108, 295, 108, 291, 241, 348, 173, 173, 173, 42, 264, 32, 10, 241, 284, 108, 295, 82, 301, 241, 248, 82, 185, 82, 114, 173, 173, 32, 290, 192, 298, 32, 277, 192, 142, 241, 277, 108, 228, 348, 108, 4, 265, 27, 173, 32, 283, 192, 142, 241, 283, 108, 228, 4, 265, 108, 266, 27, 173, 32, 284, 192, 142, 241, 284, 108, 228, 348, 108, 266, 27, 173, 32, 288, 192, 288, 74, 266, 32, 78, 32, 302, 192, 290, 74, 50, 241, 292, 173, 32, 10, 241, 286, 108, 293, 173, 32, 10, 241, 285, 108, 302, 228, 62, 108, 200, 27, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(\n    z_nv_ptr,\n    y_ptr,\n    A_t_ptr,\n    x_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n    GROUP_SIZE: tl.constexpr = 1,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_H = tl.program_id(axis=1)\n    idx_V = 0\n\n    num_idx_N, num_idx_H = tl.num_programs(0), tl.num_programs(1)\n    idx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)\n\n    x_grad_block_ptr = tl.make_block_ptr(\n        base=x_grad_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_t_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(0, 1),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = 0 + tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_range)\n    lse = tl.load(lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE))\n\n    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)\n    for _ in range(V // V_BLOCK_SIZE):\n        mask = y[:, None] == v_range[None, :]\n        A_v = tl.load(A_t_block_ptr)\n        z_j_to_k = tl.load(z_block_ptr)\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(tl.float16)\n\n        x_grad_acc = tl.dot(z_grad, A_v.trans(), x_grad_acc)\n\n        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        v_range += V_BLOCK_SIZE\n\n    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 63, 6, 108, 258, 63, 6, 108, 259, 63, 6, 108, 260, 63, 6, 108, 261, 63, 6, 192, 346, 347, 108, 262, 63, 6, 192, 346, 347, 108, 263, 63, 6, 192, 346, 347, 108, 264, 63, 6, 192, 346, 173, 63, 32, -1, 265, 192, 169, 241, 266, 192, 348, 173, 32, 267, 192, 169, 241, 266, 192, 346, 173, 32, 268, 192, 348, 32, 269, 108, 270, 192, 241, 138, 241, 348, 173, 108, 138, 241, 346, 173, 173, 32, 265, 108, 267, 192, 244, 241, 265, 108, 267, 108, 269, 108, 270, 108, 264, 173, 32, 271, 192, 217, 241, 272, 192, 248, 108, 123, 192, 241, 259, 108, 260, 173, 108, 273, 192, 241, 250, 108, 251, 173, 108, 274, 192, 241, 256, 242, 257, 74, 265, 242, 262, 108, 267, 242, 263, 173, 108, 275, 192, 241, 262, 108, 263, 173, 108, 276, 192, 241, 346, 108, 348, 173, 173, 32, 277, 192, 217, 241, 272, 192, 247, 108, 123, 192, 241, 260, 108, 258, 173, 108, 273, 192, 241, 252, 108, 253, 173, 108, 274, 192, 241, 267, 242, 263, 108, 348, 173, 108, 275, 192, 241, 263, 108, 261, 173, 108, 276, 192, 241, 348, 108, 346, 173, 173, 32, 278, 192, 217, 241, 272, 192, 245, 108, 123, 192, 241, 257, 108, 258, 173, 108, 273, 192, 241, 254, 108, 255, 173, 108, 274, 192, 241, 265, 242, 262, 108, 268, 242, 261, 173, 108, 275, 192, 241, 262, 108, 261, 173, 108, 276, 192, 241, 346, 108, 348, 173, 173, 32, 279, 192, 256, 242, 257, 74, 265, 242, 262, 74, 76, 241, 348, 108, 262, 173, 32, 280, 192, 348, 74, 76, 241, 348, 108, 261, 173, 32, 281, 192, 57, 241, 246, 74, 279, 173, 32, 282, 192, 57, 241, 249, 74, 265, 242, 262, 74, 76, 241, 348, 108, 262, 173, 173, 32, 283, 192, 174, 241, 241, 262, 108, 263, 173, 108, 144, 173, 32, 135, 284, 156, 5, 241, 258, 48, 261, 173, 63, 32, 285, 192, 281, 228, 63, 108, 200, 27, 77, 280, 228, 200, 108, 63, 27, 32, 286, 192, 57, 241, 277, 173, 32, 287, 192, 57, 241, 278, 173, 32, 288, 192, 241, 287, 4, 282, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 32, 289, 192, 241, 288, 4, 203, 241, 285, 108, 346, 108, 348, 173, 173, 82, 290, 241, 21, 173, 32, 283, 192, 15, 241, 289, 108, 286, 82, 291, 241, 173, 108, 283, 173, 32, 277, 192, 142, 241, 277, 108, 228, 348, 108, 261, 27, 173, 32, 278, 192, 142, 241, 278, 108, 228, 348, 108, 261, 27, 173, 32, 280, 170, 261, 32, 78, 32, 10, 241, 271, 108, 241, 283, 42, 259, 173, 82, 290, 241, 248, 82, 185, 82, 114, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(\n    z_nv_ptr,\n    y_ptr,\n    x_ptr,\n    A_grad_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n    GROUP_SIZE: tl.constexpr = 16,\n):\n    idx_V = tl.program_id(axis=0)\n    idx_H = tl.program_id(axis=1)\n\n    num_idx_V, num_idx_H = tl.num_programs(0), tl.num_programs(1)\n    idx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group, idx_H * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_grad_T_block_ptr = tl.make_block_ptr(\n        base=A_grad_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(0, 1),\n    )\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    N_range = tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), tl.float32)\n    for _ in range(N_group // N_BLOCK_SIZE):\n        y = tl.load(y_ptr + idx_N_group * N_group + N_range)\n        lse = tl.load(lse_ptr + N_range)\n        mask = y[:, None] == V_range[None, :]\n\n        x_chunk = tl.load(x_block_ptr)\n        z_j_to_k = tl.load(z_block_ptr)\n        softmax_z = (z_j_to_k - lse[:, None]).exp()\n        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(tl.float16)\n\n        A_grad_acc = tl.dot(x_chunk.trans(), z_grad, A_grad_acc)\n\n        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n        N_range += N_BLOCK_SIZE\n    tl.store(\n        A_grad_T_block_ptr,\n        tl.load(A_grad_T_block_ptr) + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),\n    )", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 62, 6, 108, 258, 62, 6, 108, 259, 62, 6, 108, 260, 62, 6, 108, 261, 62, 6, 192, 346, 347, 108, 262, 62, 6, 192, 346, 347, 108, 263, 62, 6, 192, 346, 347, 108, 264, 62, 6, 192, 346, 347, 173, 62, 32, -1, 265, 192, 169, 241, 266, 192, 348, 173, 32, 267, 192, 169, 241, 266, 192, 346, 173, 32, 268, 108, 269, 192, 241, 138, 241, 348, 173, 108, 138, 241, 346, 173, 173, 32, 265, 108, 267, 192, 69, 241, 265, 108, 267, 108, 268, 108, 269, 108, 264, 173, 32, 270, 192, 217, 241, 271, 192, 247, 108, 123, 192, 241, 259, 108, 260, 173, 108, 272, 192, 241, 250, 108, 251, 173, 108, 273, 192, 241, 256, 242, 257, 108, 267, 242, 263, 173, 108, 274, 192, 241, 262, 108, 263, 173, 108, 275, 192, 241, 346, 108, 348, 173, 173, 32, 276, 192, 217, 241, 271, 192, 248, 108, 123, 192, 241, 260, 108, 258, 173, 108, 272, 192, 241, 252, 108, 253, 173, 108, 273, 192, 241, 267, 242, 263, 108, 265, 242, 261, 173, 108, 274, 192, 241, 263, 108, 261, 173, 108, 275, 192, 241, 348, 108, 346, 173, 173, 32, 277, 192, 217, 241, 271, 192, 245, 108, 123, 192, 241, 257, 108, 258, 173, 108, 272, 192, 241, 254, 108, 255, 173, 108, 273, 192, 241, 348, 108, 265, 242, 261, 173, 108, 274, 192, 241, 262, 108, 261, 173, 108, 275, 192, 241, 346, 108, 348, 173, 173, 32, 278, 192, 76, 241, 348, 108, 262, 173, 32, 279, 192, 265, 242, 261, 74, 76, 241, 348, 108, 261, 173, 32, 280, 192, 174, 241, 241, 263, 108, 261, 173, 108, 144, 173, 32, 135, 281, 156, 5, 241, 257, 48, 262, 173, 62, 32, 282, 192, 55, 241, 246, 74, 256, 242, 257, 74, 278, 173, 32, 283, 192, 55, 241, 249, 74, 278, 173, 32, 284, 192, 282, 228, 62, 108, 200, 27, 77, 279, 228, 200, 108, 62, 27, 32, 285, 192, 55, 241, 270, 173, 32, 286, 192, 55, 241, 277, 173, 32, 287, 192, 241, 286, 4, 283, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 32, 288, 192, 241, 287, 4, 203, 241, 284, 108, 346, 108, 348, 173, 173, 82, 289, 241, 21, 173, 32, 280, 192, 15, 241, 285, 82, 290, 241, 173, 108, 288, 108, 280, 173, 32, 270, 192, 142, 241, 270, 108, 228, 262, 108, 348, 27, 173, 32, 277, 192, 142, 241, 277, 108, 228, 262, 108, 348, 27, 173, 32, 278, 170, 262, 32, 78, 32, 10, 241, 276, 108, 55, 241, 276, 173, 74, 241, 280, 42, 259, 173, 82, 289, 241, 248, 82, 185, 82, 114, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    stride_lse_N,\n    stride_lse_B,\n    stride_loss_Nb,\n    stride_loss_B,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n    V_TILES: tl.constexpr = 1,\n    GROUP_SIZE: tl.constexpr = 1,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)\n\n    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_ptr,\n        shape=(N_group, V // 16),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),\n        block_shape=(N_BLOCK_SIZE, 1),\n        order=(1, 0),\n    )\n    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V_TILES):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == V_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n        m = m_new\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        V_range = V_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n\n    tl.store(loss_val_ptr, loss)\n    tl.store(lse_row_ptr, lse[:, None])", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 108, 265, 63, 6, 108, 266, 63, 6, 192, 346, 347, 108, 267, 63, 6, 192, 346, 347, 108, 268, 63, 6, 192, 346, 347, 108, 269, 63, 6, 192, 346, 108, 270, 63, 6, 192, 346, 173, 63, 32, -1, 271, 192, 169, 241, 272, 192, 348, 173, 32, 273, 192, 169, 241, 272, 192, 346, 173, 32, 274, 108, 275, 192, 241, 138, 241, 348, 173, 108, 138, 241, 346, 173, 173, 32, 276, 63, 6, 192, 269, 242, 266, 32, 277, 192, 217, 241, 278, 192, 245, 108, 123, 192, 241, 264, 108, 265, 173, 108, 279, 192, 241, 251, 108, 252, 173, 108, 280, 192, 241, 261, 242, 262, 74, 271, 242, 267, 108, 348, 173, 108, 281, 192, 241, 267, 108, 268, 173, 108, 282, 192, 241, 346, 108, 348, 173, 173, 32, 283, 192, 217, 241, 278, 192, 247, 108, 123, 192, 241, 265, 108, 263, 173, 108, 279, 192, 241, 253, 108, 254, 173, 108, 280, 192, 241, 348, 108, 273, 242, 276, 173, 108, 281, 192, 241, 268, 108, 266, 173, 108, 282, 192, 241, 346, 108, 348, 173, 173, 32, 284, 192, 217, 241, 278, 192, 248, 108, 123, 192, 241, 262, 108, 263, 173, 108, 279, 192, 241, 255, 108, 256, 173, 108, 280, 192, 241, 271, 242, 267, 108, 273, 242, 276, 173, 108, 281, 192, 241, 267, 108, 266, 173, 108, 282, 192, 241, 346, 108, 348, 173, 173, 32, 285, 192, 217, 241, 278, 192, 250, 108, 123, 192, 241, 262, 108, 263, 48, 346, 347, 173, 108, 279, 192, 241, 257, 108, 258, 173, 108, 280, 192, 241, 271, 242, 267, 108, 273, 173, 108, 281, 192, 241, 267, 108, 346, 173, 108, 282, 192, 241, 346, 108, 348, 173, 173, 32, 286, 192, 249, 74, 271, 242, 259, 74, 273, 242, 260, 32, 287, 192, 261, 242, 262, 74, 271, 242, 267, 74, 76, 241, 348, 108, 267, 173, 32, 288, 192, 273, 242, 276, 74, 76, 241, 348, 108, 266, 173, 32, 289, 192, 57, 241, 246, 74, 287, 173, 32, 290, 192, 174, 241, 241, 267, 108, 173, 108, 91, 192, 144, 173, 4, 291, 241, 349, 173, 32, 292, 192, 174, 241, 241, 267, 108, 173, 108, 91, 192, 144, 173, 32, 293, 192, 348, 32, 135, 294, 156, 5, 241, 269, 173, 63, 32, 295, 192, 174, 241, 241, 267, 108, 266, 173, 108, 91, 192, 144, 173, 32, 135, 294, 156, 5, 241, 265, 48, 268, 173, 63, 32, 296, 192, 57, 241, 277, 173, 32, 297, 192, 57, 241, 283, 173, 32, 295, 192, 15, 241, 296, 108, 297, 108, 295, 173, 32, 277, 192, 142, 241, 277, 108, 228, 348, 108, 268, 27, 173, 32, 283, 192, 142, 241, 283, 108, 228, 268, 108, 348, 27, 173, 32, 78, 32, 298, 192, 190, 241, 290, 108, 12, 241, 295, 108, 346, 173, 173, 32, 299, 192, 219, 241, 107, 241, 295, 4, 298, 228, 63, 108, 200, 27, 173, 108, 272, 192, 346, 173, 32, 292, 192, 292, 242, 107, 241, 290, 4, 298, 173, 74, 299, 32, 300, 192, 289, 228, 63, 108, 200, 27, 77, 288, 228, 200, 108, 63, 27, 32, 293, 2, 219, 241, 203, 241, 300, 108, 295, 108, 291, 241, 348, 173, 173, 173, 42, 264, 32, 10, 241, 284, 108, 295, 82, 301, 241, 248, 82, 185, 82, 114, 173, 173, 32, 290, 192, 298, 32, 277, 192, 142, 241, 277, 108, 228, 348, 108, 4, 265, 27, 173, 32, 283, 192, 142, 241, 283, 108, 228, 4, 265, 108, 266, 27, 173, 32, 284, 192, 142, 241, 284, 108, 228, 348, 108, 266, 27, 173, 32, 288, 192, 288, 74, 266, 32, 78, 32, 302, 192, 290, 74, 50, 241, 292, 173, 32, 10, 241, 286, 108, 293, 173, 32, 10, 241, 285, 108, 302, 228, 63, 108, 200, 27, 173, 32, 3, 32]}, {"code": "def linear_xent_mini_bwd_prologue_kernel(\n    z_nv_ptr,\n    y_ptr,\n    lse_ptr,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1)\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n    lse = tl.load(lse_ptr + N_range)\n    z_j_to_k = tl.load(z_block_ptr)\n\n    mask = y[:, None] == v_range[None, :]\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N\n\n    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 62, 6, 108, 252, 62, 6, 108, 253, 62, 6, 108, 254, 62, 6, 108, 255, 62, 6, 173, 62, 32, -1, 256, 192, 169, 241, 257, 192, 346, 173, 32, 258, 192, 169, 241, 257, 192, 347, 173, 32, 259, 192, 217, 241, 260, 192, 245, 108, 123, 192, 241, 251, 108, 252, 173, 108, 261, 192, 241, 248, 108, 249, 173, 108, 262, 192, 241, 256, 242, 255, 108, 258, 242, 254, 173, 108, 263, 192, 241, 255, 108, 254, 173, 108, 264, 192, 241, 347, 108, 346, 173, 173, 32, 265, 192, 250, 242, 251, 74, 256, 242, 255, 74, 76, 241, 346, 108, 255, 173, 32, 266, 192, 258, 242, 254, 74, 76, 241, 346, 108, 254, 173, 32, 267, 192, 55, 241, 246, 74, 265, 173, 32, 268, 192, 55, 241, 247, 74, 265, 173, 32, 269, 192, 55, 241, 259, 173, 32, 270, 192, 267, 228, 62, 108, 200, 27, 77, 266, 228, 200, 108, 62, 27, 32, 271, 192, 241, 269, 4, 268, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 32, 272, 192, 241, 271, 4, 203, 241, 270, 108, 347, 108, 346, 173, 173, 42, 253, 32, 10, 241, 259, 108, 272, 82, 273, 241, 245, 82, 185, 82, 114, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    stride_lse_N,\n    stride_lse_B,\n    stride_loss_Nb,\n    stride_loss_B,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    V_TILES: tl.constexpr = 1,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n\n    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    lse_row_ptr = tl.make_block_ptr(\n        base=lse_ptr,\n        shape=(N, V // 64),\n        strides=(stride_lse_N, stride_lse_B),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_V_group),\n        block_shape=(N_BLOCK_SIZE, 1),\n        order=(1, 0),\n    )\n    loss_val_ptr = (\n        losses_ptr\n        + (idx_N + idx_N_group * N_group // N_BLOCK_SIZE) * stride_loss_Nb\n        + idx_V_group * stride_loss_B\n    )\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V_TILES):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == V_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n        m = m_new\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        V_range = V_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n\n    tl.store(loss_val_ptr, loss)\n    tl.store(lse_row_ptr, lse[:, None])", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 108, 265, 63, 6, 108, 266, 63, 6, 108, 267, 63, 6, 108, 268, 63, 6, 108, 269, 63, 6, 192, 346, 173, 63, 32, -1, 270, 192, 169, 241, 271, 192, 347, 173, 32, 272, 192, 169, 241, 271, 192, 346, 173, 32, 273, 63, 6, 192, 269, 242, 266, 32, 274, 192, 217, 241, 275, 192, 245, 108, 123, 192, 241, 264, 108, 265, 173, 108, 276, 192, 241, 251, 108, 252, 173, 108, 277, 192, 241, 261, 242, 262, 74, 270, 242, 267, 108, 347, 173, 108, 278, 192, 241, 267, 108, 268, 173, 108, 279, 192, 241, 346, 108, 347, 173, 173, 32, 280, 192, 217, 241, 275, 192, 247, 108, 123, 192, 241, 265, 108, 263, 173, 108, 276, 192, 241, 253, 108, 254, 173, 108, 277, 192, 241, 347, 108, 272, 242, 273, 173, 108, 278, 192, 241, 268, 108, 266, 173, 108, 279, 192, 241, 346, 108, 347, 173, 173, 32, 281, 192, 217, 241, 275, 192, 248, 108, 123, 192, 241, 262, 108, 263, 173, 108, 276, 192, 241, 255, 108, 256, 173, 108, 277, 192, 241, 270, 242, 267, 108, 272, 242, 273, 173, 108, 278, 192, 241, 267, 108, 266, 173, 108, 279, 192, 241, 346, 108, 347, 173, 173, 32, 282, 192, 217, 241, 275, 192, 250, 108, 123, 192, 241, 264, 108, 263, 48, 348, 349, 173, 108, 276, 192, 241, 257, 108, 258, 173, 108, 277, 192, 241, 261, 242, 262, 74, 270, 242, 267, 108, 272, 173, 108, 278, 192, 241, 267, 108, 346, 173, 108, 279, 192, 241, 346, 108, 347, 173, 173, 32, 283, 192, 249, 74, 241, 270, 74, 261, 242, 262, 48, 267, 173, 242, 259, 74, 272, 242, 260, 32, 284, 192, 261, 242, 262, 74, 270, 242, 267, 74, 76, 241, 347, 108, 267, 173, 32, 285, 192, 272, 242, 273, 74, 76, 241, 347, 108, 266, 173, 32, 286, 192, 57, 241, 246, 74, 284, 173, 32, 287, 192, 174, 241, 241, 267, 108, 173, 108, 91, 192, 144, 173, 4, 288, 241, 350, 173, 32, 289, 192, 174, 241, 241, 267, 108, 173, 108, 91, 192, 144, 173, 32, 290, 192, 347, 32, 135, 291, 156, 5, 241, 269, 173, 63, 32, 292, 192, 174, 241, 241, 267, 108, 266, 173, 108, 91, 192, 144, 173, 32, 135, 291, 156, 5, 241, 265, 48, 268, 173, 63, 32, 293, 192, 57, 241, 274, 173, 32, 294, 192, 57, 241, 280, 173, 32, 292, 192, 15, 241, 293, 108, 294, 108, 292, 173, 32, 274, 192, 142, 241, 274, 108, 228, 347, 108, 268, 27, 173, 32, 280, 192, 142, 241, 280, 108, 228, 268, 108, 347, 27, 173, 32, 78, 32, 295, 192, 190, 241, 287, 108, 12, 241, 292, 108, 346, 173, 173, 32, 296, 192, 219, 241, 107, 241, 292, 4, 295, 228, 63, 108, 200, 27, 173, 108, 271, 192, 346, 173, 32, 289, 192, 289, 242, 107, 241, 287, 4, 295, 173, 74, 296, 32, 297, 192, 286, 228, 63, 108, 200, 27, 77, 285, 228, 200, 108, 63, 27, 32, 290, 2, 219, 241, 203, 241, 297, 108, 292, 108, 288, 241, 347, 173, 173, 173, 42, 264, 32, 10, 241, 281, 108, 292, 82, 298, 241, 248, 82, 185, 82, 114, 173, 173, 32, 287, 192, 295, 32, 274, 192, 142, 241, 274, 108, 228, 347, 108, 4, 265, 27, 173, 32, 280, 192, 142, 241, 280, 108, 228, 4, 265, 108, 266, 27, 173, 32, 281, 192, 142, 241, 281, 108, 228, 347, 108, 266, 27, 173, 32, 285, 192, 285, 74, 266, 32, 78, 32, 299, 192, 287, 74, 50, 241, 289, 173, 32, 10, 241, 283, 108, 290, 173, 32, 10, 241, 282, 108, 299, 228, 63, 108, 200, 27, 173, 32, 3, 32]}, {"code": "def linear_xent_mini_bwd_prologue_kernel(\n    z_nv_ptr,\n    y_ptr,\n    lse_ptr,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1)\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n    lse = tl.load(lse_ptr + N_range)\n    z_j_to_k = tl.load(z_block_ptr)\n\n    mask = y[:, None] == v_range[None, :]\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N\n\n    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 62, 6, 108, 252, 62, 6, 108, 253, 62, 6, 108, 254, 62, 6, 108, 255, 62, 6, 173, 62, 32, -1, 256, 192, 169, 241, 257, 192, 346, 173, 32, 258, 192, 169, 241, 257, 192, 347, 173, 32, 259, 192, 217, 241, 260, 192, 245, 108, 123, 192, 241, 251, 108, 252, 173, 108, 261, 192, 241, 248, 108, 249, 173, 108, 262, 192, 241, 256, 242, 255, 108, 258, 242, 254, 173, 108, 263, 192, 241, 255, 108, 254, 173, 108, 264, 192, 241, 347, 108, 346, 173, 173, 32, 265, 192, 250, 242, 251, 74, 256, 242, 255, 74, 76, 241, 346, 108, 255, 173, 32, 266, 192, 258, 242, 254, 74, 76, 241, 346, 108, 254, 173, 32, 267, 192, 55, 241, 246, 74, 265, 173, 32, 268, 192, 55, 241, 247, 74, 265, 173, 32, 269, 192, 55, 241, 259, 173, 32, 270, 192, 267, 228, 62, 108, 200, 27, 77, 266, 228, 200, 108, 62, 27, 32, 271, 192, 241, 269, 4, 268, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 32, 272, 192, 241, 271, 4, 203, 241, 270, 108, 347, 108, 346, 173, 173, 42, 253, 32, 10, 241, 259, 108, 272, 82, 273, 241, 245, 82, 185, 82, 114, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    z_nv_ptr,\n    losses_ptr,\n    sumexp_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n    V_TILES: tl.constexpr = 4,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n\n    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    sumexp_row_ptr = sumexp_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V_TILES):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == V_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n\n        m = m_new\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        V_range = V_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n    sum_exp = tl.exp(lse).to(sumexp_ptr.type.element_ty)\n\n    tl.atomic_add(losses_ptr + idx_N, loss)\n    tl.atomic_add(sumexp_row_ptr, sum_exp)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 63, 6, 108, 259, 63, 6, 108, 260, 63, 6, 108, 261, 63, 6, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 108, 265, 63, 6, 192, 346, 173, 63, 32, -1, 266, 192, 169, 241, 267, 192, 347, 173, 32, 268, 192, 169, 241, 267, 192, 348, 173, 32, 269, 63, 6, 192, 265, 242, 262, 32, 270, 192, 217, 241, 271, 192, 245, 108, 123, 192, 241, 260, 108, 261, 173, 108, 272, 192, 241, 251, 108, 252, 173, 108, 273, 192, 241, 257, 242, 258, 74, 266, 242, 263, 108, 347, 173, 108, 274, 192, 241, 263, 108, 264, 173, 108, 275, 192, 241, 348, 108, 347, 173, 173, 32, 276, 192, 217, 241, 271, 192, 247, 108, 123, 192, 241, 261, 108, 259, 173, 108, 272, 192, 241, 253, 108, 254, 173, 108, 273, 192, 241, 347, 108, 268, 242, 269, 173, 108, 274, 192, 241, 264, 108, 262, 173, 108, 275, 192, 241, 348, 108, 347, 173, 173, 32, 277, 192, 217, 241, 271, 192, 248, 108, 123, 192, 241, 258, 108, 259, 173, 108, 272, 192, 241, 255, 108, 256, 173, 108, 273, 192, 241, 266, 242, 263, 108, 268, 242, 269, 173, 108, 274, 192, 241, 263, 108, 262, 173, 108, 275, 192, 241, 348, 108, 347, 173, 173, 32, 278, 192, 250, 74, 266, 242, 263, 74, 76, 241, 347, 108, 263, 173, 32, 279, 192, 257, 242, 258, 74, 266, 242, 263, 74, 76, 241, 347, 108, 263, 173, 32, 280, 192, 268, 242, 269, 74, 76, 241, 347, 108, 262, 173, 32, 281, 192, 57, 241, 246, 74, 279, 173, 32, 282, 192, 174, 241, 241, 263, 108, 173, 108, 91, 192, 144, 173, 4, 283, 241, 349, 173, 32, 284, 192, 174, 241, 241, 263, 108, 173, 108, 91, 192, 144, 173, 32, 285, 192, 347, 32, 135, 286, 156, 5, 241, 265, 173, 63, 32, 287, 192, 174, 241, 241, 263, 108, 262, 173, 108, 91, 192, 144, 173, 32, 135, 286, 156, 5, 241, 261, 48, 264, 173, 63, 32, 288, 192, 57, 241, 270, 173, 32, 289, 192, 57, 241, 276, 173, 32, 287, 192, 15, 241, 288, 108, 289, 108, 287, 173, 32, 270, 192, 142, 241, 270, 108, 228, 347, 108, 264, 27, 173, 32, 276, 192, 142, 241, 276, 108, 228, 264, 108, 347, 27, 173, 32, 78, 32, 290, 192, 190, 241, 282, 108, 12, 241, 287, 108, 348, 173, 173, 32, 291, 192, 219, 241, 107, 241, 287, 4, 290, 228, 63, 108, 200, 27, 173, 108, 267, 192, 348, 173, 32, 284, 192, 284, 242, 107, 241, 282, 4, 290, 173, 74, 291, 32, 292, 192, 281, 228, 63, 108, 200, 27, 77, 280, 228, 200, 108, 63, 27, 32, 285, 2, 219, 241, 203, 241, 292, 108, 287, 108, 283, 241, 347, 173, 173, 173, 42, 260, 32, 10, 241, 277, 108, 287, 82, 293, 241, 248, 82, 185, 82, 114, 173, 173, 32, 282, 192, 290, 32, 270, 192, 142, 241, 270, 108, 228, 347, 108, 4, 261, 27, 173, 32, 276, 192, 142, 241, 276, 108, 228, 4, 261, 108, 262, 27, 173, 32, 277, 192, 142, 241, 277, 108, 228, 347, 108, 262, 27, 173, 32, 280, 192, 280, 74, 262, 32, 78, 32, 294, 192, 282, 74, 50, 241, 284, 173, 32, 295, 192, 107, 241, 294, 173, 82, 293, 241, 250, 82, 185, 82, 114, 173, 32, 194, 241, 249, 74, 266, 108, 285, 173, 32, 194, 241, 278, 108, 295, 173, 32, 3, 32]}, {"code": "def linear_xent_mini_bwd_prologue_kernel(\n    z_nv_ptr,\n    y_ptr,\n    sumexp_ptr,\n    stride_z_N,\n    stride_z_V,\n    idx_N_group,\n    N_group: tl.constexpr,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1)\n\n    z_block_ptr = tl.make_block_ptr(\n        base=z_nv_ptr,\n        shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    N_range = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + idx_N_group * N_group + N_range)\n    lse = tl.log(tl.load(sumexp_ptr + N_range))\n    z_j_to_k = tl.load(z_block_ptr)\n\n    mask = y[:, None] == v_range[None, :]\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N\n\n    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 62, 6, 108, 252, 62, 6, 108, 253, 62, 6, 108, 254, 62, 6, 108, 255, 62, 6, 173, 62, 32, -1, 256, 192, 169, 241, 257, 192, 346, 173, 32, 258, 192, 169, 241, 257, 192, 347, 173, 32, 259, 192, 217, 241, 260, 192, 245, 108, 123, 192, 241, 251, 108, 252, 173, 108, 261, 192, 241, 248, 108, 249, 173, 108, 262, 192, 241, 256, 242, 255, 108, 258, 242, 254, 173, 108, 263, 192, 241, 255, 108, 254, 173, 108, 264, 192, 241, 347, 108, 346, 173, 173, 32, 265, 192, 256, 242, 255, 74, 76, 241, 346, 108, 255, 173, 32, 266, 192, 258, 242, 254, 74, 76, 241, 346, 108, 254, 173, 32, 267, 192, 55, 241, 246, 74, 250, 242, 251, 74, 265, 173, 32, 268, 192, 50, 241, 55, 241, 247, 74, 265, 173, 173, 32, 269, 192, 55, 241, 259, 173, 32, 270, 192, 267, 228, 62, 108, 200, 27, 77, 266, 228, 200, 108, 62, 27, 32, 271, 192, 241, 269, 4, 268, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 32, 272, 192, 241, 271, 4, 203, 241, 270, 108, 347, 108, 346, 173, 173, 42, 253, 32, 10, 241, 259, 108, 272, 82, 273, 241, 245, 82, 185, 82, 114, 173, 173, 32, 3, 32]}, {"code": "def linear_xent_fwd_kernel_matmul_t(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    losses_ptr,\n    lse_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr,\n    N_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr,\n):\n    idx = tl.program_id(axis=0)\n\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, 0),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + offsets)\n\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n\n    for _ in range(V // V_BLOCK_SIZE):\n\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        local_x_block_ptr = x_block_ptr\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(local_x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n\n        mask = y[:, None] == v_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n\n        m = m_new\n        A_block_ptr = tl.advance(\n            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]\n        )\n        v_range = v_range + V_BLOCK_SIZE\n\n    lse = m + tl.log(s)\n    loss += tl.sum(lse) / N\n    tl.store(losses_ptr + idx, loss)\n    tl.store(lse_ptr + offsets, lse)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 63, 6, 108, 255, 63, 6, 108, 256, 63, 6, 108, 257, 63, 6, 108, 258, 63, 6, 108, 259, 63, 6, 173, 63, 32, -1, 260, 192, 169, 241, 261, 192, 346, 173, 32, 83, 241, 255, 224, 258, 77, 346, 173, 32, 83, 241, 254, 224, 257, 77, 346, 173, 32, 83, 241, 256, 224, 259, 77, 346, 173, 32, 262, 192, 217, 241, 263, 192, 245, 108, 123, 192, 241, 255, 108, 256, 173, 108, 264, 192, 241, 250, 108, 251, 173, 108, 265, 192, 241, 260, 242, 258, 108, 346, 173, 108, 266, 192, 241, 258, 108, 259, 173, 108, 267, 192, 241, 347, 108, 346, 173, 173, 32, 268, 192, 217, 241, 263, 192, 247, 108, 123, 192, 241, 256, 108, 254, 173, 108, 264, 192, 241, 252, 108, 253, 173, 108, 265, 192, 241, 346, 108, 346, 173, 108, 266, 192, 241, 259, 108, 257, 173, 108, 267, 192, 241, 347, 108, 346, 173, 173, 32, 265, 192, 260, 242, 258, 74, 76, 241, 346, 108, 258, 173, 32, 269, 192, 76, 241, 346, 108, 257, 173, 32, 270, 192, 57, 241, 246, 74, 265, 173, 32, 271, 192, 174, 241, 241, 258, 108, 173, 108, 91, 192, 144, 173, 4, 272, 241, 348, 173, 32, 273, 192, 174, 241, 241, 258, 108, 173, 108, 91, 192, 144, 173, 32, 274, 192, 346, 32, 135, 275, 156, 5, 241, 254, 48, 257, 173, 63, 32, 276, 192, 174, 241, 241, 258, 108, 257, 173, 108, 91, 192, 144, 173, 32, 277, 192, 262, 32, 135, 275, 156, 5, 241, 256, 48, 259, 173, 63, 32, 278, 192, 57, 241, 277, 173, 32, 279, 192, 57, 241, 268, 173, 32, 276, 192, 15, 241, 278, 108, 279, 108, 276, 173, 32, 277, 192, 142, 241, 277, 108, 228, 346, 108, 259, 27, 173, 32, 268, 192, 142, 241, 268, 108, 228, 259, 108, 346, 27, 173, 32, 78, 32, 280, 192, 190, 241, 271, 108, 12, 241, 276, 108, 347, 173, 173, 32, 281, 192, 219, 241, 107, 241, 276, 4, 280, 228, 63, 108, 200, 27, 173, 108, 261, 192, 347, 173, 32, 273, 192, 273, 242, 107, 241, 271, 4, 280, 173, 74, 281, 32, 282, 192, 270, 228, 63, 108, 200, 27, 77, 269, 228, 200, 108, 63, 27, 32, 274, 2, 219, 241, 203, 241, 282, 108, 276, 108, 272, 241, 346, 173, 173, 173, 42, 255, 32, 271, 192, 280, 32, 268, 192, 142, 241, 268, 108, 228, 4, 259, 242, 241, 256, 48, 259, 173, 108, 257, 27, 173, 32, 269, 192, 269, 74, 257, 32, 78, 32, 283, 192, 271, 74, 50, 241, 273, 173, 32, 274, 170, 219, 241, 283, 173, 42, 255, 32, 10, 241, 248, 74, 260, 108, 274, 173, 32, 10, 241, 249, 74, 265, 108, 283, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_dA(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    A_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_V = tl.program_id(axis=0)\n    idx_H_grad = tl.program_id(axis=1)\n\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n\n    N_offsets = tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n\n    A_fwd_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_grad_block_ptr = tl.make_block_ptr(\n        base=A_grad_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H_grad * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    x_fwd_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(0 * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    x_bwd_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(0 * N_BLOCK_SIZE, idx_H_grad * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    AgradT = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), tl.float16)\n\n    for idx_N in range(N // N_BLOCK_SIZE):\n\n        y = tl.load(y_ptr + N_offsets)\n        lse = tl.load(lse_global_ptr + N_offsets)\n\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_fwd_block_ptr)\n            A_v = tl.load(A_fwd_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n\n            x_fwd_block_ptr = tl.advance(x_fwd_block_ptr, [0, H_BLOCK_SIZE])\n            A_fwd_block_ptr = tl.advance(A_fwd_block_ptr, [H_BLOCK_SIZE, 0])\n\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n\n        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n\n        x_chunk_bwd = tl.load(x_bwd_block_ptr)\n\n        AgradT += (tl.dot(x_chunk_bwd.trans(), softmax_z) / N).to(tl.float16)\n        AgradT -= (\n            tl.sum(tl.where(mask, x_chunk_bwd[:, None, :], 0.0), axis=0).trans() / N\n        ).to(tl.float16)\n\n        x_bwd_block_ptr = tl.advance(x_bwd_block_ptr, [N_BLOCK_SIZE, 0])\n        x_fwd_block_ptr = tl.advance(x_fwd_block_ptr, [N_BLOCK_SIZE, -H])\n        A_fwd_block_ptr = tl.advance(A_fwd_block_ptr, [-H, 0])\n        N_offsets += N_BLOCK_SIZE\n\n    tl.store(A_grad_block_ptr, AgradT)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 62, 6, 108, 255, 62, 6, 108, 256, 62, 6, 108, 257, 62, 6, 192, 346, 347, 108, 258, 62, 6, 192, 346, 347, 108, 259, 62, 6, 192, 346, 347, 173, 62, 32, -1, 260, 192, 169, 241, 261, 192, 348, 173, 32, 262, 192, 169, 241, 261, 192, 346, 173, 32, 83, 241, 255, 224, 258, 77, 348, 173, 32, 83, 241, 254, 224, 257, 77, 348, 173, 32, 83, 241, 256, 224, 259, 77, 348, 173, 32, 263, 192, 76, 241, 348, 108, 258, 173, 32, 264, 192, 260, 242, 257, 74, 76, 241, 348, 108, 257, 173, 32, 265, 192, 217, 241, 266, 192, 247, 108, 123, 192, 241, 256, 108, 254, 173, 108, 267, 192, 241, 252, 108, 253, 173, 108, 268, 192, 241, 348, 108, 260, 242, 257, 173, 108, 269, 192, 241, 259, 108, 257, 173, 108, 270, 192, 241, 346, 108, 348, 173, 173, 32, 271, 192, 217, 241, 266, 192, 249, 108, 123, 192, 241, 256, 108, 254, 173, 108, 267, 192, 241, 252, 108, 253, 173, 108, 268, 192, 241, 262, 242, 259, 108, 260, 242, 257, 173, 108, 269, 192, 241, 259, 108, 257, 173, 108, 270, 192, 241, 346, 108, 348, 173, 173, 32, 272, 192, 217, 241, 266, 192, 245, 108, 123, 192, 241, 255, 108, 256, 173, 108, 267, 192, 241, 250, 108, 251, 173, 108, 268, 192, 241, 348, 242, 258, 108, 348, 173, 108, 269, 192, 241, 258, 108, 259, 173, 108, 270, 192, 241, 346, 108, 348, 173, 173, 32, 273, 192, 217, 241, 266, 192, 245, 108, 123, 192, 241, 255, 108, 256, 173, 108, 267, 192, 241, 250, 108, 251, 173, 108, 268, 192, 241, 348, 242, 258, 108, 262, 242, 259, 173, 108, 269, 192, 241, 258, 108, 259, 173, 108, 270, 192, 241, 346, 108, 348, 173, 173, 32, 274, 192, 174, 241, 241, 259, 108, 257, 173, 108, 21, 173, 32, 135, 275, 156, 5, 241, 255, 48, 258, 173, 62, 32, 276, 192, 55, 241, 246, 74, 263, 173, 32, 277, 192, 55, 241, 248, 74, 263, 173, 32, 278, 192, 174, 241, 241, 258, 108, 257, 173, 108, 91, 192, 144, 173, 32, 135, 279, 156, 5, 241, 256, 48, 259, 173, 62, 32, 280, 192, 55, 241, 272, 173, 32, 281, 192, 55, 241, 265, 173, 32, 278, 192, 15, 241, 280, 108, 281, 108, 278, 173, 32, 272, 192, 142, 241, 272, 108, 228, 348, 108, 259, 27, 173, 32, 265, 192, 142, 241, 265, 108, 228, 259, 108, 348, 27, 173, 32, 78, 32, 282, 192, 241, 276, 228, 62, 108, 200, 27, 77, 264, 228, 200, 108, 62, 27, 173, 228, 62, 108, 62, 108, 200, 27, 32, 283, 192, 241, 278, 4, 277, 228, 62, 108, 200, 27, 173, 82, 205, 241, 173, 82, 284, 241, 21, 173, 32, 285, 192, 55, 241, 273, 173, 32, 274, 170, 241, 15, 241, 285, 82, 286, 241, 173, 108, 283, 173, 42, 255, 173, 82, 284, 241, 21, 173, 32, 274, 2, 241, 219, 241, 203, 241, 282, 108, 285, 228, 62, 108, 200, 108, 62, 27, 108, 348, 173, 108, 261, 192, 348, 173, 82, 286, 241, 173, 42, 255, 173, 82, 284, 241, 21, 173, 32, 273, 192, 142, 241, 273, 108, 228, 258, 108, 348, 27, 173, 32, 272, 192, 142, 241, 272, 108, 228, 258, 108, 4, 256, 27, 173, 32, 265, 192, 142, 241, 265, 108, 228, 4, 256, 108, 348, 27, 173, 32, 263, 170, 258, 32, 78, 32, 10, 241, 271, 108, 274, 173, 32, 3, 32]}, {"code": "def linear_xent_bwd_kernel_matmul_t_dx(\n    x_ptr,\n    y_ptr,\n    A_t_ptr,\n    lse_global_ptr,\n    x_grad_ptr,\n    stride_x_N,\n    stride_x_H,\n    stride_A_H,\n    stride_A_V,\n    V: tl.constexpr,\n    N: tl.constexpr,\n    H: tl.constexpr,\n    V_BLOCK_SIZE: tl.constexpr = 16,\n    N_BLOCK_SIZE: tl.constexpr = 16,\n    H_BLOCK_SIZE: tl.constexpr = 16,\n):\n    idx_N = tl.program_id(axis=0)\n    idx_H_grad = tl.program_id(axis=1)\n\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n\n    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    V_offsets = tl.arange(0, V_BLOCK_SIZE)\n\n    y = tl.load(y_ptr + N_offsets)\n    lse = tl.load(lse_global_ptr + N_offsets)\n\n    x_grad_block_ptr = tl.make_block_ptr(\n        base=x_grad_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N * N_BLOCK_SIZE, idx_H_grad * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    x_block_ptr = tl.make_block_ptr(\n        base=x_ptr,\n        shape=(N, H),\n        strides=(stride_x_N, stride_x_H),\n        offsets=(idx_N * N_BLOCK_SIZE, 0 * H_BLOCK_SIZE),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_fwd_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(0 * H_BLOCK_SIZE, 0 * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n    A_bwd_block_ptr = tl.make_block_ptr(\n        base=A_t_ptr,\n        shape=(H, V),\n        strides=(stride_A_H, stride_A_V),\n        offsets=(idx_H_grad * H_BLOCK_SIZE, 0 * V_BLOCK_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0),\n    )\n\n    x_grad = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float16)\n\n    for idx_V in range(V // V_BLOCK_SIZE):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for idx_H_1 in range(H // H_BLOCK_SIZE):\n\n            x_chunk = tl.load(x_block_ptr)\n            A_v_fwd = tl.load(A_fwd_block_ptr)\n\n            z_j_to_k = tl.dot(x_chunk, A_v_fwd, z_j_to_k)\n\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_fwd_block_ptr = tl.advance(A_fwd_block_ptr, [H_BLOCK_SIZE, 0])\n\n        mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n\n        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n\n        A_v = tl.load(A_bwd_block_ptr).trans()\n\n        x_grad += (tl.dot(softmax_z, A_v) / N).to(tl.float16)\n        x_grad -= (tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1) / N).to(\n            tl.float16\n        )\n\n        A_bwd_block_ptr = tl.advance(A_bwd_block_ptr, [0, V_BLOCK_SIZE])\n        A_fwd_block_ptr = tl.advance(A_fwd_block_ptr, [-H, V_BLOCK_SIZE])\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        V_offsets += V_BLOCK_SIZE\n    tl.store(x_grad_block_ptr, x_grad)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 63, 6, 108, 255, 63, 6, 108, 256, 63, 6, 108, 257, 63, 6, 192, 346, 347, 108, 258, 63, 6, 192, 346, 347, 108, 259, 63, 6, 192, 346, 347, 173, 63, 32, -1, 260, 192, 169, 241, 261, 192, 348, 173, 32, 262, 192, 169, 241, 261, 192, 346, 173, 32, 83, 241, 255, 224, 258, 77, 348, 173, 32, 83, 241, 254, 224, 257, 77, 348, 173, 32, 83, 241, 256, 224, 259, 77, 348, 173, 32, 263, 192, 260, 242, 258, 74, 76, 241, 348, 108, 258, 173, 32, 264, 192, 76, 241, 348, 108, 257, 173, 32, 265, 192, 57, 241, 246, 74, 263, 173, 32, 266, 192, 57, 241, 248, 74, 263, 173, 32, 267, 192, 217, 241, 268, 192, 249, 108, 123, 192, 241, 255, 108, 256, 173, 108, 269, 192, 241, 250, 108, 251, 173, 108, 270, 192, 241, 260, 242, 258, 108, 262, 242, 259, 173, 108, 271, 192, 241, 258, 108, 259, 173, 108, 272, 192, 241, 346, 108, 348, 173, 173, 32, 273, 192, 217, 241, 268, 192, 245, 108, 123, 192, 241, 255, 108, 256, 173, 108, 269, 192, 241, 250, 108, 251, 173, 108, 270, 192, 241, 260, 242, 258, 108, 348, 242, 259, 173, 108, 271, 192, 241, 258, 108, 259, 173, 108, 272, 192, 241, 346, 108, 348, 173, 173, 32, 274, 192, 217, 241, 268, 192, 247, 108, 123, 192, 241, 256, 108, 254, 173, 108, 269, 192, 241, 252, 108, 253, 173, 108, 270, 192, 241, 348, 242, 259, 108, 348, 242, 257, 173, 108, 271, 192, 241, 259, 108, 257, 173, 108, 272, 192, 241, 346, 108, 348, 173, 173, 32, 275, 192, 217, 241, 268, 192, 247, 108, 123, 192, 241, 256, 108, 254, 173, 108, 269, 192, 241, 252, 108, 253, 173, 108, 270, 192, 241, 262, 242, 259, 108, 348, 242, 257, 173, 108, 271, 192, 241, 259, 108, 257, 173, 108, 272, 192, 241, 346, 108, 348, 173, 173, 32, 276, 192, 174, 241, 241, 258, 108, 259, 173, 108, 21, 173, 32, 135, 277, 156, 5, 241, 254, 48, 257, 173, 63, 32, 278, 192, 174, 241, 241, 258, 108, 257, 173, 108, 91, 192, 144, 173, 32, 135, 279, 156, 5, 241, 256, 48, 259, 173, 63, 32, 280, 192, 57, 241, 273, 173, 32, 281, 192, 57, 241, 274, 173, 32, 278, 192, 15, 241, 280, 108, 281, 108, 278, 173, 32, 273, 192, 142, 241, 273, 108, 228, 348, 108, 259, 27, 173, 32, 274, 192, 142, 241, 274, 108, 228, 259, 108, 348, 27, 173, 32, 78, 32, 282, 192, 241, 265, 228, 63, 108, 200, 27, 77, 264, 228, 200, 108, 63, 27, 173, 228, 63, 108, 63, 108, 200, 27, 32, 283, 192, 241, 278, 4, 266, 228, 63, 108, 200, 27, 173, 82, 205, 241, 173, 82, 284, 241, 21, 173, 32, 285, 192, 57, 241, 275, 173, 82, 286, 241, 173, 32, 276, 170, 241, 15, 241, 283, 108, 285, 173, 42, 255, 173, 82, 284, 241, 21, 173, 32, 276, 2, 241, 219, 241, 203, 241, 282, 108, 285, 228, 200, 108, 63, 108, 63, 27, 108, 348, 173, 108, 261, 192, 346, 173, 42, 255, 173, 82, 284, 241, 21, 173, 32, 275, 192, 142, 241, 275, 108, 228, 348, 108, 257, 27, 173, 32, 274, 192, 142, 241, 274, 108, 228, 4, 256, 108, 257, 27, 173, 32, 273, 192, 142, 241, 273, 108, 228, 348, 108, 4, 256, 27, 173, 32, 264, 170, 257, 32, 78, 32, 10, 241, 267, 108, 276, 173, 32, 3, 32]}, {"code": "def _swiglu_fwd_kernel(\n    X,\n    Y,\n    OUT,\n    stride_x_row,\n    stride_y_row,\n    stride_out_row,\n    ncols,\n    BLOCK_N: tl.constexpr,\n):\n\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    OUT += row * stride_out_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    out = x * tl.sigmoid(x) * y\n    tl.store(OUT + cols, out, mask=cols < ncols)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 62, 6, 173, 62, 32, -1, 253, 192, 169, 241, 346, 173, 32, 254, 192, 169, 241, 347, 173, 242, 252, 32, 245, 170, 253, 242, 248, 32, 246, 170, 253, 242, 249, 32, 247, 170, 253, 242, 250, 32, 255, 192, 254, 74, 76, 241, 346, 108, 252, 173, 32, 256, 192, 55, 241, 245, 74, 255, 108, 257, 192, 255, 1, 251, 108, 258, 192, 346, 173, 82, 259, 241, 144, 173, 32, 260, 192, 55, 241, 246, 74, 255, 108, 257, 192, 255, 1, 251, 108, 258, 192, 346, 173, 82, 259, 241, 144, 173, 32, 14, 192, 256, 242, 204, 241, 256, 173, 242, 260, 32, 10, 241, 247, 74, 255, 108, 14, 108, 257, 192, 255, 1, 251, 173, 32, 3, 32]}, {"code": "def _swiglu_bwd_kernel(\n    X,\n    Y,\n    DOUT,\n    OUT,\n    DX,\n    DY,\n    stride_x_row,\n    stride_y_row,\n    stride_dout_row,\n    stride_out_row,\n    stride_dx_row,\n    stride_dy_row,\n    ncols,\n    BLOCK_N: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n):\n\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    DOUT += row * stride_dout_row\n    if RECOMPUTE_OUTPUT:\n        OUT += row * stride_out_row\n    DX += row * stride_dx_row\n    DY += row * stride_dy_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    dout = tl.load(DOUT + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    x_sigmoid = tl.sigmoid(x)\n    dx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout\n    dy = x * x_sigmoid * dout\n    tl.store(DX + cols, dx, mask=cols < ncols)\n    tl.store(DY + cols, dy, mask=cols < ncols)\n    if RECOMPUTE_OUTPUT:\n        out = x * x_sigmoid * y\n        tl.store(OUT + cols, out, mask=cols < ncols)", "encoded": [29, 345, 241, 245, 108, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 63, 6, 108, 259, 63, 6, 173, 63, 32, -1, 260, 192, 169, 241, 346, 173, 32, 261, 192, 169, 241, 347, 173, 242, 258, 32, 245, 170, 260, 242, 251, 32, 246, 170, 260, 242, 252, 32, 247, 170, 260, 242, 253, 32, 180, 259, 63, 32, 248, 170, 260, 242, 254, 32, 183, 32, 249, 170, 260, 242, 255, 32, 250, 170, 260, 242, 256, 32, 262, 192, 261, 74, 76, 241, 346, 108, 258, 173, 32, 263, 192, 57, 241, 245, 74, 262, 108, 264, 192, 262, 1, 257, 108, 265, 192, 346, 173, 82, 266, 241, 144, 173, 32, 267, 192, 57, 241, 246, 74, 262, 108, 264, 192, 262, 1, 257, 108, 265, 192, 346, 173, 82, 266, 241, 144, 173, 32, 268, 192, 57, 241, 247, 74, 262, 108, 264, 192, 262, 1, 257, 108, 265, 192, 346, 173, 82, 266, 241, 144, 173, 32, 269, 192, 204, 241, 263, 173, 32, 270, 192, 269, 242, 241, 347, 74, 263, 242, 241, 347, 4, 269, 173, 173, 242, 267, 242, 268, 32, 271, 192, 263, 242, 269, 242, 268, 32, 10, 241, 249, 74, 262, 108, 270, 108, 264, 192, 262, 1, 257, 173, 32, 10, 241, 250, 74, 262, 108, 271, 108, 264, 192, 262, 1, 257, 173, 32, 180, 259, 63, 32, 14, 192, 263, 242, 269, 242, 267, 32, 10, 241, 248, 74, 262, 108, 14, 108, 264, 192, 262, 1, 257, 173, 32, 183, 32, 3, 32]}, {"code": "def _layer_norm_fwd_1pass_kernel(\n    X,\n    Y,\n    W,\n    B,\n    RESIDUAL,\n    X1,\n    W1,\n    B1,\n    Y1,\n    RESIDUAL_OUT,\n    ROWSCALE,\n    SEEDS,\n    DROPOUT_MASK,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_y_row,\n    stride_res_row,\n    stride_res_out_row,\n    stride_x1_row,\n    stride_y1_row,\n    M,\n    N,\n    eps,\n    dropout_p,\n    IS_RMS_NORM: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    STORE_RESIDUAL_OUT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_DROPOUT: tl.constexpr,\n    STORE_DROPOUT_MASK: tl.constexpr,\n    HAS_ROWSCALE: tl.constexpr,\n    HAS_X1: tl.constexpr,\n    HAS_W1: tl.constexpr,\n    HAS_B1: tl.constexpr,\n):\n\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    if HAS_X1:\n        X1 += row * stride_x1_row\n    if HAS_W1:\n        Y1 += row * stride_y1_row\n\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_ROWSCALE:\n        rowscale = tl.load(ROWSCALE + row).to(tl.float32)\n        x *= rowscale\n    if HAS_DROPOUT:\n\n        keep_mask = (\n            tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n        )\n        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)\n        if STORE_DROPOUT_MASK:\n            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)\n    if HAS_X1:\n        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)\n        if HAS_ROWSCALE:\n            rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)\n            x1 *= rowscale\n        if HAS_DROPOUT:\n\n            keep_mask = (\n                tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)\n                > dropout_p\n            )\n            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)\n            if STORE_DROPOUT_MASK:\n                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask, mask=cols < N)\n        x += x1\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n\n    tl.store(Y + cols, y, mask=mask)\n    if HAS_W1:\n        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)\n        if HAS_B1:\n            b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)\n        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1\n        tl.store(Y1 + cols, y1, mask=mask)", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 62, 6, 108, 272, 62, 6, 108, 273, 62, 6, 108, 274, 62, 6, 108, 275, 62, 6, 108, 276, 62, 6, 108, 277, 62, 6, 108, 278, 62, 6, 108, 279, 62, 6, 108, 280, 62, 6, 108, 281, 62, 6, 174, 62, 32, -1, 282, 193, 170, 242, 347, 174, 32, 246, 171, 282, 243, 261, 32, 247, 171, 282, 243, 262, 32, 181, 273, 62, 32, 250, 171, 282, 243, 263, 32, 184, 32, 181, 274, 62, 32, 255, 171, 282, 243, 264, 32, 184, 32, 181, 279, 62, 32, 251, 171, 282, 243, 265, 32, 184, 32, 181, 280, 62, 32, 254, 171, 282, 243, 266, 32, 184, 32, 283, 193, 76, 242, 347, 108, 272, 174, 32, 284, 193, 55, 242, 246, 74, 283, 108, 285, 193, 283, 1, 268, 108, 286, 193, 347, 174, 82, 287, 242, 144, 174, 32, 181, 278, 62, 32, 288, 193, 55, 242, 256, 74, 282, 174, 82, 287, 242, 144, 174, 32, 284, 24, 288, 32, 184, 32, 181, 276, 62, 32, 289, 193, 68, 242, 55, 242, 257, 74, 282, 174, 82, 287, 242, 164, 174, 108, 283, 108, 290, 193, 348, 174, 124, 270, 32, 284, 193, 204, 242, 289, 108, 284, 42, 242, 349, 4, 270, 174, 108, 347, 174, 32, 181, 277, 62, 32, 10, 242, 258, 74, 282, 243, 268, 74, 283, 108, 289, 108, 285, 193, 283, 1, 268, 174, 32, 184, 32, 184, 32, 181, 279, 62, 32, 291, 193, 55, 242, 251, 74, 283, 108, 285, 193, 283, 1, 268, 108, 286, 193, 347, 174, 82, 287, 242, 144, 174, 32, 181, 278, 62, 32, 288, 193, 55, 242, 256, 74, 267, 74, 282, 174, 82, 287, 242, 144, 174, 32, 291, 24, 288, 32, 184, 32, 181, 276, 62, 32, 289, 193, 68, 242, 55, 242, 257, 74, 267, 74, 282, 174, 82, 287, 242, 164, 174, 108, 283, 108, 290, 193, 348, 174, 124, 270, 32, 291, 193, 204, 242, 289, 108, 291, 42, 242, 349, 4, 270, 174, 108, 347, 174, 32, 181, 277, 62, 32, 10, 242, 258, 74, 242, 267, 74, 282, 174, 243, 268, 74, 283, 108, 289, 108, 285, 193, 283, 1, 268, 174, 32, 184, 32, 184, 32, 284, 171, 291, 32, 184, 32, 181, 273, 62, 32, 292, 193, 55, 242, 250, 74, 283, 108, 285, 193, 283, 1, 268, 108, 286, 193, 347, 174, 82, 287, 242, 144, 174, 32, 284, 171, 292, 32, 184, 32, 181, 274, 62, 32, 10, 242, 255, 74, 283, 108, 284, 108, 285, 193, 283, 1, 268, 174, 32, 184, 32, 181, 63, 271, 62, 32, 293, 193, 220, 242, 284, 108, 294, 193, 347, 174, 42, 268, 32, 10, 242, 259, 74, 282, 108, 293, 174, 32, 295, 193, 204, 242, 283, 1, 268, 108, 284, 4, 293, 108, 347, 174, 32, 296, 193, 220, 242, 295, 243, 295, 108, 294, 193, 347, 174, 42, 268, 32, 184, 32, 30, 62, 32, 295, 193, 204, 242, 283, 1, 268, 108, 284, 108, 347, 174, 32, 296, 193, 220, 242, 295, 243, 295, 108, 294, 193, 347, 174, 42, 268, 32, 56, 32, 297, 193, 349, 42, 130, 242, 296, 74, 269, 174, 32, 10, 242, 260, 74, 282, 108, 297, 174, 32, 285, 193, 283, 1, 268, 32, 298, 193, 55, 242, 248, 74, 283, 108, 285, 193, 285, 174, 82, 287, 242, 144, 174, 32, 181, 275, 62, 32, 299, 193, 55, 242, 249, 74, 283, 108, 285, 193, 285, 174, 82, 287, 242, 144, 174, 32, 184, 32, 300, 193, 242, 284, 4, 293, 174, 243, 297, 181, 63, 271, 30, 284, 243, 297, 32, 301, 193, 300, 243, 298, 74, 299, 181, 275, 30, 300, 243, 298, 32, 10, 242, 247, 74, 283, 108, 301, 108, 285, 193, 285, 174, 32, 181, 280, 62, 32, 302, 193, 55, 242, 252, 74, 283, 108, 285, 193, 285, 174, 82, 287, 242, 144, 174, 32, 181, 281, 62, 32, 303, 193, 55, 242, 253, 74, 283, 108, 285, 193, 285, 174, 82, 287, 242, 144, 174, 32, 184, 32, 304, 193, 300, 243, 302, 74, 303, 181, 281, 30, 300, 243, 302, 32, 10, 242, 254, 74, 283, 108, 304, 108, 285, 193, 285, 174, 32, 184, 32, 3, 32]}, {"code": "def _layer_norm_bwd_kernel(\n    X,\n    W,\n    B,\n    Y,\n    DY,\n    DX,\n    DW,\n    DB,\n    DRESIDUAL,\n    W1,\n    DY1,\n    DX1,\n    DW1,\n    DB1,\n    DRESIDUAL_IN,\n    ROWSCALE,\n    SEEDS,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_y_row,\n    stride_dy_row,\n    stride_dx_row,\n    stride_dres_row,\n    stride_dy1_row,\n    stride_dx1_row,\n    stride_dres_in_row,\n    M,\n    N,\n    eps,\n    dropout_p,\n    rows_per_program,\n    IS_RMS_NORM: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HAS_DRESIDUAL: tl.constexpr,\n    STORE_DRESIDUAL: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_DROPOUT: tl.constexpr,\n    HAS_ROWSCALE: tl.constexpr,\n    HAS_DY1: tl.constexpr,\n    HAS_DX1: tl.constexpr,\n    HAS_B1: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n):\n\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if HAS_DY1:\n        DY1 += row_start * stride_dy1_row\n    if HAS_DX1:\n        DX1 += row_start * stride_dx1_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n    if HAS_DY1:\n        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_DY1:\n        dw1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\n        if HAS_B1:\n            db1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if HAS_DY1:\n            dy1 = tl.load(DY1 + cols, mask=mask, other=0).to(tl.float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if HAS_DY1:\n            wdy += w1 * dy1\n            dw1 += dy1 * xhat\n            if HAS_B1:\n                db1 += dy1\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)\n            dx += dres\n\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        if HAS_DX1:\n            if HAS_DROPOUT:\n                keep_mask = (\n                    tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)\n                    > dropout_p\n                )\n                dx1 = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n            else:\n                dx1 = dx\n            tl.store(DX1 + cols, dx1, mask=mask)\n        if HAS_DROPOUT:\n            keep_mask = (\n                tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7)\n                > dropout_p\n            )\n            dx = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n        if HAS_ROWSCALE:\n            rowscale = tl.load(ROWSCALE + row).to(tl.float32)\n            dx *= rowscale\n        tl.store(DX + cols, dx, mask=mask)\n\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n        if HAS_DY1:\n            DY1 += stride_dy1_row\n        if HAS_DX1:\n            DX1 += stride_dx1_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n    if HAS_DY1:\n        tl.store(DW1 + row_block_id * N + cols, dw1, mask=mask)\n        if HAS_B1:\n            tl.store(DB1 + row_block_id * N + cols, db1, mask=mask)", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 63, 6, 108, 279, 63, 6, 108, 280, 63, 6, 108, 281, 63, 6, 108, 282, 63, 6, 108, 283, 63, 6, 108, 284, 63, 6, 108, 285, 63, 6, 108, 286, 63, 6, 108, 287, 63, 6, 108, 288, 63, 6, 174, 63, 32, -1, 289, 193, 170, 242, 347, 174, 32, 290, 193, 289, 243, 277, 32, 291, 193, 76, 242, 347, 108, 279, 174, 32, 292, 193, 291, 1, 274, 32, 246, 171, 290, 243, 265, 32, 181, 280, 63, 32, 254, 171, 290, 243, 269, 32, 184, 32, 181, 281, 63, 32, 260, 171, 290, 243, 272, 32, 184, 32, 250, 171, 290, 243, 267, 32, 251, 171, 290, 243, 268, 32, 181, 285, 63, 32, 256, 171, 290, 243, 270, 32, 184, 32, 181, 286, 63, 32, 257, 171, 290, 243, 271, 32, 184, 32, 181, 288, 63, 32, 249, 171, 290, 243, 266, 32, 184, 32, 293, 193, 57, 242, 247, 74, 291, 108, 292, 193, 292, 174, 82, 294, 242, 144, 174, 32, 181, 288, 102, 282, 63, 32, 295, 193, 57, 242, 248, 74, 291, 108, 292, 193, 292, 108, 296, 193, 347, 174, 82, 294, 242, 144, 174, 32, 184, 32, 181, 285, 63, 32, 297, 193, 57, 242, 255, 74, 291, 108, 292, 193, 292, 174, 82, 294, 242, 144, 174, 32, 184, 32, 298, 193, 175, 242, 242, 279, 108, 174, 108, 91, 193, 144, 174, 32, 181, 282, 63, 32, 299, 193, 175, 242, 242, 279, 108, 174, 108, 91, 193, 144, 174, 32, 184, 32, 181, 285, 63, 32, 300, 193, 175, 242, 242, 279, 108, 174, 108, 91, 193, 144, 174, 32, 181, 287, 63, 32, 301, 193, 175, 242, 242, 279, 108, 174, 108, 91, 193, 144, 174, 32, 184, 32, 184, 32, 302, 193, 39, 242, 242, 289, 74, 348, 174, 243, 277, 108, 273, 174, 32, 135, 303, 156, 5, 242, 290, 108, 302, 174, 63, 32, 304, 193, 57, 242, 246, 74, 291, 108, 292, 193, 292, 108, 296, 193, 347, 174, 82, 294, 242, 144, 174, 32, 305, 193, 57, 242, 250, 74, 291, 108, 292, 193, 292, 108, 296, 193, 347, 174, 82, 294, 242, 144, 174, 32, 181, 285, 63, 32, 306, 193, 57, 242, 256, 74, 291, 108, 292, 193, 292, 108, 296, 193, 347, 174, 82, 294, 242, 144, 174, 32, 184, 32, 181, 64, 278, 63, 32, 307, 193, 57, 242, 263, 74, 303, 174, 32, 184, 32, 308, 193, 57, 242, 264, 74, 303, 174, 32, 309, 193, 242, 304, 4, 307, 174, 243, 308, 181, 64, 278, 30, 304, 243, 308, 32, 309, 193, 204, 242, 292, 108, 309, 108, 347, 174, 32, 181, 288, 63, 32, 310, 193, 309, 243, 293, 74, 295, 181, 282, 30, 309, 243, 293, 32, 10, 242, 249, 74, 291, 108, 310, 108, 292, 193, 292, 174, 32, 184, 32, 311, 193, 293, 243, 305, 32, 298, 171, 305, 243, 309, 32, 181, 282, 63, 32, 299, 171, 305, 32, 184, 32, 181, 285, 63, 32, 311, 171, 297, 243, 306, 32, 300, 171, 306, 243, 309, 32, 181, 287, 63, 32, 301, 171, 306, 32, 184, 32, 184, 32, 181, 64, 278, 63, 32, 312, 193, 220, 242, 309, 243, 311, 108, 313, 193, 347, 174, 42, 274, 32, 314, 193, 220, 242, 311, 108, 313, 193, 347, 174, 42, 274, 32, 315, 193, 242, 311, 4, 242, 309, 243, 312, 74, 314, 174, 174, 243, 308, 32, 184, 32, 30, 63, 32, 312, 193, 220, 242, 309, 243, 311, 108, 313, 193, 347, 174, 42, 274, 32, 315, 193, 242, 311, 4, 309, 243, 312, 174, 243, 308, 32, 56, 32, 181, 280, 63, 32, 316, 193, 57, 242, 254, 74, 291, 108, 292, 193, 292, 108, 296, 193, 347, 174, 82, 294, 242, 144, 174, 32, 315, 171, 316, 32, 184, 32, 181, 281, 63, 32, 10, 242, 260, 74, 291, 108, 315, 108, 292, 193, 292, 174, 32, 184, 32, 181, 286, 63, 32, 181, 283, 63, 32, 317, 193, 69, 242, 57, 242, 262, 74, 273, 74, 303, 174, 82, 294, 242, 164, 174, 108, 291, 108, 318, 193, 349, 174, 124, 276, 32, 319, 193, 204, 242, 317, 108, 315, 42, 242, 348, 4, 276, 174, 108, 347, 174, 32, 184, 32, 30, 63, 32, 319, 193, 315, 32, 56, 32, 10, 242, 257, 74, 291, 108, 319, 108, 292, 193, 292, 174, 32, 184, 32, 181, 283, 63, 32, 317, 193, 69, 242, 57, 242, 262, 74, 303, 174, 82, 294, 242, 164, 174, 108, 291, 108, 318, 193, 349, 174, 124, 276, 32, 315, 193, 204, 242, 317, 108, 315, 42, 242, 348, 4, 276, 174, 108, 347, 174, 32, 184, 32, 181, 284, 63, 32, 320, 193, 57, 242, 261, 74, 303, 174, 82, 294, 242, 144, 174, 32, 315, 24, 320, 32, 184, 32, 10, 242, 251, 74, 291, 108, 315, 108, 292, 193, 292, 174, 32, 246, 171, 265, 32, 181, 280, 63, 32, 254, 171, 269, 32, 184, 32, 181, 281, 63, 32, 260, 171, 272, 32, 184, 32, 181, 288, 63, 32, 249, 171, 266, 32, 184, 32, 250, 171, 267, 32, 251, 171, 268, 32, 181, 285, 63, 32, 256, 171, 270, 32, 184, 32, 181, 286, 63, 32, 257, 171, 271, 32, 184, 32, 78, 32, 10, 242, 252, 74, 289, 243, 274, 74, 291, 108, 298, 108, 292, 193, 292, 174, 32, 181, 282, 63, 32, 10, 242, 253, 74, 289, 243, 274, 74, 291, 108, 299, 108, 292, 193, 292, 174, 32, 184, 32, 181, 285, 63, 32, 10, 242, 258, 74, 289, 243, 274, 74, 291, 108, 300, 108, 292, 193, 292, 174, 32, 181, 287, 63, 32, 10, 242, 259, 74, 289, 243, 274, 74, 291, 108, 301, 108, 292, 193, 292, 174, 32, 184, 32, 184, 32, 3, 32]}, {"code": "def _layer_norm_fwd_1pass_kernel(\n    X,\n    Y,\n    W,\n    B,\n    Z,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_y_row,\n    stride_z_row,\n    M,\n    N,\n    eps,\n    BLOCK_N: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_Z: tl.constexpr,\n    NORM_BEFORE_GATE: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n):\n\n    row = tl.program_id(0)\n    group = tl.program_id(1)\n    X += row * stride_x_row + group * N\n    Y += row * stride_y_row + group * N\n    if HAS_Z:\n        Z += row * stride_z_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    if HAS_BIAS:\n        B += group * N\n\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_Z and not NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=cols < N).to(tl.float32)\n        x *= z * tl.sigmoid(z)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    if HAS_Z and NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=mask).to(tl.float32)\n        y *= z * tl.sigmoid(z)\n\n    tl.store(Y + cols, y, mask=mask)", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 62, 6, 108, 260, 62, 6, 108, 261, 62, 6, 108, 262, 62, 6, 108, 263, 62, 6, 174, 62, 32, -1, 264, 193, 170, 242, 347, 174, 32, 265, 193, 170, 242, 348, 174, 32, 246, 171, 264, 243, 253, 74, 265, 243, 257, 32, 247, 171, 264, 243, 254, 74, 265, 243, 257, 32, 181, 261, 62, 32, 250, 171, 264, 243, 255, 74, 265, 243, 257, 32, 184, 32, 181, 63, 263, 62, 32, 251, 171, 265, 243, 256, 32, 184, 32, 252, 171, 265, 243, 256, 32, 248, 171, 265, 243, 257, 32, 181, 260, 62, 32, 249, 171, 265, 243, 257, 32, 184, 32, 266, 193, 76, 242, 347, 108, 259, 174, 32, 267, 193, 55, 242, 246, 74, 266, 108, 268, 193, 266, 1, 257, 108, 269, 193, 347, 174, 82, 270, 242, 144, 174, 32, 181, 261, 102, 242, 63, 262, 174, 62, 32, 271, 193, 55, 242, 250, 74, 266, 108, 268, 193, 266, 1, 257, 174, 82, 270, 242, 144, 174, 32, 267, 24, 271, 243, 205, 242, 271, 174, 32, 184, 32, 181, 63, 263, 62, 32, 272, 193, 220, 242, 267, 108, 273, 193, 347, 174, 42, 257, 32, 10, 242, 251, 74, 264, 108, 272, 174, 32, 274, 193, 204, 242, 266, 1, 257, 108, 267, 4, 272, 108, 347, 174, 32, 275, 193, 220, 242, 274, 243, 274, 108, 273, 193, 347, 174, 42, 257, 32, 184, 32, 30, 62, 32, 274, 193, 204, 242, 266, 1, 257, 108, 267, 108, 347, 174, 32, 275, 193, 220, 242, 274, 243, 274, 108, 273, 193, 347, 174, 42, 257, 32, 56, 32, 276, 193, 348, 42, 130, 242, 275, 74, 258, 174, 32, 10, 242, 252, 74, 264, 108, 276, 174, 32, 268, 193, 266, 1, 257, 32, 277, 193, 55, 242, 248, 74, 266, 108, 268, 193, 268, 174, 82, 270, 242, 144, 174, 32, 181, 260, 62, 32, 278, 193, 55, 242, 249, 74, 266, 108, 268, 193, 268, 174, 82, 270, 242, 144, 174, 32, 184, 32, 279, 193, 242, 267, 4, 272, 174, 243, 276, 181, 63, 263, 30, 267, 243, 276, 32, 280, 193, 279, 243, 277, 74, 278, 181, 260, 30, 279, 243, 277, 32, 181, 261, 102, 262, 62, 32, 271, 193, 55, 242, 250, 74, 266, 108, 268, 193, 268, 174, 82, 270, 242, 144, 174, 32, 280, 24, 271, 243, 205, 242, 271, 174, 32, 184, 32, 10, 242, 247, 74, 266, 108, 280, 108, 268, 193, 268, 174, 32, 3, 32]}, {"code": "def _layer_norm_bwd_kernel(\n    X,\n    W,\n    B,\n    Z,\n    Y,\n    DY,\n    DX,\n    DW,\n    DB,\n    DZ,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_z_row,\n    stride_y_row,\n    stride_dy_row,\n    stride_dx_row,\n    stride_dz_row,\n    stride_dw_row,\n    stride_db_row,\n    M,\n    N,\n    eps,\n    rows_per_program,\n    NORM_BEFORE_GATE: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_Z: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n\n    row_block_id = tl.program_id(0)\n    group = tl.program_id(1)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row + group * N\n    if HAS_Z:\n        Z += row_start * stride_z_row + group * N\n        DZ += row_start * stride_dz_row + group * N\n    DY += row_start * stride_dy_row + group * N\n    DX += row_start * stride_dx_row + group * N\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:\n        B += group * N\n        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n            x_og = x\n            x = x_og * z * tl.sigmoid(z)\n        rstd = tl.load(Rstd + row)\n\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if HAS_Z and NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n            z_sigmoid = tl.sigmoid(z)\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            if RECOMPUTE_OUTPUT:\n                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)\n            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dy *= z * z_sigmoid\n        else:\n            if RECOMPUTE_OUTPUT:\n                y = xhat * w + b if HAS_BIAS else xhat * w\n                tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        if not IS_RMS_NORM:\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            dx = (wdy - xhat * c1) * rstd\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z_sigmoid = tl.sigmoid(z)\n            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dx *= z * z_sigmoid\n\n        tl.store(DX + cols, dx, mask=mask)\n\n        X += stride_x_row\n        if HAS_Z:\n            Z += stride_z_row\n            DZ += stride_dz_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db, mask=mask)", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 63, 6, 108, 271, 63, 6, 108, 272, 63, 6, 108, 273, 63, 6, 108, 274, 63, 6, 108, 275, 63, 6, 174, 63, 32, -1, 276, 193, 170, 242, 347, 174, 32, 277, 193, 170, 242, 348, 174, 32, 278, 193, 276, 243, 269, 32, 279, 193, 76, 242, 347, 108, 275, 174, 32, 280, 193, 279, 1, 267, 32, 246, 171, 278, 243, 258, 74, 277, 243, 267, 32, 181, 273, 63, 32, 249, 171, 278, 243, 259, 74, 277, 243, 267, 32, 255, 171, 278, 243, 263, 74, 277, 243, 267, 32, 184, 32, 251, 171, 278, 243, 261, 74, 277, 243, 267, 32, 252, 171, 278, 243, 262, 74, 277, 243, 267, 32, 181, 274, 63, 32, 250, 171, 278, 243, 260, 74, 277, 243, 267, 32, 184, 32, 181, 64, 271, 63, 32, 256, 171, 277, 243, 266, 32, 184, 32, 257, 171, 277, 243, 266, 32, 247, 171, 277, 243, 267, 32, 281, 193, 57, 242, 247, 74, 279, 108, 280, 193, 280, 174, 82, 282, 242, 144, 174, 32, 181, 242, 274, 140, 273, 174, 102, 272, 63, 32, 248, 171, 277, 243, 267, 32, 283, 193, 57, 242, 248, 74, 279, 108, 280, 193, 280, 108, 284, 193, 347, 174, 82, 282, 242, 144, 174, 32, 184, 32, 285, 193, 175, 242, 242, 275, 108, 174, 108, 91, 193, 144, 174, 32, 181, 272, 63, 32, 286, 193, 175, 242, 242, 275, 108, 174, 108, 91, 193, 144, 174, 32, 184, 32, 287, 193, 39, 242, 242, 276, 74, 348, 174, 243, 269, 108, 266, 174, 32, 135, 288, 156, 5, 242, 278, 108, 287, 174, 63, 32, 289, 193, 57, 242, 246, 74, 279, 108, 280, 193, 280, 108, 284, 193, 347, 174, 82, 282, 242, 144, 174, 32, 290, 193, 57, 242, 251, 74, 279, 108, 280, 193, 280, 108, 284, 193, 347, 174, 82, 282, 242, 144, 174, 32, 181, 64, 271, 63, 32, 291, 193, 57, 242, 256, 74, 288, 174, 32, 184, 32, 181, 273, 102, 242, 64, 270, 174, 63, 32, 292, 193, 57, 242, 249, 74, 279, 108, 280, 193, 280, 108, 284, 193, 347, 174, 82, 282, 242, 144, 174, 32, 293, 193, 289, 32, 289, 193, 293, 243, 292, 243, 205, 242, 292, 174, 32, 184, 32, 294, 193, 57, 242, 257, 74, 288, 174, 32, 295, 193, 242, 289, 4, 291, 174, 243, 294, 181, 64, 271, 30, 289, 243, 294, 32, 295, 193, 204, 242, 280, 108, 295, 108, 347, 174, 32, 181, 273, 102, 270, 63, 32, 292, 193, 57, 242, 249, 74, 279, 108, 280, 193, 280, 108, 284, 193, 347, 174, 82, 282, 242, 144, 174, 32, 296, 193, 205, 242, 292, 174, 32, 297, 193, 295, 243, 281, 74, 283, 181, 272, 30, 295, 243, 281, 32, 181, 274, 63, 32, 10, 242, 250, 74, 279, 108, 297, 243, 292, 243, 296, 108, 280, 193, 280, 174, 32, 184, 32, 298, 193, 290, 243, 297, 243, 296, 243, 242, 348, 74, 292, 243, 242, 348, 4, 296, 174, 174, 32, 10, 242, 255, 74, 279, 108, 298, 108, 280, 193, 280, 174, 32, 290, 24, 292, 243, 296, 32, 66, 32, 37, 274, 63, 32, 297, 193, 295, 243, 281, 74, 283, 181, 272, 30, 295, 243, 281, 32, 10, 242, 250, 74, 279, 108, 297, 108, 280, 193, 280, 174, 32, 184, 32, 299, 193, 281, 243, 290, 32, 300, 193, 220, 242, 295, 243, 299, 108, 301, 193, 347, 174, 42, 267, 32, 181, 64, 271, 63, 32, 302, 193, 220, 242, 299, 108, 301, 193, 347, 174, 42, 267, 32, 303, 193, 242, 299, 4, 242, 295, 243, 300, 74, 302, 174, 174, 243, 294, 32, 184, 32, 30, 63, 32, 303, 193, 242, 299, 4, 295, 243, 300, 174, 243, 294, 32, 56, 32, 285, 171, 290, 243, 295, 32, 181, 272, 63, 32, 286, 171, 290, 32, 184, 32, 181, 273, 102, 242, 64, 270, 174, 63, 32, 296, 193, 205, 242, 292, 174, 32, 298, 193, 303, 243, 293, 243, 296, 243, 242, 348, 74, 292, 243, 242, 348, 4, 296, 174, 174, 32, 10, 242, 255, 74, 279, 108, 298, 108, 280, 193, 280, 174, 32, 303, 24, 292, 243, 296, 32, 184, 32, 10, 242, 252, 74, 279, 108, 303, 108, 280, 193, 280, 174, 32, 246, 171, 258, 32, 181, 273, 63, 32, 249, 171, 259, 32, 255, 171, 263, 32, 184, 32, 181, 274, 63, 32, 250, 171, 260, 32, 184, 32, 251, 171, 261, 32, 252, 171, 262, 32, 78, 32, 10, 242, 253, 74, 276, 243, 264, 74, 277, 243, 267, 74, 279, 108, 285, 108, 280, 193, 280, 174, 32, 181, 272, 63, 32, 10, 242, 254, 74, 276, 243, 265, 74, 277, 243, 267, 74, 279, 108, 286, 108, 280, 193, 280, 174, 32, 184, 32, 3, 32]}, {"code": "def _selective_scan_update_kernel(\n    state_ptr,\n    x_ptr,\n    dt_ptr,\n    dt_bias_ptr,\n    A_ptr,\n    B_ptr,\n    C_ptr,\n    D_ptr,\n    z_ptr,\n    out_ptr,\n    batch,\n    nheads,\n    dim,\n    dstate,\n    nheads_ngroups_ratio,\n    stride_state_batch,\n    stride_state_head,\n    stride_state_dim,\n    stride_state_dstate,\n    stride_x_batch,\n    stride_x_head,\n    stride_x_dim,\n    stride_dt_batch,\n    stride_dt_head,\n    stride_dt_dim,\n    stride_dt_bias_head,\n    stride_dt_bias_dim,\n    stride_A_head,\n    stride_A_dim,\n    stride_A_dstate,\n    stride_B_batch,\n    stride_B_group,\n    stride_B_dstate,\n    stride_C_batch,\n    stride_C_group,\n    stride_C_dstate,\n    stride_D_head,\n    stride_D_dim,\n    stride_z_batch,\n    stride_z_head,\n    stride_z_dim,\n    stride_out_batch,\n    stride_out_head,\n    stride_out_dim,\n    DT_SOFTPLUS: tl.constexpr,\n    TIE_HDIM: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    HAS_DT_BIAS: tl.constexpr,\n    HAS_D: tl.constexpr,\n    HAS_Z: tl.constexpr,\n    BLOCK_SIZE_DSTATE: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    state_ptr += pid_b * stride_state_batch + pid_h * stride_state_head\n    x_ptr += pid_b * stride_x_batch + pid_h * stride_x_head\n    dt_ptr += pid_b * stride_dt_batch + pid_h * stride_dt_head\n    if HAS_DT_BIAS:\n        dt_bias_ptr += pid_h * stride_dt_bias_head\n    A_ptr += pid_h * stride_A_head\n    B_ptr += pid_b * stride_B_batch + (pid_h // nheads_ngroups_ratio) * stride_B_group\n    C_ptr += pid_b * stride_C_batch + (pid_h // nheads_ngroups_ratio) * stride_C_group\n    if HAS_Z:\n        z_ptr += pid_b * stride_z_batch + pid_h * stride_z_head\n    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\n    state_ptrs = state_ptr + (\n        offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate\n    )\n    x_ptrs = x_ptr + offs_m * stride_x_dim\n    dt_ptrs = dt_ptr + offs_m * stride_dt_dim\n    if HAS_DT_BIAS:\n        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\n    if HAS_D:\n        D_ptr += pid_h * stride_D_head\n    A_ptrs = A_ptr + (\n        offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate\n    )\n    B_ptrs = B_ptr + offs_n * stride_B_dstate\n    C_ptrs = C_ptr + offs_n * stride_C_dstate\n    if HAS_D:\n        D_ptrs = D_ptr + offs_m * stride_D_dim\n    if HAS_Z:\n        z_ptrs = z_ptr + offs_m * stride_z_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n\n    state = tl.load(\n        state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0\n    )\n    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    if not TIE_HDIM:\n        dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        if HAS_DT_BIAS:\n            dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        if DT_SOFTPLUS:\n            dt = softplus(dt)\n        A = tl.load(\n            A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0\n        ).to(tl.float32)\n        dA = tl.exp(A * dt[:, None])\n    else:\n        dt = tl.load(dt_ptr).to(tl.float32)\n        if HAS_DT_BIAS:\n            dt += tl.load(dt_bias_ptr).to(tl.float32)\n        if DT_SOFTPLUS:\n            dt = softplus(dt)\n        A = tl.load(A_ptr).to(tl.float32)\n        dA = tl.exp(A * dt)\n\n    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)\n    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)\n    if HAS_D:\n        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    if HAS_Z:\n        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n\n    if not TIE_HDIM:\n        dB = B[None, :] * dt[:, None]\n    else:\n        dB = B * dt\n    state = state * dA + dB * x[:, None]\n    tl.store(\n        state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate)\n    )\n    out = tl.sum(state * C[None, :], axis=1)\n    if HAS_D:\n        out += x * D\n    if HAS_Z:\n        out *= z * tl.sigmoid(z)\n    tl.store(out_ptrs, out, mask=offs_m < dim)", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 169, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 62, 6, 108, 290, 62, 6, 108, 291, 62, 6, 108, 292, 62, 6, 108, 293, 62, 6, 108, 294, 62, 6, 108, 295, 62, 6, 174, 62, 32, -1, 296, 193, 170, 242, 297, 193, 347, 174, 32, 298, 193, 170, 242, 297, 193, 348, 174, 32, 299, 193, 170, 242, 297, 193, 349, 174, 32, 246, 171, 298, 243, 260, 74, 299, 243, 261, 32, 247, 171, 298, 243, 264, 74, 299, 243, 265, 32, 248, 171, 298, 243, 267, 74, 299, 243, 268, 32, 181, 292, 62, 32, 249, 171, 299, 243, 270, 32, 184, 32, 250, 171, 299, 243, 272, 32, 251, 171, 298, 243, 275, 74, 299, 48, 259, 243, 276, 32, 252, 171, 298, 243, 278, 74, 299, 48, 259, 243, 279, 32, 181, 294, 62, 32, 254, 171, 298, 243, 283, 74, 299, 243, 284, 32, 184, 32, 255, 171, 298, 243, 286, 74, 299, 243, 287, 32, 300, 193, 296, 243, 291, 74, 76, 242, 347, 108, 291, 174, 32, 301, 193, 76, 242, 347, 108, 295, 174, 32, 302, 193, 246, 74, 242, 300, 229, 62, 108, 201, 27, 243, 262, 74, 301, 229, 201, 108, 62, 27, 243, 263, 174, 32, 303, 193, 247, 74, 300, 243, 266, 32, 304, 193, 248, 74, 300, 243, 269, 32, 181, 292, 62, 32, 305, 193, 249, 74, 300, 243, 271, 32, 184, 32, 181, 293, 62, 32, 253, 171, 299, 243, 281, 32, 184, 32, 306, 193, 250, 74, 242, 300, 229, 62, 108, 201, 27, 243, 273, 74, 301, 229, 201, 108, 62, 27, 243, 274, 174, 32, 307, 193, 251, 74, 301, 243, 277, 32, 308, 193, 252, 74, 301, 243, 280, 32, 181, 293, 62, 32, 309, 193, 253, 74, 300, 243, 282, 32, 184, 32, 181, 294, 62, 32, 310, 193, 254, 74, 300, 243, 285, 32, 184, 32, 311, 193, 255, 74, 300, 243, 288, 32, 312, 193, 55, 242, 302, 108, 313, 193, 242, 300, 229, 62, 108, 201, 27, 1, 169, 174, 172, 242, 301, 229, 201, 108, 62, 27, 1, 258, 174, 108, 314, 193, 347, 174, 32, 315, 193, 55, 242, 303, 108, 313, 193, 300, 1, 169, 108, 314, 193, 347, 174, 82, 316, 242, 144, 174, 32, 181, 63, 290, 62, 32, 317, 193, 55, 242, 304, 108, 313, 193, 300, 1, 169, 108, 314, 193, 347, 174, 82, 316, 242, 144, 174, 32, 181, 292, 62, 32, 317, 171, 55, 242, 305, 108, 313, 193, 300, 1, 169, 108, 314, 193, 347, 174, 82, 316, 242, 144, 174, 32, 184, 32, 181, 289, 62, 32, 317, 193, 318, 242, 317, 174, 32, 184, 32, 319, 193, 55, 242, 306, 108, 313, 193, 242, 300, 229, 62, 108, 201, 27, 1, 169, 174, 172, 242, 301, 229, 201, 108, 62, 27, 1, 258, 174, 108, 314, 193, 347, 174, 82, 316, 242, 144, 174, 32, 320, 193, 107, 242, 319, 243, 317, 229, 62, 108, 201, 27, 174, 32, 184, 32, 30, 62, 32, 317, 193, 55, 242, 248, 174, 82, 316, 242, 144, 174, 32, 181, 292, 62, 32, 317, 171, 55, 242, 249, 174, 82, 316, 242, 144, 174, 32, 184, 32, 181, 289, 62, 32, 317, 193, 318, 242, 317, 174, 32, 184, 32, 319, 193, 55, 242, 250, 174, 82, 316, 242, 144, 174, 32, 320, 193, 107, 242, 319, 243, 317, 174, 32, 56, 32, 321, 193, 55, 242, 307, 108, 313, 193, 301, 1, 258, 108, 314, 193, 347, 174, 82, 316, 242, 144, 174, 32, 322, 193, 55, 242, 308, 108, 313, 193, 301, 1, 258, 108, 314, 193, 347, 174, 82, 316, 242, 144, 174, 32, 181, 293, 62, 32, 323, 193, 55, 242, 309, 108, 313, 193, 300, 1, 169, 108, 314, 193, 347, 174, 82, 316, 242, 144, 174, 32, 184, 32, 181, 294, 62, 32, 324, 193, 55, 242, 310, 108, 313, 193, 300, 1, 169, 108, 314, 193, 347, 174, 82, 316, 242, 144, 174, 32, 184, 32, 181, 63, 290, 62, 32, 325, 193, 321, 229, 201, 108, 62, 27, 243, 317, 229, 62, 108, 201, 27, 32, 184, 32, 30, 62, 32, 325, 193, 321, 243, 317, 32, 56, 32, 312, 193, 312, 243, 320, 74, 325, 243, 315, 229, 62, 108, 201, 27, 32, 10, 242, 302, 108, 312, 108, 313, 193, 242, 300, 229, 62, 108, 201, 27, 1, 169, 174, 172, 242, 301, 229, 201, 108, 62, 27, 1, 258, 174, 174, 32, 14, 193, 220, 242, 312, 243, 322, 229, 201, 108, 62, 27, 108, 297, 193, 348, 174, 32, 181, 293, 62, 32, 14, 171, 315, 243, 323, 32, 184, 32, 181, 294, 62, 32, 14, 24, 324, 243, 205, 242, 324, 174, 32, 184, 32, 10, 242, 311, 108, 14, 108, 313, 193, 300, 1, 169, 174, 32, 3, 32]}, {"code": "def _bmm_chunk_fwd_kernel(\n    a_ptr,\n    b_ptr,\n    out_ptr,\n    seq_idx_ptr,\n    seqlen,\n    chunk_size,\n    K,\n    ngroups,\n    stride_a_batch,\n    stride_a_seqlen,\n    stride_a_head,\n    stride_ak,\n    stride_b_batch,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_bk,\n    stride_out_batch,\n    stride_out_chunk,\n    stride_out_head,\n    stride_outm,\n    stride_outn,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    IS_CAUSAL: tl.constexpr,\n    dot_dtype: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    if IS_CAUSAL:\n        if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n            return\n    a_ptr += (\n        pid_b * stride_a_batch\n        + pid_c * chunk_size * stride_a_seqlen\n        + pid_h * stride_a_head\n    )\n    b_ptr += (\n        pid_b * stride_b_batch\n        + pid_c * chunk_size * stride_b_seqlen\n        + pid_h * stride_b_head\n    )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_b_seqlen)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(\n            a_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit)\n            & (offs_k[None, :] < K - k * BLOCK_SIZE_K),\n            other=0.0,\n        ).to(dot_dtype)\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K)\n            & (offs_n[None, :] < chunk_size_limit),\n            other=0.0,\n        ).to(dot_dtype)\n        acc += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_SEQ_IDX:\n        chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n        seq_idx_m = tl.load(\n            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit,\n            other=-1,\n        )\n        seq_idx_n = tl.load(\n            seq_idx_ptr + offs_n * stride_seq_idx_seqlen,\n            mask=offs_n < chunk_size_limit,\n            other=-2,\n        )\n        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)\n    out = acc.to(out_ptr.dtype.element_ty)\n\n    out_ptr += (\n        pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h * stride_out_head\n    )\n    out_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] * stride_outn)\n    tl.store(\n        out_ptrs,\n        out,\n        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),\n    )", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 63, 6, 108, 270, 63, 6, 108, 271, 63, 6, 108, 272, 63, 6, 108, 273, 63, 6, 108, 274, 63, 6, 174, 63, 32, -1, 275, 193, 170, 242, 276, 193, 347, 174, 32, 277, 193, 170, 242, 276, 193, 348, 174, 32, 278, 193, 277, 48, 253, 32, 279, 193, 277, 4, 278, 243, 253, 32, 280, 193, 65, 242, 251, 108, 273, 174, 32, 281, 193, 170, 242, 276, 193, 349, 174, 48, 280, 32, 282, 193, 170, 242, 276, 193, 349, 174, 225, 280, 32, 181, 269, 63, 32, 181, 282, 243, 273, 143, 242, 281, 74, 347, 174, 243, 272, 63, 32, 230, 32, 184, 32, 184, 32, 246, 171, 275, 243, 254, 74, 278, 243, 251, 243, 255, 74, 279, 243, 256, 32, 247, 171, 275, 243, 258, 74, 278, 243, 251, 243, 259, 74, 279, 243, 260, 32, 181, 271, 63, 32, 249, 171, 275, 243, 267, 74, 278, 243, 251, 243, 268, 32, 184, 32, 283, 193, 281, 243, 272, 74, 76, 242, 349, 108, 272, 174, 32, 284, 193, 282, 243, 273, 74, 76, 242, 349, 108, 273, 174, 32, 285, 193, 76, 242, 349, 108, 274, 174, 32, 286, 193, 246, 74, 242, 283, 229, 63, 108, 201, 27, 243, 255, 74, 285, 229, 201, 108, 63, 27, 243, 257, 174, 32, 287, 193, 247, 74, 242, 285, 229, 63, 108, 201, 27, 243, 261, 74, 284, 229, 201, 108, 63, 27, 243, 259, 174, 32, 288, 193, 39, 242, 251, 108, 250, 4, 278, 243, 251, 174, 32, 289, 193, 175, 242, 242, 272, 108, 273, 174, 108, 91, 193, 144, 174, 32, 135, 290, 156, 5, 242, 349, 108, 65, 242, 252, 108, 274, 174, 174, 63, 32, 291, 193, 57, 242, 286, 108, 292, 193, 242, 283, 229, 63, 108, 201, 27, 1, 288, 174, 172, 242, 285, 229, 201, 108, 63, 27, 1, 252, 4, 290, 243, 274, 174, 108, 293, 193, 349, 174, 82, 294, 242, 270, 174, 32, 295, 193, 57, 242, 287, 108, 292, 193, 242, 285, 229, 63, 108, 201, 27, 1, 252, 4, 290, 243, 274, 174, 172, 242, 284, 229, 201, 108, 63, 27, 1, 288, 174, 108, 293, 193, 349, 174, 82, 294, 242, 270, 174, 32, 289, 171, 15, 242, 291, 108, 295, 174, 32, 286, 171, 274, 243, 257, 32, 287, 171, 274, 243, 261, 32, 78, 32, 283, 193, 281, 243, 272, 74, 76, 242, 349, 108, 272, 174, 32, 284, 193, 282, 243, 273, 74, 76, 242, 349, 108, 273, 174, 32, 181, 271, 63, 32, 288, 193, 39, 242, 251, 108, 250, 4, 278, 243, 251, 174, 32, 296, 193, 57, 242, 249, 74, 283, 243, 268, 108, 292, 193, 283, 1, 288, 108, 293, 193, 4, 347, 174, 32, 297, 193, 57, 242, 249, 74, 284, 243, 268, 108, 292, 193, 284, 1, 288, 108, 293, 193, 4, 348, 174, 32, 289, 193, 204, 242, 296, 229, 63, 108, 201, 27, 77, 297, 229, 201, 108, 63, 27, 108, 289, 108, 349, 174, 32, 184, 32, 14, 193, 289, 82, 294, 242, 248, 82, 91, 82, 114, 174, 32, 248, 171, 275, 243, 262, 74, 278, 243, 263, 74, 279, 243, 264, 32, 298, 193, 248, 74, 242, 265, 243, 283, 229, 63, 108, 201, 27, 74, 284, 229, 201, 108, 63, 27, 243, 266, 174, 32, 10, 242, 298, 108, 14, 108, 292, 193, 242, 283, 229, 63, 108, 201, 27, 1, 251, 174, 172, 242, 284, 229, 201, 108, 63, 27, 1, 251, 174, 174, 32, 3, 32]}, {"code": "def _bmm_chunk_bwd_kernel(\n    a_ptr,\n    dout_ptr,\n    db_ptr,\n    res_ptr,\n    seqlen,\n    chunk_size,\n    K,\n    ngroups,\n    stride_a_batch,\n    stride_a_seqlen,\n    stride_a_head,\n    stride_ak,\n    stride_dout_batch,\n    stride_dout_chunk,\n    stride_dout_head,\n    stride_dout_csize_m,\n    stride_dout_csize_n,\n    stride_db_batch,\n    stride_db_seqlen,\n    stride_db_head,\n    stride_db_k,\n    stride_res_batch,\n    stride_res_seqlen,\n    stride_res_head,\n    stride_res_k,\n    dot_dtype: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_CS: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(K, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n\n    a_ptr += (\n        pid_b * stride_a_batch\n        + pid_c * chunk_size * stride_a_seqlen\n        + pid_h * stride_a_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch + pid_c * stride_dout_chunk + pid_h * stride_dout_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_cs = tl.arange(0, BLOCK_SIZE_CS)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_csize_n + offs_cs[None, :] * stride_dout_csize_m\n    )\n    a_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] * stride_ak)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):\n        dout = tl.load(\n            dout_ptrs,\n            mask=(offs_m[:, None] < chunk_size)\n            & (offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS),\n            other=0.0,\n        ).to(dot_dtype)\n        a = tl.load(\n            a_ptrs,\n            mask=(offs_cs[:, None] < chunk_size_limit - cs * BLOCK_SIZE_CS)\n            & (offs_n[None, :] < K),\n            other=0.0,\n        ).to(dot_dtype)\n        acc += tl.dot(dout, a)\n        dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m\n        a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_RESIDUAL:\n        res_ptr += (\n            pid_b * stride_res_batch\n            + pid_c * chunk_size * stride_res_seqlen\n            + pid_h * stride_res_head\n        )\n        res_ptrs = res_ptr + (\n            offs_m[:, None] * stride_res_seqlen + offs_n[None, :] * stride_res_k\n        )\n        res = tl.load(\n            res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)\n        ).to(tl.float32)\n        acc += res\n    db = acc.to(db_ptr.dtype.element_ty)\n\n    db_ptr += (\n        pid_b * stride_db_batch\n        + pid_c * chunk_size * stride_db_seqlen\n        + pid_h * stride_db_head\n    )\n    db_ptrs = db_ptr + (\n        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_k\n    )\n    tl.store(\n        db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)\n    )", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 62, 6, 108, 272, 62, 6, 108, 273, 62, 6, 108, 274, 62, 6, 108, 275, 62, 6, 174, 62, 32, -1, 276, 193, 170, 242, 277, 193, 347, 174, 32, 278, 193, 170, 242, 277, 193, 348, 174, 32, 279, 193, 278, 48, 253, 32, 280, 193, 278, 4, 279, 243, 253, 32, 281, 193, 64, 242, 252, 108, 274, 174, 32, 282, 193, 170, 242, 277, 193, 349, 174, 48, 281, 32, 283, 193, 170, 242, 277, 193, 349, 174, 225, 281, 32, 246, 171, 276, 243, 254, 74, 279, 243, 251, 243, 255, 74, 280, 243, 256, 32, 247, 171, 276, 243, 258, 74, 279, 243, 259, 74, 280, 243, 260, 32, 284, 193, 282, 243, 273, 74, 76, 242, 349, 108, 273, 174, 32, 285, 193, 283, 243, 274, 74, 76, 242, 349, 108, 274, 174, 32, 286, 193, 76, 242, 349, 108, 275, 174, 32, 287, 193, 247, 74, 242, 284, 229, 62, 108, 201, 27, 243, 262, 74, 286, 229, 201, 108, 62, 27, 243, 261, 174, 32, 288, 193, 246, 74, 242, 286, 229, 62, 108, 201, 27, 243, 255, 74, 285, 229, 201, 108, 62, 27, 243, 257, 174, 32, 289, 193, 39, 242, 251, 108, 250, 4, 279, 243, 251, 174, 32, 290, 193, 175, 242, 242, 273, 108, 274, 174, 108, 91, 193, 144, 174, 32, 135, 291, 156, 5, 242, 349, 108, 64, 242, 289, 108, 275, 174, 174, 62, 32, 292, 193, 55, 242, 287, 108, 293, 193, 242, 284, 229, 62, 108, 201, 27, 1, 251, 174, 172, 242, 286, 229, 201, 108, 62, 27, 1, 289, 4, 291, 243, 275, 174, 108, 294, 193, 349, 174, 82, 295, 242, 271, 174, 32, 296, 193, 55, 242, 288, 108, 293, 193, 242, 286, 229, 62, 108, 201, 27, 1, 289, 4, 291, 243, 275, 174, 172, 242, 285, 229, 201, 108, 62, 27, 1, 252, 174, 108, 294, 193, 349, 174, 82, 295, 242, 271, 174, 32, 290, 171, 15, 242, 292, 108, 296, 174, 32, 287, 171, 275, 243, 261, 32, 288, 171, 275, 243, 255, 32, 78, 32, 284, 193, 282, 243, 273, 74, 76, 242, 349, 108, 273, 174, 32, 285, 193, 283, 243, 274, 74, 76, 242, 349, 108, 274, 174, 32, 181, 272, 62, 32, 249, 171, 276, 243, 267, 74, 279, 243, 251, 243, 268, 74, 280, 243, 269, 32, 297, 193, 249, 74, 242, 284, 229, 62, 108, 201, 27, 243, 268, 74, 285, 229, 201, 108, 62, 27, 243, 270, 174, 32, 298, 193, 55, 242, 297, 108, 293, 193, 242, 284, 229, 62, 108, 201, 27, 1, 289, 174, 172, 242, 285, 229, 201, 108, 62, 27, 1, 252, 174, 174, 82, 295, 242, 144, 174, 32, 290, 171, 298, 32, 184, 32, 299, 193, 290, 82, 295, 242, 248, 82, 91, 82, 114, 174, 32, 248, 171, 276, 243, 263, 74, 279, 243, 251, 243, 264, 74, 280, 243, 265, 32, 300, 193, 248, 74, 242, 284, 229, 62, 108, 201, 27, 243, 264, 74, 285, 229, 201, 108, 62, 27, 243, 266, 174, 32, 10, 242, 300, 108, 299, 108, 293, 193, 242, 284, 229, 62, 108, 201, 27, 1, 289, 174, 172, 242, 285, 229, 201, 108, 62, 27, 1, 252, 174, 174, 32, 3, 32]}, {"code": "def _chunk_scan_fwd_kernel(\n    cb_ptr,\n    x_ptr,\n    z_ptr,\n    out_ptr,\n    out_x_ptr,\n    dt_ptr,\n    dA_cumsum_f_ptr,\n    dA_cumsum_b_ptr,\n    C_ptr,\n    prev_states_f_ptr,\n    prev_states_b_ptr,\n    D_ptr,\n    chunk_size,\n    hdim,\n    dstate,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_k,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_z_batch,\n    stride_z_seqlen,\n    stride_z_head,\n    stride_z_hdim,\n    stride_out_batch,\n    stride_out_seqlen,\n    stride_out_head,\n    stride_out_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_f_batch,\n    stride_dA_cs_f_chunk,\n    stride_dA_cs_f_head,\n    stride_dA_cs_f_csize,\n    stride_dA_cs_b_batch,\n    stride_dA_cs_b_chunk,\n    stride_dA_cs_b_head,\n    stride_dA_cs_b_csize,\n    stride_C_batch,\n    stride_C_seqlen,\n    stride_C_head,\n    stride_C_dstate,\n    stride_states_f_batch,\n    stride_states_f_chunk,\n    stride_states_f_head,\n    stride_states_f_hdim,\n    stride_states_f_dstate,\n    stride_states_b_batch,\n    stride_states_b_chunk,\n    stride_states_b_head,\n    stride_states_b_hdim,\n    stride_states_b_dstate,\n    stride_D_head,\n    HAS_D: tl.constexpr,\n    D_HAS_HDIM: tl.constexpr,\n    HAS_Z: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_DSTATE: tl.constexpr,\n    IS_TRITON_22: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_f_ptr += (\n        pid_b * stride_dA_cs_f_batch\n        + pid_c * stride_dA_cs_f_chunk\n        + pid_h * stride_dA_cs_f_head\n    )\n    dA_cumsum_b_ptr += (\n        pid_b * stride_dA_cs_b_batch\n        + pid_c * stride_dA_cs_b_chunk\n        + pid_h * stride_dA_cs_b_head\n    )\n    C_ptr += (\n        pid_b * stride_C_batch\n        + pid_c * chunk_size * stride_C_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_C_head\n    )\n    prev_states_f_ptr += (\n        pid_b * stride_states_f_batch\n        + pid_c * stride_states_f_chunk\n        + pid_h * stride_states_f_head\n    )\n    prev_states_b_ptr += (\n        pid_b * stride_states_b_batch\n        + pid_c * stride_states_b_chunk\n        + pid_h * stride_states_b_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dA_cs_f_m = tl.load(\n        dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize,\n        mask=offs_m < chunk_size,\n        other=0.0,\n    ).to(tl.float32)\n    dA_cs_b_m = tl.load(\n        dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize,\n        mask=offs_m < chunk_size,\n        other=0.0,\n    ).to(tl.float32)\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    if IS_TRITON_22 or pid_c > -1:\n\n        offs_k_dstate = tl.arange(\n            0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K\n        )\n        C_ptrs = C_ptr + (\n            offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate\n        )\n        prev_states_f_ptrs = prev_states_f_ptr + (\n            offs_n[None, :] * stride_states_f_hdim\n            + offs_k_dstate[:, None] * stride_states_f_dstate\n        )\n        prev_states_b_ptrs = prev_states_b_ptr + (\n            offs_n[None, :] * stride_states_b_hdim\n            + offs_k_dstate[:, None] * stride_states_b_dstate\n        )\n        scale_f_m = tl.exp(dA_cs_f_m)\n        scale_b_m = tl.exp(dA_cs_b_m)\n        if BLOCK_SIZE_DSTATE <= 128:\n            C = tl.load(\n                C_ptrs,\n                mask=(offs_m[:, None] < chunk_size_limit)\n                & (offs_k_dstate[None, :] < dstate),\n                other=0.0,\n            )\n            prev_states_f = tl.load(\n                prev_states_f_ptrs,\n                mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),\n                other=0.0,\n            )\n            prev_states_f = prev_states_f.to(C_ptr.dtype.element_ty)\n            acc = tl.dot(C, prev_states_f) * scale_f_m[:, None]\n\n            prev_states_b = tl.load(\n                prev_states_b_ptrs,\n                mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),\n                other=0.0,\n            )\n            prev_states_b = prev_states_b.to(C_ptr.dtype.element_ty)\n            acc += tl.dot(C, prev_states_b) * scale_b_m[:, None]\n        else:\n            for k in range(0, dstate, BLOCK_SIZE_K):\n                C = tl.load(\n                    C_ptrs,\n                    mask=(offs_m[:, None] < chunk_size_limit)\n                    & (offs_k_dstate[None, :] < dstate - k),\n                    other=0.0,\n                )\n\n                prev_states_f = tl.load(\n                    prev_states_f_ptrs,\n                    mask=(offs_k_dstate[:, None] < dstate - k)\n                    & (offs_n[None, :] < hdim),\n                    other=0.0,\n                )\n                prev_states_f = prev_states_f.to(C_ptr.dtype.element_ty)\n                acc += tl.dot(C, prev_states_f) * scale_f_m[:, None]\n\n                prev_states_b = tl.load(\n                    prev_states_f_ptrs,\n                    mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),\n                    other=0.0,\n                )\n                prev_states_b = prev_states_b.to(C_ptr.dtype.element_ty)\n                acc += tl.dot(C, prev_states_b) * scale_b_m[:, None]\n                C_ptrs += BLOCK_SIZE_K\n                prev_states_f_ptrs += BLOCK_SIZE_K\n                prev_states_b_ptrs += BLOCK_SIZE_K\n\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k\n    )\n    x_ptrs = x_ptr + (\n        offs_k[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n    )\n    dt_ptrs = dt_ptr + offs_k * stride_dt_csize\n    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize\n    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize\n    K_F_MAX = min((pid_m) * BLOCK_SIZE_M, chunk_size_limit)\n    K_F_MAX_BEG = K_F_MAX - BLOCK_SIZE_K\n    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n        cb = tl.load(\n            cb_ptrs,\n            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < chunk_size - k),\n            other=0.0,\n        ).to(tl.float32)\n        if k <= K_F_MAX and k >= K_F_MAX_BEG:\n\n            dA_cs_f_k = tl.load(\n                dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0\n            ).to(tl.float32)\n            mask_f = offs_m[:, None] >= (k + offs_k[None, :])\n            scale_f = tl.where(\n                mask_f, tl.exp((dA_cs_f_m[:, None] - dA_cs_f_k[None, :])), 0.0\n            )\n\n            dA_cs_b_k = tl.load(\n                dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0\n            ).to(tl.float32)\n            mask_b = offs_m[:, None] <= (k + offs_k[None, :])\n            scale_b = tl.where(\n                mask_b, tl.exp((dA_cs_b_m[:, None] - dA_cs_b_k[None, :])), 0.0\n            )\n\n            cb = cb * (scale_f + scale_b)\n\n        elif k < K_F_MAX:\n            dA_cs_f_k = tl.load(\n                dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0\n            ).to(tl.float32)\n            mask = offs_m[:, None] >= k + offs_k[None, :]\n            cb *= tl.where(mask, tl.exp((dA_cs_f_m[:, None] - dA_cs_f_k[None, :])), 0.0)\n        elif k > K_F_MAX:\n            dA_cs_b_k = tl.load(\n                dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0\n            ).to(tl.float32)\n            mask = offs_m[:, None] <= k + offs_k[None, :]\n            cb *= tl.where(mask, tl.exp((dA_cs_b_m[:, None] - dA_cs_b_k[None, :])), 0.0)\n        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n        cb *= dt_k\n        cb = cb.to(x_ptr.dtype.element_ty)\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n        acc += tl.dot(cb, x)\n        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n        dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize\n        dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize\n\n    offs_out_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_out_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    if HAS_D:\n        if D_HAS_HDIM:\n            D = tl.load(\n                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0\n            ).to(tl.float32)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n        x_residual = tl.load(\n            x_ptr\n            + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim),\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n            other=0.0,\n        ).to(tl.float32)\n        acc += x_residual * D\n\n    if HAS_Z:\n        out_x_ptr += (\n            pid_b * stride_out_batch\n            + pid_c * chunk_size * stride_out_seqlen\n            + pid_h * stride_out_head\n        )\n        out_x_ptrs = out_x_ptr + (\n            stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :]\n        )\n        tl.store(\n            out_x_ptrs,\n            acc,\n            mask=(offs_out_m[:, None] < chunk_size_limit)\n            & (offs_out_n[None, :] < hdim),\n        )\n\n        z_ptr += (\n            pid_b * stride_z_batch\n            + pid_c * chunk_size * stride_z_seqlen\n            + pid_h * stride_z_head\n        )\n        z_ptrs = z_ptr + (\n            stride_z_seqlen * offs_out_m[:, None] + stride_z_hdim * offs_out_n[None, :]\n        )\n        z = tl.load(\n            z_ptrs,\n            mask=(offs_out_m[:, None] < chunk_size_limit)\n            & (offs_out_n[None, :] < hdim),\n            other=0.0,\n        ).to(tl.float32)\n        acc *= z * tl.sigmoid(z)\n\n    out_ptr += (\n        pid_b * stride_out_batch\n        + pid_c * chunk_size * stride_out_seqlen\n        + pid_h * stride_out_head\n    )\n    out_ptrs = out_ptr + (\n        stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :] * stride_out_hdim\n    )\n    tl.store(\n        out_ptrs,\n        acc,\n        mask=(offs_out_m[:, None] < chunk_size_limit) & (offs_out_n[None, :] < hdim),\n    )", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 108, 301, 108, 302, 108, 303, 108, 304, 108, 305, 108, 306, 108, 307, 108, 308, 63, 6, 108, 309, 63, 6, 108, 310, 63, 6, 108, 311, 63, 6, 108, 312, 63, 6, 108, 313, 63, 6, 108, 314, 63, 6, 108, 315, 63, 6, 174, 63, 32, -1, 316, 193, 170, 242, 317, 193, 347, 174, 32, 318, 193, 316, 48, 261, 32, 319, 193, 316, 4, 318, 243, 261, 32, 320, 193, 170, 242, 317, 193, 348, 174, 32, 321, 193, 65, 242, 259, 108, 312, 174, 32, 322, 193, 170, 242, 317, 193, 349, 174, 48, 321, 32, 323, 193, 170, 242, 317, 193, 349, 174, 225, 321, 32, 246, 171, 319, 243, 264, 74, 318, 243, 265, 74, 320, 48, 263, 243, 266, 32, 247, 171, 319, 243, 269, 74, 318, 243, 258, 243, 270, 74, 320, 243, 271, 32, 251, 171, 319, 243, 281, 74, 318, 243, 282, 74, 320, 243, 283, 32, 252, 171, 319, 243, 285, 74, 318, 243, 286, 74, 320, 243, 287, 32, 253, 171, 319, 243, 289, 74, 318, 243, 290, 74, 320, 243, 291, 32, 254, 171, 319, 243, 293, 74, 318, 243, 258, 243, 294, 74, 320, 48, 263, 243, 295, 32, 255, 171, 319, 243, 297, 74, 318, 243, 298, 74, 320, 243, 299, 32, 256, 171, 319, 243, 302, 74, 318, 243, 303, 74, 320, 243, 304, 32, 324, 193, 322, 243, 311, 74, 76, 242, 349, 108, 311, 174, 32, 325, 193, 323, 243, 312, 74, 76, 242, 349, 108, 312, 174, 32, 326, 193, 57, 242, 252, 74, 324, 243, 288, 108, 327, 193, 324, 1, 258, 108, 328, 193, 349, 174, 82, 329, 242, 144, 174, 32, 330, 193, 57, 242, 253, 74, 324, 243, 292, 108, 327, 193, 324, 1, 258, 108, 328, 193, 349, 174, 82, 329, 242, 144, 174, 32, 331, 193, 39, 242, 258, 108, 262, 4, 318, 243, 258, 174, 32, 332, 193, 175, 242, 242, 311, 108, 312, 174, 108, 91, 193, 144, 174, 32, 181, 315, 140, 318, 124, 4, 347, 63, 32, 333, 193, 76, 242, 349, 108, 314, 181, 314, 217, 347, 348, 350, 30, 313, 174, 32, 334, 193, 254, 74, 242, 324, 229, 63, 108, 201, 27, 243, 294, 74, 333, 229, 201, 108, 63, 27, 243, 296, 174, 32, 335, 193, 255, 74, 242, 325, 229, 201, 108, 63, 27, 243, 300, 74, 333, 229, 63, 108, 201, 27, 243, 301, 174, 32, 336, 193, 256, 74, 242, 325, 229, 201, 108, 63, 27, 243, 305, 74, 333, 229, 63, 108, 201, 27, 243, 306, 174, 32, 337, 193, 107, 242, 326, 174, 32, 338, 193, 107, 242, 330, 174, 32, 181, 314, 217, 347, 348, 350, 63, 32, 339, 193, 57, 242, 334, 108, 327, 193, 242, 324, 229, 63, 108, 201, 27, 1, 331, 174, 172, 242, 333, 229, 201, 108, 63, 27, 1, 260, 174, 108, 328, 193, 349, 174, 32, 340, 193, 57, 242, 335, 108, 327, 193, 242, 333, 229, 63, 108, 201, 27, 1, 260, 174, 172, 242, 325, 229, 201, 108, 63, 27, 1, 259, 174, 108, 328, 193, 349, 174, 32, 340, 193, 340, 82, 329, 242, 254, 82, 91, 82, 114, 174, 32, 332, 193, 15, 242, 339, 108, 340, 174, 243, 337, 229, 63, 108, 201, 27, 32, 341, 193, 57, 242, 336, 108, 327, 193, 242, 333, 229, 63, 108, 201, 27, 1, 260, 174, 172, 242, 325, 229, 201, 108, 63, 27, 1, 259, 174, 108, 328, 193, 349, 174, 32, 341, 193, 341, 82, 329, 242, 254, 82, 91, 82, 114, 174, 32, 332, 171, 15, 242, 339, 108, 341, 174, 243, 338, 229, 63, 108, 201, 27, 32, 184, 32, 30, 63, 32, 135, 342, 156, 5, 242, 349, 108, 260, 108, 313, 174, 63, 32, 339, 193, 57, 242, 334, 108, 327, 193, 242, 324, 229, 63, 108, 201, 27, 1, 331, 174, 172, 242, 333, 229, 201, 108, 63, 27, 1, 260, 4, 342, 174, 108, 328, 193, 349, 174, 32, 340, 193, 57, 242, 335, 108, 327, 193, 242, 333, 229, 63, 108, 201, 27, 1, 260, 4, 342, 174, 172, 242, 325, 229, 201, 108, 63, 27, 1, 259, 174, 108, 328, 193, 349, 174, 32, 340, 193, 340, 82, 329, 242, 254, 82, 91, 82, 114, 174, 32, 332, 171, 15, 242, 339, 108, 340, 174, 243, 337, 229, 63, 108, 201, 27, 32, 341, 193, 57, 242, 335, 108, 327, 193, 242, 333, 229, 63, 108, 201, 27, 1, 260, 174, 172, 242, 325, 229, 201, 108, 63, 27, 1, 259, 174, 108, 328, 193, 349, 174, 32, 341, 193, 341, 82, 329, 242, 254, 82, 91, 82, 114, 174, 32, 332, 171, 15, 242, 339, 108, 341, 174, 243, 338, 229, 63, 108, 201, 27, 32, 334, 171, 313, 32, 335, 171, 313, 32, 336, 171, 313, 32, 78, 32, 56, 32, 184, 32, 343, 193, 76, 242, 349, 108, 313, 174, 32, 344, 193, 246, 74, 242, 324, 229, 63, 108, 201, 27, 243, 267, 74, 343, 229, 201, 108, 63, 27, 243, 268, 174, 32, 345, 193, 247, 74, 242, 343, 229, 63, 108, 201, 27, 243, 270, 74, 325, 229, 201, 108, 63, 27, 243, 272, 174, 32, 351, 193, 251, 74, 343, 243, 284, 32, 352, 193, 252, 74, 343, 243, 288, 32, 353, 193, 253, 74, 343, 243, 292, 32, 354, 193, 39, 242, 322, 243, 311, 108, 331, 174, 32, 355, 193, 354, 4, 313, 32, 135, 342, 156, 5, 242, 349, 108, 331, 108, 313, 174, 63, 32, 356, 193, 57, 242, 344, 108, 327, 193, 242, 324, 229, 63, 108, 201, 27, 1, 258, 174, 172, 242, 343, 229, 201, 108, 63, 27, 1, 258, 4, 342, 174, 108, 328, 193, 349, 174, 82, 329, 242, 144, 174, 32, 181, 342, 217, 354, 102, 342, 143, 355, 63, 32, 357, 193, 57, 242, 352, 108, 327, 193, 343, 1, 258, 4, 342, 108, 328, 193, 349, 174, 82, 329, 242, 144, 174, 32, 358, 193, 324, 229, 63, 108, 201, 27, 143, 342, 74, 343, 229, 201, 108, 63, 27, 32, 359, 193, 204, 242, 358, 108, 107, 242, 326, 229, 63, 108, 201, 27, 4, 357, 229, 201, 108, 63, 27, 174, 108, 349, 174, 32, 360, 193, 57, 242, 353, 108, 327, 193, 343, 1, 258, 4, 342, 108, 328, 193, 349, 174, 82, 329, 242, 144, 174, 32, 361, 193, 324, 229, 63, 108, 201, 27, 217, 342, 74, 343, 229, 201, 108, 63, 27, 32, 362, 193, 204, 242, 361, 108, 107, 242, 330, 229, 63, 108, 201, 27, 4, 360, 229, 201, 108, 63, 27, 174, 108, 349, 174, 32, 356, 193, 356, 243, 242, 359, 74, 362, 174, 32, 66, 32, 37, 342, 1, 354, 63, 32, 357, 193, 57, 242, 352, 108, 327, 193, 343, 1, 258, 4, 342, 108, 328, 193, 349, 174, 82, 329, 242, 144, 174, 32, 327, 193, 324, 229, 63, 108, 201, 27, 143, 342, 74, 343, 229, 201, 108, 63, 27, 32, 356, 24, 204, 242, 327, 108, 107, 242, 326, 229, 63, 108, 201, 27, 4, 357, 229, 201, 108, 63, 27, 174, 108, 349, 174, 32, 66, 32, 37, 342, 124, 354, 63, 32, 360, 193, 57, 242, 353, 108, 327, 193, 343, 1, 258, 4, 342, 108, 328, 193, 349, 174, 82, 329, 242, 144, 174, 32, 327, 193, 324, 229, 63, 108, 201, 27, 217, 342, 74, 343, 229, 201, 108, 63, 27, 32, 356, 24, 204, 242, 327, 108, 107, 242, 330, 229, 63, 108, 201, 27, 4, 360, 229, 201, 108, 63, 27, 174, 108, 349, 174, 32, 184, 32, 363, 193, 57, 242, 351, 108, 327, 193, 343, 1, 258, 4, 342, 108, 328, 193, 349, 174, 82, 329, 242, 144, 174, 32, 356, 24, 363, 32, 356, 193, 356, 82, 329, 242, 247, 82, 91, 82, 114, 174, 32, 364, 193, 57, 242, 345, 108, 327, 193, 242, 343, 229, 63, 108, 201, 27, 1, 331, 4, 342, 174, 172, 242, 325, 229, 201, 108, 63, 27, 1, 259, 174, 108, 328, 193, 349, 174, 32, 332, 171, 15, 242, 356, 108, 364, 174, 32, 344, 171, 313, 243, 268, 32, 345, 171, 313, 243, 270, 32, 351, 171, 313, 243, 284, 32, 352, 171, 313, 243, 288, 32, 353, 171, 313, 243, 292, 32, 78, 32, 365, 193, 322, 243, 311, 74, 76, 242, 349, 108, 311, 174, 32, 366, 193, 323, 243, 312, 74, 76, 242, 349, 108, 312, 174, 32, 181, 308, 63, 32, 181, 309, 63, 32, 367, 193, 57, 242, 257, 74, 320, 243, 307, 74, 325, 108, 327, 193, 325, 1, 259, 108, 328, 193, 349, 174, 82, 329, 242, 144, 174, 32, 184, 32, 30, 63, 32, 367, 193, 57, 242, 257, 74, 320, 243, 307, 174, 82, 329, 242, 144, 174, 32, 56, 32, 368, 193, 57, 242, 247, 74, 242, 324, 229, 63, 108, 201, 27, 243, 270, 74, 325, 229, 201, 108, 63, 27, 243, 272, 174, 108, 327, 193, 242, 324, 229, 63, 108, 201, 27, 1, 331, 174, 172, 242, 325, 229, 201, 108, 63, 27, 1, 259, 174, 108, 328, 193, 349, 174, 82, 329, 242, 144, 174, 32, 332, 171, 368, 243, 367, 32, 184, 32, 181, 310, 63, 32, 250, 171, 319, 243, 277, 74, 318, 243, 258, 243, 278, 74, 320, 243, 279, 32, 369, 193, 250, 74, 242, 278, 243, 365, 229, 63, 108, 201, 27, 74, 366, 229, 201, 108, 63, 27, 174, 32, 10, 242, 369, 108, 332, 108, 327, 193, 242, 365, 229, 63, 108, 201, 27, 1, 331, 174, 172, 242, 366, 229, 201, 108, 63, 27, 1, 259, 174, 174, 32, 248, 171, 319, 243, 273, 74, 318, 243, 258, 243, 274, 74, 320, 243, 275, 32, 370, 193, 248, 74, 242, 274, 243, 365, 229, 63, 108, 201, 27, 74, 276, 243, 366, 229, 201, 108, 63, 27, 174, 32, 371, 193, 57, 242, 370, 108, 327, 193, 242, 365, 229, 63, 108, 201, 27, 1, 331, 174, 172, 242, 366, 229, 201, 108, 63, 27, 1, 259, 174, 108, 328, 193, 349, 174, 82, 329, 242, 144, 174, 32, 332, 24, 371, 243, 205, 242, 371, 174, 32, 184, 32, 249, 171, 319, 243, 277, 74, 318, 243, 258, 243, 278, 74, 320, 243, 279, 32, 372, 193, 249, 74, 242, 278, 243, 365, 229, 63, 108, 201, 27, 74, 366, 229, 201, 108, 63, 27, 243, 280, 174, 32, 10, 242, 372, 108, 332, 108, 327, 193, 242, 365, 229, 63, 108, 201, 27, 1, 331, 174, 172, 242, 366, 229, 201, 108, 63, 27, 1, 259, 174, 174, 32, 3, 32]}, {"code": "def _chunk_scan_fwd_kernel_wip(\n    cb_ptr,\n    x_ptr,\n    z_ptr,\n    out_ptr,\n    out_x_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    seq_idx_ptr,\n    C_ptr,\n    B_ptr,\n    prev_states_ptr,\n    D_ptr,\n    chunk_size,\n    hdim,\n    dstate,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_k,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_z_batch,\n    stride_z_seqlen,\n    stride_z_head,\n    stride_z_hdim,\n    stride_out_batch,\n    stride_out_seqlen,\n    stride_out_head,\n    stride_out_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    stride_C_batch,\n    stride_C_seqlen,\n    stride_C_head,\n    stride_C_dstate,\n    stride_B_batch,\n    stride_B_seqlen,\n    stride_B_head,\n    stride_B_dstate,\n    stride_states_batch,\n    stride_states_chunk,\n    stride_states_head,\n    stride_states_hdim,\n    stride_states_dstate,\n    stride_D_head,\n    HAS_D: tl.constexpr,\n    D_HAS_HDIM: tl.constexpr,\n    HAS_Z: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_DSTATE: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_n = tl.program_id(axis=0)\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    C_ptr += (\n        pid_b * stride_C_batch\n        + pid_c * chunk_size * stride_C_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_C_head\n    )\n    B_ptr += (\n        pid_b * stride_B_batch\n        + pid_c * chunk_size * stride_B_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_B_head\n    )\n    prev_states_ptr += (\n        pid_b * stride_states_batch\n        + pid_c * stride_states_chunk\n        + pid_h * stride_states_head\n    )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n    out_ptr += (\n        pid_b * stride_out_batch\n        + pid_c * chunk_size * stride_out_seqlen\n        + pid_h * stride_out_head\n    )\n\n    offs_m = tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE)\n\n    C_ptrs = C_ptr + (\n        offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate\n    )\n    B_ptrs = B_ptr + (\n        offs_m[None, :] * stride_B_seqlen + offs_k_dstate[:, None] * stride_B_dstate\n    )\n    prev_states_ptrs = prev_states_ptr + (\n        offs_n[None, :] * stride_states_hdim\n        + offs_k_dstate[:, None] * stride_states_dstate\n    )\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m + offs_m[None, :] * stride_cb_csize_k\n    )\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n    )\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    out_ptrs = out_ptr + (\n        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim\n    )\n\n    prev_states = tl.load(\n        prev_states_ptrs,\n        mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),\n        other=0.0,\n    )\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    for start_m in range(0, chunk_size_limit, BLOCK_SIZE_M):\n        start_m = tl.multiple_of(start_m, BLOCK_SIZE_M)\n        dA_cs_m = tl.load(\n            dA_cumsum_ptr + (start_m + offs_m) * stride_dA_cs_csize,\n            mask=offs_m < chunk_size - start_m,\n            other=0.0,\n        ).to(tl.float32)\n        if HAS_SEQ_IDX:\n            seq_idx_prev = tl.load(\n                seq_idx_ptr + start_m - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0\n            )\n            seq_idx_m = tl.load(\n                seq_idx_ptr + (start_m + offs_m) * stride_seq_idx_seqlen,\n                mask=offs_m < chunk_size_limit - start_m,\n                other=-1,\n            )\n        if not HAS_SEQ_IDX:\n            scale_m = tl.exp(dA_cs_m)\n        else:\n            scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n        C = tl.load(\n            C_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit - start_m)\n            & (offs_k_dstate[None, :] < dstate),\n            other=0.0,\n        )\n        acc = tl.dot(C, prev_states.to(C_ptr.dtype.element_ty)) * scale_m[:, None]\n\n        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size - start_m, other=0.0).to(\n            tl.float32\n        )\n\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit - start_m)\n            & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n\n        if HAS_D:\n            if D_HAS_HDIM:\n                D = tl.load(\n                    D_ptr + pid_h * stride_D_head + offs_n,\n                    mask=offs_n < hdim,\n                    other=0.0,\n                ).to(tl.float32)\n            else:\n                D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n            acc += x.to(tl.float32) * D\n\n        tl.store(\n            out_ptrs,\n            acc,\n            mask=(offs_m[:, None] < chunk_size_limit - start_m)\n            & (offs_n[None, :] < hdim),\n        )\n\n        if start_m + BLOCK_SIZE_M < chunk_size_limit:\n\n            B = tl.load(\n                B_ptrs,\n                mask=(offs_m[None, :] < chunk_size_limit - start_m)\n                & (offs_k_dstate[:, None] < dstate),\n                other=0.0,\n            )\n            dA_cs_last = tl.load(\n                dA_cumsum_ptr + (start_m + BLOCK_SIZE_M) * stride_dA_cs_csize\n            ).to(tl.float32)\n\n            scale = tl.exp((dA_cs_last - dA_cs_m)) * dt_m\n\n            B = B.to(x_ptr.dtype.element_ty)\n            tmp = tl.dot(B, x)\n            prev_states += tmp.to(prev_states.dtype)\n\n        C_ptrs += BLOCK_SIZE_M * stride_C_seqlen\n        B_ptrs += BLOCK_SIZE_M * stride_B_seqlen\n        cb_ptrs += BLOCK_SIZE_M * stride_cb_csize_m + BLOCK_SIZE_M * stride_cb_csize_k\n        x_ptrs += BLOCK_SIZE_M * stride_x_seqlen\n        dt_ptrs += BLOCK_SIZE_M * stride_dt_csize\n        out_ptrs += BLOCK_SIZE_M * stride_out_seqlen", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 108, 301, 108, 302, 108, 303, 108, 304, 108, 305, 62, 6, 108, 306, 62, 6, 108, 307, 62, 6, 108, 308, 62, 6, 108, 309, 62, 6, 108, 310, 62, 6, 108, 311, 62, 6, 174, 62, 32, -1, 312, 193, 170, 242, 313, 193, 347, 174, 32, 314, 193, 312, 48, 261, 32, 315, 193, 312, 4, 314, 243, 261, 32, 316, 193, 170, 242, 313, 193, 348, 174, 32, 317, 193, 170, 242, 313, 193, 349, 174, 32, 246, 171, 315, 243, 264, 74, 314, 243, 265, 74, 316, 48, 263, 243, 266, 32, 247, 171, 315, 243, 269, 74, 314, 243, 258, 243, 270, 74, 316, 243, 271, 32, 251, 171, 315, 243, 281, 74, 314, 243, 282, 74, 316, 243, 283, 32, 252, 171, 315, 243, 285, 74, 314, 243, 286, 74, 316, 243, 287, 32, 254, 171, 315, 243, 291, 74, 314, 243, 258, 243, 292, 74, 316, 48, 263, 243, 293, 32, 255, 171, 315, 243, 295, 74, 314, 243, 258, 243, 296, 74, 316, 48, 263, 243, 297, 32, 256, 171, 315, 243, 299, 74, 314, 243, 300, 74, 316, 243, 301, 32, 181, 308, 62, 32, 253, 171, 315, 243, 289, 74, 314, 243, 258, 243, 290, 32, 184, 32, 249, 171, 315, 243, 277, 74, 314, 243, 258, 243, 278, 74, 316, 243, 279, 32, 318, 193, 76, 242, 349, 108, 309, 174, 32, 319, 193, 317, 243, 310, 74, 76, 242, 349, 108, 310, 174, 32, 320, 193, 76, 242, 349, 108, 311, 174, 32, 321, 193, 254, 74, 242, 318, 229, 62, 108, 201, 27, 243, 292, 74, 320, 229, 201, 108, 62, 27, 243, 294, 174, 32, 322, 193, 255, 74, 242, 318, 229, 201, 108, 62, 27, 243, 296, 74, 320, 229, 62, 108, 201, 27, 243, 298, 174, 32, 323, 193, 256, 74, 242, 319, 229, 201, 108, 62, 27, 243, 302, 74, 320, 229, 62, 108, 201, 27, 243, 303, 174, 32, 324, 193, 64, 242, 259, 108, 310, 174, 32, 325, 193, 246, 74, 242, 318, 229, 62, 108, 201, 27, 243, 267, 74, 318, 229, 201, 108, 62, 27, 243, 268, 174, 32, 326, 193, 247, 74, 242, 318, 229, 62, 108, 201, 27, 243, 270, 74, 319, 229, 201, 108, 62, 27, 243, 272, 174, 32, 327, 193, 251, 74, 318, 243, 284, 32, 328, 193, 249, 74, 242, 318, 229, 62, 108, 201, 27, 243, 278, 74, 319, 229, 201, 108, 62, 27, 243, 280, 174, 32, 329, 193, 55, 242, 323, 108, 330, 193, 242, 320, 229, 62, 108, 201, 27, 1, 260, 174, 172, 242, 319, 229, 201, 108, 62, 27, 1, 259, 174, 108, 331, 193, 349, 174, 32, 332, 193, 39, 242, 258, 108, 262, 4, 314, 243, 258, 174, 32, 135, 333, 156, 5, 242, 349, 108, 332, 108, 309, 174, 62, 32, 333, 193, 57, 242, 333, 108, 309, 174, 32, 334, 193, 55, 242, 252, 74, 242, 333, 74, 318, 174, 243, 288, 108, 330, 193, 318, 1, 258, 4, 333, 108, 331, 193, 349, 174, 82, 335, 242, 144, 174, 32, 181, 308, 62, 32, 336, 193, 55, 242, 253, 74, 333, 4, 290, 108, 330, 193, 314, 143, 347, 108, 331, 193, 349, 174, 32, 337, 193, 55, 242, 253, 74, 242, 333, 74, 318, 174, 243, 290, 108, 330, 193, 318, 1, 332, 4, 333, 108, 331, 193, 4, 347, 174, 32, 184, 32, 181, 63, 308, 62, 32, 338, 193, 107, 242, 334, 174, 32, 184, 32, 30, 62, 32, 338, 193, 204, 242, 337, 77, 336, 108, 107, 242, 334, 174, 108, 349, 174, 32, 56, 32, 339, 193, 55, 242, 321, 108, 330, 193, 242, 318, 229, 62, 108, 201, 27, 1, 332, 4, 333, 174, 172, 242, 320, 229, 201, 108, 62, 27, 1, 260, 174, 108, 331, 193, 349, 174, 32, 340, 193, 15, 242, 339, 108, 329, 82, 335, 242, 254, 82, 91, 82, 114, 174, 174, 243, 338, 229, 62, 108, 201, 27, 32, 341, 193, 55, 242, 327, 108, 330, 193, 318, 1, 258, 4, 333, 108, 331, 193, 349, 174, 82, 335, 242, 144, 174, 32, 342, 193, 55, 242, 326, 108, 330, 193, 242, 318, 229, 62, 108, 201, 27, 1, 332, 4, 333, 174, 172, 242, 319, 229, 201, 108, 62, 27, 1, 259, 174, 108, 331, 193, 349, 174, 32, 181, 305, 62, 32, 181, 306, 62, 32, 343, 193, 55, 242, 257, 74, 316, 243, 304, 74, 319, 108, 330, 193, 319, 1, 259, 108, 331, 193, 349, 174, 82, 335, 242, 144, 174, 32, 184, 32, 30, 62, 32, 343, 193, 55, 242, 257, 74, 316, 243, 304, 174, 82, 335, 242, 144, 174, 32, 56, 32, 340, 171, 342, 82, 335, 242, 144, 174, 243, 343, 32, 184, 32, 10, 242, 328, 108, 340, 108, 330, 193, 242, 318, 229, 62, 108, 201, 27, 1, 332, 4, 333, 174, 172, 242, 319, 229, 201, 108, 62, 27, 1, 259, 174, 174, 32, 181, 333, 74, 309, 1, 332, 62, 32, 344, 193, 55, 242, 322, 108, 330, 193, 242, 318, 229, 201, 108, 62, 27, 1, 332, 4, 333, 174, 172, 242, 320, 229, 62, 108, 201, 27, 1, 260, 174, 108, 331, 193, 349, 174, 32, 345, 193, 55, 242, 252, 74, 242, 333, 74, 309, 174, 243, 288, 174, 82, 335, 242, 144, 174, 32, 350, 193, 107, 242, 345, 4, 334, 174, 243, 341, 32, 344, 193, 344, 82, 335, 242, 247, 82, 91, 82, 114, 174, 32, 351, 193, 15, 242, 344, 108, 342, 174, 32, 329, 171, 351, 82, 335, 242, 329, 82, 91, 174, 32, 184, 32, 321, 171, 309, 243, 292, 32, 322, 171, 309, 243, 296, 32, 325, 171, 309, 243, 267, 74, 309, 243, 268, 32, 326, 171, 309, 243, 270, 32, 327, 171, 309, 243, 284, 32, 328, 171, 309, 243, 278, 32, 78, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_dz_kernel(\n    dout_ptr,\n    out_ptr,\n    z_ptr,\n    x_ptr,\n    D_ptr,\n    outz_ptr,\n    dz_ptr,\n    dout_x_ptr,\n    dD_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_out_batch,\n    stride_out_seqlen,\n    stride_out_head,\n    stride_out_hdim,\n    stride_z_batch,\n    stride_z_seqlen,\n    stride_z_head,\n    stride_z_hdim,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_D_head,\n    stride_outz_batch,\n    stride_outz_seqlen,\n    stride_outz_head,\n    stride_outz_hdim,\n    stride_dz_batch,\n    stride_dz_seqlen,\n    stride_dz_head,\n    stride_dz_hdim,\n    stride_doutx_batch,\n    stride_doutx_seqlen,\n    stride_doutx_head,\n    stride_doutx_hdim,\n    stride_dD_batch,\n    stride_dD_chunk,\n    stride_dD_head,\n    stride_dD_csize,\n    stride_dD_hdim,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize,\n    HAS_D: tl.constexpr,\n    D_HAS_HDIM: tl.constexpr,\n    HAS_DDACS: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dout_x_ptr += (\n        pid_b * stride_doutx_batch\n        + pid_c * chunk_size * stride_doutx_seqlen\n        + pid_h * stride_doutx_head\n    )\n    out_ptr += (\n        pid_b * stride_out_batch\n        + pid_c * chunk_size * stride_out_seqlen\n        + pid_h * stride_out_head\n    )\n    z_ptr += (\n        pid_b * stride_z_batch\n        + pid_c * chunk_size * stride_z_seqlen\n        + pid_h * stride_z_head\n    )\n    dz_ptr += (\n        pid_b * stride_dz_batch\n        + pid_c * chunk_size * stride_dz_seqlen\n        + pid_h * stride_dz_head\n    )\n    if RECOMPUTE_OUTPUT:\n        outz_ptr += (\n            pid_b * stride_outz_batch\n            + pid_c * chunk_size * stride_outz_seqlen\n            + pid_h * stride_outz_head\n        )\n    if HAS_DDACS:\n        ddA_cumsum_ptr += (\n            pid_b * stride_ddA_cs_batch\n            + pid_c * stride_ddA_cs_chunk\n            + pid_h * stride_ddA_cs_head\n        )\n    if HAS_D:\n        x_ptr += (\n            pid_b * stride_x_batch\n            + pid_c * chunk_size * stride_x_seqlen\n            + pid_h * stride_x_head\n        )\n        dD_ptr += (\n            pid_b * stride_dD_batch\n            + pid_c * stride_dD_chunk\n            + pid_h * stride_dD_head\n            + pid_m * stride_dD_csize\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim\n    )\n    dout_x_ptrs = dout_x_ptr + (\n        offs_m[:, None] * stride_doutx_seqlen + offs_n[None, :] * stride_doutx_hdim\n    )\n    out_ptrs = out_ptr + (\n        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim\n    )\n    z_ptrs = z_ptr + (\n        offs_m[:, None] * stride_z_seqlen + offs_n[None, :] * stride_z_hdim\n    )\n    dz_ptrs = dz_ptr + (\n        offs_m[:, None] * stride_dz_seqlen + offs_n[None, :] * stride_dz_hdim\n    )\n    if RECOMPUTE_OUTPUT:\n        outz_ptrs = outz_ptr + (\n            offs_m[:, None] * stride_outz_seqlen + offs_n[None, :] * stride_outz_hdim\n        )\n    if HAS_D:\n        x_ptrs = x_ptr + (\n            offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n        )\n        if D_HAS_HDIM:\n            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dout = tl.load(\n        dout_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    out = tl.load(\n        out_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    z = tl.load(\n        z_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    z_sigmoid = tl.sigmoid(z)\n    if RECOMPUTE_OUTPUT:\n        outz = out * z * z_sigmoid\n        tl.store(\n            outz_ptrs,\n            outz,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        )\n    dz = dout * out * z_sigmoid * (1 + z * (1 - z_sigmoid))\n    tl.store(\n        dz_ptrs,\n        dz,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n    )\n    dout *= z * z_sigmoid\n    tl.store(\n        dout_x_ptrs,\n        dout,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n    )\n    if HAS_D:\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n            other=0.0,\n        ).to(tl.float32)\n        if D_HAS_HDIM:\n            dD = tl.sum(dout * x, axis=0)\n            tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n            D = tl.load(\n                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0\n            ).to(tl.float32)\n        else:\n            dD = tl.sum(dout * x)\n            tl.store(dD_ptr, dD)\n            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n        out -= x * D\n    if HAS_DDACS:\n        ddA_cs = tl.sum(dout * out, axis=1)\n        tl.store(\n            ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize,\n            ddA_cs,\n            mask=offs_m < chunk_size,\n        )", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 63, 6, 108, 299, 63, 6, 108, 300, 63, 6, 108, 301, 63, 6, 108, 302, 63, 6, 108, 303, 63, 6, 174, 63, 32, -1, 304, 193, 170, 242, 305, 193, 347, 174, 32, 306, 193, 304, 48, 258, 32, 307, 193, 304, 4, 306, 243, 258, 32, 308, 193, 170, 242, 305, 193, 348, 174, 32, 309, 193, 170, 242, 305, 193, 349, 174, 32, 246, 171, 307, 243, 260, 74, 306, 243, 256, 243, 261, 74, 308, 243, 262, 32, 253, 171, 307, 243, 285, 74, 306, 243, 256, 243, 286, 74, 308, 243, 287, 32, 247, 171, 307, 243, 264, 74, 306, 243, 256, 243, 265, 74, 308, 243, 266, 32, 248, 171, 307, 243, 268, 74, 306, 243, 256, 243, 269, 74, 308, 243, 270, 32, 252, 171, 307, 243, 281, 74, 306, 243, 256, 243, 282, 74, 308, 243, 283, 32, 181, 301, 63, 32, 251, 171, 307, 243, 277, 74, 306, 243, 256, 243, 278, 74, 308, 243, 279, 32, 184, 32, 181, 300, 63, 32, 255, 171, 307, 243, 294, 74, 306, 243, 295, 74, 308, 243, 296, 32, 184, 32, 181, 298, 63, 32, 249, 171, 307, 243, 272, 74, 306, 243, 256, 243, 273, 74, 308, 243, 274, 32, 254, 171, 307, 243, 289, 74, 306, 243, 290, 74, 308, 243, 291, 74, 309, 243, 292, 32, 184, 32, 310, 193, 309, 243, 302, 74, 76, 242, 349, 108, 302, 174, 32, 311, 193, 76, 242, 349, 108, 303, 174, 32, 312, 193, 246, 74, 242, 310, 229, 63, 108, 201, 27, 243, 261, 74, 311, 229, 201, 108, 63, 27, 243, 263, 174, 32, 313, 193, 253, 74, 242, 310, 229, 63, 108, 201, 27, 243, 286, 74, 311, 229, 201, 108, 63, 27, 243, 288, 174, 32, 314, 193, 247, 74, 242, 310, 229, 63, 108, 201, 27, 243, 265, 74, 311, 229, 201, 108, 63, 27, 243, 267, 174, 32, 315, 193, 248, 74, 242, 310, 229, 63, 108, 201, 27, 243, 269, 74, 311, 229, 201, 108, 63, 27, 243, 271, 174, 32, 316, 193, 252, 74, 242, 310, 229, 63, 108, 201, 27, 243, 282, 74, 311, 229, 201, 108, 63, 27, 243, 284, 174, 32, 181, 301, 63, 32, 317, 193, 251, 74, 242, 310, 229, 63, 108, 201, 27, 243, 278, 74, 311, 229, 201, 108, 63, 27, 243, 280, 174, 32, 184, 32, 181, 298, 63, 32, 318, 193, 249, 74, 242, 310, 229, 63, 108, 201, 27, 243, 273, 74, 311, 229, 201, 108, 63, 27, 243, 275, 174, 32, 181, 299, 63, 32, 319, 193, 254, 74, 311, 243, 293, 32, 184, 32, 184, 32, 320, 193, 39, 242, 256, 108, 259, 4, 306, 243, 256, 174, 32, 321, 193, 57, 242, 312, 108, 322, 193, 242, 310, 229, 63, 108, 201, 27, 1, 320, 174, 172, 242, 311, 229, 201, 108, 63, 27, 1, 257, 174, 108, 323, 193, 349, 174, 82, 324, 242, 144, 174, 32, 14, 193, 57, 242, 314, 108, 322, 193, 242, 310, 229, 63, 108, 201, 27, 1, 320, 174, 172, 242, 311, 229, 201, 108, 63, 27, 1, 257, 174, 108, 323, 193, 349, 174, 82, 324, 242, 144, 174, 32, 325, 193, 57, 242, 315, 108, 322, 193, 242, 310, 229, 63, 108, 201, 27, 1, 320, 174, 172, 242, 311, 229, 201, 108, 63, 27, 1, 257, 174, 108, 323, 193, 349, 174, 82, 324, 242, 144, 174, 32, 326, 193, 205, 242, 325, 174, 32, 181, 301, 63, 32, 327, 193, 14, 243, 325, 243, 326, 32, 10, 242, 317, 108, 327, 108, 322, 193, 242, 310, 229, 63, 108, 201, 27, 1, 320, 174, 172, 242, 311, 229, 201, 108, 63, 27, 1, 257, 174, 174, 32, 184, 32, 328, 193, 321, 243, 14, 243, 326, 243, 242, 347, 74, 325, 243, 242, 347, 4, 326, 174, 174, 32, 10, 242, 316, 108, 328, 108, 322, 193, 242, 310, 229, 63, 108, 201, 27, 1, 320, 174, 172, 242, 311, 229, 201, 108, 63, 27, 1, 257, 174, 174, 32, 321, 24, 325, 243, 326, 32, 10, 242, 313, 108, 321, 108, 322, 193, 242, 310, 229, 63, 108, 201, 27, 1, 320, 174, 172, 242, 311, 229, 201, 108, 63, 27, 1, 257, 174, 174, 32, 181, 298, 63, 32, 329, 193, 57, 242, 318, 108, 322, 193, 242, 310, 229, 63, 108, 201, 27, 1, 320, 174, 172, 242, 311, 229, 201, 108, 63, 27, 1, 257, 174, 108, 323, 193, 349, 174, 82, 324, 242, 144, 174, 32, 181, 299, 63, 32, 330, 193, 220, 242, 321, 243, 329, 108, 305, 193, 349, 174, 32, 10, 242, 319, 108, 330, 108, 322, 193, 311, 1, 257, 174, 32, 331, 193, 57, 242, 250, 74, 308, 243, 276, 74, 311, 108, 322, 193, 311, 1, 257, 108, 323, 193, 349, 174, 82, 324, 242, 144, 174, 32, 184, 32, 30, 63, 32, 330, 193, 220, 242, 321, 243, 329, 174, 32, 10, 242, 254, 108, 330, 174, 32, 331, 193, 57, 242, 250, 74, 308, 243, 276, 174, 82, 324, 242, 144, 174, 32, 56, 32, 14, 2, 329, 243, 331, 32, 184, 32, 181, 300, 63, 32, 332, 193, 220, 242, 321, 243, 14, 108, 305, 193, 347, 174, 32, 10, 242, 255, 74, 310, 243, 297, 108, 332, 108, 322, 193, 310, 1, 256, 174, 32, 184, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_dstates_kernel(\n    dout_ptr,\n    c_ptr,\n    dprev_states_f_ptr,\n    dprev_states_b_ptr,\n    dA_cumsum_f_ptr,\n    dA_cumsum_b_ptr,\n    hdim,\n    dstate,\n    chunk_size,\n    batch,\n    seqlen,\n    nchunks,\n    nheads_ngroups_ratio,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_c_batch,\n    stride_c_seqlen,\n    stride_c_head,\n    stride_c_dstate,\n    stride_dprev_states_f_batch,\n    stride_dprev_states_f_chunk,\n    stride_dprev_states_f_head,\n    stride_dprev_states_f_hdim,\n    stride_dprev_states_f_dstate,\n    stride_dprev_states_b_batch,\n    stride_dprev_states_b_chunk,\n    stride_dprev_states_b_head,\n    stride_dprev_states_b_hdim,\n    stride_dprev_states_b_dstate,\n    stride_dA_cs_f_batch,\n    stride_dA_cs_f_chunk,\n    stride_dA_cs_f_head,\n    stride_dA_cs_f_csize,\n    stride_dA_cs_b_batch,\n    stride_dA_cs_b_chunk,\n    stride_dA_cs_b_head,\n    stride_dA_cs_b_csize,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    c_ptr += (\n        pid_b * stride_c_batch\n        + pid_c * chunk_size * stride_c_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_c_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dA_cumsum_f_ptr += (\n        pid_b * stride_dA_cs_f_batch\n        + pid_c * stride_dA_cs_f_chunk\n        + pid_h * stride_dA_cs_f_head\n    )\n    dA_cumsum_b_ptr += (\n        pid_b * stride_dA_cs_b_batch\n        + pid_c * stride_dA_cs_b_chunk\n        + pid_h * stride_dA_cs_b_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_hdim + offs_k[None, :] * stride_dout_seqlen\n    )\n    c_ptrs = c_ptr + (\n        offs_n[None, :] * stride_c_dstate + offs_k[:, None] * stride_c_seqlen\n    )\n    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize\n    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc_f = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    acc_b = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n        dout = tl.load(\n            dout_ptrs,\n            mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k),\n            other=0.0,\n        ).to(tl.float32)\n        dA_cs_f_k = tl.load(\n            dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0\n        ).to(tl.float32)\n        dA_cs_b_k = tl.load(\n            dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0\n        ).to(tl.float32)\n        scale_f_k = tl.exp(dA_cs_f_k)\n        scale_b_k = tl.exp(dA_cs_b_k)\n        dout_f = (dout * scale_f_k).to(dout_ptr.dtype.element_ty)\n        dout_b = (dout * scale_b_k).to(dout_ptr.dtype.element_ty)\n        c = tl.load(\n            c_ptrs,\n            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate),\n            other=0.0,\n        )\n        acc_f += tl.dot(dout_f, c)\n        acc_b += tl.dot(dout_b, c)\n        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n        c_ptrs += BLOCK_SIZE_K * stride_c_seqlen\n        dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize\n        dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize\n    out_f = acc_f.to(dprev_states_f_ptr.dtype.element_ty)\n    out_b = acc_b.to(dprev_states_b_ptr.dtype.element_ty)\n\n    dprev_states_f_ptr += (\n        pid_b * stride_dprev_states_f_batch\n        + pid_c * stride_dprev_states_f_chunk\n        + pid_h * stride_dprev_states_f_head\n    )\n    dprev_states_b_ptr += (\n        pid_b * stride_dprev_states_b_batch\n        + pid_c * stride_dprev_states_b_chunk\n        + pid_h * stride_dprev_states_b_head\n    )\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dprev_states_f_ptrs = dprev_states_f_ptr + (\n        offs_m[:, None] * stride_dprev_states_f_hdim\n        + offs_n[None, :] * stride_dprev_states_f_dstate\n    )\n    dprev_states_b_ptrs = dprev_states_b_ptr + (\n        offs_m[:, None] * stride_dprev_states_b_hdim\n        + offs_n[None, :] * stride_dprev_states_b_dstate\n    )\n    tl.store(\n        dprev_states_f_ptrs,\n        out_f,\n        mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),\n    )\n    tl.store(\n        dprev_states_b_ptrs,\n        out_b,\n        mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),\n    )", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 62, 6, 108, 286, 62, 6, 108, 287, 62, 6, 174, 62, 32, -1, 288, 193, 170, 242, 289, 193, 347, 174, 32, 290, 193, 288, 48, 255, 32, 291, 193, 288, 4, 290, 243, 255, 32, 292, 193, 170, 242, 289, 193, 348, 174, 32, 293, 193, 64, 242, 253, 108, 286, 174, 32, 294, 193, 170, 242, 289, 193, 349, 174, 48, 293, 32, 295, 193, 170, 242, 289, 193, 349, 174, 225, 293, 32, 247, 171, 291, 243, 263, 74, 290, 243, 254, 243, 264, 74, 292, 48, 258, 243, 265, 32, 246, 171, 291, 243, 259, 74, 290, 243, 254, 243, 260, 74, 292, 243, 261, 32, 250, 171, 291, 243, 277, 74, 290, 243, 278, 74, 292, 243, 279, 32, 251, 171, 291, 243, 281, 74, 290, 243, 282, 74, 292, 243, 283, 32, 296, 193, 294, 243, 285, 74, 76, 242, 349, 108, 285, 174, 32, 297, 193, 295, 243, 286, 74, 76, 242, 349, 108, 286, 174, 32, 298, 193, 76, 242, 349, 108, 287, 174, 32, 299, 193, 246, 74, 242, 296, 229, 62, 108, 201, 27, 243, 262, 74, 298, 229, 201, 108, 62, 27, 243, 260, 174, 32, 300, 193, 247, 74, 242, 297, 229, 201, 108, 62, 27, 243, 266, 74, 298, 229, 62, 108, 201, 27, 243, 264, 174, 32, 301, 193, 250, 74, 298, 243, 280, 32, 302, 193, 251, 74, 298, 243, 284, 32, 303, 193, 39, 242, 254, 108, 256, 4, 290, 243, 254, 174, 32, 304, 193, 175, 242, 242, 285, 108, 286, 174, 108, 91, 193, 144, 174, 32, 305, 193, 175, 242, 242, 285, 108, 286, 174, 108, 91, 193, 144, 174, 32, 135, 306, 156, 5, 242, 349, 108, 303, 108, 287, 174, 62, 32, 307, 193, 55, 242, 299, 108, 308, 193, 242, 296, 229, 62, 108, 201, 27, 1, 252, 174, 172, 242, 298, 229, 201, 108, 62, 27, 1, 303, 4, 306, 174, 108, 309, 193, 349, 174, 82, 310, 242, 144, 174, 32, 311, 193, 55, 242, 301, 108, 308, 193, 298, 1, 254, 4, 306, 108, 309, 193, 349, 174, 82, 310, 242, 144, 174, 32, 312, 193, 55, 242, 302, 108, 308, 193, 298, 1, 254, 4, 306, 108, 309, 193, 349, 174, 82, 310, 242, 144, 174, 32, 313, 193, 107, 242, 311, 174, 32, 314, 193, 107, 242, 312, 174, 32, 315, 193, 242, 307, 243, 313, 174, 82, 310, 242, 246, 82, 91, 82, 114, 174, 32, 316, 193, 242, 307, 243, 314, 174, 82, 310, 242, 246, 82, 91, 82, 114, 174, 32, 317, 193, 55, 242, 300, 108, 308, 193, 242, 298, 229, 62, 108, 201, 27, 1, 303, 4, 306, 174, 172, 242, 297, 229, 201, 108, 62, 27, 1, 253, 174, 108, 309, 193, 349, 174, 32, 304, 171, 15, 242, 315, 108, 317, 174, 32, 305, 171, 15, 242, 316, 108, 317, 174, 32, 299, 171, 287, 243, 260, 32, 300, 171, 287, 243, 264, 32, 301, 171, 287, 243, 280, 32, 302, 171, 287, 243, 284, 32, 78, 32, 318, 193, 304, 82, 310, 242, 248, 82, 91, 82, 114, 174, 32, 319, 193, 305, 82, 310, 242, 249, 82, 91, 82, 114, 174, 32, 248, 171, 291, 243, 267, 74, 290, 243, 268, 74, 292, 243, 269, 32, 249, 171, 291, 243, 272, 74, 290, 243, 273, 74, 292, 243, 274, 32, 296, 193, 294, 243, 285, 74, 76, 242, 349, 108, 285, 174, 32, 297, 193, 295, 243, 286, 74, 76, 242, 349, 108, 286, 174, 32, 320, 193, 248, 74, 242, 296, 229, 62, 108, 201, 27, 243, 270, 74, 297, 229, 201, 108, 62, 27, 243, 271, 174, 32, 321, 193, 249, 74, 242, 296, 229, 62, 108, 201, 27, 243, 275, 74, 297, 229, 201, 108, 62, 27, 243, 276, 174, 32, 10, 242, 320, 108, 318, 108, 308, 193, 242, 296, 229, 62, 108, 201, 27, 1, 252, 174, 172, 242, 297, 229, 201, 108, 62, 27, 1, 253, 174, 174, 32, 10, 242, 321, 108, 319, 108, 308, 193, 242, 296, 229, 62, 108, 201, 27, 1, 252, 174, 172, 242, 297, 229, 201, 108, 62, 27, 1, 253, 174, 174, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_dc_kernel(\n    dout_ptr,\n    prev_states_f_ptr,\n    prev_states_b_ptr,\n    C_ptr,\n    dA_cumsum_f_ptr,\n    dA_cumsum_b_ptr,\n    dc_ptr,\n    ddA_cumsum_f_ptr,\n    ddA_cumsum_b_ptr,\n    chunk_size,\n    dstate,\n    hdim,\n    batch,\n    seqlen,\n    nheads,\n    nheads_per_program,\n    ngroups,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_prev_states_f_batch,\n    stride_prev_states_f_chunk,\n    stride_prev_states_f_head,\n    stride_prev_states_f_hdim,\n    stride_prev_states_f_dstate,\n    stride_prev_states_b_batch,\n    stride_prev_states_b_chunk,\n    stride_prev_states_b_head,\n    stride_prev_states_b_hdim,\n    stride_prev_states_b_dstate,\n    stride_C_batch,\n    stride_C_seqlen,\n    stride_C_head,\n    stride_C_dstate,\n    stride_dA_cs_f_batch,\n    stride_dA_cs_f_chunk,\n    stride_dA_cs_f_head,\n    stride_dA_cs_f_csize,\n    stride_dA_cs_b_batch,\n    stride_dA_cs_b_chunk,\n    stride_dA_cs_b_head,\n    stride_dA_cs_b_csize,\n    stride_dc_batch,\n    stride_dc_seqlen,\n    stride_dc_split,\n    stride_dc_group,\n    stride_dc_dstate,\n    stride_ddA_cs_f_batch,\n    stride_ddA_cs_f_chunk,\n    stride_ddA_cs_f_head,\n    stride_ddA_cs_f_csize,\n    stride_ddA_cs_b_batch,\n    stride_ddA_cs_b_chunk,\n    stride_ddA_cs_b_head,\n    stride_ddA_cs_b_csize,\n    HAS_DDA_CS: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_sg = tl.program_id(axis=2)\n    pid_s = pid_sg // ngroups\n    pid_g = pid_sg - pid_s * ngroups\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head\n    )\n    dc_ptr += (\n        pid_b * stride_dc_batch\n        + pid_c * chunk_size * stride_dc_seqlen\n        + pid_g * stride_dc_group\n        + pid_s * stride_dc_split\n    )\n    prev_states_f_ptr += (\n        pid_b * stride_prev_states_f_batch\n        + pid_c * stride_prev_states_f_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n        * stride_prev_states_f_head\n    )\n    prev_states_b_ptr += (\n        pid_b * stride_prev_states_b_batch\n        + pid_c * stride_prev_states_b_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n        * stride_prev_states_b_head\n    )\n    dA_cumsum_f_ptr += (\n        pid_b * stride_dA_cs_f_batch\n        + pid_c * stride_dA_cs_f_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n        * stride_dA_cs_f_head\n    )\n    dA_cumsum_b_ptr += (\n        pid_b * stride_dA_cs_b_batch\n        + pid_c * stride_dA_cs_b_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n        * stride_dA_cs_b_head\n    )\n    if HAS_DDA_CS:\n        C_ptr += (\n            pid_b * stride_C_batch\n            + pid_c * chunk_size * stride_C_seqlen\n            + pid_g * stride_C_head\n        )\n        ddA_cumsum_f_ptr += (\n            pid_b * stride_ddA_cs_f_batch\n            + pid_c * stride_ddA_cs_f_chunk\n            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n            * stride_ddA_cs_f_head\n        )\n        ddA_cumsum_b_ptr += (\n            pid_b * stride_ddA_cs_b_batch\n            + pid_c * stride_ddA_cs_b_chunk\n            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n            * stride_ddA_cs_b_head\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim\n    )\n    prev_states_f_ptrs = prev_states_f_ptr + (\n        offs_n[None, :] * stride_prev_states_f_dstate\n        + offs_k[:, None] * stride_prev_states_f_hdim\n    )\n    prev_states_b_ptrs = prev_states_b_ptr + (\n        offs_n[None, :] * stride_prev_states_b_dstate\n        + offs_k[:, None] * stride_prev_states_b_hdim\n    )\n    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize\n    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize\n    if HAS_DDA_CS:\n        C_ptrs = C_ptr + (\n            offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate\n        )\n        ddA_cumsum_f_ptrs = ddA_cumsum_f_ptr + offs_m * stride_ddA_cs_f_csize\n        ddA_cumsum_b_ptrs = ddA_cumsum_b_ptr + offs_m * stride_ddA_cs_b_csize\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_DDA_CS:\n        c = tl.load(\n            C_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),\n            other=0.0,\n        ).to(tl.float32)\n    nheads_iter = min(\n        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program\n    )\n    for h in range(nheads_iter):\n        dout = tl.load(\n            dout_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n            other=0.0,\n        )\n\n        prev_states_f = tl.load(\n            prev_states_f_ptrs,\n            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),\n            other=0.0,\n        )\n        prev_states_f = prev_states_f.to(dout_ptrs.dtype.element_ty)\n        dc_f = tl.dot(dout, prev_states_f)\n        dA_cs_f_m = tl.load(\n            dA_cumsum_f_ptrs, mask=offs_m < chunk_size_limit, other=0.0\n        ).to(tl.float32)\n        scale_f = tl.exp(dA_cs_f_m)\n        dc_f *= scale_f[:, None]\n        if HAS_DDA_CS:\n            ddA_cs_f = tl.sum(dc_f * c, axis=1)\n            tl.atomic_add(ddA_cumsum_f_ptrs, ddA_cs_f, mask=offs_m < chunk_size)\n\n        prev_states_b = tl.load(\n            prev_states_b_ptrs,\n            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),\n            other=0.0,\n        ).to(tl.float32)\n        prev_states_b = prev_states_b.to(dout_ptrs.dtype.element_ty)\n        dc_b = tl.dot(dout, prev_states_b)\n        dA_cs_b_m = tl.load(\n            dA_cumsum_b_ptrs, mask=offs_m < chunk_size_limit, other=0.0\n        ).to(tl.float32)\n        scale_b = tl.exp(dA_cs_b_m)\n        dc_b *= scale_b[:, None]\n        if HAS_DDA_CS:\n            ddA_cs_b = tl.sum(dc_b * c, axis=1)\n            tl.atomic_add(ddA_cumsum_b_ptrs, ddA_cs_b, mask=offs_m < chunk_size)\n\n        acc += dc_f + dc_b\n        dout_ptrs += stride_dout_head\n        prev_states_f_ptrs += stride_prev_states_f_head\n        prev_states_b_ptrs += stride_prev_states_b_head\n        dA_cumsum_f_ptrs += stride_dA_cs_f_head\n        dA_cumsum_b_ptrs += stride_dA_cs_b_head\n        if HAS_DDA_CS:\n            ddA_cumsum_f_ptrs += stride_ddA_cs_f_head\n            ddA_cumsum_b_ptrs += stride_ddA_cs_b_head\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dc_ptrs = dc_ptr + (\n        offs_m[:, None] * stride_dc_seqlen + offs_n[None, :] * stride_dc_dstate\n    )\n    tl.store(\n        dc_ptrs,\n        acc,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),\n    )", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 108, 301, 108, 302, 63, 6, 108, 303, 63, 6, 108, 304, 63, 6, 108, 305, 63, 6, 174, 63, 32, -1, 306, 193, 170, 242, 307, 193, 347, 174, 32, 308, 193, 306, 48, 258, 32, 309, 193, 306, 4, 308, 243, 258, 32, 310, 193, 170, 242, 307, 193, 348, 174, 32, 311, 193, 310, 48, 262, 32, 312, 193, 310, 4, 311, 243, 262, 32, 313, 193, 65, 242, 256, 108, 304, 174, 32, 314, 193, 170, 242, 307, 193, 349, 174, 48, 313, 32, 315, 193, 170, 242, 307, 193, 349, 174, 225, 313, 32, 246, 171, 309, 243, 263, 74, 308, 243, 255, 243, 264, 74, 242, 312, 243, 242, 260, 48, 262, 174, 74, 311, 243, 261, 174, 243, 265, 32, 252, 171, 309, 243, 289, 74, 308, 243, 255, 243, 290, 74, 312, 243, 292, 74, 311, 243, 291, 32, 247, 171, 309, 243, 267, 74, 308, 243, 268, 74, 242, 312, 243, 242, 260, 48, 262, 174, 74, 311, 243, 261, 174, 243, 269, 32, 248, 171, 309, 243, 272, 74, 308, 243, 273, 74, 242, 312, 243, 242, 260, 48, 262, 174, 74, 311, 243, 261, 174, 243, 274, 32, 250, 171, 309, 243, 281, 74, 308, 243, 282, 74, 242, 312, 243, 242, 260, 48, 262, 174, 74, 311, 243, 261, 174, 243, 283, 32, 251, 171, 309, 243, 285, 74, 308, 243, 286, 74, 242, 312, 243, 242, 260, 48, 262, 174, 74, 311, 243, 261, 174, 243, 287, 32, 181, 302, 63, 32, 249, 171, 309, 243, 277, 74, 308, 243, 255, 243, 278, 74, 312, 243, 279, 32, 253, 171, 309, 243, 294, 74, 308, 243, 295, 74, 242, 312, 243, 242, 260, 48, 262, 174, 74, 311, 243, 261, 174, 243, 296, 32, 254, 171, 309, 243, 298, 74, 308, 243, 299, 74, 242, 312, 243, 242, 260, 48, 262, 174, 74, 311, 243, 261, 174, 243, 300, 32, 184, 32, 316, 193, 314, 243, 303, 74, 76, 242, 349, 108, 303, 174, 32, 317, 193, 315, 243, 304, 74, 76, 242, 349, 108, 304, 174, 32, 318, 193, 76, 242, 349, 108, 305, 174, 32, 319, 193, 246, 74, 242, 316, 229, 63, 108, 201, 27, 243, 264, 74, 318, 229, 201, 108, 63, 27, 243, 266, 174, 32, 320, 193, 247, 74, 242, 317, 229, 201, 108, 63, 27, 243, 271, 74, 318, 229, 63, 108, 201, 27, 243, 270, 174, 32, 321, 193, 248, 74, 242, 317, 229, 201, 108, 63, 27, 243, 276, 74, 318, 229, 63, 108, 201, 27, 243, 275, 174, 32, 322, 193, 250, 74, 316, 243, 284, 32, 323, 193, 251, 74, 316, 243, 288, 32, 181, 302, 63, 32, 324, 193, 249, 74, 242, 316, 229, 63, 108, 201, 27, 243, 278, 74, 317, 229, 201, 108, 63, 27, 243, 280, 174, 32, 325, 193, 253, 74, 316, 243, 297, 32, 326, 193, 254, 74, 316, 243, 301, 32, 184, 32, 327, 193, 39, 242, 255, 108, 259, 4, 308, 243, 255, 174, 32, 328, 193, 175, 242, 242, 303, 108, 304, 174, 108, 91, 193, 144, 174, 32, 181, 302, 63, 32, 329, 193, 57, 242, 324, 108, 330, 193, 242, 316, 229, 63, 108, 201, 27, 1, 327, 174, 172, 242, 317, 229, 201, 108, 63, 27, 1, 256, 174, 108, 331, 193, 349, 174, 82, 332, 242, 144, 174, 32, 184, 32, 333, 193, 39, 242, 261, 108, 260, 48, 262, 4, 311, 243, 261, 174, 32, 135, 334, 156, 5, 242, 333, 174, 63, 32, 335, 193, 57, 242, 319, 108, 330, 193, 242, 316, 229, 63, 108, 201, 27, 1, 327, 174, 172, 242, 318, 229, 201, 108, 63, 27, 1, 257, 174, 108, 331, 193, 349, 174, 32, 336, 193, 57, 242, 320, 108, 330, 193, 242, 318, 229, 63, 108, 201, 27, 1, 257, 174, 172, 242, 317, 229, 201, 108, 63, 27, 1, 256, 174, 108, 331, 193, 349, 174, 32, 336, 193, 336, 82, 332, 242, 319, 82, 91, 82, 114, 174, 32, 337, 193, 15, 242, 335, 108, 336, 174, 32, 338, 193, 57, 242, 322, 108, 330, 193, 316, 1, 327, 108, 331, 193, 349, 174, 82, 332, 242, 144, 174, 32, 339, 193, 107, 242, 338, 174, 32, 337, 24, 339, 229, 63, 108, 201, 27, 32, 181, 302, 63, 32, 340, 193, 220, 242, 337, 243, 329, 108, 307, 193, 347, 174, 32, 195, 242, 325, 108, 340, 108, 330, 193, 316, 1, 255, 174, 32, 184, 32, 341, 193, 57, 242, 321, 108, 330, 193, 242, 318, 229, 63, 108, 201, 27, 1, 257, 174, 172, 242, 317, 229, 201, 108, 63, 27, 1, 256, 174, 108, 331, 193, 349, 174, 82, 332, 242, 144, 174, 32, 341, 193, 341, 82, 332, 242, 319, 82, 91, 82, 114, 174, 32, 342, 193, 15, 242, 335, 108, 341, 174, 32, 343, 193, 57, 242, 323, 108, 330, 193, 316, 1, 327, 108, 331, 193, 349, 174, 82, 332, 242, 144, 174, 32, 344, 193, 107, 242, 343, 174, 32, 342, 24, 344, 229, 63, 108, 201, 27, 32, 181, 302, 63, 32, 345, 193, 220, 242, 342, 243, 329, 108, 307, 193, 347, 174, 32, 195, 242, 326, 108, 345, 108, 330, 193, 316, 1, 255, 174, 32, 184, 32, 328, 171, 337, 74, 342, 32, 319, 171, 265, 32, 320, 171, 269, 32, 321, 171, 274, 32, 322, 171, 283, 32, 323, 171, 287, 32, 181, 302, 63, 32, 325, 171, 296, 32, 326, 171, 300, 32, 184, 32, 78, 32, 316, 193, 314, 243, 303, 74, 76, 242, 349, 108, 303, 174, 32, 317, 193, 315, 243, 304, 74, 76, 242, 349, 108, 304, 174, 32, 350, 193, 252, 74, 242, 316, 229, 63, 108, 201, 27, 243, 290, 74, 317, 229, 201, 108, 63, 27, 243, 293, 174, 32, 10, 242, 350, 108, 328, 108, 330, 193, 242, 316, 229, 63, 108, 201, 27, 1, 327, 174, 172, 242, 317, 229, 201, 108, 63, 27, 1, 256, 174, 174, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_dx_kernel(\n    x_ptr,\n    cb_ptr,\n    dout_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    D_ptr,\n    dx_ptr,\n    ddt_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_k,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_D_head,\n    stride_dx_batch,\n    stride_dx_seqlen,\n    stride_dx_head,\n    stride_dx_hdim,\n    stride_ddt_batch,\n    stride_ddt_chunk,\n    stride_ddt_head,\n    stride_ddt_csize,\n    HAS_D: tl.constexpr,\n    D_HAS_HDIM: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    ddt_ptr += (\n        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\n    )\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k\n    )\n    dout_ptrs = dout_ptr + (\n        offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim\n    )\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dA_cs_m = tl.load(\n        dA_cumsum_ptr + offs_m * stride_dA_cs_csize,\n        mask=offs_m < chunk_size_limit,\n        other=0.0,\n    ).to(tl.float32)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    K_MAX = chunk_size_limit\n    for k in range(0, K_MAX, BLOCK_SIZE_K):\n\n        cb = tl.load(\n            cb_ptrs,\n            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k),\n            other=0.0,\n        )\n        dout = tl.load(\n            dout_ptrs,\n            mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0).to(\n            tl.float32\n        )\n        cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])\n\n        mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)\n        cb = tl.where(mask, cb, 0.0)\n        cb = cb.to(dout_ptr.dtype.element_ty)\n        acc += tl.dot(cb, dout)\n        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\n    dx = acc * dt_m[:, None]\n    dx_ptr += (\n        pid_b * stride_dx_batch\n        + pid_c * chunk_size * stride_dx_seqlen\n        + pid_h * stride_dx_head\n    )\n    dx_ptrs = dx_ptr + (\n        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim\n    )\n    if HAS_D:\n        dout_res_ptrs = dout_ptr + (\n            offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim\n        )\n        dout_res = tl.load(\n            dout_res_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n            other=0.0,\n        ).to(tl.float32)\n        if D_HAS_HDIM:\n            D = tl.load(\n                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0\n            ).to(tl.float32)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n        dx += dout_res * D\n    tl.store(\n        dx_ptrs,\n        dx,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n    )\n\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n    )\n    x = tl.load(\n        x_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    ddt = tl.sum(acc * x, axis=1)\n    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\n    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 62, 6, 108, 290, 62, 6, 108, 291, 62, 6, 108, 292, 62, 6, 108, 293, 62, 6, 174, 62, 32, -1, 294, 193, 170, 242, 295, 193, 347, 174, 32, 296, 193, 294, 48, 256, 32, 297, 193, 294, 4, 296, 243, 256, 32, 298, 193, 170, 242, 295, 193, 348, 174, 32, 299, 193, 64, 242, 255, 108, 292, 174, 32, 300, 193, 170, 242, 295, 193, 349, 174, 48, 299, 32, 301, 193, 170, 242, 295, 193, 349, 174, 225, 299, 32, 246, 171, 297, 243, 259, 74, 296, 243, 254, 243, 260, 74, 298, 243, 261, 32, 247, 171, 297, 243, 263, 74, 296, 243, 264, 74, 298, 48, 258, 243, 265, 32, 248, 171, 297, 243, 268, 74, 296, 243, 254, 243, 269, 74, 298, 243, 270, 32, 249, 171, 297, 243, 272, 74, 296, 243, 273, 74, 298, 243, 274, 32, 253, 171, 297, 243, 285, 74, 296, 243, 286, 74, 298, 243, 287, 32, 250, 171, 297, 243, 276, 74, 296, 243, 277, 74, 298, 243, 278, 32, 302, 193, 300, 243, 291, 74, 76, 242, 349, 108, 291, 174, 32, 303, 193, 301, 243, 292, 74, 76, 242, 349, 108, 292, 174, 32, 304, 193, 76, 242, 349, 108, 293, 174, 32, 305, 193, 247, 74, 242, 302, 229, 62, 108, 201, 27, 243, 266, 74, 304, 229, 201, 108, 62, 27, 243, 267, 174, 32, 306, 193, 248, 74, 242, 304, 229, 62, 108, 201, 27, 243, 269, 74, 303, 229, 201, 108, 62, 27, 243, 271, 174, 32, 307, 193, 250, 74, 304, 243, 279, 32, 308, 193, 39, 242, 254, 108, 257, 4, 296, 243, 254, 174, 32, 309, 193, 55, 242, 250, 74, 302, 243, 279, 108, 310, 193, 302, 1, 308, 108, 311, 193, 349, 174, 82, 312, 242, 144, 174, 32, 313, 193, 175, 242, 242, 291, 108, 292, 174, 108, 91, 193, 144, 174, 32, 314, 193, 308, 32, 135, 315, 156, 5, 242, 349, 108, 314, 108, 293, 174, 62, 32, 316, 193, 55, 242, 305, 108, 310, 193, 242, 302, 229, 62, 108, 201, 27, 1, 254, 174, 172, 242, 304, 229, 201, 108, 62, 27, 1, 314, 4, 315, 174, 108, 311, 193, 349, 174, 32, 317, 193, 55, 242, 306, 108, 310, 193, 242, 304, 229, 62, 108, 201, 27, 1, 314, 4, 315, 174, 172, 242, 303, 229, 201, 108, 62, 27, 1, 255, 174, 108, 311, 193, 349, 174, 32, 318, 193, 55, 242, 307, 108, 310, 193, 304, 1, 314, 4, 315, 108, 311, 193, 349, 174, 82, 312, 242, 144, 174, 32, 316, 24, 107, 242, 318, 229, 201, 108, 62, 27, 4, 309, 229, 62, 108, 201, 27, 174, 32, 310, 193, 242, 315, 74, 304, 229, 201, 108, 62, 27, 143, 302, 229, 62, 108, 201, 27, 174, 172, 242, 315, 74, 304, 229, 201, 108, 62, 27, 1, 314, 174, 32, 316, 193, 204, 242, 310, 108, 316, 108, 349, 174, 32, 316, 193, 316, 82, 312, 242, 248, 82, 91, 82, 114, 174, 32, 313, 171, 15, 242, 316, 108, 317, 174, 32, 305, 171, 293, 243, 267, 32, 306, 171, 293, 243, 269, 32, 307, 171, 293, 243, 279, 32, 78, 32, 302, 193, 300, 243, 291, 74, 76, 242, 349, 108, 291, 174, 32, 303, 193, 301, 243, 292, 74, 76, 242, 349, 108, 292, 174, 32, 319, 193, 249, 74, 302, 243, 275, 32, 320, 193, 55, 242, 319, 108, 310, 193, 302, 1, 308, 108, 311, 193, 349, 174, 82, 312, 242, 144, 174, 32, 321, 193, 313, 243, 320, 229, 62, 108, 201, 27, 32, 252, 171, 297, 243, 281, 74, 296, 243, 254, 243, 282, 74, 298, 243, 283, 32, 322, 193, 252, 74, 242, 302, 229, 62, 108, 201, 27, 243, 282, 74, 303, 229, 201, 108, 62, 27, 243, 284, 174, 32, 181, 289, 62, 32, 323, 193, 248, 74, 242, 302, 229, 62, 108, 201, 27, 243, 269, 74, 303, 229, 201, 108, 62, 27, 243, 271, 174, 32, 324, 193, 55, 242, 323, 108, 310, 193, 242, 302, 229, 62, 108, 201, 27, 1, 308, 174, 172, 242, 303, 229, 201, 108, 62, 27, 1, 255, 174, 108, 311, 193, 349, 174, 82, 312, 242, 144, 174, 32, 181, 290, 62, 32, 325, 193, 55, 242, 251, 74, 298, 243, 280, 74, 303, 108, 310, 193, 303, 1, 255, 108, 311, 193, 349, 174, 82, 312, 242, 144, 174, 32, 184, 32, 30, 62, 32, 325, 193, 55, 242, 251, 74, 298, 243, 280, 174, 82, 312, 242, 144, 174, 32, 56, 32, 321, 171, 324, 243, 325, 32, 184, 32, 10, 242, 322, 108, 321, 108, 310, 193, 242, 302, 229, 62, 108, 201, 27, 1, 308, 174, 172, 242, 303, 229, 201, 108, 62, 27, 1, 255, 174, 174, 32, 326, 193, 246, 74, 242, 302, 229, 62, 108, 201, 27, 243, 260, 74, 303, 229, 201, 108, 62, 27, 243, 262, 174, 32, 327, 193, 55, 242, 326, 108, 310, 193, 242, 302, 229, 62, 108, 201, 27, 1, 308, 174, 172, 242, 303, 229, 201, 108, 62, 27, 1, 255, 174, 108, 311, 193, 349, 174, 82, 312, 242, 144, 174, 32, 328, 193, 220, 242, 313, 243, 327, 108, 295, 193, 347, 174, 32, 329, 193, 253, 74, 302, 243, 288, 32, 195, 242, 329, 108, 328, 108, 310, 193, 302, 1, 254, 174, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_dcb_kernel(\n    x_ptr,\n    dout_ptr,\n    dt_ptr,\n    dA_cumsum_f_ptr,\n    dA_cumsum_b_ptr,\n    dcb_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    nheads,\n    nheads_per_program,\n    ngroups,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_f_batch,\n    stride_dA_cs_f_chunk,\n    stride_dA_cs_f_head,\n    stride_dA_cs_f_csize,\n    stride_dA_cs_b_batch,\n    stride_dA_cs_b_chunk,\n    stride_dA_cs_b_head,\n    stride_dA_cs_b_csize,\n    stride_dcb_batch,\n    stride_dcb_chunk,\n    stride_dcb_split,\n    stride_dcb_group,\n    stride_dcb_csize_m,\n    stride_dcb_csize_n,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_sg = tl.program_id(axis=2)\n    pid_s = pid_sg // ngroups\n    pid_g = pid_sg - pid_s * ngroups\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head\n    )\n    dt_ptr += (\n        pid_b * stride_dt_batch\n        + pid_c * stride_dt_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head\n    )\n    dA_cumsum_f_ptr += (\n        pid_b * stride_dA_cs_f_batch\n        + pid_c * stride_dA_cs_f_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n        * stride_dA_cs_f_head\n    )\n    dA_cumsum_b_ptr += (\n        pid_b * stride_dA_cs_b_batch\n        + pid_c * stride_dA_cs_b_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n        * stride_dA_cs_b_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim\n    )\n    x_ptrs = x_ptr + (\n        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim\n    )\n    dt_ptrs = dt_ptr + offs_n * stride_dt_csize\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    nheads_iter = min(\n        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program\n    )\n    for h in range(nheads_iter):\n        dout = tl.load(\n            dout_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n            other=0.0,\n        )\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit),\n            other=0.0,\n        )\n        dcb = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)\n        dcb *= dt_n\n        if (pid_n * BLOCK_SIZE_N < (pid_m + 1) * BLOCK_SIZE_M) and (\n            (pid_n + 1) * BLOCK_SIZE_N > (pid_m) * BLOCK_SIZE_M\n        ):\n\n            dA_cs_f_m = tl.load(\n                dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize,\n                mask=offs_m < chunk_size_limit,\n                other=0.0,\n            ).to(tl.float32)\n            dA_cs_f_n = tl.load(\n                dA_cumsum_f_ptr + offs_n * stride_dA_cs_f_csize,\n                mask=offs_n < chunk_size_limit,\n                other=0.0,\n            ).to(tl.float32)\n            mask = offs_m[:, None] >= offs_n[None, :]\n            A_f = tl.where(mask, tl.exp(dA_cs_f_m[:, None] - dA_cs_f_n[None, :]), 0.0)\n            dA_cs_b_m = tl.load(\n                dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize,\n                mask=offs_m < chunk_size_limit,\n                other=0.0,\n            ).to(tl.float32)\n            dA_cs_b_n = tl.load(\n                dA_cumsum_b_ptr + offs_n * stride_dA_cs_b_csize,\n                mask=offs_n < chunk_size_limit,\n                other=0.0,\n            ).to(tl.float32)\n            mask = offs_m[:, None] <= offs_n[None, :]\n            A_b = tl.where(mask, tl.exp(dA_cs_b_m[:, None] - dA_cs_b_n[None, :]), 0.0)\n            dcb *= A_f + A_b\n        elif pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n\n            dA_cs_b_m = tl.load(\n                dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize,\n                mask=offs_m < chunk_size_limit,\n                other=0.0,\n            ).to(tl.float32)\n            dA_cs_b_n = tl.load(\n                dA_cumsum_b_ptr + offs_n * stride_dA_cs_b_csize,\n                mask=offs_n < chunk_size_limit,\n                other=0.0,\n            ).to(tl.float32)\n            mask = offs_m[:, None] <= offs_n[None, :]\n            dcb *= tl.where(mask, tl.exp(dA_cs_b_m[:, None] - dA_cs_b_n[None, :]), 0.0)\n        else:\n\n            dA_cs_f_m = tl.load(\n                dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize,\n                mask=offs_m < chunk_size_limit,\n                other=0.0,\n            ).to(tl.float32)\n            dA_cs_f_n = tl.load(\n                dA_cumsum_f_ptr + offs_n * stride_dA_cs_f_csize,\n                mask=offs_n < chunk_size_limit,\n                other=0.0,\n            ).to(tl.float32)\n            mask = offs_m[:, None] >= offs_n[None, :]\n            dcb *= tl.where(mask, tl.exp(dA_cs_f_m[:, None] - dA_cs_f_n[None, :]), 0.0)\n        acc += dcb\n        dout_ptrs += stride_dout_head\n        x_ptrs += stride_x_head\n        dt_ptrs += stride_dt_head\n        dA_cumsum_f_ptr += stride_dA_cs_f_head\n        dA_cumsum_b_ptr += stride_dA_cs_b_head\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dcb_ptr += (\n        pid_b * stride_dcb_batch\n        + pid_c * stride_dcb_chunk\n        + pid_g * stride_dcb_group\n        + pid_s * stride_dcb_split\n    )\n    dcb_ptrs = dcb_ptr + (\n        offs_m[:, None] * stride_dcb_csize_m + offs_n[None, :] * stride_dcb_csize_n\n    )\n    tl.store(\n        dcb_ptrs,\n        acc,\n        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),\n    )", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 63, 6, 108, 286, 63, 6, 108, 287, 63, 6, 174, 63, 32, -1, 288, 193, 170, 242, 289, 193, 347, 174, 32, 290, 193, 288, 48, 254, 32, 291, 193, 288, 4, 290, 243, 254, 32, 292, 193, 170, 242, 289, 193, 348, 174, 32, 293, 193, 292, 48, 258, 32, 294, 193, 292, 4, 293, 243, 258, 32, 295, 193, 65, 242, 252, 108, 286, 174, 32, 296, 193, 170, 242, 289, 193, 349, 174, 48, 295, 32, 297, 193, 170, 242, 289, 193, 349, 174, 225, 295, 32, 246, 171, 291, 243, 259, 74, 290, 243, 252, 243, 260, 74, 242, 294, 243, 242, 256, 48, 258, 174, 74, 293, 243, 257, 174, 243, 261, 32, 247, 171, 291, 243, 263, 74, 290, 243, 252, 243, 264, 74, 242, 294, 243, 242, 256, 48, 258, 174, 74, 293, 243, 257, 174, 243, 265, 32, 248, 171, 291, 243, 267, 74, 290, 243, 268, 74, 242, 294, 243, 242, 256, 48, 258, 174, 74, 293, 243, 257, 174, 243, 269, 32, 249, 171, 291, 243, 271, 74, 290, 243, 272, 74, 242, 294, 243, 242, 256, 48, 258, 174, 74, 293, 243, 257, 174, 243, 273, 32, 250, 171, 291, 243, 275, 74, 290, 243, 276, 74, 242, 294, 243, 242, 256, 48, 258, 174, 74, 293, 243, 257, 174, 243, 277, 32, 298, 193, 296, 243, 285, 74, 76, 242, 349, 108, 285, 174, 32, 299, 193, 297, 243, 286, 74, 76, 242, 349, 108, 286, 174, 32, 300, 193, 76, 242, 349, 108, 287, 174, 32, 301, 193, 247, 74, 242, 298, 229, 63, 108, 201, 27, 243, 264, 74, 300, 229, 201, 108, 63, 27, 243, 266, 174, 32, 302, 193, 246, 74, 242, 299, 229, 201, 108, 63, 27, 243, 260, 74, 300, 229, 63, 108, 201, 27, 243, 262, 174, 32, 303, 193, 248, 74, 299, 243, 270, 32, 304, 193, 39, 242, 252, 108, 255, 4, 290, 243, 252, 174, 32, 305, 193, 175, 242, 242, 285, 108, 286, 174, 108, 91, 193, 144, 174, 32, 306, 193, 39, 242, 257, 108, 256, 48, 258, 4, 293, 243, 257, 174, 32, 135, 307, 156, 5, 242, 306, 174, 63, 32, 308, 193, 57, 242, 301, 108, 309, 193, 242, 298, 229, 63, 108, 201, 27, 1, 304, 174, 172, 242, 300, 229, 201, 108, 63, 27, 1, 253, 174, 108, 310, 193, 349, 174, 32, 311, 193, 57, 242, 302, 108, 309, 193, 242, 300, 229, 63, 108, 201, 27, 1, 253, 174, 172, 242, 299, 229, 201, 108, 63, 27, 1, 304, 174, 108, 310, 193, 349, 174, 32, 312, 193, 15, 242, 308, 108, 311, 174, 32, 313, 193, 57, 242, 303, 108, 309, 193, 299, 1, 252, 108, 310, 193, 349, 174, 82, 314, 242, 144, 174, 32, 312, 24, 313, 32, 181, 297, 243, 286, 1, 242, 296, 74, 347, 174, 243, 285, 102, 242, 297, 74, 347, 174, 243, 286, 124, 296, 243, 285, 63, 32, 315, 193, 57, 242, 249, 74, 298, 243, 274, 108, 309, 193, 298, 1, 304, 108, 310, 193, 349, 174, 82, 314, 242, 144, 174, 32, 316, 193, 57, 242, 249, 74, 299, 243, 274, 108, 309, 193, 299, 1, 304, 108, 310, 193, 349, 174, 82, 314, 242, 144, 174, 32, 309, 193, 298, 229, 63, 108, 201, 27, 143, 299, 229, 201, 108, 63, 27, 32, 317, 193, 204, 242, 309, 108, 107, 242, 315, 229, 63, 108, 201, 27, 4, 316, 229, 201, 108, 63, 27, 174, 108, 349, 174, 32, 318, 193, 57, 242, 250, 74, 298, 243, 278, 108, 309, 193, 298, 1, 304, 108, 310, 193, 349, 174, 82, 314, 242, 144, 174, 32, 319, 193, 57, 242, 250, 74, 299, 243, 278, 108, 309, 193, 299, 1, 304, 108, 310, 193, 349, 174, 82, 314, 242, 144, 174, 32, 309, 193, 298, 229, 63, 108, 201, 27, 217, 299, 229, 201, 108, 63, 27, 32, 320, 193, 204, 242, 309, 108, 107, 242, 318, 229, 63, 108, 201, 27, 4, 319, 229, 201, 108, 63, 27, 174, 108, 349, 174, 32, 312, 24, 317, 74, 320, 32, 66, 32, 37, 297, 243, 286, 143, 242, 296, 74, 347, 174, 243, 285, 63, 32, 318, 193, 57, 242, 250, 74, 298, 243, 278, 108, 309, 193, 298, 1, 304, 108, 310, 193, 349, 174, 82, 314, 242, 144, 174, 32, 319, 193, 57, 242, 250, 74, 299, 243, 278, 108, 309, 193, 299, 1, 304, 108, 310, 193, 349, 174, 82, 314, 242, 144, 174, 32, 309, 193, 298, 229, 63, 108, 201, 27, 217, 299, 229, 201, 108, 63, 27, 32, 312, 24, 204, 242, 309, 108, 107, 242, 318, 229, 63, 108, 201, 27, 4, 319, 229, 201, 108, 63, 27, 174, 108, 349, 174, 32, 184, 32, 30, 63, 32, 315, 193, 57, 242, 249, 74, 298, 243, 274, 108, 309, 193, 298, 1, 304, 108, 310, 193, 349, 174, 82, 314, 242, 144, 174, 32, 316, 193, 57, 242, 249, 74, 299, 243, 274, 108, 309, 193, 299, 1, 304, 108, 310, 193, 349, 174, 82, 314, 242, 144, 174, 32, 309, 193, 298, 229, 63, 108, 201, 27, 143, 299, 229, 201, 108, 63, 27, 32, 312, 24, 204, 242, 309, 108, 107, 242, 315, 229, 63, 108, 201, 27, 4, 316, 229, 201, 108, 63, 27, 174, 108, 349, 174, 32, 56, 32, 305, 171, 312, 32, 301, 171, 265, 32, 302, 171, 261, 32, 303, 171, 269, 32, 249, 171, 273, 32, 250, 171, 277, 32, 78, 32, 298, 193, 296, 243, 285, 74, 76, 242, 349, 108, 285, 174, 32, 299, 193, 297, 243, 286, 74, 76, 242, 349, 108, 286, 174, 32, 251, 171, 291, 243, 279, 74, 290, 243, 280, 74, 294, 243, 282, 74, 293, 243, 281, 32, 321, 193, 251, 74, 242, 298, 229, 63, 108, 201, 27, 243, 283, 74, 299, 229, 201, 108, 63, 27, 243, 284, 174, 32, 10, 242, 321, 108, 305, 108, 309, 193, 242, 298, 229, 63, 108, 201, 27, 1, 252, 174, 172, 242, 299, 229, 201, 108, 63, 27, 1, 252, 174, 174, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_ddAcs_unstable_kernel(\n    dout_ptr,\n    out_ptr,\n    dt_ptr,\n    ddt_ptr,\n    x_ptr,\n    D_ptr,\n    ddA_cumsum_ptr,\n    dD_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_out_batch,\n    stride_out_seqlen,\n    stride_out_head,\n    stride_out_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_ddt_batch,\n    stride_ddt_chunk,\n    stride_ddt_head,\n    stride_ddt_csize,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_D_head,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize,\n    stride_dD_batch,\n    stride_dD_chunk,\n    stride_dD_head,\n    stride_dD_csize,\n    stride_dD_hdim,\n    HAS_D: tl.constexpr,\n    D_HAS_HDIM: tl.constexpr,\n    SUBTRACT_DDTDT: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    out_ptr += (\n        pid_b * stride_out_batch\n        + pid_c * chunk_size * stride_out_seqlen\n        + pid_h * stride_out_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    ddt_ptr += (\n        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\n    )\n    ddA_cumsum_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_c * stride_ddA_cs_chunk\n        + pid_h * stride_ddA_cs_head\n    )\n    if HAS_D:\n        x_ptr += (\n            pid_b * stride_x_batch\n            + pid_c * chunk_size * stride_x_seqlen\n            + pid_h * stride_x_head\n        )\n        dD_ptr += (\n            pid_b * stride_dD_batch\n            + pid_c * stride_dD_chunk\n            + pid_h * stride_dD_head\n            + pid_m * stride_dD_csize\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim\n    )\n    out_ptrs = out_ptr + (\n        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim\n    )\n    if HAS_D:\n        x_ptrs = x_ptr + (\n            offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n        )\n        if D_HAS_HDIM:\n            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dout = tl.load(\n        dout_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    out = tl.load(\n        out_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    if HAS_D:\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n            other=0.0,\n        ).to(tl.float32)\n        if D_HAS_HDIM:\n            dD = tl.sum(dout * x, axis=0)\n            tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n            D = tl.load(\n                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0\n            ).to(tl.float32)\n        else:\n            dD = tl.sum(dout * x)\n            tl.store(dD_ptr, dD)\n            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n        out -= x * D\n    ddA_cs = tl.sum(dout * out, axis=1)\n    if SUBTRACT_DDTDT:\n        dt = tl.load(\n            dt_ptr + offs_m * stride_dt_csize, mask=offs_m < chunk_size, other=0.0\n        ).to(tl.float32)\n        ddt = tl.load(\n            ddt_ptr + offs_m * stride_ddt_csize, mask=offs_m < chunk_size, other=0.0\n        ).to(tl.float32)\n        ddA_cs -= dt * ddt\n    tl.store(\n        ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size\n    )", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 62, 6, 108, 289, 62, 6, 108, 290, 62, 6, 108, 291, 62, 6, 108, 292, 62, 6, 174, 62, 32, -1, 293, 193, 170, 242, 294, 193, 347, 174, 32, 295, 193, 293, 48, 256, 32, 296, 193, 293, 4, 295, 243, 256, 32, 297, 193, 170, 242, 294, 193, 348, 174, 32, 298, 193, 170, 242, 294, 193, 349, 174, 32, 246, 171, 296, 243, 258, 74, 295, 243, 254, 243, 259, 74, 297, 243, 260, 32, 247, 171, 296, 243, 262, 74, 295, 243, 254, 243, 263, 74, 297, 243, 264, 32, 248, 171, 296, 243, 266, 74, 295, 243, 267, 74, 297, 243, 268, 32, 249, 171, 296, 243, 270, 74, 295, 243, 271, 74, 297, 243, 272, 32, 252, 171, 296, 243, 279, 74, 295, 243, 280, 74, 297, 243, 281, 32, 181, 288, 62, 32, 250, 171, 296, 243, 274, 74, 295, 243, 254, 243, 275, 74, 297, 243, 276, 32, 253, 171, 296, 243, 283, 74, 295, 243, 284, 74, 297, 243, 285, 74, 298, 243, 286, 32, 184, 32, 299, 193, 298, 243, 291, 74, 76, 242, 349, 108, 291, 174, 32, 300, 193, 76, 242, 349, 108, 292, 174, 32, 301, 193, 246, 74, 242, 299, 229, 62, 108, 201, 27, 243, 259, 74, 300, 229, 201, 108, 62, 27, 243, 261, 174, 32, 302, 193, 247, 74, 242, 299, 229, 62, 108, 201, 27, 243, 263, 74, 300, 229, 201, 108, 62, 27, 243, 265, 174, 32, 181, 288, 62, 32, 303, 193, 250, 74, 242, 299, 229, 62, 108, 201, 27, 243, 275, 74, 300, 229, 201, 108, 62, 27, 243, 277, 174, 32, 181, 289, 62, 32, 304, 193, 253, 74, 300, 243, 287, 32, 184, 32, 184, 32, 305, 193, 39, 242, 254, 108, 257, 4, 295, 243, 254, 174, 32, 306, 193, 55, 242, 301, 108, 307, 193, 242, 299, 229, 62, 108, 201, 27, 1, 305, 174, 172, 242, 300, 229, 201, 108, 62, 27, 1, 255, 174, 108, 308, 193, 349, 174, 82, 309, 242, 144, 174, 32, 14, 193, 55, 242, 302, 108, 307, 193, 242, 299, 229, 62, 108, 201, 27, 1, 305, 174, 172, 242, 300, 229, 201, 108, 62, 27, 1, 255, 174, 108, 308, 193, 349, 174, 82, 309, 242, 144, 174, 32, 181, 288, 62, 32, 310, 193, 55, 242, 303, 108, 307, 193, 242, 299, 229, 62, 108, 201, 27, 1, 305, 174, 172, 242, 300, 229, 201, 108, 62, 27, 1, 255, 174, 108, 308, 193, 349, 174, 82, 309, 242, 144, 174, 32, 181, 289, 62, 32, 311, 193, 220, 242, 306, 243, 310, 108, 294, 193, 349, 174, 32, 10, 242, 304, 108, 311, 108, 307, 193, 300, 1, 255, 174, 32, 312, 193, 55, 242, 251, 74, 297, 243, 278, 74, 300, 108, 307, 193, 300, 1, 255, 108, 308, 193, 349, 174, 82, 309, 242, 144, 174, 32, 184, 32, 30, 62, 32, 311, 193, 220, 242, 306, 243, 310, 174, 32, 10, 242, 253, 108, 311, 174, 32, 312, 193, 55, 242, 251, 74, 297, 243, 278, 174, 82, 309, 242, 144, 174, 32, 56, 32, 14, 2, 310, 243, 312, 32, 184, 32, 313, 193, 220, 242, 306, 243, 14, 108, 294, 193, 347, 174, 32, 181, 290, 62, 32, 314, 193, 55, 242, 248, 74, 299, 243, 269, 108, 307, 193, 299, 1, 254, 108, 308, 193, 349, 174, 82, 309, 242, 144, 174, 32, 315, 193, 55, 242, 249, 74, 299, 243, 273, 108, 307, 193, 299, 1, 254, 108, 308, 193, 349, 174, 82, 309, 242, 144, 174, 32, 313, 2, 314, 243, 315, 32, 184, 32, 10, 242, 252, 74, 299, 243, 282, 108, 313, 108, 307, 193, 299, 1, 254, 174, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_ddAcs_stable_kernel_old(\n    x_ptr,\n    dout_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    cb_ptr,\n    ddAcs_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_n,\n    stride_ddAcs_batch,\n    stride_ddAcs_chunk,\n    stride_ddAcs_head,\n    stride_ddAcs_csize_m,\n    stride_ddAcs_csize_n,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim\n    )\n    x_ptrs = x_ptr + (\n        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim\n    )\n    dt_ptrs = dt_ptr + offs_n * stride_dt_csize\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n\n    )\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    chunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)\n\n    dout = tl.load(\n        dout_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n        other=0.0,\n    )\n    x = tl.load(\n        x_ptrs,\n        mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit_n),\n        other=0.0,\n    )\n    acc = tl.dot(dout, x)\n    cb = tl.load(\n        cb_ptrs,\n        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),\n        other=0.0,\n    ).to(tl.float32)\n    acc *= cb\n    dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)\n    acc *= dt_n\n    dA_cs_m = tl.load(\n        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0\n    ).to(tl.float32)\n    dA_cs_n = tl.load(\n        dA_cumsum_ptr + offs_n * stride_dA_cs_csize, mask=offs_n < chunk_size, other=0.0\n    ).to(tl.float32)\n    acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n    mask = offs_m[:, None] >= offs_n[None, :] + 1\n    acc = tl.where(mask, acc, 0.0)\n    acc = tl.cumsum(acc, axis=1)\n    acc = tl.where(mask, acc, 0.0)\n    ddA_cs = tl.sum(acc, axis=0)\n    ddAcs_ptr += (\n        pid_b * stride_ddAcs_batch\n        + pid_c * stride_ddAcs_chunk\n        + pid_h * stride_ddAcs_head\n        + pid_m * stride_ddAcs_csize_m\n    )\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    ddAcs_ptrs = ddAcs_ptr + offs_n * stride_ddAcs_csize_n\n    tl.store(ddAcs_ptrs + stride_ddAcs_csize_n, ddA_cs, mask=offs_n < chunk_size - 1)\n    tl.store(ddAcs_ptr, 0.0)", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 63, 6, 108, 284, 63, 6, 108, 285, 63, 6, 174, 63, 32, -1, 286, 193, 170, 242, 287, 193, 347, 174, 32, 288, 193, 286, 48, 254, 32, 289, 193, 286, 4, 288, 243, 254, 32, 290, 193, 170, 242, 287, 193, 348, 174, 32, 291, 193, 65, 242, 252, 108, 284, 174, 32, 292, 193, 170, 242, 287, 193, 349, 174, 48, 291, 32, 293, 193, 170, 242, 287, 193, 349, 174, 225, 291, 32, 246, 171, 289, 243, 257, 74, 288, 243, 252, 243, 258, 74, 290, 243, 259, 32, 247, 171, 289, 243, 261, 74, 288, 243, 252, 243, 262, 74, 290, 243, 263, 32, 248, 171, 289, 243, 265, 74, 288, 243, 266, 74, 290, 243, 267, 32, 249, 171, 289, 243, 269, 74, 288, 243, 270, 74, 290, 243, 271, 32, 250, 171, 289, 243, 273, 74, 288, 243, 274, 74, 290, 48, 256, 243, 275, 32, 294, 193, 292, 243, 283, 74, 76, 242, 349, 108, 283, 174, 32, 295, 193, 293, 243, 284, 74, 76, 242, 349, 108, 284, 174, 32, 296, 193, 76, 242, 349, 108, 285, 174, 32, 297, 193, 247, 74, 242, 294, 229, 63, 108, 201, 27, 243, 262, 74, 296, 229, 201, 108, 63, 27, 243, 264, 174, 32, 298, 193, 246, 74, 242, 295, 229, 201, 108, 63, 27, 243, 258, 74, 296, 229, 63, 108, 201, 27, 243, 260, 174, 32, 299, 193, 248, 74, 295, 243, 268, 32, 300, 193, 250, 74, 242, 294, 229, 63, 108, 201, 27, 243, 276, 74, 295, 229, 201, 108, 63, 27, 243, 277, 174, 32, 301, 193, 39, 242, 252, 108, 255, 4, 288, 243, 252, 174, 32, 302, 193, 39, 242, 301, 108, 242, 292, 74, 347, 174, 243, 283, 174, 32, 303, 193, 57, 242, 297, 108, 304, 193, 242, 294, 229, 63, 108, 201, 27, 1, 301, 174, 172, 242, 296, 229, 201, 108, 63, 27, 1, 253, 174, 108, 305, 193, 349, 174, 32, 306, 193, 57, 242, 298, 108, 304, 193, 242, 296, 229, 63, 108, 201, 27, 1, 253, 174, 172, 242, 295, 229, 201, 108, 63, 27, 1, 302, 174, 108, 305, 193, 349, 174, 32, 307, 193, 15, 242, 303, 108, 306, 174, 32, 308, 193, 57, 242, 300, 108, 304, 193, 242, 294, 229, 63, 108, 201, 27, 1, 252, 174, 172, 242, 295, 229, 201, 108, 63, 27, 1, 252, 174, 108, 305, 193, 349, 174, 82, 309, 242, 144, 174, 32, 307, 24, 308, 32, 310, 193, 57, 242, 299, 108, 304, 193, 295, 1, 252, 108, 305, 193, 349, 174, 82, 309, 242, 144, 174, 32, 307, 24, 310, 32, 311, 193, 57, 242, 249, 74, 294, 243, 272, 108, 304, 193, 294, 1, 252, 108, 305, 193, 349, 174, 82, 309, 242, 144, 174, 32, 312, 193, 57, 242, 249, 74, 295, 243, 272, 108, 304, 193, 295, 1, 252, 108, 305, 193, 349, 174, 82, 309, 242, 144, 174, 32, 307, 24, 107, 242, 311, 229, 63, 108, 201, 27, 4, 312, 229, 201, 108, 63, 27, 174, 32, 304, 193, 294, 229, 63, 108, 201, 27, 143, 295, 229, 201, 108, 63, 27, 74, 347, 32, 307, 193, 204, 242, 304, 108, 307, 108, 349, 174, 32, 307, 193, 85, 242, 307, 108, 287, 193, 347, 174, 32, 307, 193, 204, 242, 304, 108, 307, 108, 349, 174, 32, 313, 193, 220, 242, 307, 108, 287, 193, 349, 174, 32, 251, 171, 289, 243, 278, 74, 288, 243, 279, 74, 290, 243, 280, 74, 292, 243, 281, 32, 295, 193, 293, 243, 284, 74, 76, 242, 349, 108, 284, 174, 32, 314, 193, 251, 74, 295, 243, 282, 32, 10, 242, 314, 74, 282, 108, 313, 108, 304, 193, 295, 1, 252, 4, 347, 174, 32, 10, 242, 251, 108, 349, 174, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_ddAcs_stable_bwd_kernel(\n    x_ptr,\n    dout_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    cb_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_n,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize_m,\n    stride_ddA_cs_csize_n,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n    ddA_cumsum_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_c * stride_ddA_cs_chunk\n        + pid_h * stride_ddA_cs_head\n        + pid_m * stride_ddA_cs_csize_m\n    )\n\n    start = chunk_size - BLOCK_SIZE_N\n\n    offs_m = (chunk_size - (pid_m + 1) * BLOCK_SIZE_M) + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim\n    )\n    x_ptrs = x_ptr + (\n        (start + offs_n[None, :]) * stride_x_seqlen + offs_k[:, None] * stride_x_hdim\n    )\n    dt_ptrs = dt_ptr + (start + offs_n) * stride_dt_csize\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m\n        + (start + offs_n[None, :]) * stride_cb_csize_n\n    )\n    ddAcs_ptrs = ddA_cumsum_ptr + (start + offs_n) * stride_ddA_cs_csize_n\n    tl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    dout = tl.load(\n        dout_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit)\n        & (offs_m[:, None] >= 0)\n        & (offs_k[None, :] < hdim),\n        other=0.0,\n    )\n    dA_cs_m = tl.load(\n        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m >= 0, other=0.0\n    ).to(tl.float32)\n\n    lo, hi = 0, (pid_m + 1) * BLOCK_SIZE_M\n\n    for start_n in range(lo, hi, BLOCK_SIZE_N):\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_k[:, None] < hdim)\n            & (offs_n[None, :] < chunk_size_limit + start_n - start)\n            & (offs_n >= start_n - start),\n            other=0.0,\n        )\n        acc = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n >= start_n - start, other=0.0).to(\n            tl.float32\n        )\n        acc *= dt_n\n\n        cb = tl.load(\n            cb_ptrs,\n            mask=(offs_m[:, None] >= 0) & (offs_n[None, :] >= start_n - start),\n            other=0.0,\n        ).to(tl.float32)\n        acc *= cb\n        dA_cs_n = tl.load(\n            dA_cumsum_ptr + (start - start_n + offs_n) * stride_dA_cs_csize,\n            mask=(offs_n >= start_n - start),\n            other=0.0,\n        ).to(tl.float32)\n        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        mask = offs_m[:, None] <= start - start_n + offs_n[None, :] - 1\n        acc = tl.where(mask, acc, 0.0)\n        rowsum_new = rowsum + tl.sum(acc, axis=1)\n        acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)\n        rowsum = rowsum_new\n        acc = tl.where(mask, acc, 0.0)\n        ddA_cs = tl.sum(acc, axis=0)\n\n        tl.store(\n            ddAcs_ptrs - stride_ddA_cs_csize_n,\n            ddA_cs,\n            mask=(offs_n >= 1 + start_n - start),\n        )\n        x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen\n        dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize\n        cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n\n        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n\n\n    for start_n in range(hi, chunk_size, BLOCK_SIZE_N):\n        tl.store(\n            ddAcs_ptrs - stride_ddA_cs_csize_n,\n            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),\n            mask=offs_n >= 1 + start_n - start,\n        )\n        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 62, 6, 108, 284, 62, 6, 108, 285, 62, 6, 174, 62, 32, -1, 286, 193, 170, 242, 287, 193, 347, 174, 32, 288, 193, 286, 48, 254, 32, 289, 193, 286, 4, 288, 243, 254, 32, 290, 193, 170, 242, 287, 193, 348, 174, 32, 291, 193, 170, 242, 287, 193, 349, 174, 32, 246, 171, 289, 243, 257, 74, 288, 243, 252, 243, 258, 74, 290, 243, 259, 32, 247, 171, 289, 243, 261, 74, 288, 243, 252, 243, 262, 74, 290, 243, 263, 32, 248, 171, 289, 243, 265, 74, 288, 243, 266, 74, 290, 243, 267, 32, 249, 171, 289, 243, 269, 74, 288, 243, 270, 74, 290, 243, 271, 32, 250, 171, 289, 243, 273, 74, 288, 243, 274, 74, 290, 48, 256, 243, 275, 32, 251, 171, 289, 243, 278, 74, 288, 243, 279, 74, 290, 243, 280, 74, 291, 243, 281, 32, 292, 193, 252, 4, 284, 32, 293, 193, 252, 4, 242, 291, 74, 347, 174, 243, 283, 74, 76, 242, 349, 108, 283, 174, 32, 294, 193, 76, 242, 349, 108, 284, 174, 32, 295, 193, 76, 242, 349, 108, 285, 174, 32, 296, 193, 247, 74, 242, 293, 229, 62, 108, 201, 27, 243, 262, 74, 295, 229, 201, 108, 62, 27, 243, 264, 174, 32, 297, 193, 246, 74, 242, 242, 292, 74, 294, 229, 201, 108, 62, 27, 174, 243, 258, 74, 295, 229, 62, 108, 201, 27, 243, 260, 174, 32, 298, 193, 248, 74, 242, 292, 74, 294, 174, 243, 268, 32, 299, 193, 250, 74, 242, 293, 229, 62, 108, 201, 27, 243, 276, 74, 242, 292, 74, 294, 229, 201, 108, 62, 27, 174, 243, 277, 174, 32, 300, 193, 251, 74, 242, 292, 74, 294, 174, 243, 282, 32, 10, 242, 251, 74, 242, 252, 4, 347, 174, 243, 282, 108, 349, 174, 32, 301, 193, 39, 242, 252, 108, 255, 4, 288, 243, 252, 174, 32, 302, 193, 175, 242, 242, 283, 108, 174, 108, 91, 193, 144, 174, 32, 303, 193, 55, 242, 296, 108, 304, 193, 242, 293, 229, 62, 108, 201, 27, 1, 301, 174, 172, 242, 293, 229, 62, 108, 201, 27, 143, 349, 174, 172, 242, 295, 229, 201, 108, 62, 27, 1, 253, 174, 108, 305, 193, 349, 174, 32, 306, 193, 55, 242, 249, 74, 293, 243, 272, 108, 304, 193, 293, 143, 349, 108, 305, 193, 349, 174, 82, 307, 242, 144, 174, 32, 308, 108, 309, 193, 242, 349, 108, 242, 291, 74, 347, 174, 243, 283, 174, 32, 135, 310, 156, 5, 242, 308, 108, 309, 108, 284, 174, 62, 32, 311, 193, 55, 242, 297, 108, 304, 193, 242, 295, 229, 62, 108, 201, 27, 1, 253, 174, 172, 242, 294, 229, 201, 108, 62, 27, 1, 301, 74, 310, 4, 292, 174, 172, 242, 294, 143, 310, 4, 292, 174, 108, 305, 193, 349, 174, 32, 312, 193, 15, 242, 303, 108, 311, 174, 32, 313, 193, 55, 242, 298, 108, 304, 193, 294, 143, 310, 4, 292, 108, 305, 193, 349, 174, 82, 307, 242, 144, 174, 32, 312, 24, 313, 32, 314, 193, 55, 242, 299, 108, 304, 193, 242, 293, 229, 62, 108, 201, 27, 143, 349, 174, 172, 242, 294, 229, 201, 108, 62, 27, 143, 310, 4, 292, 174, 108, 305, 193, 349, 174, 82, 307, 242, 144, 174, 32, 312, 24, 314, 32, 315, 193, 55, 242, 249, 74, 242, 292, 4, 310, 74, 294, 174, 243, 272, 108, 304, 193, 294, 143, 310, 4, 292, 108, 305, 193, 349, 174, 82, 307, 242, 144, 174, 32, 312, 24, 107, 242, 306, 229, 62, 108, 201, 27, 4, 315, 229, 201, 108, 62, 27, 174, 32, 304, 193, 293, 229, 62, 108, 201, 27, 217, 292, 4, 310, 74, 294, 229, 201, 108, 62, 27, 4, 347, 32, 312, 193, 204, 242, 304, 108, 312, 108, 349, 174, 32, 316, 193, 302, 74, 220, 242, 312, 108, 287, 193, 347, 174, 32, 312, 193, 302, 229, 62, 108, 201, 27, 74, 85, 242, 312, 108, 287, 193, 347, 108, 317, 193, 159, 174, 32, 302, 193, 316, 32, 312, 193, 204, 242, 304, 108, 312, 108, 349, 174, 32, 318, 193, 220, 242, 312, 108, 287, 193, 349, 174, 32, 10, 242, 300, 4, 282, 108, 318, 108, 304, 193, 294, 143, 347, 74, 310, 4, 292, 174, 32, 297, 2, 284, 243, 258, 32, 298, 2, 284, 243, 268, 32, 299, 2, 284, 243, 277, 32, 300, 2, 284, 243, 282, 32, 78, 32, 135, 310, 156, 5, 242, 309, 108, 252, 108, 284, 174, 62, 32, 10, 242, 300, 4, 282, 108, 175, 242, 242, 284, 108, 174, 108, 91, 193, 144, 174, 108, 304, 193, 294, 143, 347, 74, 310, 4, 292, 174, 32, 300, 2, 284, 243, 282, 32, 78, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_ddAcs_stable_bwd_slow_kernel(\n    x_ptr,\n    dout_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    cb_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_n,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize_m,\n    stride_ddA_cs_csize_n,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n    ddA_cumsum_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_c * stride_ddA_cs_chunk\n        + pid_h * stride_ddA_cs_head\n        + pid_m * stride_ddA_cs_csize_m\n    )\n\n    start = (chunk_size - 1 // BLOCK_SIZE_N) * BLOCK_SIZE_N\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim\n    )\n    x_ptrs = x_ptr + (\n        (start + offs_n[None, :]) * stride_x_seqlen + offs_k[:, None] * stride_x_hdim\n    )\n    dt_ptrs = dt_ptr + (start + offs_n) * stride_dt_csize\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m\n        + (start + offs_n[None, :]) * stride_cb_csize_n\n    )\n    ddAcs_ptrs = ddA_cumsum_ptr + (start + offs_n) * stride_ddA_cs_csize_n\n    tl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    dout = tl.load(\n        dout_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n        other=0.0,\n    )\n    dA_cs_m = tl.load(\n        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0\n    ).to(tl.float32)\n\n    lo, hi = start, ((pid_m * BLOCK_SIZE_M) // BLOCK_SIZE_N) * BLOCK_SIZE_N - 1\n\n    for start_n in range(lo, hi, -BLOCK_SIZE_N):\n        tl.multiple_of(start_n, BLOCK_SIZE_N)\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_k[:, None] < hdim)\n            & (offs_n[None, :] < chunk_size_limit - start_n),\n            other=0.0,\n        )\n        acc = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0).to(\n            tl.float32\n        )\n        acc *= dt_n\n\n        cb = tl.load(\n            cb_ptrs,\n            mask=(offs_m[:, None] < chunk_size)\n            & (offs_n[None, :] < chunk_size - start_n),\n            other=0.0,\n        ).to(tl.float32)\n        acc *= cb\n        dA_cs_n = tl.load(\n            dA_cumsum_ptr + (start_n + offs_n) * stride_dA_cs_csize,\n            mask=(offs_n < chunk_size - start_n),\n            other=0.0,\n        ).to(tl.float32)\n        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        mask = offs_m[:, None] <= start_n + offs_n[None, :] - 1\n        acc = tl.where(mask, acc, 0.0)\n        rowsum_new = rowsum + tl.sum(acc, axis=1)\n        acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)\n        rowsum = rowsum_new\n        acc = tl.where(mask, acc, 0.0)\n        ddA_cs = tl.sum(acc, axis=0)\n\n        tl.store(\n            ddAcs_ptrs - stride_ddA_cs_csize_n,\n            ddA_cs,\n            mask=(offs_n < chunk_size - start_n) & (offs_n >= 1 - start_n),\n        )\n        x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen\n        dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize\n        cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n\n        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n\n\n    for start_n in range(hi, -1, -BLOCK_SIZE_N):\n        tl.store(\n            ddAcs_ptrs - stride_ddA_cs_csize_n,\n            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),\n            mask=offs_n >= 1 - start_n,\n        )\n        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 63, 6, 108, 284, 63, 6, 108, 285, 63, 6, 174, 63, 32, -1, 286, 193, 170, 242, 287, 193, 347, 174, 32, 288, 193, 286, 48, 254, 32, 289, 193, 286, 4, 288, 243, 254, 32, 290, 193, 170, 242, 287, 193, 348, 174, 32, 291, 193, 170, 242, 287, 193, 349, 174, 32, 246, 171, 289, 243, 257, 74, 288, 243, 252, 243, 258, 74, 290, 243, 259, 32, 247, 171, 289, 243, 261, 74, 288, 243, 252, 243, 262, 74, 290, 243, 263, 32, 248, 171, 289, 243, 265, 74, 288, 243, 266, 74, 290, 243, 267, 32, 249, 171, 289, 243, 269, 74, 288, 243, 270, 74, 290, 243, 271, 32, 250, 171, 289, 243, 273, 74, 288, 243, 274, 74, 290, 48, 256, 243, 275, 32, 251, 171, 289, 243, 278, 74, 288, 243, 279, 74, 290, 243, 280, 74, 291, 243, 281, 32, 292, 193, 242, 252, 4, 347, 48, 284, 174, 243, 284, 32, 293, 193, 291, 243, 283, 74, 76, 242, 349, 108, 283, 174, 32, 294, 193, 76, 242, 349, 108, 284, 174, 32, 295, 193, 76, 242, 349, 108, 285, 174, 32, 296, 193, 247, 74, 242, 293, 229, 63, 108, 201, 27, 243, 262, 74, 295, 229, 201, 108, 63, 27, 243, 264, 174, 32, 297, 193, 246, 74, 242, 242, 292, 74, 294, 229, 201, 108, 63, 27, 174, 243, 258, 74, 295, 229, 63, 108, 201, 27, 243, 260, 174, 32, 298, 193, 248, 74, 242, 292, 74, 294, 174, 243, 268, 32, 299, 193, 250, 74, 242, 293, 229, 63, 108, 201, 27, 243, 276, 74, 242, 292, 74, 294, 229, 201, 108, 63, 27, 174, 243, 277, 174, 32, 300, 193, 251, 74, 242, 292, 74, 294, 174, 243, 282, 32, 10, 242, 251, 74, 242, 252, 4, 347, 174, 243, 282, 108, 349, 174, 32, 301, 193, 39, 242, 252, 108, 255, 4, 288, 243, 252, 174, 32, 302, 193, 175, 242, 242, 283, 108, 174, 108, 91, 193, 144, 174, 32, 303, 193, 57, 242, 296, 108, 304, 193, 242, 293, 229, 63, 108, 201, 27, 1, 301, 174, 172, 242, 295, 229, 201, 108, 63, 27, 1, 253, 174, 108, 305, 193, 349, 174, 32, 306, 193, 57, 242, 249, 74, 293, 243, 272, 108, 304, 193, 293, 1, 252, 108, 305, 193, 349, 174, 82, 307, 242, 144, 174, 32, 308, 108, 309, 193, 242, 292, 108, 291, 243, 283, 48, 284, 243, 284, 4, 347, 174, 32, 135, 310, 156, 5, 242, 308, 108, 309, 108, 4, 284, 174, 63, 32, 55, 242, 310, 108, 284, 174, 32, 311, 193, 57, 242, 297, 108, 304, 193, 242, 295, 229, 63, 108, 201, 27, 1, 253, 174, 172, 242, 294, 229, 201, 108, 63, 27, 1, 301, 4, 310, 174, 108, 305, 193, 349, 174, 32, 312, 193, 15, 242, 303, 108, 311, 174, 32, 313, 193, 57, 242, 298, 108, 304, 193, 294, 1, 252, 4, 310, 108, 305, 193, 349, 174, 82, 307, 242, 144, 174, 32, 312, 24, 313, 32, 314, 193, 57, 242, 299, 108, 304, 193, 242, 293, 229, 63, 108, 201, 27, 1, 252, 174, 172, 242, 294, 229, 201, 108, 63, 27, 1, 252, 4, 310, 174, 108, 305, 193, 349, 174, 82, 307, 242, 144, 174, 32, 312, 24, 314, 32, 315, 193, 57, 242, 249, 74, 242, 310, 74, 294, 174, 243, 272, 108, 304, 193, 294, 1, 252, 4, 310, 108, 305, 193, 349, 174, 82, 307, 242, 144, 174, 32, 312, 24, 107, 242, 306, 229, 63, 108, 201, 27, 4, 315, 229, 201, 108, 63, 27, 174, 32, 304, 193, 293, 229, 63, 108, 201, 27, 217, 310, 74, 294, 229, 201, 108, 63, 27, 4, 347, 32, 312, 193, 204, 242, 304, 108, 312, 108, 349, 174, 32, 316, 193, 302, 74, 220, 242, 312, 108, 287, 193, 347, 174, 32, 312, 193, 302, 229, 63, 108, 201, 27, 74, 85, 242, 312, 108, 287, 193, 347, 108, 317, 193, 159, 174, 32, 302, 193, 316, 32, 312, 193, 204, 242, 304, 108, 312, 108, 349, 174, 32, 318, 193, 220, 242, 312, 108, 287, 193, 349, 174, 32, 10, 242, 300, 4, 282, 108, 318, 108, 304, 193, 242, 294, 1, 252, 4, 310, 174, 172, 242, 294, 143, 347, 4, 310, 174, 174, 32, 297, 2, 284, 243, 258, 32, 298, 2, 284, 243, 268, 32, 299, 2, 284, 243, 277, 32, 300, 2, 284, 243, 282, 32, 78, 32, 135, 310, 156, 5, 242, 309, 108, 4, 347, 108, 4, 284, 174, 63, 32, 10, 242, 300, 4, 282, 108, 175, 242, 242, 284, 108, 174, 108, 91, 193, 144, 174, 108, 304, 193, 294, 143, 347, 4, 310, 174, 32, 300, 2, 284, 243, 282, 32, 78, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_ddAcs_stable_bwd_old_kernel(\n    x_ptr,\n    dout_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    cb_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_n,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize_m,\n    stride_ddA_cs_csize_n,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n    ddA_cumsum_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_c * stride_ddA_cs_chunk\n        + pid_h * stride_ddA_cs_head\n        + pid_m * stride_ddA_cs_csize_m\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    end_offset = chunk_size - BLOCK_SIZE_N\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim\n    )\n    x_ptrs = x_ptr + (\n        (end_offset + offs_n[None, :]) * stride_x_seqlen\n        + offs_k[:, None] * stride_x_hdim\n    )\n    dt_ptrs = dt_ptr + (end_offset + offs_n) * stride_dt_csize\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m\n        + (end_offset + offs_n[None, :]) * stride_cb_csize_n\n    )\n    ddAcs_ptrs = ddA_cumsum_ptr + (end_offset + offs_n) * stride_ddA_cs_csize_n\n    tl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    dout = tl.load(\n        dout_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n        other=0.0,\n    )\n    dA_cs_m = tl.load(\n        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0\n    ).to(tl.float32)\n\n    for start_n in range(0, chunk_size - pid_m * BLOCK_SIZE_M, BLOCK_SIZE_N):\n        start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)\n        offset = (chunk_size - BLOCK_SIZE_N) - start_n\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_k[:, None] < hdim)\n            & (offs_n[None, :] < chunk_size_limit - offset)\n            & (offs_n[None, :] >= -offset),\n            other=0.0,\n        )\n        acc = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n >= (-offset), other=0.0).to(tl.float32)\n        acc *= dt_n\n\n        cb = tl.load(\n            cb_ptrs,\n            mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] >= (-offset)),\n            other=0.0,\n        ).to(tl.float32)\n        acc *= cb\n        dA_cs_n = tl.load(\n            dA_cumsum_ptr + offset + offs_n * stride_dA_cs_csize,\n            mask=offs_n >= (-offset),\n            other=0.0,\n        ).to(tl.float32)\n        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        mask = offs_m[:, None] <= offset + offs_n[None, :] - 1\n        acc = tl.where(mask, acc, 0.0)\n        rowsum_new = rowsum + tl.sum(acc, axis=1)\n        acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)\n        rowsum = rowsum_new\n        acc = tl.where(mask, acc, 0.0)\n        ddA_cs = tl.sum(acc, axis=0)\n        tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, ddA_cs, mask=offs_n >= 1 - offset)\n        x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen\n        dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize\n        cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n\n        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n\n\n    for start_n in range(pid_m * BLOCK_SIZE_M, 0, -BLOCK_SIZE_N):\n        tl.store(\n            ddAcs_ptrs - stride_ddA_cs_csize_n,\n            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),\n            mask=offs_n >= 1 - start_n,\n        )\n        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 62, 6, 108, 284, 62, 6, 108, 285, 62, 6, 174, 62, 32, -1, 286, 193, 170, 242, 287, 193, 347, 174, 32, 288, 193, 286, 48, 254, 32, 289, 193, 286, 4, 288, 243, 254, 32, 290, 193, 170, 242, 287, 193, 348, 174, 32, 291, 193, 170, 242, 287, 193, 349, 174, 32, 246, 171, 289, 243, 257, 74, 288, 243, 252, 243, 258, 74, 290, 243, 259, 32, 247, 171, 289, 243, 261, 74, 288, 243, 252, 243, 262, 74, 290, 243, 263, 32, 248, 171, 289, 243, 265, 74, 288, 243, 266, 74, 290, 243, 267, 32, 249, 171, 289, 243, 269, 74, 288, 243, 270, 74, 290, 243, 271, 32, 250, 171, 289, 243, 273, 74, 288, 243, 274, 74, 290, 48, 256, 243, 275, 32, 251, 171, 289, 243, 278, 74, 288, 243, 279, 74, 290, 243, 280, 74, 291, 243, 281, 32, 292, 193, 291, 243, 283, 74, 76, 242, 349, 108, 283, 174, 32, 293, 193, 76, 242, 349, 108, 284, 174, 32, 294, 193, 76, 242, 349, 108, 285, 174, 32, 295, 193, 252, 4, 284, 32, 296, 193, 247, 74, 242, 292, 229, 62, 108, 201, 27, 243, 262, 74, 294, 229, 201, 108, 62, 27, 243, 264, 174, 32, 297, 193, 246, 74, 242, 242, 295, 74, 293, 229, 201, 108, 62, 27, 174, 243, 258, 74, 294, 229, 62, 108, 201, 27, 243, 260, 174, 32, 298, 193, 248, 74, 242, 295, 74, 293, 174, 243, 268, 32, 299, 193, 250, 74, 242, 292, 229, 62, 108, 201, 27, 243, 276, 74, 242, 295, 74, 293, 229, 201, 108, 62, 27, 174, 243, 277, 174, 32, 300, 193, 251, 74, 242, 295, 74, 293, 174, 243, 282, 32, 10, 242, 251, 74, 242, 252, 4, 347, 174, 243, 282, 108, 349, 174, 32, 301, 193, 39, 242, 252, 108, 255, 4, 288, 243, 252, 174, 32, 302, 193, 175, 242, 242, 283, 108, 174, 108, 91, 193, 144, 174, 32, 303, 193, 55, 242, 296, 108, 304, 193, 242, 292, 229, 62, 108, 201, 27, 1, 301, 174, 172, 242, 294, 229, 201, 108, 62, 27, 1, 253, 174, 108, 305, 193, 349, 174, 32, 306, 193, 55, 242, 249, 74, 292, 243, 272, 108, 304, 193, 292, 1, 252, 108, 305, 193, 349, 174, 82, 307, 242, 144, 174, 32, 135, 308, 156, 5, 242, 349, 108, 252, 4, 291, 243, 283, 108, 284, 174, 62, 32, 308, 193, 57, 242, 308, 108, 284, 174, 32, 309, 193, 252, 4, 284, 4, 308, 32, 310, 193, 55, 242, 297, 108, 304, 193, 242, 294, 229, 62, 108, 201, 27, 1, 253, 174, 172, 242, 293, 229, 201, 108, 62, 27, 1, 301, 4, 309, 174, 172, 242, 293, 229, 201, 108, 62, 27, 143, 4, 309, 174, 108, 305, 193, 349, 174, 32, 311, 193, 15, 242, 303, 108, 310, 174, 32, 312, 193, 55, 242, 298, 108, 304, 193, 293, 143, 4, 309, 108, 305, 193, 349, 174, 82, 307, 242, 144, 174, 32, 311, 24, 312, 32, 313, 193, 55, 242, 299, 108, 304, 193, 242, 292, 229, 62, 108, 201, 27, 1, 252, 174, 172, 242, 293, 229, 201, 108, 62, 27, 143, 4, 309, 174, 108, 305, 193, 349, 174, 82, 307, 242, 144, 174, 32, 311, 24, 313, 32, 314, 193, 55, 242, 249, 74, 309, 74, 293, 243, 272, 108, 304, 193, 293, 143, 4, 309, 108, 305, 193, 349, 174, 82, 307, 242, 144, 174, 32, 311, 24, 107, 242, 306, 229, 62, 108, 201, 27, 4, 314, 229, 201, 108, 62, 27, 174, 32, 304, 193, 292, 229, 62, 108, 201, 27, 217, 309, 74, 293, 229, 201, 108, 62, 27, 4, 347, 32, 311, 193, 204, 242, 304, 108, 311, 108, 349, 174, 32, 315, 193, 302, 74, 220, 242, 311, 108, 287, 193, 347, 174, 32, 311, 193, 302, 229, 62, 108, 201, 27, 74, 85, 242, 311, 108, 287, 193, 347, 108, 316, 193, 159, 174, 32, 302, 193, 315, 32, 311, 193, 204, 242, 304, 108, 311, 108, 349, 174, 32, 317, 193, 220, 242, 311, 108, 287, 193, 349, 174, 32, 10, 242, 300, 4, 282, 108, 317, 108, 304, 193, 293, 143, 347, 4, 309, 174, 32, 297, 2, 284, 243, 258, 32, 298, 2, 284, 243, 268, 32, 299, 2, 284, 243, 277, 32, 300, 2, 284, 243, 282, 32, 78, 32, 135, 308, 156, 5, 242, 291, 243, 283, 108, 349, 108, 4, 284, 174, 62, 32, 10, 242, 300, 4, 282, 108, 175, 242, 242, 284, 108, 174, 108, 91, 193, 144, 174, 108, 304, 193, 293, 143, 347, 4, 308, 174, 32, 300, 2, 284, 243, 282, 32, 78, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_ddAcs_stable_fwd_kernel(\n    x_ptr,\n    dout_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    cb_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_n,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize_m,\n    stride_ddA_cs_csize_n,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n    ddA_cumsum_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_c * stride_ddA_cs_chunk\n        + pid_h * stride_ddA_cs_head\n        + pid_m * stride_ddA_cs_csize_m\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim\n    )\n    x_ptrs = x_ptr + (\n        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim\n    )\n    dt_ptrs = dt_ptr + offs_n * stride_dt_csize\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n\n    )\n    ddAcs_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n\n    tl.store(ddA_cumsum_ptr, 0.0)\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    dout = tl.load(\n        dout_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n        other=0.0,\n    )\n    dA_cs_m = tl.load(\n        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0\n    ).to(tl.float32)\n\n    lo, hi = 0, (pid_m + 1) * BLOCK_SIZE_M\n\n    for start_n in range(lo, hi, BLOCK_SIZE_N):\n        start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)\n\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_k[:, None] < hdim)\n            & (offs_n[None, :] < chunk_size_limit - start_n),\n            other=0.0,\n        )\n        acc = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0).to(\n            tl.float32\n        )\n        acc *= dt_n\n\n        cb = tl.load(\n            cb_ptrs,\n            mask=(offs_m[:, None] < chunk_size)\n            & (offs_n[None, :] < chunk_size - start_n),\n            other=0.0,\n        ).to(tl.float32)\n        acc *= cb\n        dA_cs_n = tl.load(\n            dA_cumsum_ptr + start_n + offs_n * stride_dA_cs_csize,\n            mask=offs_n < chunk_size - start_n,\n            other=0.0,\n        ).to(tl.float32)\n        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        mask = offs_m[:, None] >= start_n + offs_n[None, :] + 1\n        acc = tl.where(mask, acc, 0.0)\n        rowsum_new = rowsum + tl.sum(acc, axis=1)\n        acc = rowsum[:, None] + tl.cumsum(acc, axis=1)\n        rowsum = rowsum_new\n        acc = tl.where(mask, acc, 0.0)\n        ddA_cs = tl.sum(acc, axis=0)\n        tl.store(\n            ddAcs_ptrs + stride_ddA_cs_csize_n,\n            ddA_cs,\n            mask=offs_n < chunk_size - start_n - 1,\n        )\n        x_ptrs += BLOCK_SIZE_N * stride_x_seqlen\n        dt_ptrs += BLOCK_SIZE_N * stride_dt_csize\n        cb_ptrs += BLOCK_SIZE_N * stride_cb_csize_n\n        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n\n\n    for start_n in range(hi, chunk_size, BLOCK_SIZE_N):\n        tl.store(\n            ddAcs_ptrs + stride_ddA_cs_csize_n,\n            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),\n            mask=offs_n < chunk_size - start_n - 1,\n        )\n        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 63, 6, 108, 284, 63, 6, 108, 285, 63, 6, 174, 63, 32, -1, 286, 193, 170, 242, 287, 193, 347, 174, 32, 288, 193, 286, 48, 254, 32, 289, 193, 286, 4, 288, 243, 254, 32, 290, 193, 170, 242, 287, 193, 348, 174, 32, 291, 193, 170, 242, 287, 193, 349, 174, 32, 246, 171, 289, 243, 257, 74, 288, 243, 252, 243, 258, 74, 290, 243, 259, 32, 247, 171, 289, 243, 261, 74, 288, 243, 252, 243, 262, 74, 290, 243, 263, 32, 248, 171, 289, 243, 265, 74, 288, 243, 266, 74, 290, 243, 267, 32, 249, 171, 289, 243, 269, 74, 288, 243, 270, 74, 290, 243, 271, 32, 250, 171, 289, 243, 273, 74, 288, 243, 274, 74, 290, 48, 256, 243, 275, 32, 251, 171, 289, 243, 278, 74, 288, 243, 279, 74, 290, 243, 280, 74, 291, 243, 281, 32, 292, 193, 291, 243, 283, 74, 76, 242, 349, 108, 283, 174, 32, 293, 193, 76, 242, 349, 108, 284, 174, 32, 294, 193, 76, 242, 349, 108, 285, 174, 32, 295, 193, 247, 74, 242, 292, 229, 63, 108, 201, 27, 243, 262, 74, 294, 229, 201, 108, 63, 27, 243, 264, 174, 32, 296, 193, 246, 74, 242, 293, 229, 201, 108, 63, 27, 243, 258, 74, 294, 229, 63, 108, 201, 27, 243, 260, 174, 32, 297, 193, 248, 74, 293, 243, 268, 32, 298, 193, 250, 74, 242, 292, 229, 63, 108, 201, 27, 243, 276, 74, 293, 229, 201, 108, 63, 27, 243, 277, 174, 32, 299, 193, 251, 74, 293, 243, 282, 32, 10, 242, 251, 108, 349, 174, 32, 300, 193, 39, 242, 252, 108, 255, 4, 288, 243, 252, 174, 32, 301, 193, 175, 242, 242, 283, 108, 174, 108, 91, 193, 144, 174, 32, 302, 193, 57, 242, 295, 108, 303, 193, 242, 292, 229, 63, 108, 201, 27, 1, 300, 174, 172, 242, 294, 229, 201, 108, 63, 27, 1, 253, 174, 108, 304, 193, 349, 174, 32, 305, 193, 57, 242, 249, 74, 292, 243, 272, 108, 303, 193, 292, 1, 252, 108, 304, 193, 349, 174, 82, 306, 242, 144, 174, 32, 307, 108, 308, 193, 242, 349, 108, 242, 291, 74, 347, 174, 243, 283, 174, 32, 135, 309, 156, 5, 242, 307, 108, 308, 108, 284, 174, 63, 32, 309, 193, 55, 242, 309, 108, 284, 174, 32, 310, 193, 57, 242, 296, 108, 303, 193, 242, 294, 229, 63, 108, 201, 27, 1, 253, 174, 172, 242, 293, 229, 201, 108, 63, 27, 1, 300, 4, 309, 174, 108, 304, 193, 349, 174, 32, 311, 193, 15, 242, 302, 108, 310, 174, 32, 312, 193, 57, 242, 297, 108, 303, 193, 293, 1, 252, 4, 309, 108, 304, 193, 349, 174, 82, 306, 242, 144, 174, 32, 311, 24, 312, 32, 313, 193, 57, 242, 298, 108, 303, 193, 242, 292, 229, 63, 108, 201, 27, 1, 252, 174, 172, 242, 293, 229, 201, 108, 63, 27, 1, 252, 4, 309, 174, 108, 304, 193, 349, 174, 82, 306, 242, 144, 174, 32, 311, 24, 313, 32, 314, 193, 57, 242, 249, 74, 309, 74, 293, 243, 272, 108, 303, 193, 293, 1, 252, 4, 309, 108, 304, 193, 349, 174, 82, 306, 242, 144, 174, 32, 311, 24, 107, 242, 305, 229, 63, 108, 201, 27, 4, 314, 229, 201, 108, 63, 27, 174, 32, 303, 193, 292, 229, 63, 108, 201, 27, 143, 309, 74, 293, 229, 201, 108, 63, 27, 74, 347, 32, 311, 193, 204, 242, 303, 108, 311, 108, 349, 174, 32, 315, 193, 301, 74, 220, 242, 311, 108, 287, 193, 347, 174, 32, 311, 193, 301, 229, 63, 108, 201, 27, 74, 85, 242, 311, 108, 287, 193, 347, 174, 32, 301, 193, 315, 32, 311, 193, 204, 242, 303, 108, 311, 108, 349, 174, 32, 316, 193, 220, 242, 311, 108, 287, 193, 349, 174, 32, 10, 242, 299, 74, 282, 108, 316, 108, 303, 193, 293, 1, 252, 4, 309, 4, 347, 174, 32, 296, 171, 284, 243, 258, 32, 297, 171, 284, 243, 268, 32, 298, 171, 284, 243, 277, 32, 299, 171, 284, 243, 282, 32, 78, 32, 135, 309, 156, 5, 242, 308, 108, 252, 108, 284, 174, 63, 32, 10, 242, 299, 74, 282, 108, 175, 242, 242, 284, 108, 174, 108, 91, 193, 144, 174, 108, 303, 193, 293, 1, 252, 4, 309, 4, 347, 174, 32, 299, 171, 284, 243, 282, 32, 78, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_ddAcs_prev_kernel(\n    dout_ptr,\n    prev_states_ptr,\n    C_ptr,\n    dA_cumsum_ptr,\n    seq_idx_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    dstate,\n    hdim,\n    batch,\n    seqlen,\n    nchunks,\n    nheads_ngroups_ratio,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_prev_states_batch,\n    stride_prev_states_chunk,\n    stride_prev_states_head,\n    stride_prev_states_hdim,\n    stride_prev_states_dstate,\n    stride_C_batch,\n    stride_C_seqlen,\n    stride_C_head,\n    stride_C_dstate,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    prev_states_ptr += (\n        pid_b * stride_prev_states_batch\n        + pid_c * stride_prev_states_chunk\n        + pid_h * stride_prev_states_head\n    )\n    C_ptr += (\n        pid_b * stride_C_batch\n        + pid_c * chunk_size * stride_C_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_C_head\n    )\n    ddA_cumsum_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_c * stride_ddA_cs_chunk\n        + pid_h * stride_ddA_cs_head\n    )\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim\n    )\n    prev_states_ptrs = prev_states_ptr + (\n        offs_n[None, :] * stride_prev_states_dstate\n        + offs_k[:, None] * stride_prev_states_hdim\n    )\n    C_ptrs = C_ptr + (\n        offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate\n    )\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dout = tl.load(\n        dout_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n        other=0.0,\n    )\n    prev_states = tl.load(\n        prev_states_ptrs,\n        mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),\n        other=0.0,\n    )\n    prev_states = prev_states.to(dout_ptrs.dtype.element_ty)\n    acc = tl.dot(dout, prev_states)\n    c = tl.load(\n        C_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),\n        other=0.0,\n    ).to(tl.float32)\n    ddA_cs = tl.sum(acc * c, axis=1)\n    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(\n        tl.float32\n    )\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_m)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(\n            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0\n        )\n        seq_idx_m = tl.load(\n            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit,\n            other=-1,\n        )\n        scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n    ddA_cs *= scale\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)", "encoded": [29, 346, 242, 246, 108, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 62, 6, 108, 283, 62, 6, 108, 284, 62, 6, 108, 285, 62, 6, 174, 62, 32, -1, 286, 193, 170, 242, 287, 193, 347, 174, 32, 288, 193, 286, 48, 255, 32, 289, 193, 286, 4, 288, 243, 255, 32, 290, 193, 170, 242, 287, 193, 348, 174, 32, 291, 193, 64, 242, 253, 108, 284, 174, 32, 292, 193, 170, 242, 287, 193, 349, 174, 48, 291, 32, 293, 193, 170, 242, 287, 193, 349, 174, 225, 291, 32, 246, 171, 289, 243, 259, 74, 288, 243, 252, 243, 260, 74, 290, 243, 261, 32, 247, 171, 289, 243, 263, 74, 288, 243, 264, 74, 290, 243, 265, 32, 248, 171, 289, 243, 268, 74, 288, 243, 252, 243, 269, 74, 290, 48, 258, 243, 270, 32, 251, 171, 289, 243, 278, 74, 288, 243, 279, 74, 290, 243, 280, 32, 249, 171, 289, 243, 272, 74, 288, 243, 273, 74, 290, 243, 274, 32, 181, 282, 62, 32, 250, 171, 289, 243, 276, 74, 288, 243, 252, 243, 277, 32, 184, 32, 294, 193, 292, 243, 283, 74, 76, 242, 349, 108, 283, 174, 32, 295, 193, 293, 243, 284, 74, 76, 242, 349, 108, 284, 174, 32, 296, 193, 76, 242, 349, 108, 285, 174, 32, 297, 193, 246, 74, 242, 294, 229, 62, 108, 201, 27, 243, 260, 74, 296, 229, 201, 108, 62, 27, 243, 262, 174, 32, 298, 193, 247, 74, 242, 295, 229, 201, 108, 62, 27, 243, 267, 74, 296, 229, 62, 108, 201, 27, 243, 266, 174, 32, 299, 193, 248, 74, 242, 294, 229, 62, 108, 201, 27, 243, 269, 74, 295, 229, 201, 108, 62, 27, 243, 271, 174, 32, 300, 193, 249, 74, 294, 243, 275, 32, 301, 193, 39, 242, 252, 108, 256, 4, 288, 243, 252, 174, 32, 302, 193, 55, 242, 297, 108, 303, 193, 242, 294, 229, 62, 108, 201, 27, 1, 301, 174, 172, 242, 296, 229, 201, 108, 62, 27, 1, 254, 174, 108, 304, 193, 349, 174, 32, 305, 193, 55, 242, 298, 108, 303, 193, 242, 296, 229, 62, 108, 201, 27, 1, 254, 174, 172, 242, 295, 229, 201, 108, 62, 27, 1, 253, 174, 108, 304, 193, 349, 174, 32, 305, 193, 305, 82, 306, 242, 297, 82, 91, 82, 114, 174, 32, 307, 193, 15, 242, 302, 108, 305, 174, 32, 308, 193, 55, 242, 299, 108, 303, 193, 242, 294, 229, 62, 108, 201, 27, 1, 301, 174, 172, 242, 295, 229, 201, 108, 62, 27, 1, 253, 174, 108, 304, 193, 349, 174, 82, 306, 242, 144, 174, 32, 309, 193, 220, 242, 307, 243, 308, 108, 287, 193, 347, 174, 32, 310, 193, 55, 242, 300, 108, 303, 193, 294, 1, 301, 108, 304, 193, 349, 174, 82, 306, 242, 144, 174, 32, 181, 63, 282, 62, 32, 311, 193, 107, 242, 310, 174, 32, 184, 32, 181, 282, 62, 32, 312, 193, 55, 242, 250, 4, 277, 108, 303, 193, 288, 143, 347, 108, 304, 193, 349, 174, 32, 313, 193, 55, 242, 250, 74, 294, 243, 277, 108, 303, 193, 294, 1, 301, 108, 304, 193, 4, 347, 174, 32, 311, 193, 204, 242, 313, 77, 312, 108, 107, 242, 310, 174, 108, 349, 174, 32, 184, 32, 309, 24, 311, 32, 295, 193, 293, 243, 284, 74, 76, 242, 349, 108, 284, 174, 32, 314, 193, 251, 74, 294, 243, 281, 32, 195, 242, 314, 108, 309, 108, 303, 193, 294, 1, 252, 174, 32, 3, 32]}, {"code": "def _chunk_cumsum_fwd_kernel(\n    dt_ptr,\n    A_ptr,\n    dt_bias_ptr,\n    dt_out_ptr,\n    dA_cumsum_f_ptr,\n    dA_cumsum_b_ptr,\n    batch,\n    seqlen,\n    nheads,\n    chunk_size,\n    dt_min,\n    dt_max,\n    stride_dt_batch,\n    stride_dt_seqlen,\n    stride_dt_head,\n    stride_A_head,\n    stride_dt_bias_head,\n    stride_dt_out_batch,\n    stride_dt_out_chunk,\n    stride_dt_out_head,\n    stride_dt_out_csize,\n    stride_dA_cs_f_batch,\n    stride_dA_cs_f_chunk,\n    stride_dA_cs_f_head,\n    stride_dA_cs_f_csize,\n    stride_dA_cs_b_batch,\n    stride_dA_cs_b_chunk,\n    stride_dA_cs_b_head,\n    stride_dA_cs_b_csize,\n    DT_SOFTPLUS: tl.constexpr,\n    HAS_DT_BIAS: tl.constexpr,\n    BLOCK_SIZE_H: tl.constexpr,\n    BLOCK_SIZE_CHUNK: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=0)\n    pid_c = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\n    dt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk\n    dA_cumsum_f_ptr += pid_b * stride_dA_cs_f_batch + pid_c * stride_dA_cs_f_chunk\n    dA_cumsum_b_ptr += pid_b * stride_dA_cs_b_batch + pid_c * stride_dA_cs_b_chunk\n\n    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\n    dt_ptrs = dt_ptr + (\n        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen\n    )\n    A_ptrs = A_ptr + offs_h * stride_A_head\n    dt_out_ptrs = dt_out_ptr + (\n        offs_h[:, None] * stride_dt_out_head + offs_c[None, :] * stride_dt_out_csize\n    )\n    dA_cs_f_ptrs = dA_cumsum_f_ptr + (\n        offs_h[:, None] * stride_dA_cs_f_head + offs_c[None, :] * stride_dA_cs_f_csize\n    )\n    dA_cs_b_ptrs = dA_cumsum_b_ptr + (\n        offs_h[:, None] * stride_dA_cs_b_head + offs_c[None, :] * stride_dA_cs_b_csize\n    )\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    dt = tl.load(\n        dt_ptrs,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),\n        other=0.0,\n    ).to(tl.float32)\n    if HAS_DT_BIAS:\n        dt_bias = tl.load(\n            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0\n        ).to(tl.float32)\n        dt += dt_bias[:, None]\n    if DT_SOFTPLUS:\n        dt = softplus(dt)\n\n    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\n    dt = tl.where(\n        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0\n    )\n    tl.store(\n        dt_out_ptrs,\n        dt,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),\n    )\n    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)\n    dA = dt * A[:, None]\n    dA_cs_f = tl.cumsum(dA, axis=1)\n    tl.store(\n        dA_cs_f_ptrs,\n        dA_cs_f,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),\n    )\n\n    dA_cs_b = tl.flip(tl.cumsum(tl.flip(dA, dim=1), axis=1))\n\n    tl.store(\n        dA_cs_b_ptrs,\n        dA_cs_b,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 63, 6, 108, 277, 63, 6, 108, 278, 63, 6, 108, 279, 63, 6, 175, 63, 32, -1, 280, 194, 171, 243, 281, 194, 348, 175, 32, 282, 194, 171, 243, 281, 194, 349, 175, 32, 283, 194, 171, 243, 281, 194, 350, 175, 32, 247, 172, 280, 244, 259, 74, 282, 244, 256, 244, 260, 32, 250, 172, 280, 244, 264, 74, 282, 244, 265, 32, 251, 172, 280, 244, 268, 74, 282, 244, 269, 32, 252, 172, 280, 244, 272, 74, 282, 244, 273, 32, 284, 194, 283, 244, 278, 74, 76, 243, 348, 108, 278, 175, 32, 285, 194, 76, 243, 348, 108, 279, 175, 32, 286, 194, 247, 74, 243, 284, 230, 63, 108, 202, 27, 244, 261, 74, 285, 230, 202, 108, 63, 27, 244, 260, 175, 32, 287, 194, 248, 74, 284, 244, 262, 32, 288, 194, 250, 74, 243, 284, 230, 63, 108, 202, 27, 244, 266, 74, 285, 230, 202, 108, 63, 27, 244, 267, 175, 32, 289, 194, 251, 74, 243, 284, 230, 63, 108, 202, 27, 244, 270, 74, 285, 230, 202, 108, 63, 27, 244, 271, 175, 32, 290, 194, 252, 74, 243, 284, 230, 63, 108, 202, 27, 244, 274, 74, 285, 230, 202, 108, 63, 27, 244, 275, 175, 32, 291, 194, 39, 243, 256, 108, 254, 4, 282, 244, 256, 175, 32, 292, 194, 57, 243, 286, 108, 293, 194, 243, 284, 230, 63, 108, 202, 27, 1, 255, 175, 173, 243, 285, 230, 202, 108, 63, 27, 1, 291, 175, 108, 294, 194, 348, 175, 82, 295, 243, 145, 175, 32, 182, 277, 63, 32, 296, 194, 57, 243, 249, 74, 284, 244, 263, 108, 293, 194, 284, 1, 255, 108, 294, 194, 348, 175, 82, 295, 243, 145, 175, 32, 292, 172, 296, 230, 63, 108, 202, 27, 32, 185, 32, 182, 276, 63, 32, 292, 194, 297, 243, 292, 175, 32, 185, 32, 292, 194, 67, 243, 192, 243, 292, 108, 257, 175, 108, 258, 175, 32, 292, 194, 205, 243, 243, 284, 230, 63, 108, 202, 27, 1, 255, 175, 173, 243, 285, 230, 202, 108, 63, 27, 1, 291, 175, 108, 292, 108, 348, 175, 32, 10, 243, 288, 108, 292, 108, 293, 194, 243, 284, 230, 63, 108, 202, 27, 1, 255, 175, 173, 243, 285, 230, 202, 108, 63, 27, 1, 256, 175, 175, 32, 298, 194, 57, 243, 287, 108, 293, 194, 284, 1, 255, 108, 294, 194, 348, 175, 82, 295, 243, 145, 175, 32, 299, 194, 292, 244, 298, 230, 63, 108, 202, 27, 32, 300, 194, 85, 243, 299, 108, 281, 194, 349, 175, 32, 10, 243, 289, 108, 300, 108, 293, 194, 243, 284, 230, 63, 108, 202, 27, 1, 255, 175, 173, 243, 285, 230, 202, 108, 63, 27, 1, 256, 175, 175, 32, 301, 194, 139, 243, 85, 243, 139, 243, 299, 108, 170, 194, 349, 175, 108, 281, 194, 349, 175, 175, 32, 10, 243, 290, 108, 301, 108, 293, 194, 243, 284, 230, 63, 108, 202, 27, 1, 255, 175, 173, 243, 285, 230, 202, 108, 63, 27, 1, 256, 175, 175, 32, 3, 32]}, {"code": "def _chunk_cumsum_bwd_kernel(\n    ddA_ptr,\n    ddt_out_ptr,\n    dt_ptr,\n    A_ptr,\n    dt_bias_ptr,\n    ddt_ptr,\n    dA_ptr,\n    ddt_bias_ptr,\n    batch,\n    seqlen,\n    nheads,\n    chunk_size,\n    dt_min,\n    dt_max,\n    stride_ddA_batch,\n    stride_ddA_chunk,\n    stride_ddA_head,\n    stride_ddA_csize,\n    stride_ddt_out_batch,\n    stride_ddt_out_chunk,\n    stride_ddt_out_head,\n    stride_ddt_out_csize,\n    stride_dt_batch,\n    stride_dt_seqlen,\n    stride_dt_head,\n    stride_A_head,\n    stride_dt_bias_head,\n    stride_ddt_batch,\n    stride_ddt_seqlen,\n    stride_ddt_head,\n    stride_dA_head,\n    stride_ddt_bias_head,\n    DT_SOFTPLUS: tl.constexpr,\n    HAS_DT_BIAS: tl.constexpr,\n    BLOCK_SIZE_H: tl.constexpr,\n    BLOCK_SIZE_CHUNK: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=0)\n    pid_c = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    ddt_out_ptr += pid_b * stride_ddt_out_batch + pid_c * stride_ddt_out_chunk\n    ddA_ptr += pid_b * stride_ddA_batch + pid_c * stride_ddA_chunk\n    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\n    ddt_ptr += pid_b * stride_ddt_batch + pid_c * chunk_size * stride_ddt_seqlen\n\n    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\n    ddt_out_ptrs = ddt_out_ptr + (\n        offs_h[:, None] * stride_ddt_out_head + offs_c[None, :] * stride_ddt_out_csize\n    )\n    ddA_ptrs = ddA_ptr + (\n        offs_h[:, None] * stride_ddA_head + offs_c[None, :] * stride_ddA_csize\n    )\n    dt_ptrs = dt_ptr + (\n        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen\n    )\n    ddt_ptrs = ddt_ptr + (\n        offs_h[:, None] * stride_ddt_head + offs_c[None, :] * stride_ddt_seqlen\n    )\n    A_ptrs = A_ptr + offs_h * stride_A_head\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    ddA = tl.load(\n        ddA_ptrs,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),\n        other=0.0,\n    ).to(tl.float32)\n    ddt_out = tl.load(\n        ddt_out_ptrs,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),\n        other=0.0,\n    ).to(tl.float32)\n    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)\n    ddt = ddA * A[:, None] + ddt_out\n    dt = tl.load(\n        dt_ptrs,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),\n        other=0.0,\n    ).to(tl.float32)\n    if HAS_DT_BIAS:\n        dt_bias = tl.load(\n            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0\n        ).to(tl.float32)\n        dt += dt_bias[:, None]\n    if DT_SOFTPLUS:\n        dt_presoftplus = dt\n        dt = softplus(dt)\n    clamp_mask = (dt < dt_min) | (dt > dt_max)\n\n    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\n    dt = tl.where(\n        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0\n    )\n    ddt = tl.where(\n        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), ddt, 0.0\n    )\n    ddt = tl.where(clamp_mask, 0.0, ddt)\n    if DT_SOFTPLUS:\n        ddt = tl.where(dt_presoftplus <= 20.0, ddt * tl.sigmoid(dt_presoftplus), ddt)\n    tl.store(\n        ddt_ptrs,\n        ddt,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),\n    )\n    dA = tl.sum(ddA * dt, axis=1)\n    tl.atomic_add(dA_ptr + offs_h * stride_dA_head, dA, mask=offs_h < nheads)\n    if HAS_DT_BIAS:\n        ddt_bias = tl.sum(ddt, axis=1)\n        tl.atomic_add(\n            ddt_bias_ptr + offs_h * stride_ddt_bias_head, ddt_bias, mask=offs_h < nheads\n        )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 62, 6, 108, 280, 62, 6, 108, 281, 62, 6, 108, 282, 62, 6, 175, 62, 32, -1, 283, 194, 171, 243, 284, 194, 348, 175, 32, 285, 194, 171, 243, 284, 194, 349, 175, 32, 286, 194, 171, 243, 284, 194, 350, 175, 32, 248, 172, 283, 244, 265, 74, 285, 244, 266, 32, 247, 172, 283, 244, 261, 74, 285, 244, 262, 32, 249, 172, 283, 244, 269, 74, 285, 244, 258, 244, 270, 32, 252, 172, 283, 244, 274, 74, 285, 244, 258, 244, 275, 32, 287, 194, 286, 244, 281, 74, 76, 243, 348, 108, 281, 175, 32, 288, 194, 76, 243, 348, 108, 282, 175, 32, 289, 194, 248, 74, 243, 287, 230, 62, 108, 202, 27, 244, 267, 74, 288, 230, 202, 108, 62, 27, 244, 268, 175, 32, 290, 194, 247, 74, 243, 287, 230, 62, 108, 202, 27, 244, 263, 74, 288, 230, 202, 108, 62, 27, 244, 264, 175, 32, 291, 194, 249, 74, 243, 287, 230, 62, 108, 202, 27, 244, 271, 74, 288, 230, 202, 108, 62, 27, 244, 270, 175, 32, 292, 194, 252, 74, 243, 287, 230, 62, 108, 202, 27, 244, 276, 74, 288, 230, 202, 108, 62, 27, 244, 275, 175, 32, 293, 194, 250, 74, 287, 244, 272, 32, 294, 194, 39, 243, 258, 108, 256, 4, 285, 244, 258, 175, 32, 295, 194, 55, 243, 290, 108, 296, 194, 243, 287, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 288, 230, 202, 108, 62, 27, 1, 294, 175, 108, 297, 194, 348, 175, 82, 298, 243, 145, 175, 32, 299, 194, 55, 243, 289, 108, 296, 194, 243, 287, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 288, 230, 202, 108, 62, 27, 1, 294, 175, 108, 297, 194, 348, 175, 82, 298, 243, 145, 175, 32, 300, 194, 55, 243, 293, 108, 296, 194, 287, 1, 257, 108, 297, 194, 348, 175, 82, 298, 243, 145, 175, 32, 301, 194, 295, 244, 300, 230, 62, 108, 202, 27, 74, 299, 32, 302, 194, 55, 243, 291, 108, 296, 194, 243, 287, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 288, 230, 202, 108, 62, 27, 1, 294, 175, 108, 297, 194, 348, 175, 82, 298, 243, 145, 175, 32, 182, 280, 62, 32, 303, 194, 55, 243, 251, 74, 287, 244, 273, 108, 296, 194, 287, 1, 257, 108, 297, 194, 348, 175, 82, 298, 243, 145, 175, 32, 302, 172, 303, 230, 62, 108, 202, 27, 32, 185, 32, 182, 279, 62, 32, 304, 194, 302, 32, 302, 194, 305, 243, 302, 175, 32, 185, 32, 306, 194, 243, 302, 1, 259, 175, 152, 243, 302, 124, 260, 175, 32, 302, 194, 66, 243, 192, 243, 302, 108, 259, 175, 108, 260, 175, 32, 302, 194, 205, 243, 243, 287, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 288, 230, 202, 108, 62, 27, 1, 294, 175, 108, 302, 108, 348, 175, 32, 301, 194, 205, 243, 243, 287, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 288, 230, 202, 108, 62, 27, 1, 294, 175, 108, 301, 108, 348, 175, 32, 301, 194, 205, 243, 306, 108, 348, 108, 301, 175, 32, 182, 279, 62, 32, 301, 194, 205, 243, 304, 218, 351, 108, 301, 244, 206, 243, 304, 175, 108, 301, 175, 32, 185, 32, 10, 243, 292, 108, 301, 108, 296, 194, 243, 287, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 288, 230, 202, 108, 62, 27, 1, 294, 175, 175, 32, 307, 194, 221, 243, 295, 244, 302, 108, 284, 194, 349, 175, 32, 196, 243, 253, 74, 287, 244, 277, 108, 307, 108, 296, 194, 287, 1, 257, 175, 32, 182, 280, 62, 32, 308, 194, 221, 243, 301, 108, 284, 194, 349, 175, 32, 196, 243, 254, 74, 287, 244, 278, 108, 308, 108, 296, 194, 287, 1, 257, 175, 32, 185, 32, 3, 32]}, {"code": "def _chunk_state_fwd_kernel(\n    x_ptr,\n    b_ptr,\n    states_f_ptr,\n    states_b_ptr,\n    dt_ptr,\n    dA_cumsum_f_ptr,\n    dA_cumsum_b_ptr,\n    hdim,\n    dstate,\n    chunk_size,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_b_batch,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_b_dstate,\n    stride_states_f_batch,\n    stride_states_f_chunk,\n    stride_states_f_head,\n    stride_states_f_hdim,\n    stride_states_f_dstate,\n    stride_states_b_batch,\n    stride_states_b_chunk,\n    stride_states_b_head,\n    stride_states_b_hdim,\n    stride_states_b_dstate,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_f_batch,\n    stride_dA_cs_f_chunk,\n    stride_dA_cs_f_head,\n    stride_dA_cs_f_csize,\n    stride_dA_cs_b_batch,\n    stride_dA_cs_b_chunk,\n    stride_dA_cs_b_head,\n    stride_dA_cs_b_csize,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    b_ptr += (\n        pid_b * stride_b_batch\n        + pid_c * chunk_size * stride_b_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_b_head\n    )\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_f_ptr += (\n        pid_b * stride_dA_cs_f_batch\n        + pid_c * stride_dA_cs_f_chunk\n        + pid_h * stride_dA_cs_f_head\n    )\n    dA_cumsum_b_ptr += (\n        pid_b * stride_dA_cs_b_batch\n        + pid_c * stride_dA_cs_b_chunk\n        + pid_h * stride_dA_cs_b_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen\n    )\n    b_ptrs = b_ptr + (\n        offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen\n    )\n    dt_ptrs = dt_ptr + offs_k * stride_dt_csize\n    dA_cs_f_last = tl.load(\n        dA_cumsum_f_ptr + (chunk_size - 1) * stride_dA_cs_f_csize\n    ).to(tl.float32)\n    dA_cs_b_last = tl.load(dA_cumsum_b_ptr).to(tl.float32)\n    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize\n    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    acc_f = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    acc_b = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k),\n            other=0.0,\n        )\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate),\n            other=0.0,\n        ).to(tl.float32)\n        dA_cs_f_k = tl.load(\n            dA_cumsum_f_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0\n        ).to(tl.float32)\n        dA_cs_b_k = tl.load(\n            dA_cumsum_b_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0\n        ).to(tl.float32)\n        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(\n            tl.float32\n        )\n        scale_f = tl.exp((dA_cs_f_last - dA_cs_f_k)) * dt_k\n        scale_b = tl.exp((dA_cs_b_last - dA_cs_b_k)) * dt_k\n        b_f = b * scale_f[:, None]\n        b_b = b * scale_b[:, None]\n        b_f = b_f.to(x_ptr.dtype.element_ty)\n        b_b = b_b.to(x_ptr.dtype.element_ty)\n        acc_f += tl.dot(x, b_f)\n        acc_b += tl.dot(x, b_b)\n        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen\n        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n        dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize\n        dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize\n    states_f = acc_f.to(states_f_ptr.dtype.element_ty)\n    states_b = acc_b.to(states_b_ptr.dtype.element_ty)\n\n    states_f_ptr += (\n        pid_b * stride_states_f_batch\n        + pid_c * stride_states_f_chunk\n        + pid_h * stride_states_f_head\n    )\n    states_b_ptr += (\n        pid_b * stride_states_b_batch\n        + pid_c * stride_states_b_chunk\n        + pid_h * stride_states_b_head\n    )\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    states_f_ptrs = states_f_ptr + (\n        offs_m[:, None] * stride_states_f_hdim\n        + offs_n[None, :] * stride_states_f_dstate\n    )\n    states_b_ptrs = states_b_ptr + (\n        offs_m[:, None] * stride_states_b_hdim\n        + offs_n[None, :] * stride_states_b_dstate\n    )\n    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)\n    tl.store(states_f_ptrs, states_f, mask=c_mask)\n    tl.store(states_b_ptrs, states_b, mask=c_mask)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 63, 6, 108, 291, 63, 6, 108, 292, 63, 6, 175, 63, 32, -1, 293, 194, 171, 243, 294, 194, 348, 175, 32, 295, 194, 293, 48, 257, 32, 296, 194, 293, 4, 295, 244, 257, 32, 297, 194, 171, 243, 294, 194, 349, 175, 32, 298, 194, 65, 243, 255, 108, 291, 175, 32, 299, 194, 171, 243, 294, 194, 350, 175, 48, 298, 32, 300, 194, 171, 243, 294, 194, 350, 175, 226, 298, 32, 248, 172, 296, 244, 264, 74, 295, 244, 256, 244, 265, 74, 297, 48, 259, 244, 266, 32, 247, 172, 296, 244, 260, 74, 295, 244, 256, 244, 261, 74, 297, 244, 262, 32, 251, 172, 296, 244, 278, 74, 295, 244, 279, 74, 297, 244, 280, 32, 252, 172, 296, 244, 282, 74, 295, 244, 283, 74, 297, 244, 284, 32, 253, 172, 296, 244, 286, 74, 295, 244, 287, 74, 297, 244, 288, 32, 301, 194, 299, 244, 290, 74, 76, 243, 350, 108, 290, 175, 32, 302, 194, 300, 244, 291, 74, 76, 243, 350, 108, 291, 175, 32, 303, 194, 76, 243, 350, 108, 292, 175, 32, 304, 194, 247, 74, 243, 301, 230, 63, 108, 202, 27, 244, 263, 74, 303, 230, 202, 108, 63, 27, 244, 261, 175, 32, 305, 194, 248, 74, 243, 302, 230, 202, 108, 63, 27, 244, 267, 74, 303, 230, 63, 108, 202, 27, 244, 265, 175, 32, 306, 194, 251, 74, 303, 244, 281, 32, 307, 194, 57, 243, 252, 74, 243, 256, 4, 348, 175, 244, 285, 175, 82, 308, 243, 145, 175, 32, 309, 194, 57, 243, 253, 175, 82, 308, 243, 145, 175, 32, 310, 194, 252, 74, 303, 244, 285, 32, 311, 194, 253, 74, 303, 244, 289, 32, 312, 194, 39, 243, 256, 108, 258, 4, 295, 244, 256, 175, 32, 313, 194, 176, 243, 243, 290, 108, 291, 175, 108, 91, 194, 145, 175, 32, 314, 194, 176, 243, 243, 290, 108, 291, 175, 108, 91, 194, 145, 175, 32, 135, 315, 157, 5, 243, 350, 108, 312, 108, 292, 175, 63, 32, 316, 194, 57, 243, 304, 108, 317, 194, 243, 301, 230, 63, 108, 202, 27, 1, 254, 175, 173, 243, 303, 230, 202, 108, 63, 27, 1, 312, 4, 315, 175, 108, 318, 194, 350, 175, 32, 319, 194, 57, 243, 305, 108, 317, 194, 243, 303, 230, 63, 108, 202, 27, 1, 312, 4, 315, 175, 173, 243, 302, 230, 202, 108, 63, 27, 1, 255, 175, 108, 318, 194, 350, 175, 82, 308, 243, 145, 175, 32, 320, 194, 57, 243, 310, 108, 317, 194, 303, 1, 312, 4, 315, 108, 318, 194, 350, 175, 82, 308, 243, 145, 175, 32, 321, 194, 57, 243, 311, 108, 317, 194, 303, 1, 312, 4, 315, 108, 318, 194, 350, 175, 82, 308, 243, 145, 175, 32, 322, 194, 57, 243, 306, 108, 317, 194, 303, 1, 312, 4, 315, 108, 318, 194, 350, 175, 82, 308, 243, 145, 175, 32, 323, 194, 107, 243, 307, 4, 320, 175, 244, 322, 32, 324, 194, 107, 243, 309, 4, 321, 175, 244, 322, 32, 325, 194, 319, 244, 323, 230, 63, 108, 202, 27, 32, 326, 194, 319, 244, 324, 230, 63, 108, 202, 27, 32, 325, 194, 325, 82, 308, 243, 247, 82, 91, 82, 114, 175, 32, 326, 194, 326, 82, 308, 243, 247, 82, 91, 82, 114, 175, 32, 313, 172, 15, 243, 316, 108, 325, 175, 32, 314, 172, 15, 243, 316, 108, 326, 175, 32, 304, 172, 292, 244, 261, 32, 305, 172, 292, 244, 265, 32, 306, 172, 292, 244, 281, 32, 310, 172, 292, 244, 285, 32, 311, 172, 292, 244, 289, 32, 78, 32, 327, 194, 313, 82, 308, 243, 249, 82, 91, 82, 114, 175, 32, 328, 194, 314, 82, 308, 243, 250, 82, 91, 82, 114, 175, 32, 249, 172, 296, 244, 268, 74, 295, 244, 269, 74, 297, 244, 270, 32, 250, 172, 296, 244, 273, 74, 295, 244, 274, 74, 297, 244, 275, 32, 301, 194, 299, 244, 290, 74, 76, 243, 350, 108, 290, 175, 32, 302, 194, 300, 244, 291, 74, 76, 243, 350, 108, 291, 175, 32, 329, 194, 249, 74, 243, 301, 230, 63, 108, 202, 27, 244, 271, 74, 302, 230, 202, 108, 63, 27, 244, 272, 175, 32, 330, 194, 250, 74, 243, 301, 230, 63, 108, 202, 27, 244, 276, 74, 302, 230, 202, 108, 63, 27, 244, 277, 175, 32, 331, 194, 243, 301, 230, 63, 108, 202, 27, 1, 254, 175, 173, 243, 302, 230, 202, 108, 63, 27, 1, 255, 175, 32, 10, 243, 329, 108, 327, 108, 317, 194, 331, 175, 32, 10, 243, 330, 108, 328, 108, 317, 194, 331, 175, 32, 3, 32]}, {"code": "def _chunk_state_bwd_dx_kernel(\n    x_ptr,\n    b_ptr,\n    dstates_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    dx_ptr,\n    ddt_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    hdim,\n    dstate,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_b_batch,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_b_dstate,\n    stride_dstates_batch,\n    stride_dstates_chunk,\n    stride_states_head,\n    stride_states_hdim,\n    stride_states_dstate,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_dx_batch,\n    stride_dx_seqlen,\n    stride_dx_head,\n    stride_dx_hdim,\n    stride_ddt_batch,\n    stride_ddt_chunk,\n    stride_ddt_head,\n    stride_ddt_csize,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_DSTATE: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    b_ptr += (\n        pid_b * stride_b_batch\n        + pid_c * chunk_size * stride_b_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_b_head\n    )\n    dstates_ptr += (\n        pid_b * stride_dstates_batch\n        + pid_c * stride_dstates_chunk\n        + pid_h * stride_states_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    ddt_ptr += (\n        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\n    )\n    ddA_cumsum_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_c * stride_ddA_cs_chunk\n        + pid_h * stride_ddA_cs_head\n    )\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    offs_k = tl.arange(\n        0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K\n    )\n    b_ptrs = b_ptr + (\n        offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate\n    )\n    dstates_ptrs = dstates_ptr + (\n        offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate\n    )\n    if BLOCK_SIZE_DSTATE <= 128:\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate),\n            other=0.0,\n        )\n        dstates = tl.load(\n            dstates_ptrs,\n            mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n        dstates = dstates.to(b_ptr.dtype.element_ty)\n        acc = tl.dot(b, dstates)\n    else:\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            b = tl.load(\n                b_ptrs,\n                mask=(offs_m[:, None] < chunk_size_limit)\n                & (offs_k[None, :] < dstate - k),\n                other=0.0,\n            )\n            dstates = tl.load(\n                dstates_ptrs,\n                mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim),\n                other=0.0,\n            )\n            dstates = dstates.to(b_ptr.dtype.element_ty)\n            acc += tl.dot(b, dstates)\n            b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(\n        tl.float32\n    )\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0).to(\n        tl.float32\n    )\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n    acc *= tl.exp(dA_cs_last - dA_cs_m)[:, None]\n\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n    )\n    x = tl.load(\n        x_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    ddt = tl.sum(acc * x, axis=1)\n    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\n    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)\n    ddA_cs = -(ddt * dt_m)\n    ddA_cs_last = -tl.sum(ddA_cs)\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)\n    tl.atomic_add(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize, ddA_cs_last)\n\n    dx = (acc * dt_m[:, None]).to(dx_ptr.dtype.element_ty)\n    dx_ptr += (\n        pid_b * stride_dx_batch\n        + pid_c * chunk_size * stride_dx_seqlen\n        + pid_h * stride_dx_head\n    )\n    dx_ptrs = dx_ptr + (\n        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim\n    )\n    tl.store(\n        dx_ptrs,\n        dx,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 62, 6, 108, 295, 62, 6, 108, 296, 62, 6, 108, 297, 62, 6, 175, 62, 32, -1, 298, 194, 171, 243, 299, 194, 348, 175, 32, 300, 194, 298, 48, 258, 32, 301, 194, 298, 4, 300, 244, 258, 32, 302, 194, 171, 243, 299, 194, 349, 175, 32, 303, 194, 64, 243, 256, 108, 295, 175, 32, 304, 194, 171, 243, 299, 194, 350, 175, 48, 303, 32, 305, 194, 171, 243, 299, 194, 350, 175, 226, 303, 32, 247, 172, 301, 244, 261, 74, 300, 244, 255, 244, 262, 74, 302, 244, 263, 32, 248, 172, 301, 244, 265, 74, 300, 244, 255, 244, 266, 74, 302, 48, 260, 244, 267, 32, 249, 172, 301, 244, 269, 74, 300, 244, 270, 74, 302, 244, 271, 32, 250, 172, 301, 244, 274, 74, 300, 244, 275, 74, 302, 244, 276, 32, 253, 172, 301, 244, 286, 74, 300, 244, 287, 74, 302, 244, 288, 32, 254, 172, 301, 244, 290, 74, 300, 244, 291, 74, 302, 244, 292, 32, 251, 172, 301, 244, 278, 74, 300, 244, 279, 74, 302, 244, 280, 32, 306, 194, 304, 244, 294, 74, 76, 243, 350, 108, 294, 175, 32, 307, 194, 305, 244, 295, 74, 76, 243, 350, 108, 295, 175, 32, 308, 194, 39, 243, 255, 108, 259, 4, 300, 244, 255, 175, 32, 309, 194, 76, 243, 350, 108, 297, 182, 297, 218, 348, 349, 351, 30, 296, 175, 32, 310, 194, 248, 74, 243, 306, 230, 62, 108, 202, 27, 244, 266, 74, 309, 230, 202, 108, 62, 27, 244, 268, 175, 32, 311, 194, 249, 74, 243, 307, 230, 202, 108, 62, 27, 244, 272, 74, 309, 230, 62, 108, 202, 27, 244, 273, 175, 32, 182, 297, 218, 348, 349, 351, 62, 32, 312, 194, 55, 243, 310, 108, 313, 194, 243, 306, 230, 62, 108, 202, 27, 1, 308, 175, 173, 243, 309, 230, 202, 108, 62, 27, 1, 257, 175, 108, 314, 194, 350, 175, 32, 315, 194, 55, 243, 311, 108, 313, 194, 243, 309, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 307, 230, 202, 108, 62, 27, 1, 256, 175, 108, 314, 194, 350, 175, 32, 315, 194, 315, 82, 316, 243, 248, 82, 91, 82, 114, 175, 32, 317, 194, 15, 243, 312, 108, 315, 175, 32, 185, 32, 30, 62, 32, 317, 194, 176, 243, 243, 294, 108, 295, 175, 108, 91, 194, 145, 175, 32, 135, 318, 157, 5, 243, 350, 108, 257, 108, 296, 175, 62, 32, 312, 194, 55, 243, 310, 108, 313, 194, 243, 306, 230, 62, 108, 202, 27, 1, 308, 175, 173, 243, 309, 230, 202, 108, 62, 27, 1, 257, 4, 318, 175, 108, 314, 194, 350, 175, 32, 315, 194, 55, 243, 311, 108, 313, 194, 243, 309, 230, 62, 108, 202, 27, 1, 257, 4, 318, 175, 173, 243, 307, 230, 202, 108, 62, 27, 1, 256, 175, 108, 314, 194, 350, 175, 32, 315, 194, 315, 82, 316, 243, 248, 82, 91, 82, 114, 175, 32, 317, 172, 15, 243, 312, 108, 315, 175, 32, 310, 172, 296, 244, 268, 32, 311, 172, 296, 244, 273, 32, 78, 32, 56, 32, 306, 194, 304, 244, 294, 74, 76, 243, 350, 108, 294, 175, 32, 307, 194, 305, 244, 295, 74, 76, 243, 350, 108, 295, 175, 32, 319, 194, 55, 243, 251, 74, 243, 255, 4, 348, 175, 244, 281, 175, 82, 316, 243, 145, 175, 32, 320, 194, 250, 74, 306, 244, 277, 32, 321, 194, 251, 74, 306, 244, 281, 32, 322, 194, 55, 243, 321, 108, 313, 194, 306, 1, 255, 108, 314, 194, 350, 175, 82, 316, 243, 145, 175, 32, 323, 194, 55, 243, 320, 108, 313, 194, 306, 1, 255, 108, 314, 194, 350, 175, 82, 316, 243, 145, 175, 32, 317, 24, 107, 243, 319, 4, 322, 175, 230, 62, 108, 202, 27, 32, 324, 194, 247, 74, 243, 306, 230, 62, 108, 202, 27, 244, 262, 74, 307, 230, 202, 108, 62, 27, 244, 264, 175, 32, 325, 194, 55, 243, 324, 108, 313, 194, 243, 306, 230, 62, 108, 202, 27, 1, 308, 175, 173, 243, 307, 230, 202, 108, 62, 27, 1, 256, 175, 108, 314, 194, 350, 175, 82, 316, 243, 145, 175, 32, 326, 194, 221, 243, 317, 244, 325, 108, 299, 194, 348, 175, 32, 327, 194, 253, 74, 306, 244, 289, 32, 196, 243, 327, 108, 326, 108, 313, 194, 306, 1, 255, 175, 32, 328, 194, 4, 243, 326, 244, 323, 175, 32, 329, 194, 4, 221, 243, 328, 175, 32, 330, 194, 254, 74, 306, 244, 293, 32, 196, 243, 330, 108, 328, 108, 313, 194, 306, 1, 255, 175, 32, 196, 243, 254, 74, 243, 255, 4, 348, 175, 244, 293, 108, 329, 175, 32, 331, 194, 243, 317, 244, 323, 230, 62, 108, 202, 27, 175, 82, 316, 243, 252, 82, 91, 82, 114, 175, 32, 252, 172, 301, 244, 282, 74, 300, 244, 255, 244, 283, 74, 302, 244, 284, 32, 332, 194, 252, 74, 243, 306, 230, 62, 108, 202, 27, 244, 283, 74, 307, 230, 202, 108, 62, 27, 244, 285, 175, 32, 10, 243, 332, 108, 331, 108, 313, 194, 243, 306, 230, 62, 108, 202, 27, 1, 308, 175, 173, 243, 307, 230, 202, 108, 62, 27, 1, 256, 175, 175, 32, 3, 32]}, {"code": "def _chunk_state_bwd_db_kernel(\n    x_ptr,\n    dstates_f_ptr,\n    dstates_b_ptr,\n    b_ptr,\n    dt_ptr,\n    dA_cumsum_f_ptr,\n    dA_cumsum_b_ptr,\n    db_ptr,\n    ddA_cumsum_f_ptr,\n    ddA_cumsum_b_ptr,\n    chunk_size,\n    dstate,\n    hdim,\n    batch,\n    seqlen,\n    nheads,\n    nheads_per_program,\n    ngroups,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_dstates_f_batch,\n    stride_dstates_f_chunk,\n    stride_dstates_f_head,\n    stride_dstates_f_hdim,\n    stride_dstates_f_dstate,\n    stride_dstates_b_batch,\n    stride_dstates_b_chunk,\n    stride_dstates_b_head,\n    stride_dstates_b_hdim,\n    stride_dstates_b_dstate,\n    stride_b_batch,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_b_dstate,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_f_batch,\n    stride_dA_cs_f_chunk,\n    stride_dA_cs_f_head,\n    stride_dA_cs_f_csize,\n    stride_dA_cs_b_batch,\n    stride_dA_cs_b_chunk,\n    stride_dA_cs_b_head,\n    stride_dA_cs_b_csize,\n    stride_db_batch,\n    stride_db_seqlen,\n    stride_db_split,\n    stride_db_group,\n    stride_db_dstate,\n    stride_ddA_cs_f_batch,\n    stride_ddA_cs_f_chunk,\n    stride_ddA_cs_f_head,\n    stride_ddA_cs_f_csize,\n    stride_ddA_cs_b_batch,\n    stride_ddA_cs_b_chunk,\n    stride_ddA_cs_b_head,\n    stride_ddA_cs_b_csize,\n    HAS_DDA_CS: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_sg = tl.program_id(axis=2)\n    pid_s = pid_sg // ngroups\n    pid_g = pid_sg - pid_s * ngroups\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head\n    )\n    db_ptr += (\n        pid_b * stride_db_batch\n        + pid_c * chunk_size * stride_db_seqlen\n        + pid_g * stride_db_group\n        + pid_s * stride_db_split\n    )\n    dstates_f_ptr += (\n        pid_b * stride_dstates_f_batch\n        + pid_c * stride_dstates_f_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n        * stride_dstates_f_head\n    )\n    dstates_b_ptr += (\n        pid_b * stride_dstates_b_batch\n        + pid_c * stride_dstates_b_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n        * stride_dstates_b_head\n    )\n    dt_ptr += (\n        pid_b * stride_dt_batch\n        + pid_c * stride_dt_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head\n    )\n    dA_cumsum_f_ptr += (\n        pid_b * stride_dA_cs_f_batch\n        + pid_c * stride_dA_cs_f_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n        * stride_dA_cs_f_head\n    )\n    dA_cumsum_b_ptr += (\n        pid_b * stride_dA_cs_b_batch\n        + pid_c * stride_dA_cs_b_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n        * stride_dA_cs_b_head\n    )\n    if HAS_DDA_CS:\n        b_ptr += (\n            pid_b * stride_b_batch\n            + pid_c * chunk_size * stride_b_seqlen\n            + pid_g * stride_b_head\n        )\n        ddA_cumsum_f_ptr += (\n            pid_b * stride_ddA_cs_f_batch\n            + pid_c * stride_ddA_cs_f_chunk\n            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n            * stride_ddA_cs_f_head\n        )\n        ddA_cumsum_b_ptr += (\n            pid_b * stride_ddA_cs_b_batch\n            + pid_c * stride_ddA_cs_b_chunk\n            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n            * stride_ddA_cs_b_head\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_seqlen + offs_k[None, :] * stride_x_hdim\n    )\n    dstates_f_ptrs = dstates_f_ptr + (\n        offs_n[None, :] * stride_dstates_f_dstate\n        + offs_k[:, None] * stride_dstates_f_hdim\n    )\n    dstates_b_ptrs = dstates_b_ptr + (\n        offs_n[None, :] * stride_dstates_b_dstate\n        + offs_k[:, None] * stride_dstates_b_hdim\n    )\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize\n    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize\n    if HAS_DDA_CS:\n        b_ptrs = b_ptr + (\n            offs_m[:, None] * stride_b_seqlen + offs_n[None, :] * stride_b_dstate\n        )\n        ddA_cumsum_f_ptrs = ddA_cumsum_f_ptr + offs_m * stride_ddA_cs_f_csize\n        ddA_cumsum_b_ptrs = ddA_cumsum_b_ptr + offs_m * stride_ddA_cs_b_csize\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_DDA_CS:\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),\n            other=0.0,\n        ).to(tl.float32)\n    nheads_iter = min(\n        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program\n    )\n    for h in range(nheads_iter):\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n            other=0.0,\n        )\n\n        dstates_f = tl.load(\n            dstates_f_ptrs,\n            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),\n            other=0.0,\n        )\n        dstates_f = dstates_f.to(x_ptrs.dtype.element_ty)\n        db_f = tl.dot(x, dstates_f)\n        dA_cs_f_last = tl.load(\n            dA_cumsum_f_ptr + (chunk_size - 1) * stride_dA_cs_f_csize\n        ).to(tl.float32)\n        dA_cs_f_m = tl.load(dA_cumsum_f_ptrs, mask=offs_m < chunk_size, other=0.0).to(\n            tl.float32\n        )\n        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n        scale_f = tl.exp(dA_cs_f_last - dA_cs_f_m)\n        db_f *= (scale_f * dt_m)[:, None]\n        if HAS_DDA_CS:\n\n            ddA_cs_f = tl.sum(db_f * b, axis=1)\n\n            tl.atomic_add(\n                ddA_cumsum_f_ptrs + stride_ddA_cs_f_csize,\n                ddA_cs_f,\n                mask=offs_m < chunk_size - 1,\n            )\n\n        dstates_b = tl.load(\n            dstates_b_ptrs,\n            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),\n            other=0.0,\n        )\n        dstates_b = dstates_b.to(x_ptrs.dtype.element_ty)\n        db_b = tl.dot(x, dstates_b)\n        dA_cs_b_last = tl.load(dA_cumsum_b_ptr).to(tl.float32)\n        dA_cs_b_m = tl.load(dA_cumsum_b_ptrs, mask=(offs_m < chunk_size), other=0.0).to(\n            tl.float32\n        )\n        scale_b = tl.exp(dA_cs_b_last - dA_cs_b_m)\n        db_b *= (scale_b * dt_m)[:, None]\n        if HAS_DDA_CS:\n\n            ddA_cs_b = tl.sum(db_b * b, axis=1)\n\n            tl.atomic_add(\n                ddA_cumsum_b_ptrs - stride_ddA_cs_b_csize,\n                ddA_cs_b,\n                mask=(offs_m >= 1) & (offs_m < chunk_size),\n            )\n\n        acc += db_f + db_b\n        x_ptrs += stride_x_head\n        dstates_f_ptrs += stride_dstates_f_head\n        dstates_b_ptrs += stride_dstates_b_head\n        dt_ptrs += stride_dt_head\n        dA_cumsum_f_ptr += stride_dA_cs_f_head\n        dA_cumsum_f_ptrs += stride_dA_cs_f_head\n        dA_cumsum_b_ptr += stride_dA_cs_b_head\n        dA_cumsum_b_ptrs += stride_dA_cs_b_head\n        if HAS_DDA_CS:\n            ddA_cumsum_f_ptrs += stride_ddA_cs_f_head\n            ddA_cumsum_b_ptrs += stride_ddA_cs_b_head\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    db_ptrs = db_ptr + (\n        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_dstate\n    )\n    tl.store(\n        db_ptrs,\n        acc,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 108, 301, 108, 302, 108, 303, 108, 304, 108, 305, 108, 306, 108, 307, 108, 308, 63, 6, 108, 309, 63, 6, 108, 310, 63, 6, 108, 311, 63, 6, 175, 63, 32, -1, 312, 194, 171, 243, 313, 194, 348, 175, 32, 314, 194, 312, 48, 260, 32, 315, 194, 312, 4, 314, 244, 260, 32, 316, 194, 171, 243, 313, 194, 349, 175, 32, 317, 194, 316, 48, 264, 32, 318, 194, 316, 4, 317, 244, 264, 32, 319, 194, 65, 243, 258, 108, 310, 175, 32, 320, 194, 171, 243, 313, 194, 350, 175, 48, 319, 32, 321, 194, 171, 243, 313, 194, 350, 175, 226, 319, 32, 247, 172, 315, 244, 265, 74, 314, 244, 257, 244, 266, 74, 243, 318, 244, 243, 262, 48, 264, 175, 74, 317, 244, 263, 175, 244, 267, 32, 254, 172, 315, 244, 295, 74, 314, 244, 257, 244, 296, 74, 318, 244, 298, 74, 317, 244, 297, 32, 248, 172, 315, 244, 269, 74, 314, 244, 270, 74, 243, 318, 244, 243, 262, 48, 264, 175, 74, 317, 244, 263, 175, 244, 271, 32, 249, 172, 315, 244, 274, 74, 314, 244, 275, 74, 243, 318, 244, 243, 262, 48, 264, 175, 74, 317, 244, 263, 175, 244, 276, 32, 251, 172, 315, 244, 283, 74, 314, 244, 284, 74, 243, 318, 244, 243, 262, 48, 264, 175, 74, 317, 244, 263, 175, 244, 285, 32, 252, 172, 315, 244, 287, 74, 314, 244, 288, 74, 243, 318, 244, 243, 262, 48, 264, 175, 74, 317, 244, 263, 175, 244, 289, 32, 253, 172, 315, 244, 291, 74, 314, 244, 292, 74, 243, 318, 244, 243, 262, 48, 264, 175, 74, 317, 244, 263, 175, 244, 293, 32, 182, 308, 63, 32, 250, 172, 315, 244, 279, 74, 314, 244, 257, 244, 280, 74, 318, 244, 281, 32, 255, 172, 315, 244, 300, 74, 314, 244, 301, 74, 243, 318, 244, 243, 262, 48, 264, 175, 74, 317, 244, 263, 175, 244, 302, 32, 256, 172, 315, 244, 304, 74, 314, 244, 305, 74, 243, 318, 244, 243, 262, 48, 264, 175, 74, 317, 244, 263, 175, 244, 306, 32, 185, 32, 322, 194, 320, 244, 309, 74, 76, 243, 350, 108, 309, 175, 32, 323, 194, 321, 244, 310, 74, 76, 243, 350, 108, 310, 175, 32, 324, 194, 76, 243, 350, 108, 311, 175, 32, 325, 194, 247, 74, 243, 322, 230, 63, 108, 202, 27, 244, 266, 74, 324, 230, 202, 108, 63, 27, 244, 268, 175, 32, 326, 194, 248, 74, 243, 323, 230, 202, 108, 63, 27, 244, 273, 74, 324, 230, 63, 108, 202, 27, 244, 272, 175, 32, 327, 194, 249, 74, 243, 323, 230, 202, 108, 63, 27, 244, 278, 74, 324, 230, 63, 108, 202, 27, 244, 277, 175, 32, 328, 194, 251, 74, 322, 244, 286, 32, 329, 194, 252, 74, 322, 244, 290, 32, 330, 194, 253, 74, 322, 244, 294, 32, 182, 308, 63, 32, 331, 194, 250, 74, 243, 322, 230, 63, 108, 202, 27, 244, 280, 74, 323, 230, 202, 108, 63, 27, 244, 282, 175, 32, 332, 194, 255, 74, 322, 244, 303, 32, 333, 194, 256, 74, 322, 244, 307, 32, 185, 32, 334, 194, 39, 243, 257, 108, 261, 4, 314, 244, 257, 175, 32, 335, 194, 176, 243, 243, 309, 108, 310, 175, 108, 91, 194, 145, 175, 32, 182, 308, 63, 32, 336, 194, 57, 243, 331, 108, 337, 194, 243, 322, 230, 63, 108, 202, 27, 1, 334, 175, 173, 243, 323, 230, 202, 108, 63, 27, 1, 258, 175, 108, 338, 194, 350, 175, 82, 339, 243, 145, 175, 32, 185, 32, 340, 194, 39, 243, 263, 108, 262, 48, 264, 4, 317, 244, 263, 175, 32, 135, 341, 157, 5, 243, 340, 175, 63, 32, 342, 194, 57, 243, 325, 108, 337, 194, 243, 322, 230, 63, 108, 202, 27, 1, 334, 175, 173, 243, 324, 230, 202, 108, 63, 27, 1, 259, 175, 108, 338, 194, 350, 175, 32, 343, 194, 57, 243, 326, 108, 337, 194, 243, 324, 230, 63, 108, 202, 27, 1, 259, 175, 173, 243, 323, 230, 202, 108, 63, 27, 1, 258, 175, 108, 338, 194, 350, 175, 32, 343, 194, 343, 82, 339, 243, 325, 82, 91, 82, 114, 175, 32, 344, 194, 15, 243, 342, 108, 343, 175, 32, 345, 194, 57, 243, 252, 74, 243, 257, 4, 348, 175, 244, 290, 175, 82, 339, 243, 145, 175, 32, 346, 194, 57, 243, 329, 108, 337, 194, 322, 1, 257, 108, 338, 194, 350, 175, 82, 339, 243, 145, 175, 32, 351, 194, 57, 243, 328, 108, 337, 194, 322, 1, 257, 108, 338, 194, 350, 175, 82, 339, 243, 145, 175, 32, 352, 194, 107, 243, 345, 4, 346, 175, 32, 344, 24, 243, 352, 244, 351, 175, 230, 63, 108, 202, 27, 32, 182, 308, 63, 32, 353, 194, 221, 243, 344, 244, 336, 108, 313, 194, 348, 175, 32, 196, 243, 332, 74, 303, 108, 353, 108, 337, 194, 322, 1, 257, 4, 348, 175, 32, 185, 32, 354, 194, 57, 243, 327, 108, 337, 194, 243, 324, 230, 63, 108, 202, 27, 1, 259, 175, 173, 243, 323, 230, 202, 108, 63, 27, 1, 258, 175, 108, 338, 194, 350, 175, 32, 354, 194, 354, 82, 339, 243, 325, 82, 91, 82, 114, 175, 32, 355, 194, 15, 243, 342, 108, 354, 175, 32, 356, 194, 57, 243, 253, 175, 82, 339, 243, 145, 175, 32, 357, 194, 57, 243, 330, 108, 337, 194, 322, 1, 257, 108, 338, 194, 350, 175, 82, 339, 243, 145, 175, 32, 358, 194, 107, 243, 356, 4, 357, 175, 32, 355, 24, 243, 358, 244, 351, 175, 230, 63, 108, 202, 27, 32, 182, 308, 63, 32, 359, 194, 221, 243, 355, 244, 336, 108, 313, 194, 348, 175, 32, 196, 243, 333, 4, 307, 108, 359, 108, 337, 194, 243, 322, 144, 348, 175, 173, 243, 322, 1, 257, 175, 175, 32, 185, 32, 335, 172, 344, 74, 355, 32, 325, 172, 267, 32, 326, 172, 271, 32, 327, 172, 276, 32, 328, 172, 285, 32, 252, 172, 289, 32, 329, 172, 289, 32, 253, 172, 293, 32, 330, 172, 293, 32, 182, 308, 63, 32, 332, 172, 302, 32, 333, 172, 306, 32, 185, 32, 78, 32, 322, 194, 320, 244, 309, 74, 76, 243, 350, 108, 309, 175, 32, 323, 194, 321, 244, 310, 74, 76, 243, 350, 108, 310, 175, 32, 360, 194, 254, 74, 243, 322, 230, 63, 108, 202, 27, 244, 296, 74, 323, 230, 202, 108, 63, 27, 244, 299, 175, 32, 10, 243, 360, 108, 335, 108, 337, 194, 243, 322, 230, 63, 108, 202, 27, 1, 334, 175, 173, 243, 323, 230, 202, 108, 63, 27, 1, 258, 175, 175, 32, 3, 32]}, {"code": "def _chunk_state_bwd_ddAcs_stable_kernel(\n    x_ptr,\n    b_ptr,\n    dstates_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    seq_idx_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    hdim,\n    dstate,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_b_batch,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_b_dstate,\n    stride_dstates_batch,\n    stride_dstates_chunk,\n    stride_states_head,\n    stride_states_hdim,\n    stride_states_dstate,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_DSTATE: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    b_ptr += (\n        pid_b * stride_b_batch\n        + pid_c * chunk_size * stride_b_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_b_head\n    )\n    dstates_ptr += (\n        pid_b * stride_dstates_batch\n        + pid_c * stride_dstates_chunk\n        + pid_h * stride_states_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    ddA_cumsum_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_c * stride_ddA_cs_chunk\n        + pid_h * stride_ddA_cs_head\n    )\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    offs_k = tl.arange(\n        0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K\n    )\n    b_ptrs = b_ptr + (\n        offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate\n    )\n    dstates_ptrs = dstates_ptr + (\n        offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate\n    )\n    if BLOCK_SIZE_DSTATE <= 128:\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate),\n            other=0.0,\n        )\n        dstates = tl.load(\n            dstates_ptrs,\n            mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n        dstates = dstates.to(b_ptr.dtype.element_ty)\n        acc = tl.dot(b, dstates)\n    else:\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            b = tl.load(\n                b_ptrs,\n                mask=(offs_m[:, None] < chunk_size_limit)\n                & (offs_k[None, :] < dstate - k),\n                other=0.0,\n            )\n            dstates = tl.load(\n                dstates_ptrs,\n                mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim),\n                other=0.0,\n            )\n            dstates = dstates.to(b_ptr.dtype.element_ty)\n            acc += tl.dot(b, dstates)\n            b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    dA_cs_m = tl.load(\n        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0\n    ).to(tl.float32)\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(\n        tl.float32\n    )\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_last - dA_cs_m)\n    else:\n        seq_idx_m = tl.load(\n            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit,\n            other=-1,\n        )\n        seq_idx_last = tl.load(\n            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen\n        )\n        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0)\n    acc *= scale[:, None]\n\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n    )\n    x = tl.load(\n        x_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n    ddt = tl.sum(acc * x, axis=1)\n\n    ddA_cs = ddt * dt_m\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n\n    tl.atomic_add(\n        ddA_cumsum_ptrs + stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size - 1\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 62, 6, 108, 288, 62, 6, 108, 289, 62, 6, 108, 290, 62, 6, 108, 291, 62, 6, 175, 62, 32, -1, 292, 194, 171, 243, 293, 194, 348, 175, 32, 294, 194, 292, 48, 257, 32, 295, 194, 292, 4, 294, 244, 257, 32, 296, 194, 171, 243, 293, 194, 349, 175, 32, 297, 194, 64, 243, 255, 108, 289, 175, 32, 298, 194, 171, 243, 293, 194, 350, 175, 48, 297, 32, 299, 194, 171, 243, 293, 194, 350, 175, 226, 297, 32, 247, 172, 295, 244, 260, 74, 294, 244, 254, 244, 261, 74, 296, 244, 262, 32, 248, 172, 295, 244, 264, 74, 294, 244, 254, 244, 265, 74, 296, 48, 259, 244, 266, 32, 249, 172, 295, 244, 268, 74, 294, 244, 269, 74, 296, 244, 270, 32, 250, 172, 295, 244, 273, 74, 294, 244, 274, 74, 296, 244, 275, 32, 253, 172, 295, 244, 283, 74, 294, 244, 284, 74, 296, 244, 285, 32, 251, 172, 295, 244, 277, 74, 294, 244, 278, 74, 296, 244, 279, 32, 182, 287, 62, 32, 252, 172, 295, 244, 281, 74, 294, 244, 254, 244, 282, 32, 185, 32, 300, 194, 298, 244, 288, 74, 76, 243, 350, 108, 288, 175, 32, 301, 194, 299, 244, 289, 74, 76, 243, 350, 108, 289, 175, 32, 302, 194, 39, 243, 254, 108, 258, 4, 294, 244, 254, 175, 32, 303, 194, 76, 243, 350, 108, 291, 182, 291, 218, 348, 349, 351, 30, 290, 175, 32, 304, 194, 248, 74, 243, 300, 230, 62, 108, 202, 27, 244, 265, 74, 303, 230, 202, 108, 62, 27, 244, 267, 175, 32, 305, 194, 249, 74, 243, 301, 230, 202, 108, 62, 27, 244, 271, 74, 303, 230, 62, 108, 202, 27, 244, 272, 175, 32, 182, 291, 218, 348, 349, 351, 62, 32, 306, 194, 55, 243, 304, 108, 307, 194, 243, 300, 230, 62, 108, 202, 27, 1, 302, 175, 173, 243, 303, 230, 202, 108, 62, 27, 1, 256, 175, 108, 308, 194, 350, 175, 32, 309, 194, 55, 243, 305, 108, 307, 194, 243, 303, 230, 62, 108, 202, 27, 1, 256, 175, 173, 243, 301, 230, 202, 108, 62, 27, 1, 255, 175, 108, 308, 194, 350, 175, 32, 309, 194, 309, 82, 310, 243, 248, 82, 91, 82, 114, 175, 32, 311, 194, 15, 243, 306, 108, 309, 175, 32, 185, 32, 30, 62, 32, 311, 194, 176, 243, 243, 288, 108, 289, 175, 108, 91, 194, 145, 175, 32, 135, 312, 157, 5, 243, 350, 108, 256, 108, 290, 175, 62, 32, 306, 194, 55, 243, 304, 108, 307, 194, 243, 300, 230, 62, 108, 202, 27, 1, 302, 175, 173, 243, 303, 230, 202, 108, 62, 27, 1, 256, 4, 312, 175, 108, 308, 194, 350, 175, 32, 309, 194, 55, 243, 305, 108, 307, 194, 243, 303, 230, 62, 108, 202, 27, 1, 256, 4, 312, 175, 173, 243, 301, 230, 202, 108, 62, 27, 1, 255, 175, 108, 308, 194, 350, 175, 32, 309, 194, 309, 82, 310, 243, 248, 82, 91, 82, 114, 175, 32, 311, 172, 15, 243, 306, 108, 309, 175, 32, 304, 172, 290, 244, 267, 32, 305, 172, 290, 244, 272, 32, 78, 32, 56, 32, 300, 194, 298, 244, 288, 74, 76, 243, 350, 108, 288, 175, 32, 301, 194, 299, 244, 289, 74, 76, 243, 350, 108, 289, 175, 32, 313, 194, 55, 243, 251, 74, 300, 244, 280, 108, 307, 194, 300, 1, 254, 108, 308, 194, 350, 175, 82, 310, 243, 145, 175, 32, 314, 194, 55, 243, 251, 74, 243, 254, 4, 348, 175, 244, 280, 175, 82, 310, 243, 145, 175, 32, 182, 63, 287, 62, 32, 315, 194, 107, 243, 314, 4, 313, 175, 32, 185, 32, 30, 62, 32, 316, 194, 55, 243, 252, 74, 300, 244, 282, 108, 307, 194, 300, 1, 302, 108, 308, 194, 4, 348, 175, 32, 317, 194, 55, 243, 252, 74, 243, 302, 4, 348, 175, 244, 282, 175, 32, 315, 194, 205, 243, 316, 77, 317, 108, 107, 243, 314, 4, 313, 175, 108, 350, 175, 32, 56, 32, 311, 24, 315, 230, 62, 108, 202, 27, 32, 318, 194, 247, 74, 243, 300, 230, 62, 108, 202, 27, 244, 261, 74, 301, 230, 202, 108, 62, 27, 244, 263, 175, 32, 319, 194, 55, 243, 318, 108, 307, 194, 243, 300, 230, 62, 108, 202, 27, 1, 302, 175, 173, 243, 301, 230, 202, 108, 62, 27, 1, 255, 175, 108, 308, 194, 350, 175, 82, 310, 243, 145, 175, 32, 320, 194, 250, 74, 300, 244, 276, 32, 321, 194, 55, 243, 320, 108, 307, 194, 300, 1, 254, 108, 308, 194, 350, 175, 82, 310, 243, 145, 175, 32, 322, 194, 221, 243, 311, 244, 319, 108, 293, 194, 348, 175, 32, 323, 194, 322, 244, 321, 32, 324, 194, 253, 74, 300, 244, 286, 32, 196, 243, 324, 74, 286, 108, 323, 108, 307, 194, 300, 1, 254, 4, 348, 175, 32, 3, 32]}, {"code": "def _chunk_state_varlen_kernel(\n    x_ptr,\n    b_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    chunk_states_ptr,\n    cu_seqlens_ptr,\n    states_ptr,\n    hdim,\n    dstate,\n    chunk_size,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_b_dstate,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_chunk_states_chunk,\n    stride_chunk_states_head,\n    stride_chunk_states_hdim,\n    stride_chunk_states_dstate,\n    stride_states_batch,\n    stride_states_head,\n    stride_states_hdim,\n    stride_states_dstate,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    end_idx = tl.load(cu_seqlens_ptr + pid_b + 1)\n    pid_c = (end_idx - 1) // chunk_size\n    b_ptr += (\n        pid_c * chunk_size * stride_b_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_b_head\n    )\n    x_ptr += pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\n    dt_ptr += pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\n    chunk_states_ptr += (\n        pid_c * stride_chunk_states_chunk + pid_h * stride_chunk_states_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen\n    )\n    b_ptrs = b_ptr + (\n        offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen\n    )\n    dt_ptrs = dt_ptr + offs_k * stride_dt_csize\n    dA_cs_last = tl.load(\n        dA_cumsum_ptr + (end_idx - pid_c * chunk_size - 1) * stride_dA_cs_csize\n    ).to(tl.float32)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n\n    chunk_size_limit = end_idx - pid_c * chunk_size\n    start_idx = tl.load(cu_seqlens_ptr + pid_b)\n    start_idx_cur = tl.maximum(start_idx - pid_c * chunk_size, 0)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_m[:, None] < hdim)\n            & (offs_k[None, :] < chunk_size_limit - k)\n            & (offs_k[None, :] >= start_idx_cur - k),\n            other=0.0,\n        )\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_k[:, None] < chunk_size_limit - k)\n            & (offs_n[None, :] < dstate)\n            & (offs_k[:, None] >= start_idx_cur - k),\n            other=0.0,\n        ).to(tl.float32)\n        dA_cs_k = tl.load(\n            dA_cumsum_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0\n        ).to(tl.float32)\n        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(\n            tl.float32\n        )\n        scale = tl.where(\n            (offs_k >= start_idx_cur - k) & (offs_k < chunk_size_limit - k),\n            tl.exp((dA_cs_last - dA_cs_k)) * dt_k,\n            0.0,\n        )\n        b *= scale[:, None]\n        b = b.to(x_ptr.dtype.element_ty)\n        acc += tl.dot(x, b)\n        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen\n        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n\n    if start_idx < pid_c * chunk_size:\n        chunk_states_ptrs = chunk_states_ptr + (\n            offs_m[:, None] * stride_chunk_states_hdim\n            + offs_n[None, :] * stride_chunk_states_dstate\n        )\n        chunk_states = tl.load(\n            chunk_states_ptrs,\n            mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),\n            other=0.0,\n        ).to(tl.float32)\n\n        scale = tl.exp(dA_cs_last)\n        acc += chunk_states * scale\n\n    states = acc.to(states_ptr.dtype.element_ty)\n\n    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    states_ptrs = states_ptr + (\n        offs_m[:, None] * stride_states_hdim + offs_n[None, :] * stride_states_dstate\n    )\n    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)\n    tl.store(states_ptrs, states, mask=c_mask)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 63, 6, 108, 280, 63, 6, 108, 281, 63, 6, 175, 63, 32, -1, 282, 194, 171, 243, 283, 194, 348, 175, 32, 284, 194, 171, 243, 283, 194, 349, 175, 32, 285, 194, 65, 243, 255, 108, 280, 175, 32, 286, 194, 171, 243, 283, 194, 350, 175, 48, 285, 32, 287, 194, 171, 243, 283, 194, 350, 175, 226, 285, 32, 288, 194, 57, 243, 252, 74, 282, 74, 348, 175, 32, 289, 194, 243, 288, 4, 348, 175, 48, 256, 32, 248, 172, 289, 244, 256, 244, 262, 74, 284, 48, 258, 244, 263, 32, 247, 172, 289, 244, 256, 244, 259, 74, 284, 244, 260, 32, 249, 172, 289, 244, 265, 74, 284, 244, 266, 32, 250, 172, 289, 244, 268, 74, 284, 244, 269, 32, 251, 172, 289, 244, 271, 74, 284, 244, 272, 32, 290, 194, 286, 244, 279, 74, 76, 243, 350, 108, 279, 175, 32, 291, 194, 287, 244, 280, 74, 76, 243, 350, 108, 280, 175, 32, 292, 194, 76, 243, 350, 108, 281, 175, 32, 293, 194, 247, 74, 243, 290, 230, 63, 108, 202, 27, 244, 261, 74, 292, 230, 202, 108, 63, 27, 244, 259, 175, 32, 294, 194, 248, 74, 243, 291, 230, 202, 108, 63, 27, 244, 264, 74, 292, 230, 63, 108, 202, 27, 244, 262, 175, 32, 295, 194, 249, 74, 292, 244, 267, 32, 296, 194, 57, 243, 250, 74, 243, 288, 4, 289, 244, 256, 4, 348, 175, 244, 270, 175, 82, 297, 243, 145, 175, 32, 298, 194, 250, 74, 292, 244, 270, 32, 299, 194, 288, 4, 289, 244, 256, 32, 300, 194, 57, 243, 252, 74, 282, 175, 32, 301, 194, 192, 243, 300, 4, 289, 244, 256, 108, 350, 175, 32, 302, 194, 176, 243, 243, 279, 108, 280, 175, 108, 91, 194, 145, 175, 32, 135, 303, 157, 5, 243, 350, 108, 299, 108, 281, 175, 63, 32, 304, 194, 57, 243, 293, 108, 305, 194, 243, 290, 230, 63, 108, 202, 27, 1, 254, 175, 173, 243, 292, 230, 202, 108, 63, 27, 1, 299, 4, 303, 175, 173, 243, 292, 230, 202, 108, 63, 27, 144, 301, 4, 303, 175, 108, 306, 194, 350, 175, 32, 307, 194, 57, 243, 294, 108, 305, 194, 243, 292, 230, 63, 108, 202, 27, 1, 299, 4, 303, 175, 173, 243, 291, 230, 202, 108, 63, 27, 1, 255, 175, 173, 243, 292, 230, 63, 108, 202, 27, 144, 301, 4, 303, 175, 108, 306, 194, 350, 175, 82, 297, 243, 145, 175, 32, 308, 194, 57, 243, 298, 108, 305, 194, 292, 1, 299, 4, 303, 108, 306, 194, 350, 175, 82, 297, 243, 145, 175, 32, 309, 194, 57, 243, 295, 108, 305, 194, 292, 1, 299, 4, 303, 108, 306, 194, 350, 175, 82, 297, 243, 145, 175, 32, 310, 194, 205, 243, 243, 292, 144, 301, 4, 303, 175, 173, 243, 292, 1, 299, 4, 303, 175, 108, 107, 243, 296, 4, 308, 175, 244, 309, 108, 350, 175, 32, 307, 24, 310, 230, 63, 108, 202, 27, 32, 307, 194, 307, 82, 297, 243, 247, 82, 91, 82, 114, 175, 32, 302, 172, 15, 243, 304, 108, 307, 175, 32, 293, 172, 281, 244, 259, 32, 294, 172, 281, 244, 262, 32, 295, 172, 281, 244, 267, 32, 298, 172, 281, 244, 270, 32, 78, 32, 182, 300, 1, 289, 244, 256, 63, 32, 311, 194, 251, 74, 243, 290, 230, 63, 108, 202, 27, 244, 273, 74, 291, 230, 202, 108, 63, 27, 244, 274, 175, 32, 312, 194, 57, 243, 311, 108, 305, 194, 243, 290, 230, 63, 108, 202, 27, 1, 254, 175, 173, 243, 291, 230, 202, 108, 63, 27, 1, 255, 175, 108, 306, 194, 350, 175, 82, 297, 243, 145, 175, 32, 310, 194, 107, 243, 296, 175, 32, 302, 172, 312, 244, 310, 32, 185, 32, 313, 194, 302, 82, 297, 243, 253, 82, 91, 82, 114, 175, 32, 253, 172, 282, 244, 275, 74, 284, 244, 276, 32, 290, 194, 286, 244, 279, 74, 76, 243, 350, 108, 279, 175, 32, 291, 194, 287, 244, 280, 74, 76, 243, 350, 108, 280, 175, 32, 314, 194, 253, 74, 243, 290, 230, 63, 108, 202, 27, 244, 277, 74, 291, 230, 202, 108, 63, 27, 244, 278, 175, 32, 315, 194, 243, 290, 230, 63, 108, 202, 27, 1, 254, 175, 173, 243, 291, 230, 202, 108, 63, 27, 1, 255, 175, 32, 10, 243, 314, 108, 313, 108, 305, 194, 315, 175, 32, 3, 32]}, {"code": "def _chunk_scan_chunk_state_bwd_dx_kernel(\n    x_ptr,\n    cb_ptr,\n    dout_ptr,\n    dt_ptr,\n    dA_cumsum_f_ptr,\n    dA_cumsum_b_ptr,\n    D_ptr,\n    b_ptr,\n    dstates_f_ptr,\n    dstates_b_ptr,\n    dx_ptr,\n    ddt_ptr,\n    dD_ptr,\n    chunk_size,\n    hdim,\n    dstate,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_k,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_f_batch,\n    stride_dA_cs_f_chunk,\n    stride_dA_cs_f_head,\n    stride_dA_cs_f_csize,\n    stride_dA_cs_b_batch,\n    stride_dA_cs_b_chunk,\n    stride_dA_cs_b_head,\n    stride_dA_cs_b_csize,\n    stride_D_head,\n    stride_b_batch,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_b_dstate,\n    stride_dstates_f_batch,\n    stride_dstates_f_chunk,\n    stride_dstates_f_head,\n    stride_dstates_f_hdim,\n    stride_dstates_f_dstate,\n    stride_dstates_b_batch,\n    stride_dstates_b_chunk,\n    stride_dstates_b_head,\n    stride_dstates_b_hdim,\n    stride_dstates_b_dstate,\n    stride_dx_batch,\n    stride_dx_seqlen,\n    stride_dx_head,\n    stride_dx_hdim,\n    stride_ddt_batch,\n    stride_ddt_chunk,\n    stride_ddt_head,\n    stride_ddt_csize,\n    stride_dD_batch,\n    stride_dD_chunk,\n    stride_dD_head,\n    stride_dD_csize,\n    stride_dD_hdim,\n    HAS_D: tl.constexpr,\n    D_HAS_HDIM: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_DSTATE: tl.constexpr,\n    IS_TRITON_22: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    ddt_ptr += (\n        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\n    )\n    dA_cumsum_f_ptr += (\n        pid_b * stride_dA_cs_f_batch\n        + pid_c * stride_dA_cs_f_chunk\n        + pid_h * stride_dA_cs_f_head\n    )\n    dA_cumsum_b_ptr += (\n        pid_b * stride_dA_cs_b_batch\n        + pid_c * stride_dA_cs_b_chunk\n        + pid_h * stride_dA_cs_b_head\n    )\n    b_ptr += (\n        pid_b * stride_b_batch\n        + pid_c * chunk_size * stride_b_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_b_head\n    )\n    dstates_f_ptr += (\n        pid_b * stride_dstates_f_batch\n        + pid_c * stride_dstates_f_chunk\n        + pid_h * stride_dstates_f_head\n    )\n    dstates_b_ptr += (\n        pid_b * stride_dstates_b_batch\n        + pid_c * stride_dstates_b_chunk\n        + pid_h * stride_dstates_b_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    dA_cs_f_m = tl.load(\n        dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize,\n        mask=offs_m < chunk_size_limit,\n        other=0.0,\n    ).to(tl.float32)\n    dA_cs_b_m = tl.load(\n        dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize,\n        mask=offs_m < chunk_size_limit,\n        other=0.0,\n    ).to(tl.float32)\n\n    dA_cs_f_last = tl.load(\n        dA_cumsum_f_ptr + (chunk_size - 1) * stride_dA_cs_f_csize\n    ).to(tl.float32)\n    dA_cs_b_last = tl.load(dA_cumsum_b_ptr).to(tl.float32)\n    scale_f = tl.exp(dA_cs_f_last - dA_cs_f_m)\n    scale_b = tl.exp(dA_cs_b_last - dA_cs_b_m)\n\n    offs_dstate = tl.arange(\n        0,\n        (\n            BLOCK_SIZE_DSTATE\n            if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128\n            else BLOCK_SIZE_K\n        ),\n    )\n    b_ptrs = b_ptr + (\n        offs_m[:, None] * stride_b_seqlen + offs_dstate[None, :] * stride_b_dstate\n    )\n    dstates_f_ptrs = dstates_f_ptr + (\n        offs_n[None, :] * stride_dstates_f_hdim\n        + offs_dstate[:, None] * stride_dstates_f_dstate\n    )\n    dstates_b_ptrs = dstates_b_ptr + (\n        offs_n[None, :] * stride_dstates_b_hdim\n        + offs_dstate[:, None] * stride_dstates_b_dstate\n    )\n    if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128:\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_dstate[None, :] < dstate),\n            other=0.0,\n        )\n        dstates_f = tl.load(\n            dstates_f_ptrs,\n            mask=(offs_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n        dstates_f = dstates_f.to(b_ptr.dtype.element_ty)\n        acc = tl.dot(b, dstates_f) * scale_f[:, None]\n        dstates_b = tl.load(\n            dstates_b_ptrs,\n            mask=(offs_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n        dstates_b = dstates_b.to(b_ptr.dtype.element_ty)\n        acc += tl.dot(b, dstates_b) * scale_b[:, None]\n    else:\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            b = tl.load(\n                b_ptrs,\n                mask=(offs_m[:, None] < chunk_size_limit)\n                & (offs_dstate[None, :] < dstate - k),\n                other=0.0,\n            )\n            dstates_f = tl.load(\n                dstates_f_ptrs,\n                mask=(offs_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim),\n                other=0.0,\n            )\n            dstates_f = dstates_f.to(b_ptr.dtype.element_ty)\n            acc += tl.dot(b, dstates_f) * scale_f[:, None]\n            dstates_b = tl.load(\n                dstates_b_ptrs,\n                mask=(offs_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim),\n                other=0.0,\n            )\n            dstates_b = dstates_b.to(b_ptr.dtype.element_ty)\n            acc += tl.dot(b, dstates_b) * scale_b[:, None]\n            b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n            dstates_f_ptrs += BLOCK_SIZE_K * stride_dstates_f_dstate\n            dstates_b_ptrs += BLOCK_SIZE_K * stride_dstates_b_dstate\n\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k\n    )\n    dout_ptrs = dout_ptr + (\n        offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim\n    )\n    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize\n    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize\n    K_MAX = chunk_size_limit\n\n    K_F_MAX = min((pid_m) * BLOCK_SIZE_M, chunk_size_limit)\n    for k in range(0, K_MAX, BLOCK_SIZE_K):\n        k = tl.multiple_of(k, BLOCK_SIZE_K)\n\n        cb = tl.load(\n            cb_ptrs,\n            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k),\n            other=0.0,\n        )\n        dout = tl.load(\n            dout_ptrs,\n            mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n\n        if (k <= K_F_MAX + BLOCK_SIZE_M) or (k + BLOCK_SIZE_K >= K_F_MAX):\n            dA_cs_f_k = tl.load(\n                dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0\n            ).to(tl.float32)\n            mask_f = (k + offs_k[None, :] >= offs_m[:, None]) & (\n                k + offs_k[None, :] < K_MAX\n            )\n            a_f = tl.where(\n                mask_f, tl.exp((dA_cs_f_k[None, :] - dA_cs_f_m[:, None])), 0.0\n            )\n            dA_cs_b_k = tl.load(\n                dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0\n            ).to(tl.float32)\n            mask_b = k + offs_k[None, :] <= offs_m[:, None]\n            cb *= a_f + tl.where(\n                mask_b, tl.exp((dA_cs_b_k[None, :] - dA_cs_b_m[:, None])), 0.0\n            )\n\n        elif k < K_F_MAX:\n            dA_cs_b_k = tl.load(\n                dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0\n            ).to(tl.float32)\n            mask_b = k + offs_k[None, :] <= offs_m[:, None]\n            cb *= tl.where(\n                mask_b, tl.exp((dA_cs_b_k[None, :] - dA_cs_b_m[:, None])), 0.0\n            )\n        else:\n            dA_cs_f_k = tl.load(\n                dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0\n            ).to(tl.float32)\n            mask_f = (k + offs_k[None, :] >= offs_m[:, None]) & (\n                k + offs_k[None, :] < K_MAX\n            )\n            cb *= tl.where(\n                mask_f, tl.exp((dA_cs_f_k[None, :] - dA_cs_f_m[:, None])), 0.0\n            )\n        cb = cb.to(dout_ptr.dtype.element_ty)\n        acc += tl.dot(cb, dout)\n        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n        dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize\n        dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\n    dx = acc * dt_m[:, None]\n    dx_ptr += (\n        pid_b * stride_dx_batch\n        + pid_c * chunk_size * stride_dx_seqlen\n        + pid_h * stride_dx_head\n    )\n    dx_ptrs = dx_ptr + (\n        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim\n    )\n    if HAS_D:\n        dout_res_ptrs = dout_ptr + (\n            offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim\n        )\n        dout_res = tl.load(\n            dout_res_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n            other=0.0,\n        ).to(tl.float32)\n        if D_HAS_HDIM:\n            D = tl.load(\n                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0\n            ).to(tl.float32)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n        dx += dout_res * D\n    tl.store(\n        dx_ptrs,\n        dx,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n    )\n\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n    )\n    x = tl.load(\n        x_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    if HAS_D:\n        dD_ptr += (\n            pid_b * stride_dD_batch\n            + pid_c * stride_dD_chunk\n            + pid_h * stride_dD_head\n            + pid_m * stride_dD_csize\n        )\n        if D_HAS_HDIM:\n            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n            dD = tl.sum(dout_res * x, axis=0)\n            tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n        else:\n            dD = tl.sum(dout_res * x)\n            tl.store(dD_ptr, dD)\n    ddt = tl.sum(acc * x, axis=1)\n    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\n    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 108, 301, 108, 302, 108, 303, 108, 304, 108, 305, 108, 306, 108, 307, 108, 308, 108, 309, 108, 310, 108, 311, 108, 312, 108, 313, 108, 314, 108, 315, 108, 316, 108, 317, 108, 318, 108, 319, 62, 6, 108, 320, 62, 6, 108, 321, 62, 6, 108, 322, 62, 6, 108, 323, 62, 6, 108, 324, 62, 6, 108, 325, 62, 6, 175, 62, 32, -1, 326, 194, 171, 243, 327, 194, 348, 175, 32, 328, 194, 326, 48, 263, 32, 329, 194, 326, 4, 328, 244, 263, 32, 330, 194, 171, 243, 327, 194, 349, 175, 32, 331, 194, 64, 243, 261, 108, 322, 175, 32, 332, 194, 171, 243, 327, 194, 350, 175, 48, 331, 32, 333, 194, 171, 243, 327, 194, 350, 175, 226, 331, 32, 247, 172, 329, 244, 266, 74, 328, 244, 260, 244, 267, 74, 330, 244, 268, 32, 248, 172, 329, 244, 270, 74, 328, 244, 271, 74, 330, 48, 265, 244, 272, 32, 249, 172, 329, 244, 275, 74, 328, 244, 260, 244, 276, 74, 330, 244, 277, 32, 250, 172, 329, 244, 279, 74, 328, 244, 280, 74, 330, 244, 281, 32, 258, 172, 329, 244, 310, 74, 328, 244, 311, 74, 330, 244, 312, 32, 251, 172, 329, 244, 283, 74, 328, 244, 284, 74, 330, 244, 285, 32, 252, 172, 329, 244, 287, 74, 328, 244, 288, 74, 330, 244, 289, 32, 254, 172, 329, 244, 292, 74, 328, 244, 260, 244, 293, 74, 330, 48, 265, 244, 294, 32, 255, 172, 329, 244, 296, 74, 328, 244, 297, 74, 330, 244, 298, 32, 256, 172, 329, 244, 301, 74, 328, 244, 302, 74, 330, 244, 303, 32, 334, 194, 332, 244, 321, 74, 76, 243, 350, 108, 321, 175, 32, 335, 194, 333, 244, 322, 74, 76, 243, 350, 108, 322, 175, 32, 336, 194, 39, 243, 260, 108, 264, 4, 328, 244, 260, 175, 32, 337, 194, 176, 243, 243, 321, 108, 322, 175, 108, 91, 194, 145, 175, 32, 338, 194, 55, 243, 251, 74, 334, 244, 286, 108, 339, 194, 334, 1, 336, 108, 340, 194, 350, 175, 82, 341, 243, 145, 175, 32, 342, 194, 55, 243, 252, 74, 334, 244, 290, 108, 339, 194, 334, 1, 336, 108, 340, 194, 350, 175, 82, 341, 243, 145, 175, 32, 343, 194, 55, 243, 251, 74, 243, 260, 4, 348, 175, 244, 286, 175, 82, 341, 243, 145, 175, 32, 344, 194, 55, 243, 252, 175, 82, 341, 243, 145, 175, 32, 345, 194, 107, 243, 343, 4, 338, 175, 32, 346, 194, 107, 243, 344, 4, 342, 175, 32, 351, 194, 76, 243, 350, 108, 324, 182, 325, 102, 324, 218, 348, 349, 352, 30, 323, 175, 32, 353, 194, 254, 74, 243, 334, 230, 62, 108, 202, 27, 244, 293, 74, 351, 230, 202, 108, 62, 27, 244, 295, 175, 32, 354, 194, 255, 74, 243, 335, 230, 202, 108, 62, 27, 244, 299, 74, 351, 230, 62, 108, 202, 27, 244, 300, 175, 32, 355, 194, 256, 74, 243, 335, 230, 202, 108, 62, 27, 244, 304, 74, 351, 230, 62, 108, 202, 27, 244, 305, 175, 32, 182, 325, 102, 324, 218, 348, 349, 352, 62, 32, 356, 194, 55, 243, 353, 108, 339, 194, 243, 334, 230, 62, 108, 202, 27, 1, 336, 175, 173, 243, 351, 230, 202, 108, 62, 27, 1, 262, 175, 108, 340, 194, 350, 175, 32, 357, 194, 55, 243, 354, 108, 339, 194, 243, 351, 230, 62, 108, 202, 27, 1, 262, 175, 173, 243, 335, 230, 202, 108, 62, 27, 1, 261, 175, 108, 340, 194, 350, 175, 32, 357, 194, 357, 82, 341, 243, 254, 82, 91, 82, 114, 175, 32, 337, 194, 15, 243, 356, 108, 357, 175, 244, 345, 230, 62, 108, 202, 27, 32, 358, 194, 55, 243, 355, 108, 339, 194, 243, 351, 230, 62, 108, 202, 27, 1, 262, 175, 173, 243, 335, 230, 202, 108, 62, 27, 1, 261, 175, 108, 340, 194, 350, 175, 32, 358, 194, 358, 82, 341, 243, 254, 82, 91, 82, 114, 175, 32, 337, 172, 15, 243, 356, 108, 358, 175, 244, 346, 230, 62, 108, 202, 27, 32, 185, 32, 30, 62, 32, 135, 359, 157, 5, 243, 350, 108, 262, 108, 323, 175, 62, 32, 356, 194, 55, 243, 353, 108, 339, 194, 243, 334, 230, 62, 108, 202, 27, 1, 336, 175, 173, 243, 351, 230, 202, 108, 62, 27, 1, 262, 4, 359, 175, 108, 340, 194, 350, 175, 32, 357, 194, 55, 243, 354, 108, 339, 194, 243, 351, 230, 62, 108, 202, 27, 1, 262, 4, 359, 175, 173, 243, 335, 230, 202, 108, 62, 27, 1, 261, 175, 108, 340, 194, 350, 175, 32, 357, 194, 357, 82, 341, 243, 254, 82, 91, 82, 114, 175, 32, 337, 172, 15, 243, 356, 108, 357, 175, 244, 345, 230, 62, 108, 202, 27, 32, 358, 194, 55, 243, 355, 108, 339, 194, 243, 351, 230, 62, 108, 202, 27, 1, 262, 4, 359, 175, 173, 243, 335, 230, 202, 108, 62, 27, 1, 261, 175, 108, 340, 194, 350, 175, 32, 358, 194, 358, 82, 341, 243, 254, 82, 91, 82, 114, 175, 32, 337, 172, 15, 243, 356, 108, 358, 175, 244, 346, 230, 62, 108, 202, 27, 32, 353, 172, 323, 244, 295, 32, 354, 172, 323, 244, 300, 32, 355, 172, 323, 244, 305, 32, 78, 32, 56, 32, 360, 194, 76, 243, 350, 108, 323, 175, 32, 361, 194, 248, 74, 243, 334, 230, 62, 108, 202, 27, 244, 273, 74, 360, 230, 202, 108, 62, 27, 244, 274, 175, 32, 362, 194, 249, 74, 243, 360, 230, 62, 108, 202, 27, 244, 276, 74, 335, 230, 202, 108, 62, 27, 244, 278, 175, 32, 363, 194, 251, 74, 360, 244, 286, 32, 364, 194, 252, 74, 360, 244, 290, 32, 365, 194, 336, 32, 366, 194, 39, 243, 332, 244, 321, 108, 336, 175, 32, 135, 359, 157, 5, 243, 350, 108, 365, 108, 323, 175, 62, 32, 359, 194, 57, 243, 359, 108, 323, 175, 32, 367, 194, 55, 243, 361, 108, 339, 194, 243, 334, 230, 62, 108, 202, 27, 1, 260, 175, 173, 243, 360, 230, 202, 108, 62, 27, 1, 365, 4, 359, 175, 108, 340, 194, 350, 175, 32, 368, 194, 55, 243, 362, 108, 339, 194, 243, 360, 230, 62, 108, 202, 27, 1, 365, 4, 359, 175, 173, 243, 335, 230, 202, 108, 62, 27, 1, 261, 175, 108, 340, 194, 350, 175, 32, 182, 359, 218, 366, 74, 321, 141, 359, 74, 323, 144, 366, 62, 32, 369, 194, 55, 243, 363, 108, 339, 194, 360, 1, 260, 4, 359, 108, 340, 194, 350, 175, 82, 341, 243, 145, 175, 32, 370, 194, 243, 359, 74, 360, 230, 202, 108, 62, 27, 144, 334, 230, 62, 108, 202, 27, 175, 173, 243, 359, 74, 360, 230, 202, 108, 62, 27, 1, 365, 175, 32, 371, 194, 205, 243, 370, 108, 107, 243, 369, 230, 202, 108, 62, 27, 4, 338, 230, 62, 108, 202, 27, 175, 108, 350, 175, 32, 372, 194, 55, 243, 364, 108, 339, 194, 360, 1, 260, 4, 359, 108, 340, 194, 350, 175, 82, 341, 243, 145, 175, 32, 373, 194, 359, 74, 360, 230, 202, 108, 62, 27, 218, 334, 230, 62, 108, 202, 27, 32, 367, 24, 371, 74, 205, 243, 373, 108, 107, 243, 372, 230, 202, 108, 62, 27, 4, 342, 230, 62, 108, 202, 27, 175, 108, 350, 175, 32, 65, 32, 37, 359, 1, 366, 62, 32, 372, 194, 55, 243, 364, 108, 339, 194, 360, 1, 260, 4, 359, 108, 340, 194, 350, 175, 82, 341, 243, 145, 175, 32, 373, 194, 359, 74, 360, 230, 202, 108, 62, 27, 218, 334, 230, 62, 108, 202, 27, 32, 367, 24, 205, 243, 373, 108, 107, 243, 372, 230, 202, 108, 62, 27, 4, 342, 230, 62, 108, 202, 27, 175, 108, 350, 175, 32, 185, 32, 30, 62, 32, 369, 194, 55, 243, 363, 108, 339, 194, 360, 1, 260, 4, 359, 108, 340, 194, 350, 175, 82, 341, 243, 145, 175, 32, 370, 194, 243, 359, 74, 360, 230, 202, 108, 62, 27, 144, 334, 230, 62, 108, 202, 27, 175, 173, 243, 359, 74, 360, 230, 202, 108, 62, 27, 1, 365, 175, 32, 367, 24, 205, 243, 370, 108, 107, 243, 369, 230, 202, 108, 62, 27, 4, 338, 230, 62, 108, 202, 27, 175, 108, 350, 175, 32, 56, 32, 367, 194, 367, 82, 341, 243, 249, 82, 91, 82, 114, 175, 32, 337, 172, 15, 243, 367, 108, 368, 175, 32, 361, 172, 323, 244, 274, 32, 362, 172, 323, 244, 276, 32, 363, 172, 323, 244, 286, 32, 364, 172, 323, 244, 290, 32, 78, 32, 334, 194, 332, 244, 321, 74, 76, 243, 350, 108, 321, 175, 32, 335, 194, 333, 244, 322, 74, 76, 243, 350, 108, 322, 175, 32, 374, 194, 250, 74, 334, 244, 282, 32, 375, 194, 55, 243, 374, 108, 339, 194, 334, 1, 336, 108, 340, 194, 350, 175, 82, 341, 243, 145, 175, 32, 376, 194, 337, 244, 375, 230, 62, 108, 202, 27, 32, 257, 172, 329, 244, 306, 74, 328, 244, 260, 244, 307, 74, 330, 244, 308, 32, 377, 194, 257, 74, 243, 334, 230, 62, 108, 202, 27, 244, 307, 74, 335, 230, 202, 108, 62, 27, 244, 309, 175, 32, 182, 319, 62, 32, 378, 194, 249, 74, 243, 334, 230, 62, 108, 202, 27, 244, 276, 74, 335, 230, 202, 108, 62, 27, 244, 278, 175, 32, 379, 194, 55, 243, 378, 108, 339, 194, 243, 334, 230, 62, 108, 202, 27, 1, 336, 175, 173, 243, 335, 230, 202, 108, 62, 27, 1, 261, 175, 108, 340, 194, 350, 175, 82, 341, 243, 145, 175, 32, 182, 320, 62, 32, 380, 194, 55, 243, 253, 74, 330, 244, 291, 74, 335, 108, 339, 194, 335, 1, 261, 108, 340, 194, 350, 175, 82, 341, 243, 145, 175, 32, 185, 32, 30, 62, 32, 380, 194, 55, 243, 253, 74, 330, 244, 291, 175, 82, 341, 243, 145, 175, 32, 56, 32, 376, 172, 379, 244, 380, 32, 185, 32, 10, 243, 377, 108, 376, 108, 339, 194, 243, 334, 230, 62, 108, 202, 27, 1, 336, 175, 173, 243, 335, 230, 202, 108, 62, 27, 1, 261, 175, 175, 32, 381, 194, 247, 74, 243, 334, 230, 62, 108, 202, 27, 244, 267, 74, 335, 230, 202, 108, 62, 27, 244, 269, 175, 32, 382, 194, 55, 243, 381, 108, 339, 194, 243, 334, 230, 62, 108, 202, 27, 1, 336, 175, 173, 243, 335, 230, 202, 108, 62, 27, 1, 261, 175, 108, 340, 194, 350, 175, 82, 341, 243, 145, 175, 32, 182, 319, 62, 32, 259, 172, 329, 244, 314, 74, 328, 244, 315, 74, 330, 244, 316, 74, 332, 244, 317, 32, 182, 320, 62, 32, 383, 194, 259, 74, 335, 244, 318, 32, 384, 194, 221, 243, 379, 244, 382, 108, 327, 194, 350, 175, 32, 10, 243, 383, 108, 384, 108, 339, 194, 335, 1, 261, 175, 32, 185, 32, 30, 62, 32, 384, 194, 221, 243, 379, 244, 382, 175, 32, 10, 243, 259, 108, 384, 175, 32, 56, 32, 185, 32, 385, 194, 221, 243, 337, 244, 382, 108, 327, 194, 348, 175, 32, 386, 194, 258, 74, 334, 244, 313, 32, 196, 243, 386, 108, 385, 108, 339, 194, 334, 1, 260, 175, 32, 3, 32]}, {"code": "def _state_passing_fwd_kernel(\n    states_ptr,\n    out_ptr,\n    final_states_ptr,\n    dA_cs_ptr,\n    dim,\n    nchunks,\n    seqlen,\n    chunk_size,\n    stride_states_batch,\n    stride_states_chunk,\n    stride_states_head,\n    stride_states_dim,\n    stride_out_batch,\n    stride_out_chunk,\n    stride_out_head,\n    stride_out_dim,\n    stride_final_states_batch,\n    stride_final_states_head,\n    stride_final_states_dim,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    REVERSE: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head\n    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head\n    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\n    final_states_ptr += (\n        pid_b * stride_final_states_batch + pid_h * stride_final_states_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    states_ptrs = states_ptr + offs_m * stride_states_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    final_states_ptrs = final_states_ptr + offs_m * stride_final_states_dim\n\n    if not REVERSE:\n\n        states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        tl.store(out_ptrs, states, mask=offs_m < dim)\n        out_ptrs += stride_out_chunk\n        for c in range(nchunks):\n            new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0).to(\n                tl.float32\n            )\n            dA_cs = tl.load(dA_cs_ptr).to(tl.float32)\n            scale = tl.exp(dA_cs)\n            states = scale * states + new_states\n            if c < nchunks - 1:\n                tl.store(out_ptrs, states, mask=offs_m < dim)\n            else:\n                tl.store(final_states_ptrs, states, mask=offs_m < dim)\n            states_ptrs += stride_states_chunk\n            dA_cs_ptr += stride_dA_cs_chunk\n            out_ptrs += stride_out_chunk\n    else:\n\n        states_ptrs += (nchunks - 1) * stride_states_chunk\n        dA_cs_ptr += (nchunks - 1) * stride_dA_cs_chunk\n        out_ptrs += (nchunks - 1) * stride_out_chunk\n\n        states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        tl.store(out_ptrs, states, mask=offs_m < dim)\n        out_ptrs -= stride_out_chunk\n        for c in range(nchunks - 1, -1, -1):\n            new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0).to(\n                tl.float32\n            )\n            dA_cs = tl.load(dA_cs_ptr).to(tl.float32)\n            scale = tl.exp(dA_cs)\n            states = scale * states + new_states\n            if c > 0:\n                tl.store(out_ptrs, states, mask=offs_m < dim)\n            else:\n                tl.store(final_states_ptrs, states, mask=offs_m < dim)\n            states_ptrs -= stride_states_chunk\n            dA_cs_ptr -= stride_dA_cs_chunk\n            out_ptrs -= stride_out_chunk", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 170, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 63, 6, 108, 269, 63, 6, 175, 63, 32, -1, 270, 194, 171, 243, 271, 194, 348, 175, 32, 272, 194, 171, 243, 271, 194, 349, 175, 32, 273, 194, 171, 243, 271, 194, 350, 175, 32, 247, 172, 270, 244, 254, 74, 272, 244, 256, 32, 250, 172, 270, 244, 265, 74, 272, 244, 267, 32, 248, 172, 270, 244, 258, 74, 272, 244, 260, 32, 249, 172, 270, 244, 262, 74, 272, 244, 263, 32, 274, 194, 273, 244, 269, 74, 76, 243, 350, 108, 269, 175, 32, 275, 194, 247, 74, 274, 244, 257, 32, 276, 194, 248, 74, 274, 244, 261, 32, 277, 194, 249, 74, 274, 244, 264, 32, 182, 64, 268, 63, 32, 278, 194, 176, 243, 243, 269, 108, 175, 108, 91, 194, 145, 175, 32, 10, 243, 276, 108, 278, 108, 279, 194, 274, 1, 170, 175, 32, 276, 172, 259, 32, 135, 280, 157, 5, 243, 251, 175, 63, 32, 281, 194, 57, 243, 275, 108, 279, 194, 274, 1, 170, 108, 282, 194, 350, 175, 82, 283, 243, 145, 175, 32, 284, 194, 57, 243, 250, 175, 82, 283, 243, 145, 175, 32, 285, 194, 107, 243, 284, 175, 32, 278, 194, 285, 244, 278, 74, 281, 32, 182, 280, 1, 251, 4, 348, 63, 32, 10, 243, 276, 108, 278, 108, 279, 194, 274, 1, 170, 175, 32, 185, 32, 30, 63, 32, 10, 243, 277, 108, 278, 108, 279, 194, 274, 1, 170, 175, 32, 56, 32, 275, 172, 255, 32, 250, 172, 266, 32, 276, 172, 259, 32, 78, 32, 185, 32, 30, 63, 32, 275, 172, 243, 251, 4, 348, 175, 244, 255, 32, 250, 172, 243, 251, 4, 348, 175, 244, 266, 32, 276, 172, 243, 251, 4, 348, 175, 244, 259, 32, 278, 194, 176, 243, 243, 269, 108, 175, 108, 91, 194, 145, 175, 32, 10, 243, 276, 108, 278, 108, 279, 194, 274, 1, 170, 175, 32, 276, 2, 259, 32, 135, 280, 157, 5, 243, 251, 4, 348, 108, 4, 348, 108, 4, 348, 175, 63, 32, 281, 194, 57, 243, 275, 108, 279, 194, 274, 1, 170, 108, 282, 194, 350, 175, 82, 283, 243, 145, 175, 32, 284, 194, 57, 243, 250, 175, 82, 283, 243, 145, 175, 32, 285, 194, 107, 243, 284, 175, 32, 278, 194, 285, 244, 278, 74, 281, 32, 182, 280, 124, 350, 63, 32, 10, 243, 276, 108, 278, 108, 279, 194, 274, 1, 170, 175, 32, 185, 32, 30, 63, 32, 10, 243, 277, 108, 278, 108, 279, 194, 274, 1, 170, 175, 32, 56, 32, 275, 2, 255, 32, 250, 2, 266, 32, 276, 2, 259, 32, 78, 32, 56, 32, 3, 32]}, {"code": "def _state_passing_bwd_kernel(\n    dout_ptr,\n    out_ptr,\n    dA_cs_ptr,\n    dstates_ptr,\n    ddA_cs_ptr,\n    states_converted_ptr,\n    dim,\n    nchunks,\n    seqlen,\n    chunk_size,\n    stride_dout_batch,\n    stride_dout_chunk,\n    stride_dout_head,\n    stride_dout_dim,\n    stride_out_batch,\n    stride_out_chunk,\n    stride_out_head,\n    stride_out_dim,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dstates_batch,\n    stride_dstates_chunk,\n    stride_dstates_head,\n    stride_dstates_dim,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    CONVERT_STATES: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    REVERSE: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    dstates_ptr += pid_b * stride_dstates_batch + pid_h * stride_dstates_head\n    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head\n    ddA_cs_ptr += pid_b * stride_ddA_cs_batch + pid_h * stride_ddA_cs_head\n    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\n    dout_ptr += pid_b * stride_dout_batch + pid_h * stride_dout_head\n    if not REVERSE:\n        dstates_ptr += (nchunks - 1) * stride_dstates_chunk\n        dA_cs_ptr += (nchunks - 1) * stride_dA_cs_chunk\n        ddA_cs_ptr += (nchunks - 1) * stride_ddA_cs_chunk\n        out_ptr += (nchunks - 1) * stride_out_chunk\n        dout_ptr += (nchunks - 1) * stride_dout_chunk\n\n    if CONVERT_STATES:\n        states_converted_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\n        if not REVERSE:\n            states_converted_ptr += (nchunks - 1) * stride_out_chunk\n\n    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    dstates_ptrs = dstates_ptr + offs_m * stride_dstates_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    dout_ptrs = dout_ptr + offs_m * stride_dout_dim\n    if CONVERT_STATES:\n        states_converted_ptrs = states_converted_ptr + offs_m * stride_out_dim\n\n    dstates = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    tl.store(dstates_ptrs, dstates, mask=offs_m < dim)\n    if not REVERSE:\n        dstates_ptrs -= stride_dstates_chunk\n    else:\n        dstates_ptrs += stride_dstates_chunk\n    for c in range(nchunks - 1):\n        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)\n        scale = tl.exp(dA_cs)\n        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        if CONVERT_STATES:\n            tl.store(states_converted_ptrs, out, mask=offs_m < dim)\n        ddA = tl.sum(out * dstates) * scale\n        tl.store(ddA_cs_ptr, ddA)\n        dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        dstates = scale * dstates + dout\n        tl.store(dstates_ptrs, dstates, mask=offs_m < dim)\n        if not REVERSE:\n            dout_ptrs -= stride_dout_chunk\n            dstates_ptrs -= stride_dstates_chunk\n            dA_cs_ptr -= stride_dA_cs_chunk\n            ddA_cs_ptr -= stride_ddA_cs_chunk\n            out_ptrs -= stride_out_chunk\n            if CONVERT_STATES:\n                states_converted_ptrs -= stride_out_chunk\n        else:\n            dout_ptrs += stride_dout_chunk\n            dstates_ptrs += stride_dstates_chunk\n            dA_cs_ptr += stride_dA_cs_chunk\n            ddA_cs_ptr += stride_ddA_cs_chunk\n            out_ptrs += stride_out_chunk\n            if CONVERT_STATES:\n                states_converted_ptrs += stride_out_chunk\n    if CONVERT_STATES:\n        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        tl.store(states_converted_ptrs, out, mask=offs_m < dim)\n    tl.store(ddA_cs_ptr, 0.0)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 170, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 62, 6, 108, 275, 62, 6, 108, 276, 62, 6, 175, 62, 32, -1, 277, 194, 171, 243, 278, 194, 348, 175, 32, 279, 194, 171, 243, 278, 194, 349, 175, 32, 280, 194, 171, 243, 278, 194, 350, 175, 32, 250, 172, 277, 244, 267, 74, 279, 244, 269, 32, 249, 172, 277, 244, 264, 74, 279, 244, 266, 32, 251, 172, 277, 244, 271, 74, 279, 244, 273, 32, 248, 172, 277, 244, 260, 74, 279, 244, 262, 32, 247, 172, 277, 244, 256, 74, 279, 244, 258, 32, 182, 63, 276, 62, 32, 250, 172, 243, 253, 4, 348, 175, 244, 268, 32, 249, 172, 243, 253, 4, 348, 175, 244, 265, 32, 251, 172, 243, 253, 4, 348, 175, 244, 272, 32, 248, 172, 243, 253, 4, 348, 175, 244, 261, 32, 247, 172, 243, 253, 4, 348, 175, 244, 257, 32, 185, 32, 182, 274, 62, 32, 252, 172, 277, 244, 260, 74, 279, 244, 262, 32, 182, 63, 276, 62, 32, 252, 172, 243, 253, 4, 348, 175, 244, 261, 32, 185, 32, 185, 32, 281, 194, 280, 244, 275, 74, 76, 243, 350, 108, 275, 175, 32, 282, 194, 250, 74, 281, 244, 270, 32, 283, 194, 248, 74, 281, 244, 263, 32, 284, 194, 247, 74, 281, 244, 259, 32, 182, 274, 62, 32, 285, 194, 252, 74, 281, 244, 263, 32, 185, 32, 286, 194, 176, 243, 243, 275, 108, 175, 108, 91, 194, 145, 175, 32, 10, 243, 282, 108, 286, 108, 287, 194, 281, 1, 170, 175, 32, 182, 63, 276, 62, 32, 282, 2, 268, 32, 185, 32, 30, 62, 32, 282, 172, 268, 32, 56, 32, 135, 288, 157, 5, 243, 253, 4, 348, 175, 62, 32, 289, 194, 55, 243, 249, 175, 82, 290, 243, 145, 175, 32, 291, 194, 107, 243, 289, 175, 32, 14, 194, 55, 243, 283, 108, 287, 194, 281, 1, 170, 108, 292, 194, 350, 175, 82, 290, 243, 145, 175, 32, 182, 274, 62, 32, 10, 243, 285, 108, 14, 108, 287, 194, 281, 1, 170, 175, 32, 185, 32, 293, 194, 221, 243, 14, 244, 286, 175, 244, 291, 32, 10, 243, 251, 108, 293, 175, 32, 294, 194, 55, 243, 284, 108, 287, 194, 281, 1, 170, 108, 292, 194, 350, 175, 82, 290, 243, 145, 175, 32, 286, 194, 291, 244, 286, 74, 294, 32, 10, 243, 282, 108, 286, 108, 287, 194, 281, 1, 170, 175, 32, 182, 63, 276, 62, 32, 284, 2, 257, 32, 282, 2, 268, 32, 249, 2, 265, 32, 251, 2, 272, 32, 283, 2, 261, 32, 182, 274, 62, 32, 285, 2, 261, 32, 185, 32, 185, 32, 30, 62, 32, 284, 172, 257, 32, 282, 172, 268, 32, 249, 172, 265, 32, 251, 172, 272, 32, 283, 172, 261, 32, 182, 274, 62, 32, 285, 172, 261, 32, 185, 32, 56, 32, 78, 32, 182, 274, 62, 32, 14, 194, 55, 243, 283, 108, 287, 194, 281, 1, 170, 108, 292, 194, 350, 175, 82, 290, 243, 145, 175, 32, 10, 243, 285, 108, 14, 108, 287, 194, 281, 1, 170, 175, 32, 185, 32, 10, 243, 251, 108, 350, 175, 32, 3, 32]}, {"code": "def _swiglu_fwd_kernel(\n    X,\n    Y,\n    OUT,\n    stride_x_row,\n    stride_y_row,\n    stride_out_row,\n    ncols,\n    BLOCK_N: tl.constexpr,\n):\n\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    OUT += row * stride_out_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    out = x * tl.sigmoid(x) * y\n    tl.store(OUT + cols, out, mask=cols < ncols)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 63, 6, 175, 63, 32, -1, 255, 194, 171, 243, 348, 175, 32, 256, 194, 171, 243, 349, 175, 244, 254, 32, 247, 172, 255, 244, 250, 32, 248, 172, 255, 244, 251, 32, 249, 172, 255, 244, 252, 32, 257, 194, 256, 74, 76, 243, 348, 108, 254, 175, 32, 258, 194, 57, 243, 247, 74, 257, 108, 259, 194, 257, 1, 253, 108, 260, 194, 348, 175, 82, 261, 243, 145, 175, 32, 262, 194, 57, 243, 248, 74, 257, 108, 259, 194, 257, 1, 253, 108, 260, 194, 348, 175, 82, 261, 243, 145, 175, 32, 14, 194, 258, 244, 206, 243, 258, 175, 244, 262, 32, 10, 243, 249, 74, 257, 108, 14, 108, 259, 194, 257, 1, 253, 175, 32, 3, 32]}, {"code": "def _swiglu_bwd_kernel(\n    X,\n    Y,\n    DOUT,\n    OUT,\n    DX,\n    DY,\n    stride_x_row,\n    stride_y_row,\n    stride_dout_row,\n    stride_out_row,\n    stride_dx_row,\n    stride_dy_row,\n    ncols,\n    BLOCK_N: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n):\n\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    DOUT += row * stride_dout_row\n    if RECOMPUTE_OUTPUT:\n        OUT += row * stride_out_row\n    DX += row * stride_dx_row\n    DY += row * stride_dy_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    dout = tl.load(DOUT + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    x_sigmoid = tl.sigmoid(x)\n    dx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout\n    dy = x * x_sigmoid * dout\n    tl.store(DX + cols, dx, mask=cols < ncols)\n    tl.store(DY + cols, dy, mask=cols < ncols)\n    if RECOMPUTE_OUTPUT:\n        out = x * x_sigmoid * y\n        tl.store(OUT + cols, out, mask=cols < ncols)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 62, 6, 108, 261, 62, 6, 175, 62, 32, -1, 262, 194, 171, 243, 348, 175, 32, 263, 194, 171, 243, 349, 175, 244, 260, 32, 247, 172, 262, 244, 253, 32, 248, 172, 262, 244, 254, 32, 249, 172, 262, 244, 255, 32, 182, 261, 62, 32, 250, 172, 262, 244, 256, 32, 185, 32, 251, 172, 262, 244, 257, 32, 252, 172, 262, 244, 258, 32, 264, 194, 263, 74, 76, 243, 348, 108, 260, 175, 32, 265, 194, 55, 243, 247, 74, 264, 108, 266, 194, 264, 1, 259, 108, 267, 194, 348, 175, 82, 268, 243, 145, 175, 32, 269, 194, 55, 243, 248, 74, 264, 108, 266, 194, 264, 1, 259, 108, 267, 194, 348, 175, 82, 268, 243, 145, 175, 32, 270, 194, 55, 243, 249, 74, 264, 108, 266, 194, 264, 1, 259, 108, 267, 194, 348, 175, 82, 268, 243, 145, 175, 32, 271, 194, 206, 243, 265, 175, 32, 272, 194, 271, 244, 243, 349, 74, 265, 244, 243, 349, 4, 271, 175, 175, 244, 269, 244, 270, 32, 273, 194, 265, 244, 271, 244, 270, 32, 10, 243, 251, 74, 264, 108, 272, 108, 266, 194, 264, 1, 259, 175, 32, 10, 243, 252, 74, 264, 108, 273, 108, 266, 194, 264, 1, 259, 175, 32, 182, 261, 62, 32, 14, 194, 265, 244, 271, 244, 269, 32, 10, 243, 250, 74, 264, 108, 14, 108, 266, 194, 264, 1, 259, 175, 32, 185, 32, 3, 32]}, {"code": "def _layer_norm_fwd_1pass_kernel(\n    X,\n    Y,\n    W,\n    B,\n    RESIDUAL,\n    X1,\n    W1,\n    B1,\n    Y1,\n    RESIDUAL_OUT,\n    ROWSCALE,\n    SEEDS,\n    DROPOUT_MASK,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_y_row,\n    stride_res_row,\n    stride_res_out_row,\n    stride_x1_row,\n    stride_y1_row,\n    M,\n    N,\n    eps,\n    dropout_p,\n    IS_RMS_NORM: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    STORE_RESIDUAL_OUT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_DROPOUT: tl.constexpr,\n    STORE_DROPOUT_MASK: tl.constexpr,\n    HAS_ROWSCALE: tl.constexpr,\n    HAS_X1: tl.constexpr,\n    HAS_W1: tl.constexpr,\n    HAS_B1: tl.constexpr,\n):\n\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    if HAS_X1:\n        X1 += row * stride_x1_row\n    if HAS_W1:\n        Y1 += row * stride_y1_row\n\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_ROWSCALE:\n        rowscale = tl.load(ROWSCALE + row).to(tl.float32)\n        x *= rowscale\n    if HAS_DROPOUT:\n\n        keep_mask = (\n            tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n        )\n        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)\n        if STORE_DROPOUT_MASK:\n            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)\n    if HAS_X1:\n        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)\n        if HAS_ROWSCALE:\n            rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)\n            x1 *= rowscale\n        if HAS_DROPOUT:\n\n            keep_mask = (\n                tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)\n                > dropout_p\n            )\n            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)\n            if STORE_DROPOUT_MASK:\n                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask, mask=cols < N)\n        x += x1\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n\n    tl.store(Y + cols, y, mask=mask)\n    if HAS_W1:\n        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)\n        if HAS_B1:\n            b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)\n        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1\n        tl.store(Y1 + cols, y1, mask=mask)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 63, 6, 108, 273, 63, 6, 108, 274, 63, 6, 108, 275, 63, 6, 108, 276, 63, 6, 108, 277, 63, 6, 108, 278, 63, 6, 108, 279, 63, 6, 108, 280, 63, 6, 108, 281, 63, 6, 108, 282, 63, 6, 175, 63, 32, -1, 283, 194, 171, 243, 348, 175, 32, 247, 172, 283, 244, 262, 32, 248, 172, 283, 244, 263, 32, 182, 274, 63, 32, 251, 172, 283, 244, 264, 32, 185, 32, 182, 275, 63, 32, 256, 172, 283, 244, 265, 32, 185, 32, 182, 280, 63, 32, 252, 172, 283, 244, 266, 32, 185, 32, 182, 281, 63, 32, 255, 172, 283, 244, 267, 32, 185, 32, 284, 194, 76, 243, 348, 108, 273, 175, 32, 285, 194, 57, 243, 247, 74, 284, 108, 286, 194, 284, 1, 269, 108, 287, 194, 348, 175, 82, 288, 243, 145, 175, 32, 182, 279, 63, 32, 289, 194, 57, 243, 257, 74, 283, 175, 82, 288, 243, 145, 175, 32, 285, 24, 289, 32, 185, 32, 182, 277, 63, 32, 290, 194, 69, 243, 57, 243, 258, 74, 283, 175, 82, 288, 243, 165, 175, 108, 284, 108, 291, 194, 349, 175, 124, 271, 32, 285, 194, 205, 243, 290, 108, 285, 42, 243, 350, 4, 271, 175, 108, 348, 175, 32, 182, 278, 63, 32, 10, 243, 259, 74, 283, 244, 269, 74, 284, 108, 290, 108, 286, 194, 284, 1, 269, 175, 32, 185, 32, 185, 32, 182, 280, 63, 32, 292, 194, 57, 243, 252, 74, 284, 108, 286, 194, 284, 1, 269, 108, 287, 194, 348, 175, 82, 288, 243, 145, 175, 32, 182, 279, 63, 32, 289, 194, 57, 243, 257, 74, 268, 74, 283, 175, 82, 288, 243, 145, 175, 32, 292, 24, 289, 32, 185, 32, 182, 277, 63, 32, 290, 194, 69, 243, 57, 243, 258, 74, 268, 74, 283, 175, 82, 288, 243, 165, 175, 108, 284, 108, 291, 194, 349, 175, 124, 271, 32, 292, 194, 205, 243, 290, 108, 292, 42, 243, 350, 4, 271, 175, 108, 348, 175, 32, 182, 278, 63, 32, 10, 243, 259, 74, 243, 268, 74, 283, 175, 244, 269, 74, 284, 108, 290, 108, 286, 194, 284, 1, 269, 175, 32, 185, 32, 185, 32, 285, 172, 292, 32, 185, 32, 182, 274, 63, 32, 293, 194, 57, 243, 251, 74, 284, 108, 286, 194, 284, 1, 269, 108, 287, 194, 348, 175, 82, 288, 243, 145, 175, 32, 285, 172, 293, 32, 185, 32, 182, 275, 63, 32, 10, 243, 256, 74, 284, 108, 285, 108, 286, 194, 284, 1, 269, 175, 32, 185, 32, 182, 64, 272, 63, 32, 294, 194, 221, 243, 285, 108, 295, 194, 348, 175, 42, 269, 32, 10, 243, 260, 74, 283, 108, 294, 175, 32, 296, 194, 205, 243, 284, 1, 269, 108, 285, 4, 294, 108, 348, 175, 32, 297, 194, 221, 243, 296, 244, 296, 108, 295, 194, 348, 175, 42, 269, 32, 185, 32, 30, 63, 32, 296, 194, 205, 243, 284, 1, 269, 108, 285, 108, 348, 175, 32, 297, 194, 221, 243, 296, 244, 296, 108, 295, 194, 348, 175, 42, 269, 32, 56, 32, 298, 194, 350, 42, 130, 243, 297, 74, 270, 175, 32, 10, 243, 261, 74, 283, 108, 298, 175, 32, 286, 194, 284, 1, 269, 32, 299, 194, 57, 243, 249, 74, 284, 108, 286, 194, 286, 175, 82, 288, 243, 145, 175, 32, 182, 276, 63, 32, 300, 194, 57, 243, 250, 74, 284, 108, 286, 194, 286, 175, 82, 288, 243, 145, 175, 32, 185, 32, 301, 194, 243, 285, 4, 294, 175, 244, 298, 182, 64, 272, 30, 285, 244, 298, 32, 302, 194, 301, 244, 299, 74, 300, 182, 276, 30, 301, 244, 299, 32, 10, 243, 248, 74, 284, 108, 302, 108, 286, 194, 286, 175, 32, 182, 281, 63, 32, 303, 194, 57, 243, 253, 74, 284, 108, 286, 194, 286, 175, 82, 288, 243, 145, 175, 32, 182, 282, 63, 32, 304, 194, 57, 243, 254, 74, 284, 108, 286, 194, 286, 175, 82, 288, 243, 145, 175, 32, 185, 32, 305, 194, 301, 244, 303, 74, 304, 182, 282, 30, 301, 244, 303, 32, 10, 243, 255, 74, 284, 108, 305, 108, 286, 194, 286, 175, 32, 185, 32, 3, 32]}, {"code": "def _layer_norm_bwd_kernel(\n    X,\n    W,\n    B,\n    Y,\n    DY,\n    DX,\n    DW,\n    DB,\n    DRESIDUAL,\n    W1,\n    DY1,\n    DX1,\n    DW1,\n    DB1,\n    DRESIDUAL_IN,\n    ROWSCALE,\n    SEEDS,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_y_row,\n    stride_dy_row,\n    stride_dx_row,\n    stride_dres_row,\n    stride_dy1_row,\n    stride_dx1_row,\n    stride_dres_in_row,\n    M,\n    N,\n    eps,\n    dropout_p,\n    rows_per_program,\n    IS_RMS_NORM: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HAS_DRESIDUAL: tl.constexpr,\n    STORE_DRESIDUAL: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_DROPOUT: tl.constexpr,\n    HAS_ROWSCALE: tl.constexpr,\n    HAS_DY1: tl.constexpr,\n    HAS_DX1: tl.constexpr,\n    HAS_B1: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n):\n\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if HAS_DY1:\n        DY1 += row_start * stride_dy1_row\n    if HAS_DX1:\n        DX1 += row_start * stride_dx1_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n    if HAS_DY1:\n        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_DY1:\n        dw1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\n        if HAS_B1:\n            db1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if HAS_DY1:\n            dy1 = tl.load(DY1 + cols, mask=mask, other=0).to(tl.float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if HAS_DY1:\n            wdy += w1 * dy1\n            dw1 += dy1 * xhat\n            if HAS_B1:\n                db1 += dy1\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)\n            dx += dres\n\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        if HAS_DX1:\n            if HAS_DROPOUT:\n                keep_mask = (\n                    tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)\n                    > dropout_p\n                )\n                dx1 = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n            else:\n                dx1 = dx\n            tl.store(DX1 + cols, dx1, mask=mask)\n        if HAS_DROPOUT:\n            keep_mask = (\n                tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7)\n                > dropout_p\n            )\n            dx = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n        if HAS_ROWSCALE:\n            rowscale = tl.load(ROWSCALE + row).to(tl.float32)\n            dx *= rowscale\n        tl.store(DX + cols, dx, mask=mask)\n\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n        if HAS_DY1:\n            DY1 += stride_dy1_row\n        if HAS_DX1:\n            DX1 += stride_dx1_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n    if HAS_DY1:\n        tl.store(DW1 + row_block_id * N + cols, dw1, mask=mask)\n        if HAS_B1:\n            tl.store(DB1 + row_block_id * N + cols, db1, mask=mask)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 62, 6, 108, 280, 62, 6, 108, 281, 62, 6, 108, 282, 62, 6, 108, 283, 62, 6, 108, 284, 62, 6, 108, 285, 62, 6, 108, 286, 62, 6, 108, 287, 62, 6, 108, 288, 62, 6, 108, 289, 62, 6, 175, 62, 32, -1, 290, 194, 171, 243, 348, 175, 32, 291, 194, 290, 244, 278, 32, 292, 194, 76, 243, 348, 108, 280, 175, 32, 293, 194, 292, 1, 275, 32, 247, 172, 291, 244, 266, 32, 182, 281, 62, 32, 255, 172, 291, 244, 270, 32, 185, 32, 182, 282, 62, 32, 261, 172, 291, 244, 273, 32, 185, 32, 251, 172, 291, 244, 268, 32, 252, 172, 291, 244, 269, 32, 182, 286, 62, 32, 257, 172, 291, 244, 271, 32, 185, 32, 182, 287, 62, 32, 258, 172, 291, 244, 272, 32, 185, 32, 182, 289, 62, 32, 250, 172, 291, 244, 267, 32, 185, 32, 294, 194, 55, 243, 248, 74, 292, 108, 293, 194, 293, 175, 82, 295, 243, 145, 175, 32, 182, 289, 102, 283, 62, 32, 296, 194, 55, 243, 249, 74, 292, 108, 293, 194, 293, 108, 297, 194, 348, 175, 82, 295, 243, 145, 175, 32, 185, 32, 182, 286, 62, 32, 298, 194, 55, 243, 256, 74, 292, 108, 293, 194, 293, 175, 82, 295, 243, 145, 175, 32, 185, 32, 299, 194, 176, 243, 243, 280, 108, 175, 108, 91, 194, 145, 175, 32, 182, 283, 62, 32, 300, 194, 176, 243, 243, 280, 108, 175, 108, 91, 194, 145, 175, 32, 185, 32, 182, 286, 62, 32, 301, 194, 176, 243, 243, 280, 108, 175, 108, 91, 194, 145, 175, 32, 182, 288, 62, 32, 302, 194, 176, 243, 243, 280, 108, 175, 108, 91, 194, 145, 175, 32, 185, 32, 185, 32, 303, 194, 39, 243, 243, 290, 74, 349, 175, 244, 278, 108, 274, 175, 32, 135, 304, 157, 5, 243, 291, 108, 303, 175, 62, 32, 305, 194, 55, 243, 247, 74, 292, 108, 293, 194, 293, 108, 297, 194, 348, 175, 82, 295, 243, 145, 175, 32, 306, 194, 55, 243, 251, 74, 292, 108, 293, 194, 293, 108, 297, 194, 348, 175, 82, 295, 243, 145, 175, 32, 182, 286, 62, 32, 307, 194, 55, 243, 257, 74, 292, 108, 293, 194, 293, 108, 297, 194, 348, 175, 82, 295, 243, 145, 175, 32, 185, 32, 182, 63, 279, 62, 32, 308, 194, 55, 243, 264, 74, 304, 175, 32, 185, 32, 309, 194, 55, 243, 265, 74, 304, 175, 32, 310, 194, 243, 305, 4, 308, 175, 244, 309, 182, 63, 279, 30, 305, 244, 309, 32, 310, 194, 205, 243, 293, 108, 310, 108, 348, 175, 32, 182, 289, 62, 32, 311, 194, 310, 244, 294, 74, 296, 182, 283, 30, 310, 244, 294, 32, 10, 243, 250, 74, 292, 108, 311, 108, 293, 194, 293, 175, 32, 185, 32, 312, 194, 294, 244, 306, 32, 299, 172, 306, 244, 310, 32, 182, 283, 62, 32, 300, 172, 306, 32, 185, 32, 182, 286, 62, 32, 312, 172, 298, 244, 307, 32, 301, 172, 307, 244, 310, 32, 182, 288, 62, 32, 302, 172, 307, 32, 185, 32, 185, 32, 182, 63, 279, 62, 32, 313, 194, 221, 243, 310, 244, 312, 108, 314, 194, 348, 175, 42, 275, 32, 315, 194, 221, 243, 312, 108, 314, 194, 348, 175, 42, 275, 32, 316, 194, 243, 312, 4, 243, 310, 244, 313, 74, 315, 175, 175, 244, 309, 32, 185, 32, 30, 62, 32, 313, 194, 221, 243, 310, 244, 312, 108, 314, 194, 348, 175, 42, 275, 32, 316, 194, 243, 312, 4, 310, 244, 313, 175, 244, 309, 32, 56, 32, 182, 281, 62, 32, 317, 194, 55, 243, 255, 74, 292, 108, 293, 194, 293, 108, 297, 194, 348, 175, 82, 295, 243, 145, 175, 32, 316, 172, 317, 32, 185, 32, 182, 282, 62, 32, 10, 243, 261, 74, 292, 108, 316, 108, 293, 194, 293, 175, 32, 185, 32, 182, 287, 62, 32, 182, 284, 62, 32, 318, 194, 68, 243, 55, 243, 263, 74, 274, 74, 304, 175, 82, 295, 243, 165, 175, 108, 292, 108, 319, 194, 350, 175, 124, 277, 32, 320, 194, 205, 243, 318, 108, 316, 42, 243, 349, 4, 277, 175, 108, 348, 175, 32, 185, 32, 30, 62, 32, 320, 194, 316, 32, 56, 32, 10, 243, 258, 74, 292, 108, 320, 108, 293, 194, 293, 175, 32, 185, 32, 182, 284, 62, 32, 318, 194, 68, 243, 55, 243, 263, 74, 304, 175, 82, 295, 243, 165, 175, 108, 292, 108, 319, 194, 350, 175, 124, 277, 32, 316, 194, 205, 243, 318, 108, 316, 42, 243, 349, 4, 277, 175, 108, 348, 175, 32, 185, 32, 182, 285, 62, 32, 321, 194, 55, 243, 262, 74, 304, 175, 82, 295, 243, 145, 175, 32, 316, 24, 321, 32, 185, 32, 10, 243, 252, 74, 292, 108, 316, 108, 293, 194, 293, 175, 32, 247, 172, 266, 32, 182, 281, 62, 32, 255, 172, 270, 32, 185, 32, 182, 282, 62, 32, 261, 172, 273, 32, 185, 32, 182, 289, 62, 32, 250, 172, 267, 32, 185, 32, 251, 172, 268, 32, 252, 172, 269, 32, 182, 286, 62, 32, 257, 172, 271, 32, 185, 32, 182, 287, 62, 32, 258, 172, 272, 32, 185, 32, 78, 32, 10, 243, 253, 74, 290, 244, 275, 74, 292, 108, 299, 108, 293, 194, 293, 175, 32, 182, 283, 62, 32, 10, 243, 254, 74, 290, 244, 275, 74, 292, 108, 300, 108, 293, 194, 293, 175, 32, 185, 32, 182, 286, 62, 32, 10, 243, 259, 74, 290, 244, 275, 74, 292, 108, 301, 108, 293, 194, 293, 175, 32, 182, 288, 62, 32, 10, 243, 260, 74, 290, 244, 275, 74, 292, 108, 302, 108, 293, 194, 293, 175, 32, 185, 32, 185, 32, 3, 32]}, {"code": "def _layer_norm_fwd_1pass_kernel(\n    X,\n    Y,\n    W,\n    B,\n    Z,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_y_row,\n    stride_z_row,\n    M,\n    N,\n    eps,\n    BLOCK_N: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_Z: tl.constexpr,\n    NORM_BEFORE_GATE: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n):\n\n    row = tl.program_id(0)\n    group = tl.program_id(1)\n    X += row * stride_x_row + group * N\n    Y += row * stride_y_row + group * N\n    if HAS_Z:\n        Z += row * stride_z_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    if HAS_BIAS:\n        B += group * N\n\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_Z and not NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=cols < N).to(tl.float32)\n        x *= z * tl.sigmoid(z)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    if HAS_Z and NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=mask).to(tl.float32)\n        y *= z * tl.sigmoid(z)\n\n    tl.store(Y + cols, y, mask=mask)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 63, 6, 108, 261, 63, 6, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 175, 63, 32, -1, 265, 194, 171, 243, 348, 175, 32, 266, 194, 171, 243, 349, 175, 32, 247, 172, 265, 244, 254, 74, 266, 244, 258, 32, 248, 172, 265, 244, 255, 74, 266, 244, 258, 32, 182, 262, 63, 32, 251, 172, 265, 244, 256, 74, 266, 244, 258, 32, 185, 32, 182, 64, 264, 63, 32, 252, 172, 266, 244, 257, 32, 185, 32, 253, 172, 266, 244, 257, 32, 249, 172, 266, 244, 258, 32, 182, 261, 63, 32, 250, 172, 266, 244, 258, 32, 185, 32, 267, 194, 76, 243, 348, 108, 260, 175, 32, 268, 194, 57, 243, 247, 74, 267, 108, 269, 194, 267, 1, 258, 108, 270, 194, 348, 175, 82, 271, 243, 145, 175, 32, 182, 262, 102, 243, 64, 263, 175, 63, 32, 272, 194, 57, 243, 251, 74, 267, 108, 269, 194, 267, 1, 258, 175, 82, 271, 243, 145, 175, 32, 268, 24, 272, 244, 206, 243, 272, 175, 32, 185, 32, 182, 64, 264, 63, 32, 273, 194, 221, 243, 268, 108, 274, 194, 348, 175, 42, 258, 32, 10, 243, 252, 74, 265, 108, 273, 175, 32, 275, 194, 205, 243, 267, 1, 258, 108, 268, 4, 273, 108, 348, 175, 32, 276, 194, 221, 243, 275, 244, 275, 108, 274, 194, 348, 175, 42, 258, 32, 185, 32, 30, 63, 32, 275, 194, 205, 243, 267, 1, 258, 108, 268, 108, 348, 175, 32, 276, 194, 221, 243, 275, 244, 275, 108, 274, 194, 348, 175, 42, 258, 32, 56, 32, 277, 194, 349, 42, 130, 243, 276, 74, 259, 175, 32, 10, 243, 253, 74, 265, 108, 277, 175, 32, 269, 194, 267, 1, 258, 32, 278, 194, 57, 243, 249, 74, 267, 108, 269, 194, 269, 175, 82, 271, 243, 145, 175, 32, 182, 261, 63, 32, 279, 194, 57, 243, 250, 74, 267, 108, 269, 194, 269, 175, 82, 271, 243, 145, 175, 32, 185, 32, 280, 194, 243, 268, 4, 273, 175, 244, 277, 182, 64, 264, 30, 268, 244, 277, 32, 281, 194, 280, 244, 278, 74, 279, 182, 261, 30, 280, 244, 278, 32, 182, 262, 102, 263, 63, 32, 272, 194, 57, 243, 251, 74, 267, 108, 269, 194, 269, 175, 82, 271, 243, 145, 175, 32, 281, 24, 272, 244, 206, 243, 272, 175, 32, 185, 32, 10, 243, 248, 74, 267, 108, 281, 108, 269, 194, 269, 175, 32, 3, 32]}, {"code": "def _layer_norm_bwd_kernel(\n    X,\n    W,\n    B,\n    Z,\n    Y,\n    DY,\n    DX,\n    DW,\n    DB,\n    DZ,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_z_row,\n    stride_y_row,\n    stride_dy_row,\n    stride_dx_row,\n    stride_dz_row,\n    stride_dw_row,\n    stride_db_row,\n    M,\n    N,\n    eps,\n    rows_per_program,\n    NORM_BEFORE_GATE: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_Z: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n\n    row_block_id = tl.program_id(0)\n    group = tl.program_id(1)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row + group * N\n    if HAS_Z:\n        Z += row_start * stride_z_row + group * N\n        DZ += row_start * stride_dz_row + group * N\n    DY += row_start * stride_dy_row + group * N\n    DX += row_start * stride_dx_row + group * N\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:\n        B += group * N\n        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n            x_og = x\n            x = x_og * z * tl.sigmoid(z)\n        rstd = tl.load(Rstd + row)\n\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if HAS_Z and NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n            z_sigmoid = tl.sigmoid(z)\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            if RECOMPUTE_OUTPUT:\n                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)\n            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dy *= z * z_sigmoid\n        else:\n            if RECOMPUTE_OUTPUT:\n                y = xhat * w + b if HAS_BIAS else xhat * w\n                tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        if not IS_RMS_NORM:\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            dx = (wdy - xhat * c1) * rstd\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z_sigmoid = tl.sigmoid(z)\n            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dx *= z * z_sigmoid\n\n        tl.store(DX + cols, dx, mask=mask)\n\n        X += stride_x_row\n        if HAS_Z:\n            Z += stride_z_row\n            DZ += stride_dz_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db, mask=mask)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 62, 6, 108, 272, 62, 6, 108, 273, 62, 6, 108, 274, 62, 6, 108, 275, 62, 6, 108, 276, 62, 6, 175, 62, 32, -1, 277, 194, 171, 243, 348, 175, 32, 278, 194, 171, 243, 349, 175, 32, 279, 194, 277, 244, 270, 32, 280, 194, 76, 243, 348, 108, 276, 175, 32, 281, 194, 280, 1, 268, 32, 247, 172, 279, 244, 259, 74, 278, 244, 268, 32, 182, 274, 62, 32, 250, 172, 279, 244, 260, 74, 278, 244, 268, 32, 256, 172, 279, 244, 264, 74, 278, 244, 268, 32, 185, 32, 252, 172, 279, 244, 262, 74, 278, 244, 268, 32, 253, 172, 279, 244, 263, 74, 278, 244, 268, 32, 182, 275, 62, 32, 251, 172, 279, 244, 261, 74, 278, 244, 268, 32, 185, 32, 182, 63, 272, 62, 32, 257, 172, 278, 244, 267, 32, 185, 32, 258, 172, 278, 244, 267, 32, 248, 172, 278, 244, 268, 32, 282, 194, 55, 243, 248, 74, 280, 108, 281, 194, 281, 175, 82, 283, 243, 145, 175, 32, 182, 243, 275, 141, 274, 175, 102, 273, 62, 32, 249, 172, 278, 244, 268, 32, 284, 194, 55, 243, 249, 74, 280, 108, 281, 194, 281, 108, 285, 194, 348, 175, 82, 283, 243, 145, 175, 32, 185, 32, 286, 194, 176, 243, 243, 276, 108, 175, 108, 91, 194, 145, 175, 32, 182, 273, 62, 32, 287, 194, 176, 243, 243, 276, 108, 175, 108, 91, 194, 145, 175, 32, 185, 32, 288, 194, 39, 243, 243, 277, 74, 349, 175, 244, 270, 108, 267, 175, 32, 135, 289, 157, 5, 243, 279, 108, 288, 175, 62, 32, 290, 194, 55, 243, 247, 74, 280, 108, 281, 194, 281, 108, 285, 194, 348, 175, 82, 283, 243, 145, 175, 32, 291, 194, 55, 243, 252, 74, 280, 108, 281, 194, 281, 108, 285, 194, 348, 175, 82, 283, 243, 145, 175, 32, 182, 63, 272, 62, 32, 292, 194, 55, 243, 257, 74, 289, 175, 32, 185, 32, 182, 274, 102, 243, 63, 271, 175, 62, 32, 293, 194, 55, 243, 250, 74, 280, 108, 281, 194, 281, 108, 285, 194, 348, 175, 82, 283, 243, 145, 175, 32, 294, 194, 290, 32, 290, 194, 294, 244, 293, 244, 206, 243, 293, 175, 32, 185, 32, 295, 194, 55, 243, 258, 74, 289, 175, 32, 296, 194, 243, 290, 4, 292, 175, 244, 295, 182, 63, 272, 30, 290, 244, 295, 32, 296, 194, 205, 243, 281, 108, 296, 108, 348, 175, 32, 182, 274, 102, 271, 62, 32, 293, 194, 55, 243, 250, 74, 280, 108, 281, 194, 281, 108, 285, 194, 348, 175, 82, 283, 243, 145, 175, 32, 297, 194, 206, 243, 293, 175, 32, 298, 194, 296, 244, 282, 74, 284, 182, 273, 30, 296, 244, 282, 32, 182, 275, 62, 32, 10, 243, 251, 74, 280, 108, 298, 244, 293, 244, 297, 108, 281, 194, 281, 175, 32, 185, 32, 299, 194, 291, 244, 298, 244, 297, 244, 243, 349, 74, 293, 244, 243, 349, 4, 297, 175, 175, 32, 10, 243, 256, 74, 280, 108, 299, 108, 281, 194, 281, 175, 32, 291, 24, 293, 244, 297, 32, 65, 32, 37, 275, 62, 32, 298, 194, 296, 244, 282, 74, 284, 182, 273, 30, 296, 244, 282, 32, 10, 243, 251, 74, 280, 108, 298, 108, 281, 194, 281, 175, 32, 185, 32, 300, 194, 282, 244, 291, 32, 301, 194, 221, 243, 296, 244, 300, 108, 302, 194, 348, 175, 42, 268, 32, 182, 63, 272, 62, 32, 303, 194, 221, 243, 300, 108, 302, 194, 348, 175, 42, 268, 32, 304, 194, 243, 300, 4, 243, 296, 244, 301, 74, 303, 175, 175, 244, 295, 32, 185, 32, 30, 62, 32, 304, 194, 243, 300, 4, 296, 244, 301, 175, 244, 295, 32, 56, 32, 286, 172, 291, 244, 296, 32, 182, 273, 62, 32, 287, 172, 291, 32, 185, 32, 182, 274, 102, 243, 63, 271, 175, 62, 32, 297, 194, 206, 243, 293, 175, 32, 299, 194, 304, 244, 294, 244, 297, 244, 243, 349, 74, 293, 244, 243, 349, 4, 297, 175, 175, 32, 10, 243, 256, 74, 280, 108, 299, 108, 281, 194, 281, 175, 32, 304, 24, 293, 244, 297, 32, 185, 32, 10, 243, 253, 74, 280, 108, 304, 108, 281, 194, 281, 175, 32, 247, 172, 259, 32, 182, 274, 62, 32, 250, 172, 260, 32, 256, 172, 264, 32, 185, 32, 182, 275, 62, 32, 251, 172, 261, 32, 185, 32, 252, 172, 262, 32, 253, 172, 263, 32, 78, 32, 10, 243, 254, 74, 277, 244, 265, 74, 278, 244, 268, 74, 280, 108, 286, 108, 281, 194, 281, 175, 32, 182, 273, 62, 32, 10, 243, 255, 74, 277, 244, 266, 74, 278, 244, 268, 74, 280, 108, 287, 108, 281, 194, 281, 175, 32, 185, 32, 3, 32]}, {"code": "def _selective_scan_update_kernel(\n    state_ptr,\n    x_ptr,\n    dt_ptr,\n    dt_bias_ptr,\n    A_ptr,\n    B_ptr,\n    C_ptr,\n    D_ptr,\n    z_ptr,\n    out_ptr,\n    batch,\n    nheads,\n    dim,\n    dstate,\n    nheads_ngroups_ratio,\n    stride_state_batch,\n    stride_state_head,\n    stride_state_dim,\n    stride_state_dstate,\n    stride_x_batch,\n    stride_x_head,\n    stride_x_dim,\n    stride_dt_batch,\n    stride_dt_head,\n    stride_dt_dim,\n    stride_dt_bias_head,\n    stride_dt_bias_dim,\n    stride_A_head,\n    stride_A_dim,\n    stride_A_dstate,\n    stride_B_batch,\n    stride_B_group,\n    stride_B_dstate,\n    stride_C_batch,\n    stride_C_group,\n    stride_C_dstate,\n    stride_D_head,\n    stride_D_dim,\n    stride_z_batch,\n    stride_z_head,\n    stride_z_dim,\n    stride_out_batch,\n    stride_out_head,\n    stride_out_dim,\n    DT_SOFTPLUS: tl.constexpr,\n    TIE_HDIM: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    HAS_DT_BIAS: tl.constexpr,\n    HAS_D: tl.constexpr,\n    HAS_Z: tl.constexpr,\n    BLOCK_SIZE_DSTATE: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    state_ptr += pid_b * stride_state_batch + pid_h * stride_state_head\n    x_ptr += pid_b * stride_x_batch + pid_h * stride_x_head\n    dt_ptr += pid_b * stride_dt_batch + pid_h * stride_dt_head\n    if HAS_DT_BIAS:\n        dt_bias_ptr += pid_h * stride_dt_bias_head\n    A_ptr += pid_h * stride_A_head\n    B_ptr += pid_b * stride_B_batch + (pid_h // nheads_ngroups_ratio) * stride_B_group\n    C_ptr += pid_b * stride_C_batch + (pid_h // nheads_ngroups_ratio) * stride_C_group\n    if HAS_Z:\n        z_ptr += pid_b * stride_z_batch + pid_h * stride_z_head\n    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\n    state_ptrs = state_ptr + (\n        offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate\n    )\n    x_ptrs = x_ptr + offs_m * stride_x_dim\n    dt_ptrs = dt_ptr + offs_m * stride_dt_dim\n    if HAS_DT_BIAS:\n        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\n    if HAS_D:\n        D_ptr += pid_h * stride_D_head\n    A_ptrs = A_ptr + (\n        offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate\n    )\n    B_ptrs = B_ptr + offs_n * stride_B_dstate\n    C_ptrs = C_ptr + offs_n * stride_C_dstate\n    if HAS_D:\n        D_ptrs = D_ptr + offs_m * stride_D_dim\n    if HAS_Z:\n        z_ptrs = z_ptr + offs_m * stride_z_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n\n    state = tl.load(\n        state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0\n    )\n    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    if not TIE_HDIM:\n        dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        if HAS_DT_BIAS:\n            dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        if DT_SOFTPLUS:\n            dt = softplus(dt)\n        A = tl.load(\n            A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0\n        ).to(tl.float32)\n        dA = tl.exp(A * dt[:, None])\n    else:\n        dt = tl.load(dt_ptr).to(tl.float32)\n        if HAS_DT_BIAS:\n            dt += tl.load(dt_bias_ptr).to(tl.float32)\n        if DT_SOFTPLUS:\n            dt = softplus(dt)\n        A = tl.load(A_ptr).to(tl.float32)\n        dA = tl.exp(A * dt)\n\n    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)\n    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)\n    if HAS_D:\n        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    if HAS_Z:\n        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n\n    if not TIE_HDIM:\n        dB = B[None, :] * dt[:, None]\n    else:\n        dB = B * dt\n    state = state * dA + dB * x[:, None]\n    tl.store(\n        state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate)\n    )\n    out = tl.sum(state * C[None, :], axis=1)\n    if HAS_D:\n        out += x * D\n    if HAS_Z:\n        out *= z * tl.sigmoid(z)\n    tl.store(out_ptrs, out, mask=offs_m < dim)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 170, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 63, 6, 108, 291, 63, 6, 108, 292, 63, 6, 108, 293, 63, 6, 108, 294, 63, 6, 108, 295, 63, 6, 108, 296, 63, 6, 175, 63, 32, -1, 297, 194, 171, 243, 298, 194, 348, 175, 32, 299, 194, 171, 243, 298, 194, 349, 175, 32, 300, 194, 171, 243, 298, 194, 350, 175, 32, 247, 172, 299, 244, 261, 74, 300, 244, 262, 32, 248, 172, 299, 244, 265, 74, 300, 244, 266, 32, 249, 172, 299, 244, 268, 74, 300, 244, 269, 32, 182, 293, 63, 32, 250, 172, 300, 244, 271, 32, 185, 32, 251, 172, 300, 244, 273, 32, 252, 172, 299, 244, 276, 74, 300, 48, 260, 244, 277, 32, 253, 172, 299, 244, 279, 74, 300, 48, 260, 244, 280, 32, 182, 295, 63, 32, 255, 172, 299, 244, 284, 74, 300, 244, 285, 32, 185, 32, 256, 172, 299, 244, 287, 74, 300, 244, 288, 32, 301, 194, 297, 244, 292, 74, 76, 243, 348, 108, 292, 175, 32, 302, 194, 76, 243, 348, 108, 296, 175, 32, 303, 194, 247, 74, 243, 301, 230, 63, 108, 202, 27, 244, 263, 74, 302, 230, 202, 108, 63, 27, 244, 264, 175, 32, 304, 194, 248, 74, 301, 244, 267, 32, 305, 194, 249, 74, 301, 244, 270, 32, 182, 293, 63, 32, 306, 194, 250, 74, 301, 244, 272, 32, 185, 32, 182, 294, 63, 32, 254, 172, 300, 244, 282, 32, 185, 32, 307, 194, 251, 74, 243, 301, 230, 63, 108, 202, 27, 244, 274, 74, 302, 230, 202, 108, 63, 27, 244, 275, 175, 32, 308, 194, 252, 74, 302, 244, 278, 32, 309, 194, 253, 74, 302, 244, 281, 32, 182, 294, 63, 32, 310, 194, 254, 74, 301, 244, 283, 32, 185, 32, 182, 295, 63, 32, 311, 194, 255, 74, 301, 244, 286, 32, 185, 32, 312, 194, 256, 74, 301, 244, 289, 32, 313, 194, 57, 243, 303, 108, 314, 194, 243, 301, 230, 63, 108, 202, 27, 1, 170, 175, 173, 243, 302, 230, 202, 108, 63, 27, 1, 259, 175, 108, 315, 194, 348, 175, 32, 316, 194, 57, 243, 304, 108, 314, 194, 301, 1, 170, 108, 315, 194, 348, 175, 82, 317, 243, 145, 175, 32, 182, 64, 291, 63, 32, 318, 194, 57, 243, 305, 108, 314, 194, 301, 1, 170, 108, 315, 194, 348, 175, 82, 317, 243, 145, 175, 32, 182, 293, 63, 32, 318, 172, 57, 243, 306, 108, 314, 194, 301, 1, 170, 108, 315, 194, 348, 175, 82, 317, 243, 145, 175, 32, 185, 32, 182, 290, 63, 32, 318, 194, 319, 243, 318, 175, 32, 185, 32, 320, 194, 57, 243, 307, 108, 314, 194, 243, 301, 230, 63, 108, 202, 27, 1, 170, 175, 173, 243, 302, 230, 202, 108, 63, 27, 1, 259, 175, 108, 315, 194, 348, 175, 82, 317, 243, 145, 175, 32, 321, 194, 107, 243, 320, 244, 318, 230, 63, 108, 202, 27, 175, 32, 185, 32, 30, 63, 32, 318, 194, 57, 243, 249, 175, 82, 317, 243, 145, 175, 32, 182, 293, 63, 32, 318, 172, 57, 243, 250, 175, 82, 317, 243, 145, 175, 32, 185, 32, 182, 290, 63, 32, 318, 194, 319, 243, 318, 175, 32, 185, 32, 320, 194, 57, 243, 251, 175, 82, 317, 243, 145, 175, 32, 321, 194, 107, 243, 320, 244, 318, 175, 32, 56, 32, 322, 194, 57, 243, 308, 108, 314, 194, 302, 1, 259, 108, 315, 194, 348, 175, 82, 317, 243, 145, 175, 32, 323, 194, 57, 243, 309, 108, 314, 194, 302, 1, 259, 108, 315, 194, 348, 175, 82, 317, 243, 145, 175, 32, 182, 294, 63, 32, 324, 194, 57, 243, 310, 108, 314, 194, 301, 1, 170, 108, 315, 194, 348, 175, 82, 317, 243, 145, 175, 32, 185, 32, 182, 295, 63, 32, 325, 194, 57, 243, 311, 108, 314, 194, 301, 1, 170, 108, 315, 194, 348, 175, 82, 317, 243, 145, 175, 32, 185, 32, 182, 64, 291, 63, 32, 326, 194, 322, 230, 202, 108, 63, 27, 244, 318, 230, 63, 108, 202, 27, 32, 185, 32, 30, 63, 32, 326, 194, 322, 244, 318, 32, 56, 32, 313, 194, 313, 244, 321, 74, 326, 244, 316, 230, 63, 108, 202, 27, 32, 10, 243, 303, 108, 313, 108, 314, 194, 243, 301, 230, 63, 108, 202, 27, 1, 170, 175, 173, 243, 302, 230, 202, 108, 63, 27, 1, 259, 175, 175, 32, 14, 194, 221, 243, 313, 244, 323, 230, 202, 108, 63, 27, 108, 298, 194, 349, 175, 32, 182, 294, 63, 32, 14, 172, 316, 244, 324, 32, 185, 32, 182, 295, 63, 32, 14, 24, 325, 244, 206, 243, 325, 175, 32, 185, 32, 10, 243, 312, 108, 14, 108, 314, 194, 301, 1, 170, 175, 32, 3, 32]}, {"code": "def _bmm_chunk_fwd_kernel(\n    a_ptr,\n    b_ptr,\n    out_ptr,\n    seq_idx_ptr,\n    seqlen,\n    chunk_size,\n    K,\n    ngroups,\n    stride_a_batch,\n    stride_a_seqlen,\n    stride_a_head,\n    stride_ak,\n    stride_b_batch,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_bk,\n    stride_out_batch,\n    stride_out_chunk,\n    stride_out_head,\n    stride_outm,\n    stride_outn,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    IS_CAUSAL: tl.constexpr,\n    dot_dtype: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    if IS_CAUSAL:\n        if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n            return\n    a_ptr += (\n        pid_b * stride_a_batch\n        + pid_c * chunk_size * stride_a_seqlen\n        + pid_h * stride_a_head\n    )\n    b_ptr += (\n        pid_b * stride_b_batch\n        + pid_c * chunk_size * stride_b_seqlen\n        + pid_h * stride_b_head\n    )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_b_seqlen)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(\n            a_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit)\n            & (offs_k[None, :] < K - k * BLOCK_SIZE_K),\n            other=0.0,\n        ).to(dot_dtype)\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K)\n            & (offs_n[None, :] < chunk_size_limit),\n            other=0.0,\n        ).to(dot_dtype)\n        acc += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_SEQ_IDX:\n        chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n        seq_idx_m = tl.load(\n            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit,\n            other=-1,\n        )\n        seq_idx_n = tl.load(\n            seq_idx_ptr + offs_n * stride_seq_idx_seqlen,\n            mask=offs_n < chunk_size_limit,\n            other=-2,\n        )\n        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)\n    out = acc.to(out_ptr.dtype.element_ty)\n\n    out_ptr += (\n        pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h * stride_out_head\n    )\n    out_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] * stride_outn)\n    tl.store(\n        out_ptrs,\n        out,\n        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 62, 6, 108, 271, 62, 6, 108, 272, 62, 6, 108, 273, 62, 6, 108, 274, 62, 6, 108, 275, 62, 6, 175, 62, 32, -1, 276, 194, 171, 243, 277, 194, 348, 175, 32, 278, 194, 171, 243, 277, 194, 349, 175, 32, 279, 194, 278, 48, 254, 32, 280, 194, 278, 4, 279, 244, 254, 32, 281, 194, 64, 243, 252, 108, 274, 175, 32, 282, 194, 171, 243, 277, 194, 350, 175, 48, 281, 32, 283, 194, 171, 243, 277, 194, 350, 175, 226, 281, 32, 182, 270, 62, 32, 182, 283, 244, 274, 144, 243, 282, 74, 348, 175, 244, 273, 62, 32, 231, 32, 185, 32, 185, 32, 247, 172, 276, 244, 255, 74, 279, 244, 252, 244, 256, 74, 280, 244, 257, 32, 248, 172, 276, 244, 259, 74, 279, 244, 252, 244, 260, 74, 280, 244, 261, 32, 182, 272, 62, 32, 250, 172, 276, 244, 268, 74, 279, 244, 252, 244, 269, 32, 185, 32, 284, 194, 282, 244, 273, 74, 76, 243, 350, 108, 273, 175, 32, 285, 194, 283, 244, 274, 74, 76, 243, 350, 108, 274, 175, 32, 286, 194, 76, 243, 350, 108, 275, 175, 32, 287, 194, 247, 74, 243, 284, 230, 62, 108, 202, 27, 244, 256, 74, 286, 230, 202, 108, 62, 27, 244, 258, 175, 32, 288, 194, 248, 74, 243, 286, 230, 62, 108, 202, 27, 244, 262, 74, 285, 230, 202, 108, 62, 27, 244, 260, 175, 32, 289, 194, 39, 243, 252, 108, 251, 4, 279, 244, 252, 175, 32, 290, 194, 176, 243, 243, 273, 108, 274, 175, 108, 91, 194, 145, 175, 32, 135, 291, 157, 5, 243, 350, 108, 64, 243, 253, 108, 275, 175, 175, 62, 32, 292, 194, 55, 243, 287, 108, 293, 194, 243, 284, 230, 62, 108, 202, 27, 1, 289, 175, 173, 243, 286, 230, 202, 108, 62, 27, 1, 253, 4, 291, 244, 275, 175, 108, 294, 194, 350, 175, 82, 295, 243, 271, 175, 32, 296, 194, 55, 243, 288, 108, 293, 194, 243, 286, 230, 62, 108, 202, 27, 1, 253, 4, 291, 244, 275, 175, 173, 243, 285, 230, 202, 108, 62, 27, 1, 289, 175, 108, 294, 194, 350, 175, 82, 295, 243, 271, 175, 32, 290, 172, 15, 243, 292, 108, 296, 175, 32, 287, 172, 275, 244, 258, 32, 288, 172, 275, 244, 262, 32, 78, 32, 284, 194, 282, 244, 273, 74, 76, 243, 350, 108, 273, 175, 32, 285, 194, 283, 244, 274, 74, 76, 243, 350, 108, 274, 175, 32, 182, 272, 62, 32, 289, 194, 39, 243, 252, 108, 251, 4, 279, 244, 252, 175, 32, 297, 194, 55, 243, 250, 74, 284, 244, 269, 108, 293, 194, 284, 1, 289, 108, 294, 194, 4, 348, 175, 32, 298, 194, 55, 243, 250, 74, 285, 244, 269, 108, 293, 194, 285, 1, 289, 108, 294, 194, 4, 349, 175, 32, 290, 194, 205, 243, 297, 230, 62, 108, 202, 27, 77, 298, 230, 202, 108, 62, 27, 108, 290, 108, 350, 175, 32, 185, 32, 14, 194, 290, 82, 295, 243, 249, 82, 91, 82, 114, 175, 32, 249, 172, 276, 244, 263, 74, 279, 244, 264, 74, 280, 244, 265, 32, 299, 194, 249, 74, 243, 266, 244, 284, 230, 62, 108, 202, 27, 74, 285, 230, 202, 108, 62, 27, 244, 267, 175, 32, 10, 243, 299, 108, 14, 108, 293, 194, 243, 284, 230, 62, 108, 202, 27, 1, 252, 175, 173, 243, 285, 230, 202, 108, 62, 27, 1, 252, 175, 175, 32, 3, 32]}, {"code": "def _bmm_chunk_bwd_kernel(\n    a_ptr,\n    dout_ptr,\n    db_ptr,\n    res_ptr,\n    seqlen,\n    chunk_size,\n    K,\n    ngroups,\n    stride_a_batch,\n    stride_a_seqlen,\n    stride_a_head,\n    stride_ak,\n    stride_dout_batch,\n    stride_dout_chunk,\n    stride_dout_head,\n    stride_dout_csize_m,\n    stride_dout_csize_n,\n    stride_db_batch,\n    stride_db_seqlen,\n    stride_db_head,\n    stride_db_k,\n    stride_res_batch,\n    stride_res_seqlen,\n    stride_res_head,\n    stride_res_k,\n    dot_dtype: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_CS: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(K, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n\n    a_ptr += (\n        pid_b * stride_a_batch\n        + pid_c * chunk_size * stride_a_seqlen\n        + pid_h * stride_a_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch + pid_c * stride_dout_chunk + pid_h * stride_dout_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_cs = tl.arange(0, BLOCK_SIZE_CS)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_csize_n + offs_cs[None, :] * stride_dout_csize_m\n    )\n    a_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] * stride_ak)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):\n        dout = tl.load(\n            dout_ptrs,\n            mask=(offs_m[:, None] < chunk_size)\n            & (offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS),\n            other=0.0,\n        ).to(dot_dtype)\n        a = tl.load(\n            a_ptrs,\n            mask=(offs_cs[:, None] < chunk_size_limit - cs * BLOCK_SIZE_CS)\n            & (offs_n[None, :] < K),\n            other=0.0,\n        ).to(dot_dtype)\n        acc += tl.dot(dout, a)\n        dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m\n        a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_RESIDUAL:\n        res_ptr += (\n            pid_b * stride_res_batch\n            + pid_c * chunk_size * stride_res_seqlen\n            + pid_h * stride_res_head\n        )\n        res_ptrs = res_ptr + (\n            offs_m[:, None] * stride_res_seqlen + offs_n[None, :] * stride_res_k\n        )\n        res = tl.load(\n            res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)\n        ).to(tl.float32)\n        acc += res\n    db = acc.to(db_ptr.dtype.element_ty)\n\n    db_ptr += (\n        pid_b * stride_db_batch\n        + pid_c * chunk_size * stride_db_seqlen\n        + pid_h * stride_db_head\n    )\n    db_ptrs = db_ptr + (\n        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_k\n    )\n    tl.store(\n        db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 63, 6, 108, 273, 63, 6, 108, 274, 63, 6, 108, 275, 63, 6, 108, 276, 63, 6, 175, 63, 32, -1, 277, 194, 171, 243, 278, 194, 348, 175, 32, 279, 194, 171, 243, 278, 194, 349, 175, 32, 280, 194, 279, 48, 254, 32, 281, 194, 279, 4, 280, 244, 254, 32, 282, 194, 65, 243, 253, 108, 275, 175, 32, 283, 194, 171, 243, 278, 194, 350, 175, 48, 282, 32, 284, 194, 171, 243, 278, 194, 350, 175, 226, 282, 32, 247, 172, 277, 244, 255, 74, 280, 244, 252, 244, 256, 74, 281, 244, 257, 32, 248, 172, 277, 244, 259, 74, 280, 244, 260, 74, 281, 244, 261, 32, 285, 194, 283, 244, 274, 74, 76, 243, 350, 108, 274, 175, 32, 286, 194, 284, 244, 275, 74, 76, 243, 350, 108, 275, 175, 32, 287, 194, 76, 243, 350, 108, 276, 175, 32, 288, 194, 248, 74, 243, 285, 230, 63, 108, 202, 27, 244, 263, 74, 287, 230, 202, 108, 63, 27, 244, 262, 175, 32, 289, 194, 247, 74, 243, 287, 230, 63, 108, 202, 27, 244, 256, 74, 286, 230, 202, 108, 63, 27, 244, 258, 175, 32, 290, 194, 39, 243, 252, 108, 251, 4, 280, 244, 252, 175, 32, 291, 194, 176, 243, 243, 274, 108, 275, 175, 108, 91, 194, 145, 175, 32, 135, 292, 157, 5, 243, 350, 108, 65, 243, 290, 108, 276, 175, 175, 63, 32, 293, 194, 57, 243, 288, 108, 294, 194, 243, 285, 230, 63, 108, 202, 27, 1, 252, 175, 173, 243, 287, 230, 202, 108, 63, 27, 1, 290, 4, 292, 244, 276, 175, 108, 295, 194, 350, 175, 82, 296, 243, 272, 175, 32, 297, 194, 57, 243, 289, 108, 294, 194, 243, 287, 230, 63, 108, 202, 27, 1, 290, 4, 292, 244, 276, 175, 173, 243, 286, 230, 202, 108, 63, 27, 1, 253, 175, 108, 295, 194, 350, 175, 82, 296, 243, 272, 175, 32, 291, 172, 15, 243, 293, 108, 297, 175, 32, 288, 172, 276, 244, 262, 32, 289, 172, 276, 244, 256, 32, 78, 32, 285, 194, 283, 244, 274, 74, 76, 243, 350, 108, 274, 175, 32, 286, 194, 284, 244, 275, 74, 76, 243, 350, 108, 275, 175, 32, 182, 273, 63, 32, 250, 172, 277, 244, 268, 74, 280, 244, 252, 244, 269, 74, 281, 244, 270, 32, 298, 194, 250, 74, 243, 285, 230, 63, 108, 202, 27, 244, 269, 74, 286, 230, 202, 108, 63, 27, 244, 271, 175, 32, 299, 194, 57, 243, 298, 108, 294, 194, 243, 285, 230, 63, 108, 202, 27, 1, 290, 175, 173, 243, 286, 230, 202, 108, 63, 27, 1, 253, 175, 175, 82, 296, 243, 145, 175, 32, 291, 172, 299, 32, 185, 32, 300, 194, 291, 82, 296, 243, 249, 82, 91, 82, 114, 175, 32, 249, 172, 277, 244, 264, 74, 280, 244, 252, 244, 265, 74, 281, 244, 266, 32, 301, 194, 249, 74, 243, 285, 230, 63, 108, 202, 27, 244, 265, 74, 286, 230, 202, 108, 63, 27, 244, 267, 175, 32, 10, 243, 301, 108, 300, 108, 294, 194, 243, 285, 230, 63, 108, 202, 27, 1, 290, 175, 173, 243, 286, 230, 202, 108, 63, 27, 1, 253, 175, 175, 32, 3, 32]}, {"code": "def _chunk_scan_fwd_kernel(\n    cb_ptr,\n    x_ptr,\n    z_ptr,\n    out_ptr,\n    out_x_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    seq_idx_ptr,\n    C_ptr,\n    prev_states_ptr,\n    D_ptr,\n    chunk_size,\n    hdim,\n    dstate,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_k,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_z_batch,\n    stride_z_seqlen,\n    stride_z_head,\n    stride_z_hdim,\n    stride_out_batch,\n    stride_out_seqlen,\n    stride_out_head,\n    stride_out_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    stride_C_batch,\n    stride_C_seqlen,\n    stride_C_head,\n    stride_C_dstate,\n    stride_states_batch,\n    stride_states_chunk,\n    stride_states_head,\n    stride_states_hdim,\n    stride_states_dstate,\n    stride_D_head,\n    IS_CAUSAL: tl.constexpr,\n    HAS_D: tl.constexpr,\n    D_HAS_HDIM: tl.constexpr,\n    HAS_Z: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_DSTATE: tl.constexpr,\n    IS_TRITON_22: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    C_ptr += (\n        pid_b * stride_C_batch\n        + pid_c * chunk_size * stride_C_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_C_head\n    )\n    prev_states_ptr += (\n        pid_b * stride_states_batch\n        + pid_c * stride_states_chunk\n        + pid_h * stride_states_head\n    )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dA_cs_m = tl.load(\n        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0\n    ).to(tl.float32)\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(\n            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0\n        )\n        seq_idx_m = tl.load(\n            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit,\n            other=-1,\n        )\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    if IS_TRITON_22 or pid_c > -1:\n\n        offs_k_dstate = tl.arange(\n            0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K\n        )\n        C_ptrs = C_ptr + (\n            offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate\n        )\n        prev_states_ptrs = prev_states_ptr + (\n            offs_n[None, :] * stride_states_hdim\n            + offs_k_dstate[:, None] * stride_states_dstate\n        )\n        if not HAS_SEQ_IDX:\n            scale_m = tl.exp(dA_cs_m)\n        else:\n            scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n        if BLOCK_SIZE_DSTATE <= 128:\n            C = tl.load(\n                C_ptrs,\n                mask=(offs_m[:, None] < chunk_size_limit)\n                & (offs_k_dstate[None, :] < dstate),\n                other=0.0,\n            )\n            prev_states = tl.load(\n                prev_states_ptrs,\n                mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),\n                other=0.0,\n            )\n            prev_states = prev_states.to(C_ptr.dtype.element_ty)\n            acc = tl.dot(C, prev_states) * scale_m[:, None]\n        else:\n            for k in range(0, dstate, BLOCK_SIZE_K):\n                C = tl.load(\n                    C_ptrs,\n                    mask=(offs_m[:, None] < chunk_size_limit)\n                    & (offs_k_dstate[None, :] < dstate - k),\n                    other=0.0,\n                )\n\n                prev_states = tl.load(\n                    prev_states_ptrs,\n                    mask=(offs_k_dstate[:, None] < dstate - k)\n                    & (offs_n[None, :] < hdim),\n                    other=0.0,\n                )\n                prev_states = prev_states.to(C_ptr.dtype.element_ty)\n                acc += tl.dot(C, prev_states)\n                C_ptrs += BLOCK_SIZE_K\n                prev_states_ptrs += BLOCK_SIZE_K\n            acc *= scale_m[:, None]\n\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k\n    )\n    x_ptrs = x_ptr + (\n        offs_k[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n    )\n    dt_ptrs = dt_ptr + offs_k * stride_dt_csize\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    K_MAX = (\n        chunk_size_limit\n        if not IS_CAUSAL\n        else min((pid_m + 1) * BLOCK_SIZE_M, chunk_size_limit)\n    )\n    for k in range(0, K_MAX, BLOCK_SIZE_K):\n        cb = tl.load(\n            cb_ptrs,\n            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < chunk_size - k),\n            other=0.0,\n        ).to(tl.float32)\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(\n            tl.float32\n        )\n\n        cb *= tl.exp((dA_cs_m[:, None] - dA_cs_k[None, :]))\n        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n        cb *= dt_k\n        if IS_CAUSAL:\n            mask = offs_m[:, None] >= k + offs_k[None, :]\n            cb = tl.where(mask, cb, 0.0)\n        cb = cb.to(x_ptr.dtype.element_ty)\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n        acc += tl.dot(cb, x)\n        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n\n    offs_out_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_out_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    if HAS_D:\n        if D_HAS_HDIM:\n            D = tl.load(\n                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0\n            ).to(tl.float32)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n        x_residual = tl.load(\n            x_ptr\n            + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim),\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n            other=0.0,\n        ).to(tl.float32)\n        acc += x_residual * D\n\n    if HAS_Z:\n        out_x_ptr += (\n            pid_b * stride_out_batch\n            + pid_c * chunk_size * stride_out_seqlen\n            + pid_h * stride_out_head\n        )\n        out_x_ptrs = out_x_ptr + (\n            stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :]\n        )\n        tl.store(\n            out_x_ptrs,\n            acc,\n            mask=(offs_out_m[:, None] < chunk_size_limit)\n            & (offs_out_n[None, :] < hdim),\n        )\n\n        z_ptr += (\n            pid_b * stride_z_batch\n            + pid_c * chunk_size * stride_z_seqlen\n            + pid_h * stride_z_head\n        )\n        z_ptrs = z_ptr + (\n            stride_z_seqlen * offs_out_m[:, None] + stride_z_hdim * offs_out_n[None, :]\n        )\n        z = tl.load(\n            z_ptrs,\n            mask=(offs_out_m[:, None] < chunk_size_limit)\n            & (offs_out_n[None, :] < hdim),\n            other=0.0,\n        ).to(tl.float32)\n        acc *= z * tl.sigmoid(z)\n\n    out_ptr += (\n        pid_b * stride_out_batch\n        + pid_c * chunk_size * stride_out_seqlen\n        + pid_h * stride_out_head\n    )\n    out_ptrs = out_ptr + (\n        stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :] * stride_out_hdim\n    )\n    tl.store(\n        out_ptrs,\n        acc,\n        mask=(offs_out_m[:, None] < chunk_size_limit) & (offs_out_n[None, :] < hdim),\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 108, 301, 62, 6, 108, 302, 62, 6, 108, 303, 62, 6, 108, 304, 62, 6, 108, 305, 62, 6, 108, 306, 62, 6, 108, 307, 62, 6, 108, 308, 62, 6, 108, 309, 62, 6, 108, 310, 62, 6, 175, 62, 32, -1, 311, 194, 171, 243, 312, 194, 348, 175, 32, 313, 194, 311, 48, 261, 32, 314, 194, 311, 4, 313, 244, 261, 32, 315, 194, 171, 243, 312, 194, 349, 175, 32, 316, 194, 64, 243, 259, 108, 307, 175, 32, 317, 194, 171, 243, 312, 194, 350, 175, 48, 316, 32, 318, 194, 171, 243, 312, 194, 350, 175, 226, 316, 32, 247, 172, 314, 244, 264, 74, 313, 244, 265, 74, 315, 48, 263, 244, 266, 32, 248, 172, 314, 244, 269, 74, 313, 244, 258, 244, 270, 74, 315, 244, 271, 32, 252, 172, 314, 244, 281, 74, 313, 244, 282, 74, 315, 244, 283, 32, 253, 172, 314, 244, 285, 74, 313, 244, 286, 74, 315, 244, 287, 32, 255, 172, 314, 244, 291, 74, 313, 244, 258, 244, 292, 74, 315, 48, 263, 244, 293, 32, 256, 172, 314, 244, 295, 74, 313, 244, 296, 74, 315, 244, 297, 32, 182, 305, 62, 32, 254, 172, 314, 244, 289, 74, 313, 244, 258, 244, 290, 32, 185, 32, 319, 194, 317, 244, 306, 74, 76, 243, 350, 108, 306, 175, 32, 320, 194, 318, 244, 307, 74, 76, 243, 350, 108, 307, 175, 32, 321, 194, 55, 243, 253, 74, 319, 244, 288, 108, 322, 194, 319, 1, 258, 108, 323, 194, 350, 175, 82, 324, 243, 145, 175, 32, 325, 194, 39, 243, 258, 108, 262, 4, 313, 244, 258, 175, 32, 182, 305, 62, 32, 326, 194, 55, 243, 254, 4, 290, 108, 322, 194, 313, 144, 348, 108, 323, 194, 350, 175, 32, 327, 194, 55, 243, 254, 74, 319, 244, 290, 108, 322, 194, 319, 1, 325, 108, 323, 194, 4, 348, 175, 32, 185, 32, 328, 194, 176, 243, 243, 306, 108, 307, 175, 108, 91, 194, 145, 175, 32, 182, 310, 141, 313, 124, 4, 348, 62, 32, 329, 194, 76, 243, 350, 108, 309, 182, 309, 218, 348, 349, 351, 30, 308, 175, 32, 330, 194, 255, 74, 243, 319, 230, 62, 108, 202, 27, 244, 292, 74, 329, 230, 202, 108, 62, 27, 244, 294, 175, 32, 331, 194, 256, 74, 243, 320, 230, 202, 108, 62, 27, 244, 298, 74, 329, 230, 62, 108, 202, 27, 244, 299, 175, 32, 182, 63, 305, 62, 32, 332, 194, 107, 243, 321, 175, 32, 185, 32, 30, 62, 32, 332, 194, 205, 243, 327, 77, 326, 108, 107, 243, 321, 175, 108, 350, 175, 32, 56, 32, 182, 309, 218, 348, 349, 351, 62, 32, 333, 194, 55, 243, 330, 108, 322, 194, 243, 319, 230, 62, 108, 202, 27, 1, 325, 175, 173, 243, 329, 230, 202, 108, 62, 27, 1, 260, 175, 108, 323, 194, 350, 175, 32, 334, 194, 55, 243, 331, 108, 322, 194, 243, 329, 230, 62, 108, 202, 27, 1, 260, 175, 173, 243, 320, 230, 202, 108, 62, 27, 1, 259, 175, 108, 323, 194, 350, 175, 32, 334, 194, 334, 82, 324, 243, 255, 82, 91, 82, 114, 175, 32, 328, 194, 15, 243, 333, 108, 334, 175, 244, 332, 230, 62, 108, 202, 27, 32, 185, 32, 30, 62, 32, 135, 335, 157, 5, 243, 350, 108, 260, 108, 308, 175, 62, 32, 333, 194, 55, 243, 330, 108, 322, 194, 243, 319, 230, 62, 108, 202, 27, 1, 325, 175, 173, 243, 329, 230, 202, 108, 62, 27, 1, 260, 4, 335, 175, 108, 323, 194, 350, 175, 32, 334, 194, 55, 243, 331, 108, 322, 194, 243, 329, 230, 62, 108, 202, 27, 1, 260, 4, 335, 175, 173, 243, 320, 230, 202, 108, 62, 27, 1, 259, 175, 108, 323, 194, 350, 175, 32, 334, 194, 334, 82, 324, 243, 255, 82, 91, 82, 114, 175, 32, 328, 172, 15, 243, 333, 108, 334, 175, 32, 330, 172, 308, 32, 331, 172, 308, 32, 78, 32, 328, 24, 332, 230, 62, 108, 202, 27, 32, 56, 32, 185, 32, 336, 194, 76, 243, 350, 108, 308, 175, 32, 337, 194, 247, 74, 243, 319, 230, 62, 108, 202, 27, 244, 267, 74, 336, 230, 202, 108, 62, 27, 244, 268, 175, 32, 338, 194, 248, 74, 243, 336, 230, 62, 108, 202, 27, 244, 270, 74, 320, 230, 202, 108, 62, 27, 244, 272, 175, 32, 339, 194, 252, 74, 336, 244, 284, 32, 340, 194, 253, 74, 336, 244, 288, 32, 341, 194, 325, 182, 63, 301, 30, 39, 243, 243, 317, 74, 348, 175, 244, 306, 108, 325, 175, 32, 135, 335, 157, 5, 243, 350, 108, 341, 108, 308, 175, 62, 32, 342, 194, 55, 243, 337, 108, 322, 194, 243, 319, 230, 62, 108, 202, 27, 1, 258, 175, 173, 243, 336, 230, 202, 108, 62, 27, 1, 258, 4, 335, 175, 108, 323, 194, 350, 175, 82, 324, 243, 145, 175, 32, 343, 194, 55, 243, 340, 108, 322, 194, 336, 1, 258, 4, 335, 108, 323, 194, 350, 175, 82, 324, 243, 145, 175, 32, 342, 24, 107, 243, 321, 230, 62, 108, 202, 27, 4, 343, 230, 202, 108, 62, 27, 175, 32, 344, 194, 55, 243, 339, 108, 322, 194, 336, 1, 258, 4, 335, 108, 323, 194, 350, 175, 82, 324, 243, 145, 175, 32, 342, 24, 344, 32, 182, 301, 62, 32, 322, 194, 319, 230, 62, 108, 202, 27, 144, 335, 74, 336, 230, 202, 108, 62, 27, 32, 342, 194, 205, 243, 322, 108, 342, 108, 350, 175, 32, 185, 32, 342, 194, 342, 82, 324, 243, 248, 82, 91, 82, 114, 175, 32, 345, 194, 55, 243, 338, 108, 322, 194, 243, 336, 230, 62, 108, 202, 27, 1, 325, 4, 335, 175, 173, 243, 320, 230, 202, 108, 62, 27, 1, 259, 175, 108, 323, 194, 350, 175, 32, 328, 172, 15, 243, 342, 108, 345, 175, 32, 337, 172, 308, 244, 268, 32, 338, 172, 308, 244, 270, 32, 339, 172, 308, 244, 284, 32, 340, 172, 308, 244, 288, 32, 78, 32, 346, 194, 317, 244, 306, 74, 76, 243, 350, 108, 306, 175, 32, 352, 194, 318, 244, 307, 74, 76, 243, 350, 108, 307, 175, 32, 182, 302, 62, 32, 182, 303, 62, 32, 353, 194, 55, 243, 257, 74, 315, 244, 300, 74, 320, 108, 322, 194, 320, 1, 259, 108, 323, 194, 350, 175, 82, 324, 243, 145, 175, 32, 185, 32, 30, 62, 32, 353, 194, 55, 243, 257, 74, 315, 244, 300, 175, 82, 324, 243, 145, 175, 32, 56, 32, 354, 194, 55, 243, 248, 74, 243, 319, 230, 62, 108, 202, 27, 244, 270, 74, 320, 230, 202, 108, 62, 27, 244, 272, 175, 108, 322, 194, 243, 319, 230, 62, 108, 202, 27, 1, 325, 175, 173, 243, 320, 230, 202, 108, 62, 27, 1, 259, 175, 108, 323, 194, 350, 175, 82, 324, 243, 145, 175, 32, 328, 172, 354, 244, 353, 32, 185, 32, 182, 304, 62, 32, 251, 172, 314, 244, 277, 74, 313, 244, 258, 244, 278, 74, 315, 244, 279, 32, 355, 194, 251, 74, 243, 278, 244, 346, 230, 62, 108, 202, 27, 74, 352, 230, 202, 108, 62, 27, 175, 32, 10, 243, 355, 108, 328, 108, 322, 194, 243, 346, 230, 62, 108, 202, 27, 1, 325, 175, 173, 243, 352, 230, 202, 108, 62, 27, 1, 259, 175, 175, 32, 249, 172, 314, 244, 273, 74, 313, 244, 258, 244, 274, 74, 315, 244, 275, 32, 356, 194, 249, 74, 243, 274, 244, 346, 230, 62, 108, 202, 27, 74, 276, 244, 352, 230, 202, 108, 62, 27, 175, 32, 357, 194, 55, 243, 356, 108, 322, 194, 243, 346, 230, 62, 108, 202, 27, 1, 325, 175, 173, 243, 352, 230, 202, 108, 62, 27, 1, 259, 175, 108, 323, 194, 350, 175, 82, 324, 243, 145, 175, 32, 328, 24, 357, 244, 206, 243, 357, 175, 32, 185, 32, 250, 172, 314, 244, 277, 74, 313, 244, 258, 244, 278, 74, 315, 244, 279, 32, 358, 194, 250, 74, 243, 278, 244, 346, 230, 62, 108, 202, 27, 74, 352, 230, 202, 108, 62, 27, 244, 280, 175, 32, 10, 243, 358, 108, 328, 108, 322, 194, 243, 346, 230, 62, 108, 202, 27, 1, 325, 175, 173, 243, 352, 230, 202, 108, 62, 27, 1, 259, 175, 175, 32, 3, 32]}, {"code": "def _chunk_scan_fwd_kernel_wip(\n    cb_ptr,\n    x_ptr,\n    z_ptr,\n    out_ptr,\n    out_x_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    seq_idx_ptr,\n    C_ptr,\n    B_ptr,\n    prev_states_ptr,\n    D_ptr,\n    chunk_size,\n    hdim,\n    dstate,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_k,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_z_batch,\n    stride_z_seqlen,\n    stride_z_head,\n    stride_z_hdim,\n    stride_out_batch,\n    stride_out_seqlen,\n    stride_out_head,\n    stride_out_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    stride_C_batch,\n    stride_C_seqlen,\n    stride_C_head,\n    stride_C_dstate,\n    stride_B_batch,\n    stride_B_seqlen,\n    stride_B_head,\n    stride_B_dstate,\n    stride_states_batch,\n    stride_states_chunk,\n    stride_states_head,\n    stride_states_hdim,\n    stride_states_dstate,\n    stride_D_head,\n    HAS_D: tl.constexpr,\n    D_HAS_HDIM: tl.constexpr,\n    HAS_Z: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_DSTATE: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_n = tl.program_id(axis=0)\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    C_ptr += (\n        pid_b * stride_C_batch\n        + pid_c * chunk_size * stride_C_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_C_head\n    )\n    B_ptr += (\n        pid_b * stride_B_batch\n        + pid_c * chunk_size * stride_B_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_B_head\n    )\n    prev_states_ptr += (\n        pid_b * stride_states_batch\n        + pid_c * stride_states_chunk\n        + pid_h * stride_states_head\n    )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n    out_ptr += (\n        pid_b * stride_out_batch\n        + pid_c * chunk_size * stride_out_seqlen\n        + pid_h * stride_out_head\n    )\n\n    offs_m = tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE)\n\n    C_ptrs = C_ptr + (\n        offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate\n    )\n    B_ptrs = B_ptr + (\n        offs_m[None, :] * stride_B_seqlen + offs_k_dstate[:, None] * stride_B_dstate\n    )\n    prev_states_ptrs = prev_states_ptr + (\n        offs_n[None, :] * stride_states_hdim\n        + offs_k_dstate[:, None] * stride_states_dstate\n    )\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m + offs_m[None, :] * stride_cb_csize_k\n    )\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n    )\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    out_ptrs = out_ptr + (\n        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim\n    )\n\n    prev_states = tl.load(\n        prev_states_ptrs,\n        mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),\n        other=0.0,\n    )\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    for start_m in range(0, chunk_size_limit, BLOCK_SIZE_M):\n        start_m = tl.multiple_of(start_m, BLOCK_SIZE_M)\n        dA_cs_m = tl.load(\n            dA_cumsum_ptr + (start_m + offs_m) * stride_dA_cs_csize,\n            mask=offs_m < chunk_size - start_m,\n            other=0.0,\n        ).to(tl.float32)\n        if HAS_SEQ_IDX:\n            seq_idx_prev = tl.load(\n                seq_idx_ptr + start_m - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0\n            )\n            seq_idx_m = tl.load(\n                seq_idx_ptr + (start_m + offs_m) * stride_seq_idx_seqlen,\n                mask=offs_m < chunk_size_limit - start_m,\n                other=-1,\n            )\n        if not HAS_SEQ_IDX:\n            scale_m = tl.exp(dA_cs_m)\n        else:\n            scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n        C = tl.load(\n            C_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit - start_m)\n            & (offs_k_dstate[None, :] < dstate),\n            other=0.0,\n        )\n        acc = tl.dot(C, prev_states.to(C_ptr.dtype.element_ty)) * scale_m[:, None]\n\n        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size - start_m, other=0.0).to(\n            tl.float32\n        )\n\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit - start_m)\n            & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n\n        if HAS_D:\n            if D_HAS_HDIM:\n                D = tl.load(\n                    D_ptr + pid_h * stride_D_head + offs_n,\n                    mask=offs_n < hdim,\n                    other=0.0,\n                ).to(tl.float32)\n            else:\n                D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n            acc += x.to(tl.float32) * D\n\n        tl.store(\n            out_ptrs,\n            acc,\n            mask=(offs_m[:, None] < chunk_size_limit - start_m)\n            & (offs_n[None, :] < hdim),\n        )\n\n        if start_m + BLOCK_SIZE_M < chunk_size_limit:\n\n            B = tl.load(\n                B_ptrs,\n                mask=(offs_m[None, :] < chunk_size_limit - start_m)\n                & (offs_k_dstate[:, None] < dstate),\n                other=0.0,\n            )\n            dA_cs_last = tl.load(\n                dA_cumsum_ptr + (start_m + BLOCK_SIZE_M) * stride_dA_cs_csize\n            ).to(tl.float32)\n\n            scale = tl.exp((dA_cs_last - dA_cs_m)) * dt_m\n\n            B = B.to(x_ptr.dtype.element_ty)\n            tmp = tl.dot(B, x)\n            prev_states += tmp.to(prev_states.dtype)\n\n        C_ptrs += BLOCK_SIZE_M * stride_C_seqlen\n        B_ptrs += BLOCK_SIZE_M * stride_B_seqlen\n        cb_ptrs += BLOCK_SIZE_M * stride_cb_csize_m + BLOCK_SIZE_M * stride_cb_csize_k\n        x_ptrs += BLOCK_SIZE_M * stride_x_seqlen\n        dt_ptrs += BLOCK_SIZE_M * stride_dt_csize\n        out_ptrs += BLOCK_SIZE_M * stride_out_seqlen", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 108, 301, 108, 302, 108, 303, 108, 304, 108, 305, 108, 306, 63, 6, 108, 307, 63, 6, 108, 308, 63, 6, 108, 309, 63, 6, 108, 310, 63, 6, 108, 311, 63, 6, 108, 312, 63, 6, 175, 63, 32, -1, 313, 194, 171, 243, 314, 194, 348, 175, 32, 315, 194, 313, 48, 262, 32, 316, 194, 313, 4, 315, 244, 262, 32, 317, 194, 171, 243, 314, 194, 349, 175, 32, 318, 194, 171, 243, 314, 194, 350, 175, 32, 247, 172, 316, 244, 265, 74, 315, 244, 266, 74, 317, 48, 264, 244, 267, 32, 248, 172, 316, 244, 270, 74, 315, 244, 259, 244, 271, 74, 317, 244, 272, 32, 252, 172, 316, 244, 282, 74, 315, 244, 283, 74, 317, 244, 284, 32, 253, 172, 316, 244, 286, 74, 315, 244, 287, 74, 317, 244, 288, 32, 255, 172, 316, 244, 292, 74, 315, 244, 259, 244, 293, 74, 317, 48, 264, 244, 294, 32, 256, 172, 316, 244, 296, 74, 315, 244, 259, 244, 297, 74, 317, 48, 264, 244, 298, 32, 257, 172, 316, 244, 300, 74, 315, 244, 301, 74, 317, 244, 302, 32, 182, 309, 63, 32, 254, 172, 316, 244, 290, 74, 315, 244, 259, 244, 291, 32, 185, 32, 250, 172, 316, 244, 278, 74, 315, 244, 259, 244, 279, 74, 317, 244, 280, 32, 319, 194, 76, 243, 350, 108, 310, 175, 32, 320, 194, 318, 244, 311, 74, 76, 243, 350, 108, 311, 175, 32, 321, 194, 76, 243, 350, 108, 312, 175, 32, 322, 194, 255, 74, 243, 319, 230, 63, 108, 202, 27, 244, 293, 74, 321, 230, 202, 108, 63, 27, 244, 295, 175, 32, 323, 194, 256, 74, 243, 319, 230, 202, 108, 63, 27, 244, 297, 74, 321, 230, 63, 108, 202, 27, 244, 299, 175, 32, 324, 194, 257, 74, 243, 320, 230, 202, 108, 63, 27, 244, 303, 74, 321, 230, 63, 108, 202, 27, 244, 304, 175, 32, 325, 194, 65, 243, 260, 108, 311, 175, 32, 326, 194, 247, 74, 243, 319, 230, 63, 108, 202, 27, 244, 268, 74, 319, 230, 202, 108, 63, 27, 244, 269, 175, 32, 327, 194, 248, 74, 243, 319, 230, 63, 108, 202, 27, 244, 271, 74, 320, 230, 202, 108, 63, 27, 244, 273, 175, 32, 328, 194, 252, 74, 319, 244, 285, 32, 329, 194, 250, 74, 243, 319, 230, 63, 108, 202, 27, 244, 279, 74, 320, 230, 202, 108, 63, 27, 244, 281, 175, 32, 330, 194, 57, 243, 324, 108, 331, 194, 243, 321, 230, 63, 108, 202, 27, 1, 261, 175, 173, 243, 320, 230, 202, 108, 63, 27, 1, 260, 175, 108, 332, 194, 350, 175, 32, 333, 194, 39, 243, 259, 108, 263, 4, 315, 244, 259, 175, 32, 135, 334, 157, 5, 243, 350, 108, 333, 108, 310, 175, 63, 32, 334, 194, 55, 243, 334, 108, 310, 175, 32, 335, 194, 57, 243, 253, 74, 243, 334, 74, 319, 175, 244, 289, 108, 331, 194, 319, 1, 259, 4, 334, 108, 332, 194, 350, 175, 82, 336, 243, 145, 175, 32, 182, 309, 63, 32, 337, 194, 57, 243, 254, 74, 334, 4, 291, 108, 331, 194, 315, 144, 348, 108, 332, 194, 350, 175, 32, 338, 194, 57, 243, 254, 74, 243, 334, 74, 319, 175, 244, 291, 108, 331, 194, 319, 1, 333, 4, 334, 108, 332, 194, 4, 348, 175, 32, 185, 32, 182, 64, 309, 63, 32, 339, 194, 107, 243, 335, 175, 32, 185, 32, 30, 63, 32, 339, 194, 205, 243, 338, 77, 337, 108, 107, 243, 335, 175, 108, 350, 175, 32, 56, 32, 340, 194, 57, 243, 322, 108, 331, 194, 243, 319, 230, 63, 108, 202, 27, 1, 333, 4, 334, 175, 173, 243, 321, 230, 202, 108, 63, 27, 1, 261, 175, 108, 332, 194, 350, 175, 32, 341, 194, 15, 243, 340, 108, 330, 82, 336, 243, 255, 82, 91, 82, 114, 175, 175, 244, 339, 230, 63, 108, 202, 27, 32, 342, 194, 57, 243, 328, 108, 331, 194, 319, 1, 259, 4, 334, 108, 332, 194, 350, 175, 82, 336, 243, 145, 175, 32, 343, 194, 57, 243, 327, 108, 331, 194, 243, 319, 230, 63, 108, 202, 27, 1, 333, 4, 334, 175, 173, 243, 320, 230, 202, 108, 63, 27, 1, 260, 175, 108, 332, 194, 350, 175, 32, 182, 306, 63, 32, 182, 307, 63, 32, 344, 194, 57, 243, 258, 74, 317, 244, 305, 74, 320, 108, 331, 194, 320, 1, 260, 108, 332, 194, 350, 175, 82, 336, 243, 145, 175, 32, 185, 32, 30, 63, 32, 344, 194, 57, 243, 258, 74, 317, 244, 305, 175, 82, 336, 243, 145, 175, 32, 56, 32, 341, 172, 343, 82, 336, 243, 145, 175, 244, 344, 32, 185, 32, 10, 243, 329, 108, 341, 108, 331, 194, 243, 319, 230, 63, 108, 202, 27, 1, 333, 4, 334, 175, 173, 243, 320, 230, 202, 108, 63, 27, 1, 260, 175, 175, 32, 182, 334, 74, 310, 1, 333, 63, 32, 345, 194, 57, 243, 323, 108, 331, 194, 243, 319, 230, 202, 108, 63, 27, 1, 333, 4, 334, 175, 173, 243, 321, 230, 63, 108, 202, 27, 1, 261, 175, 108, 332, 194, 350, 175, 32, 346, 194, 57, 243, 253, 74, 243, 334, 74, 310, 175, 244, 289, 175, 82, 336, 243, 145, 175, 32, 351, 194, 107, 243, 346, 4, 335, 175, 244, 342, 32, 345, 194, 345, 82, 336, 243, 248, 82, 91, 82, 114, 175, 32, 352, 194, 15, 243, 345, 108, 343, 175, 32, 330, 172, 352, 82, 336, 243, 330, 82, 91, 175, 32, 185, 32, 322, 172, 310, 244, 293, 32, 323, 172, 310, 244, 297, 32, 326, 172, 310, 244, 268, 74, 310, 244, 269, 32, 327, 172, 310, 244, 271, 32, 328, 172, 310, 244, 285, 32, 329, 172, 310, 244, 279, 32, 78, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_dz_kernel(\n    dout_ptr,\n    out_ptr,\n    z_ptr,\n    x_ptr,\n    D_ptr,\n    outz_ptr,\n    dz_ptr,\n    dout_x_ptr,\n    dD_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_out_batch,\n    stride_out_seqlen,\n    stride_out_head,\n    stride_out_hdim,\n    stride_z_batch,\n    stride_z_seqlen,\n    stride_z_head,\n    stride_z_hdim,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_D_head,\n    stride_outz_batch,\n    stride_outz_seqlen,\n    stride_outz_head,\n    stride_outz_hdim,\n    stride_dz_batch,\n    stride_dz_seqlen,\n    stride_dz_head,\n    stride_dz_hdim,\n    stride_doutx_batch,\n    stride_doutx_seqlen,\n    stride_doutx_head,\n    stride_doutx_hdim,\n    stride_dD_batch,\n    stride_dD_chunk,\n    stride_dD_head,\n    stride_dD_csize,\n    stride_dD_hdim,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize,\n    HAS_D: tl.constexpr,\n    D_HAS_HDIM: tl.constexpr,\n    HAS_DDACS: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dout_x_ptr += (\n        pid_b * stride_doutx_batch\n        + pid_c * chunk_size * stride_doutx_seqlen\n        + pid_h * stride_doutx_head\n    )\n    out_ptr += (\n        pid_b * stride_out_batch\n        + pid_c * chunk_size * stride_out_seqlen\n        + pid_h * stride_out_head\n    )\n    z_ptr += (\n        pid_b * stride_z_batch\n        + pid_c * chunk_size * stride_z_seqlen\n        + pid_h * stride_z_head\n    )\n    dz_ptr += (\n        pid_b * stride_dz_batch\n        + pid_c * chunk_size * stride_dz_seqlen\n        + pid_h * stride_dz_head\n    )\n    if RECOMPUTE_OUTPUT:\n        outz_ptr += (\n            pid_b * stride_outz_batch\n            + pid_c * chunk_size * stride_outz_seqlen\n            + pid_h * stride_outz_head\n        )\n    if HAS_DDACS:\n        ddA_cumsum_ptr += (\n            pid_b * stride_ddA_cs_batch\n            + pid_c * stride_ddA_cs_chunk\n            + pid_h * stride_ddA_cs_head\n        )\n    if HAS_D:\n        x_ptr += (\n            pid_b * stride_x_batch\n            + pid_c * chunk_size * stride_x_seqlen\n            + pid_h * stride_x_head\n        )\n        dD_ptr += (\n            pid_b * stride_dD_batch\n            + pid_c * stride_dD_chunk\n            + pid_h * stride_dD_head\n            + pid_m * stride_dD_csize\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim\n    )\n    dout_x_ptrs = dout_x_ptr + (\n        offs_m[:, None] * stride_doutx_seqlen + offs_n[None, :] * stride_doutx_hdim\n    )\n    out_ptrs = out_ptr + (\n        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim\n    )\n    z_ptrs = z_ptr + (\n        offs_m[:, None] * stride_z_seqlen + offs_n[None, :] * stride_z_hdim\n    )\n    dz_ptrs = dz_ptr + (\n        offs_m[:, None] * stride_dz_seqlen + offs_n[None, :] * stride_dz_hdim\n    )\n    if RECOMPUTE_OUTPUT:\n        outz_ptrs = outz_ptr + (\n            offs_m[:, None] * stride_outz_seqlen + offs_n[None, :] * stride_outz_hdim\n        )\n    if HAS_D:\n        x_ptrs = x_ptr + (\n            offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n        )\n        if D_HAS_HDIM:\n            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dout = tl.load(\n        dout_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    out = tl.load(\n        out_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    z = tl.load(\n        z_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    z_sigmoid = tl.sigmoid(z)\n    if RECOMPUTE_OUTPUT:\n        outz = out * z * z_sigmoid\n        tl.store(\n            outz_ptrs,\n            outz,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        )\n    dz = dout * out * z_sigmoid * (1 + z * (1 - z_sigmoid))\n    tl.store(\n        dz_ptrs,\n        dz,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n    )\n    dout *= z * z_sigmoid\n    tl.store(\n        dout_x_ptrs,\n        dout,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n    )\n    if HAS_D:\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n            other=0.0,\n        ).to(tl.float32)\n        if D_HAS_HDIM:\n            dD = tl.sum(dout * x, axis=0)\n            tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n            D = tl.load(\n                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0\n            ).to(tl.float32)\n        else:\n            dD = tl.sum(dout * x)\n            tl.store(dD_ptr, dD)\n            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n        out -= x * D\n    if HAS_DDACS:\n        ddA_cs = tl.sum(dout * out, axis=1)\n        tl.store(\n            ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize,\n            ddA_cs,\n            mask=offs_m < chunk_size,\n        )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 62, 6, 108, 300, 62, 6, 108, 301, 62, 6, 108, 302, 62, 6, 108, 303, 62, 6, 108, 304, 62, 6, 175, 62, 32, -1, 305, 194, 171, 243, 306, 194, 348, 175, 32, 307, 194, 305, 48, 259, 32, 308, 194, 305, 4, 307, 244, 259, 32, 309, 194, 171, 243, 306, 194, 349, 175, 32, 310, 194, 171, 243, 306, 194, 350, 175, 32, 247, 172, 308, 244, 261, 74, 307, 244, 257, 244, 262, 74, 309, 244, 263, 32, 254, 172, 308, 244, 286, 74, 307, 244, 257, 244, 287, 74, 309, 244, 288, 32, 248, 172, 308, 244, 265, 74, 307, 244, 257, 244, 266, 74, 309, 244, 267, 32, 249, 172, 308, 244, 269, 74, 307, 244, 257, 244, 270, 74, 309, 244, 271, 32, 253, 172, 308, 244, 282, 74, 307, 244, 257, 244, 283, 74, 309, 244, 284, 32, 182, 302, 62, 32, 252, 172, 308, 244, 278, 74, 307, 244, 257, 244, 279, 74, 309, 244, 280, 32, 185, 32, 182, 301, 62, 32, 256, 172, 308, 244, 295, 74, 307, 244, 296, 74, 309, 244, 297, 32, 185, 32, 182, 299, 62, 32, 250, 172, 308, 244, 273, 74, 307, 244, 257, 244, 274, 74, 309, 244, 275, 32, 255, 172, 308, 244, 290, 74, 307, 244, 291, 74, 309, 244, 292, 74, 310, 244, 293, 32, 185, 32, 311, 194, 310, 244, 303, 74, 76, 243, 350, 108, 303, 175, 32, 312, 194, 76, 243, 350, 108, 304, 175, 32, 313, 194, 247, 74, 243, 311, 230, 62, 108, 202, 27, 244, 262, 74, 312, 230, 202, 108, 62, 27, 244, 264, 175, 32, 314, 194, 254, 74, 243, 311, 230, 62, 108, 202, 27, 244, 287, 74, 312, 230, 202, 108, 62, 27, 244, 289, 175, 32, 315, 194, 248, 74, 243, 311, 230, 62, 108, 202, 27, 244, 266, 74, 312, 230, 202, 108, 62, 27, 244, 268, 175, 32, 316, 194, 249, 74, 243, 311, 230, 62, 108, 202, 27, 244, 270, 74, 312, 230, 202, 108, 62, 27, 244, 272, 175, 32, 317, 194, 253, 74, 243, 311, 230, 62, 108, 202, 27, 244, 283, 74, 312, 230, 202, 108, 62, 27, 244, 285, 175, 32, 182, 302, 62, 32, 318, 194, 252, 74, 243, 311, 230, 62, 108, 202, 27, 244, 279, 74, 312, 230, 202, 108, 62, 27, 244, 281, 175, 32, 185, 32, 182, 299, 62, 32, 319, 194, 250, 74, 243, 311, 230, 62, 108, 202, 27, 244, 274, 74, 312, 230, 202, 108, 62, 27, 244, 276, 175, 32, 182, 300, 62, 32, 320, 194, 255, 74, 312, 244, 294, 32, 185, 32, 185, 32, 321, 194, 39, 243, 257, 108, 260, 4, 307, 244, 257, 175, 32, 322, 194, 55, 243, 313, 108, 323, 194, 243, 311, 230, 62, 108, 202, 27, 1, 321, 175, 173, 243, 312, 230, 202, 108, 62, 27, 1, 258, 175, 108, 324, 194, 350, 175, 82, 325, 243, 145, 175, 32, 14, 194, 55, 243, 315, 108, 323, 194, 243, 311, 230, 62, 108, 202, 27, 1, 321, 175, 173, 243, 312, 230, 202, 108, 62, 27, 1, 258, 175, 108, 324, 194, 350, 175, 82, 325, 243, 145, 175, 32, 326, 194, 55, 243, 316, 108, 323, 194, 243, 311, 230, 62, 108, 202, 27, 1, 321, 175, 173, 243, 312, 230, 202, 108, 62, 27, 1, 258, 175, 108, 324, 194, 350, 175, 82, 325, 243, 145, 175, 32, 327, 194, 206, 243, 326, 175, 32, 182, 302, 62, 32, 328, 194, 14, 244, 326, 244, 327, 32, 10, 243, 318, 108, 328, 108, 323, 194, 243, 311, 230, 62, 108, 202, 27, 1, 321, 175, 173, 243, 312, 230, 202, 108, 62, 27, 1, 258, 175, 175, 32, 185, 32, 329, 194, 322, 244, 14, 244, 327, 244, 243, 348, 74, 326, 244, 243, 348, 4, 327, 175, 175, 32, 10, 243, 317, 108, 329, 108, 323, 194, 243, 311, 230, 62, 108, 202, 27, 1, 321, 175, 173, 243, 312, 230, 202, 108, 62, 27, 1, 258, 175, 175, 32, 322, 24, 326, 244, 327, 32, 10, 243, 314, 108, 322, 108, 323, 194, 243, 311, 230, 62, 108, 202, 27, 1, 321, 175, 173, 243, 312, 230, 202, 108, 62, 27, 1, 258, 175, 175, 32, 182, 299, 62, 32, 330, 194, 55, 243, 319, 108, 323, 194, 243, 311, 230, 62, 108, 202, 27, 1, 321, 175, 173, 243, 312, 230, 202, 108, 62, 27, 1, 258, 175, 108, 324, 194, 350, 175, 82, 325, 243, 145, 175, 32, 182, 300, 62, 32, 331, 194, 221, 243, 322, 244, 330, 108, 306, 194, 350, 175, 32, 10, 243, 320, 108, 331, 108, 323, 194, 312, 1, 258, 175, 32, 332, 194, 55, 243, 251, 74, 309, 244, 277, 74, 312, 108, 323, 194, 312, 1, 258, 108, 324, 194, 350, 175, 82, 325, 243, 145, 175, 32, 185, 32, 30, 62, 32, 331, 194, 221, 243, 322, 244, 330, 175, 32, 10, 243, 255, 108, 331, 175, 32, 332, 194, 55, 243, 251, 74, 309, 244, 277, 175, 82, 325, 243, 145, 175, 32, 56, 32, 14, 2, 330, 244, 332, 32, 185, 32, 182, 301, 62, 32, 333, 194, 221, 243, 322, 244, 14, 108, 306, 194, 348, 175, 32, 10, 243, 256, 74, 311, 244, 298, 108, 333, 108, 323, 194, 311, 1, 257, 175, 32, 185, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_dstates_kernel(\n    dout_ptr,\n    c_ptr,\n    dprev_states_ptr,\n    dA_cumsum_ptr,\n    seq_idx_ptr,\n    hdim,\n    dstate,\n    chunk_size,\n    batch,\n    seqlen,\n    nchunks,\n    nheads_ngroups_ratio,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_c_batch,\n    stride_c_seqlen,\n    stride_c_head,\n    stride_c_dstate,\n    stride_dprev_states_batch,\n    stride_dprev_states_chunk,\n    stride_dprev_states_head,\n    stride_dprev_states_hdim,\n    stride_dprev_states_dstate,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    c_ptr += (\n        pid_b * stride_c_batch\n        + pid_c * chunk_size * stride_c_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_c_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_hdim + offs_k[None, :] * stride_dout_seqlen\n    )\n    c_ptrs = c_ptr + (\n        offs_n[None, :] * stride_c_dstate + offs_k[:, None] * stride_c_seqlen\n    )\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    if HAS_SEQ_IDX:\n        seq_idx_ptrs = seq_idx_ptr + offs_k * stride_seq_idx_seqlen\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(\n            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0\n        )\n    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n        dout = tl.load(\n            dout_ptrs,\n            mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k),\n            other=0.0,\n        ).to(tl.float32)\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(\n            tl.float32\n        )\n        if not HAS_SEQ_IDX:\n            scale_k = tl.exp(dA_cs_k)\n        else:\n            seq_idx_k = tl.load(\n                seq_idx_ptrs, mask=offs_k < chunk_size_limit - k, other=-1\n            )\n            scale_k = tl.where(seq_idx_k == seq_idx_prev, tl.exp(dA_cs_k), 0.0)\n        dout = (dout * scale_k).to(dout_ptr.dtype.element_ty)\n        c = tl.load(\n            c_ptrs,\n            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate),\n            other=0.0,\n        )\n        acc += tl.dot(dout, c)\n        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n        c_ptrs += BLOCK_SIZE_K * stride_c_seqlen\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n        if HAS_SEQ_IDX:\n            seq_idx_ptrs += BLOCK_SIZE_K * stride_seq_idx_seqlen\n    out = acc.to(dprev_states_ptr.dtype.element_ty)\n\n    dprev_states_ptr += (\n        pid_b * stride_dprev_states_batch\n        + pid_c * stride_dprev_states_chunk\n        + pid_h * stride_dprev_states_head\n    )\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dprev_states_ptrs = dprev_states_ptr + (\n        offs_m[:, None] * stride_dprev_states_hdim\n        + offs_n[None, :] * stride_dprev_states_dstate\n    )\n    tl.store(\n        dprev_states_ptrs,\n        out,\n        mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 63, 6, 108, 279, 63, 6, 108, 280, 63, 6, 108, 281, 63, 6, 175, 63, 32, -1, 282, 194, 171, 243, 283, 194, 348, 175, 32, 284, 194, 282, 48, 255, 32, 285, 194, 282, 4, 284, 244, 255, 32, 286, 194, 171, 243, 283, 194, 349, 175, 32, 287, 194, 65, 243, 253, 108, 280, 175, 32, 288, 194, 171, 243, 283, 194, 350, 175, 48, 287, 32, 289, 194, 171, 243, 283, 194, 350, 175, 226, 287, 32, 248, 172, 285, 244, 263, 74, 284, 244, 254, 244, 264, 74, 286, 48, 258, 244, 265, 32, 247, 172, 285, 244, 259, 74, 284, 244, 254, 244, 260, 74, 286, 244, 261, 32, 250, 172, 285, 244, 272, 74, 284, 244, 273, 74, 286, 244, 274, 32, 182, 278, 63, 32, 251, 172, 285, 244, 276, 74, 284, 244, 254, 244, 277, 32, 185, 32, 290, 194, 288, 244, 279, 74, 76, 243, 350, 108, 279, 175, 32, 291, 194, 289, 244, 280, 74, 76, 243, 350, 108, 280, 175, 32, 292, 194, 76, 243, 350, 108, 281, 175, 32, 293, 194, 247, 74, 243, 290, 230, 63, 108, 202, 27, 244, 262, 74, 292, 230, 202, 108, 63, 27, 244, 260, 175, 32, 294, 194, 248, 74, 243, 291, 230, 202, 108, 63, 27, 244, 266, 74, 292, 230, 63, 108, 202, 27, 244, 264, 175, 32, 295, 194, 250, 74, 292, 244, 275, 32, 182, 278, 63, 32, 296, 194, 251, 74, 292, 244, 277, 32, 185, 32, 297, 194, 39, 243, 254, 108, 256, 4, 284, 244, 254, 175, 32, 298, 194, 176, 243, 243, 279, 108, 280, 175, 108, 91, 194, 145, 175, 32, 182, 278, 63, 32, 299, 194, 57, 243, 251, 4, 277, 108, 300, 194, 284, 144, 348, 108, 301, 194, 350, 175, 32, 185, 32, 135, 302, 157, 5, 243, 350, 108, 297, 108, 281, 175, 63, 32, 303, 194, 57, 243, 293, 108, 300, 194, 243, 290, 230, 63, 108, 202, 27, 1, 252, 175, 173, 243, 292, 230, 202, 108, 63, 27, 1, 297, 4, 302, 175, 108, 301, 194, 350, 175, 82, 304, 243, 145, 175, 32, 305, 194, 57, 243, 295, 108, 300, 194, 292, 1, 254, 4, 302, 108, 301, 194, 350, 175, 82, 304, 243, 145, 175, 32, 182, 64, 278, 63, 32, 306, 194, 107, 243, 305, 175, 32, 185, 32, 30, 63, 32, 307, 194, 57, 243, 296, 108, 300, 194, 292, 1, 297, 4, 302, 108, 301, 194, 4, 348, 175, 32, 306, 194, 205, 243, 307, 77, 299, 108, 107, 243, 305, 175, 108, 350, 175, 32, 56, 32, 303, 194, 243, 303, 244, 306, 175, 82, 304, 243, 247, 82, 91, 82, 114, 175, 32, 308, 194, 57, 243, 294, 108, 300, 194, 243, 292, 230, 63, 108, 202, 27, 1, 297, 4, 302, 175, 173, 243, 291, 230, 202, 108, 63, 27, 1, 253, 175, 108, 301, 194, 350, 175, 32, 298, 172, 15, 243, 303, 108, 308, 175, 32, 293, 172, 281, 244, 260, 32, 294, 172, 281, 244, 264, 32, 295, 172, 281, 244, 275, 32, 182, 278, 63, 32, 296, 172, 281, 244, 277, 32, 185, 32, 78, 32, 14, 194, 298, 82, 304, 243, 249, 82, 91, 82, 114, 175, 32, 249, 172, 285, 244, 267, 74, 284, 244, 268, 74, 286, 244, 269, 32, 290, 194, 288, 244, 279, 74, 76, 243, 350, 108, 279, 175, 32, 291, 194, 289, 244, 280, 74, 76, 243, 350, 108, 280, 175, 32, 309, 194, 249, 74, 243, 290, 230, 63, 108, 202, 27, 244, 270, 74, 291, 230, 202, 108, 63, 27, 244, 271, 175, 32, 10, 243, 309, 108, 14, 108, 300, 194, 243, 290, 230, 63, 108, 202, 27, 1, 252, 175, 173, 243, 291, 230, 202, 108, 63, 27, 1, 253, 175, 175, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_dc_kernel(\n    dout_ptr,\n    prev_states_ptr,\n    C_ptr,\n    dA_cumsum_ptr,\n    seq_idx_ptr,\n    dc_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    dstate,\n    hdim,\n    batch,\n    seqlen,\n    nheads,\n    nheads_per_program,\n    ngroups,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_prev_states_batch,\n    stride_prev_states_chunk,\n    stride_prev_states_head,\n    stride_prev_states_hdim,\n    stride_prev_states_dstate,\n    stride_C_batch,\n    stride_C_seqlen,\n    stride_C_head,\n    stride_C_dstate,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    stride_dc_batch,\n    stride_dc_seqlen,\n    stride_dc_split,\n    stride_dc_group,\n    stride_dc_dstate,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize,\n    HAS_DDA_CS: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_sg = tl.program_id(axis=2)\n    pid_s = pid_sg // ngroups\n    pid_g = pid_sg - pid_s * ngroups\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head\n    )\n    dc_ptr += (\n        pid_b * stride_dc_batch\n        + pid_c * chunk_size * stride_dc_seqlen\n        + pid_g * stride_dc_group\n        + pid_s * stride_dc_split\n    )\n    prev_states_ptr += (\n        pid_b * stride_prev_states_batch\n        + pid_c * stride_prev_states_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n        * stride_prev_states_head\n    )\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_head\n    )\n    if HAS_DDA_CS:\n        C_ptr += (\n            pid_b * stride_C_batch\n            + pid_c * chunk_size * stride_C_seqlen\n            + pid_g * stride_C_head\n        )\n        ddA_cumsum_ptr += (\n            pid_b * stride_ddA_cs_batch\n            + pid_c * stride_ddA_cs_chunk\n            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n            * stride_ddA_cs_head\n        )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim\n    )\n    prev_states_ptrs = prev_states_ptr + (\n        offs_n[None, :] * stride_prev_states_dstate\n        + offs_k[:, None] * stride_prev_states_hdim\n    )\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n    if HAS_DDA_CS:\n        C_ptrs = C_ptr + (\n            offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate\n        )\n        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_DDA_CS:\n        c = tl.load(\n            C_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),\n            other=0.0,\n        ).to(tl.float32)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(\n            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0\n        )\n        seq_idx_m = tl.load(\n            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit,\n            other=-1,\n        )\n    nheads_iter = min(\n        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program\n    )\n    for h in range(nheads_iter):\n        dout = tl.load(\n            dout_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n            other=0.0,\n        )\n        prev_states = tl.load(\n            prev_states_ptrs,\n            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),\n            other=0.0,\n        )\n        prev_states = prev_states.to(dout_ptrs.dtype.element_ty)\n        dc = tl.dot(dout, prev_states)\n        dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(\n            tl.float32\n        )\n        if not HAS_SEQ_IDX:\n            scale = tl.exp(dA_cs_m)\n        else:\n            scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n        dc *= scale[:, None]\n        if HAS_DDA_CS:\n            ddA_cs = tl.sum(dc * c, axis=1)\n            tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)\n        acc += dc\n        dout_ptrs += stride_dout_head\n        prev_states_ptrs += stride_prev_states_head\n        dA_cumsum_ptrs += stride_dA_cs_head\n        if HAS_DDA_CS:\n            ddA_cumsum_ptrs += stride_ddA_cs_head\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dc_ptrs = dc_ptr + (\n        offs_m[:, None] * stride_dc_seqlen + offs_n[None, :] * stride_dc_dstate\n    )\n    tl.store(\n        dc_ptrs,\n        acc,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 62, 6, 108, 291, 62, 6, 108, 292, 62, 6, 108, 293, 62, 6, 108, 294, 62, 6, 175, 62, 32, -1, 295, 194, 171, 243, 296, 194, 348, 175, 32, 297, 194, 295, 48, 257, 32, 298, 194, 295, 4, 297, 244, 257, 32, 299, 194, 171, 243, 296, 194, 349, 175, 32, 300, 194, 299, 48, 261, 32, 301, 194, 299, 4, 300, 244, 261, 32, 302, 194, 64, 243, 255, 108, 293, 175, 32, 303, 194, 171, 243, 296, 194, 350, 175, 48, 302, 32, 304, 194, 171, 243, 296, 194, 350, 175, 226, 302, 32, 247, 172, 298, 244, 262, 74, 297, 244, 254, 244, 263, 74, 243, 301, 244, 243, 259, 48, 261, 175, 74, 300, 244, 260, 175, 244, 264, 32, 252, 172, 298, 244, 281, 74, 297, 244, 254, 244, 282, 74, 301, 244, 284, 74, 300, 244, 283, 32, 248, 172, 298, 244, 266, 74, 297, 244, 267, 74, 243, 301, 244, 243, 259, 48, 261, 175, 74, 300, 244, 260, 175, 244, 268, 32, 250, 172, 298, 244, 275, 74, 297, 244, 276, 74, 243, 301, 244, 243, 259, 48, 261, 175, 74, 300, 244, 260, 175, 244, 277, 32, 182, 290, 62, 32, 249, 172, 298, 244, 271, 74, 297, 244, 254, 244, 272, 74, 301, 244, 273, 32, 253, 172, 298, 244, 286, 74, 297, 244, 287, 74, 243, 301, 244, 243, 259, 48, 261, 175, 74, 300, 244, 260, 175, 244, 288, 32, 185, 32, 182, 291, 62, 32, 251, 172, 298, 244, 279, 74, 297, 244, 254, 244, 280, 32, 185, 32, 305, 194, 303, 244, 292, 74, 76, 243, 350, 108, 292, 175, 32, 306, 194, 304, 244, 293, 74, 76, 243, 350, 108, 293, 175, 32, 307, 194, 76, 243, 350, 108, 294, 175, 32, 308, 194, 247, 74, 243, 305, 230, 62, 108, 202, 27, 244, 263, 74, 307, 230, 202, 108, 62, 27, 244, 265, 175, 32, 309, 194, 248, 74, 243, 306, 230, 202, 108, 62, 27, 244, 270, 74, 307, 230, 62, 108, 202, 27, 244, 269, 175, 32, 310, 194, 250, 74, 305, 244, 278, 32, 182, 290, 62, 32, 311, 194, 249, 74, 243, 305, 230, 62, 108, 202, 27, 244, 272, 74, 306, 230, 202, 108, 62, 27, 244, 274, 175, 32, 312, 194, 253, 74, 305, 244, 289, 32, 185, 32, 313, 194, 39, 243, 254, 108, 258, 4, 297, 244, 254, 175, 32, 314, 194, 176, 243, 243, 292, 108, 293, 175, 108, 91, 194, 145, 175, 32, 182, 290, 62, 32, 315, 194, 55, 243, 311, 108, 316, 194, 243, 305, 230, 62, 108, 202, 27, 1, 313, 175, 173, 243, 306, 230, 202, 108, 62, 27, 1, 255, 175, 108, 317, 194, 350, 175, 82, 318, 243, 145, 175, 32, 185, 32, 182, 291, 62, 32, 319, 194, 55, 243, 251, 4, 280, 108, 316, 194, 297, 144, 348, 108, 317, 194, 350, 175, 32, 320, 194, 55, 243, 251, 74, 305, 244, 280, 108, 316, 194, 305, 1, 313, 108, 317, 194, 4, 348, 175, 32, 185, 32, 321, 194, 39, 243, 260, 108, 259, 48, 261, 4, 300, 244, 260, 175, 32, 135, 322, 157, 5, 243, 321, 175, 62, 32, 323, 194, 55, 243, 308, 108, 316, 194, 243, 305, 230, 62, 108, 202, 27, 1, 313, 175, 173, 243, 307, 230, 202, 108, 62, 27, 1, 256, 175, 108, 317, 194, 350, 175, 32, 324, 194, 55, 243, 309, 108, 316, 194, 243, 307, 230, 62, 108, 202, 27, 1, 256, 175, 173, 243, 306, 230, 202, 108, 62, 27, 1, 255, 175, 108, 317, 194, 350, 175, 32, 324, 194, 324, 82, 318, 243, 308, 82, 91, 82, 114, 175, 32, 325, 194, 15, 243, 323, 108, 324, 175, 32, 326, 194, 55, 243, 310, 108, 316, 194, 305, 1, 313, 108, 317, 194, 350, 175, 82, 318, 243, 145, 175, 32, 182, 63, 291, 62, 32, 327, 194, 107, 243, 326, 175, 32, 185, 32, 30, 62, 32, 327, 194, 205, 243, 320, 77, 319, 108, 107, 243, 326, 175, 108, 350, 175, 32, 56, 32, 325, 24, 327, 230, 62, 108, 202, 27, 32, 182, 290, 62, 32, 328, 194, 221, 243, 325, 244, 315, 108, 296, 194, 348, 175, 32, 196, 243, 312, 108, 328, 108, 316, 194, 305, 1, 254, 175, 32, 185, 32, 314, 172, 325, 32, 308, 172, 264, 32, 309, 172, 268, 32, 310, 172, 277, 32, 182, 290, 62, 32, 312, 172, 288, 32, 185, 32, 78, 32, 305, 194, 303, 244, 292, 74, 76, 243, 350, 108, 292, 175, 32, 306, 194, 304, 244, 293, 74, 76, 243, 350, 108, 293, 175, 32, 329, 194, 252, 74, 243, 305, 230, 62, 108, 202, 27, 244, 282, 74, 306, 230, 202, 108, 62, 27, 244, 285, 175, 32, 10, 243, 329, 108, 314, 108, 316, 194, 243, 305, 230, 62, 108, 202, 27, 1, 313, 175, 173, 243, 306, 230, 202, 108, 62, 27, 1, 255, 175, 175, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_dx_kernel(\n    x_ptr,\n    cb_ptr,\n    dout_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    D_ptr,\n    dx_ptr,\n    ddt_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_k,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_D_head,\n    stride_dx_batch,\n    stride_dx_seqlen,\n    stride_dx_head,\n    stride_dx_hdim,\n    stride_ddt_batch,\n    stride_ddt_chunk,\n    stride_ddt_head,\n    stride_ddt_csize,\n    HAS_D: tl.constexpr,\n    D_HAS_HDIM: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    ddt_ptr += (\n        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\n    )\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k\n    )\n    dout_ptrs = dout_ptr + (\n        offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim\n    )\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dA_cs_m = tl.load(\n        dA_cumsum_ptr + offs_m * stride_dA_cs_csize,\n        mask=offs_m < chunk_size_limit,\n        other=0.0,\n    ).to(tl.float32)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    K_MAX = chunk_size_limit\n    for k in range(0, K_MAX, BLOCK_SIZE_K):\n\n        cb = tl.load(\n            cb_ptrs,\n            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k),\n            other=0.0,\n        )\n        dout = tl.load(\n            dout_ptrs,\n            mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0).to(\n            tl.float32\n        )\n        cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])\n\n        mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)\n        cb = tl.where(mask, cb, 0.0)\n        cb = cb.to(dout_ptr.dtype.element_ty)\n        acc += tl.dot(cb, dout)\n        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\n    dx = acc * dt_m[:, None]\n    dx_ptr += (\n        pid_b * stride_dx_batch\n        + pid_c * chunk_size * stride_dx_seqlen\n        + pid_h * stride_dx_head\n    )\n    dx_ptrs = dx_ptr + (\n        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim\n    )\n    if HAS_D:\n        dout_res_ptrs = dout_ptr + (\n            offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim\n        )\n        dout_res = tl.load(\n            dout_res_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n            other=0.0,\n        ).to(tl.float32)\n        if D_HAS_HDIM:\n            D = tl.load(\n                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0\n            ).to(tl.float32)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n        dx += dout_res * D\n    tl.store(\n        dx_ptrs,\n        dx,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n    )\n\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n    )\n    x = tl.load(\n        x_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    ddt = tl.sum(acc * x, axis=1)\n    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\n    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 63, 6, 108, 291, 63, 6, 108, 292, 63, 6, 108, 293, 63, 6, 108, 294, 63, 6, 175, 63, 32, -1, 295, 194, 171, 243, 296, 194, 348, 175, 32, 297, 194, 295, 48, 257, 32, 298, 194, 295, 4, 297, 244, 257, 32, 299, 194, 171, 243, 296, 194, 349, 175, 32, 300, 194, 65, 243, 256, 108, 293, 175, 32, 301, 194, 171, 243, 296, 194, 350, 175, 48, 300, 32, 302, 194, 171, 243, 296, 194, 350, 175, 226, 300, 32, 247, 172, 298, 244, 260, 74, 297, 244, 255, 244, 261, 74, 299, 244, 262, 32, 248, 172, 298, 244, 264, 74, 297, 244, 265, 74, 299, 48, 259, 244, 266, 32, 249, 172, 298, 244, 269, 74, 297, 244, 255, 244, 270, 74, 299, 244, 271, 32, 250, 172, 298, 244, 273, 74, 297, 244, 274, 74, 299, 244, 275, 32, 254, 172, 298, 244, 286, 74, 297, 244, 287, 74, 299, 244, 288, 32, 251, 172, 298, 244, 277, 74, 297, 244, 278, 74, 299, 244, 279, 32, 303, 194, 301, 244, 292, 74, 76, 243, 350, 108, 292, 175, 32, 304, 194, 302, 244, 293, 74, 76, 243, 350, 108, 293, 175, 32, 305, 194, 76, 243, 350, 108, 294, 175, 32, 306, 194, 248, 74, 243, 303, 230, 63, 108, 202, 27, 244, 267, 74, 305, 230, 202, 108, 63, 27, 244, 268, 175, 32, 307, 194, 249, 74, 243, 305, 230, 63, 108, 202, 27, 244, 270, 74, 304, 230, 202, 108, 63, 27, 244, 272, 175, 32, 308, 194, 251, 74, 305, 244, 280, 32, 309, 194, 39, 243, 255, 108, 258, 4, 297, 244, 255, 175, 32, 310, 194, 57, 243, 251, 74, 303, 244, 280, 108, 311, 194, 303, 1, 309, 108, 312, 194, 350, 175, 82, 313, 243, 145, 175, 32, 314, 194, 176, 243, 243, 292, 108, 293, 175, 108, 91, 194, 145, 175, 32, 315, 194, 309, 32, 135, 316, 157, 5, 243, 350, 108, 315, 108, 294, 175, 63, 32, 317, 194, 57, 243, 306, 108, 311, 194, 243, 303, 230, 63, 108, 202, 27, 1, 255, 175, 173, 243, 305, 230, 202, 108, 63, 27, 1, 315, 4, 316, 175, 108, 312, 194, 350, 175, 32, 318, 194, 57, 243, 307, 108, 311, 194, 243, 305, 230, 63, 108, 202, 27, 1, 315, 4, 316, 175, 173, 243, 304, 230, 202, 108, 63, 27, 1, 256, 175, 108, 312, 194, 350, 175, 32, 319, 194, 57, 243, 308, 108, 311, 194, 305, 1, 315, 4, 316, 108, 312, 194, 350, 175, 82, 313, 243, 145, 175, 32, 317, 24, 107, 243, 319, 230, 202, 108, 63, 27, 4, 310, 230, 63, 108, 202, 27, 175, 32, 311, 194, 243, 316, 74, 305, 230, 202, 108, 63, 27, 144, 303, 230, 63, 108, 202, 27, 175, 173, 243, 316, 74, 305, 230, 202, 108, 63, 27, 1, 315, 175, 32, 317, 194, 205, 243, 311, 108, 317, 108, 350, 175, 32, 317, 194, 317, 82, 313, 243, 249, 82, 91, 82, 114, 175, 32, 314, 172, 15, 243, 317, 108, 318, 175, 32, 306, 172, 294, 244, 268, 32, 307, 172, 294, 244, 270, 32, 308, 172, 294, 244, 280, 32, 78, 32, 303, 194, 301, 244, 292, 74, 76, 243, 350, 108, 292, 175, 32, 304, 194, 302, 244, 293, 74, 76, 243, 350, 108, 293, 175, 32, 320, 194, 250, 74, 303, 244, 276, 32, 321, 194, 57, 243, 320, 108, 311, 194, 303, 1, 309, 108, 312, 194, 350, 175, 82, 313, 243, 145, 175, 32, 322, 194, 314, 244, 321, 230, 63, 108, 202, 27, 32, 253, 172, 298, 244, 282, 74, 297, 244, 255, 244, 283, 74, 299, 244, 284, 32, 323, 194, 253, 74, 243, 303, 230, 63, 108, 202, 27, 244, 283, 74, 304, 230, 202, 108, 63, 27, 244, 285, 175, 32, 182, 290, 63, 32, 324, 194, 249, 74, 243, 303, 230, 63, 108, 202, 27, 244, 270, 74, 304, 230, 202, 108, 63, 27, 244, 272, 175, 32, 325, 194, 57, 243, 324, 108, 311, 194, 243, 303, 230, 63, 108, 202, 27, 1, 309, 175, 173, 243, 304, 230, 202, 108, 63, 27, 1, 256, 175, 108, 312, 194, 350, 175, 82, 313, 243, 145, 175, 32, 182, 291, 63, 32, 326, 194, 57, 243, 252, 74, 299, 244, 281, 74, 304, 108, 311, 194, 304, 1, 256, 108, 312, 194, 350, 175, 82, 313, 243, 145, 175, 32, 185, 32, 30, 63, 32, 326, 194, 57, 243, 252, 74, 299, 244, 281, 175, 82, 313, 243, 145, 175, 32, 56, 32, 322, 172, 325, 244, 326, 32, 185, 32, 10, 243, 323, 108, 322, 108, 311, 194, 243, 303, 230, 63, 108, 202, 27, 1, 309, 175, 173, 243, 304, 230, 202, 108, 63, 27, 1, 256, 175, 175, 32, 327, 194, 247, 74, 243, 303, 230, 63, 108, 202, 27, 244, 261, 74, 304, 230, 202, 108, 63, 27, 244, 263, 175, 32, 328, 194, 57, 243, 327, 108, 311, 194, 243, 303, 230, 63, 108, 202, 27, 1, 309, 175, 173, 243, 304, 230, 202, 108, 63, 27, 1, 256, 175, 108, 312, 194, 350, 175, 82, 313, 243, 145, 175, 32, 329, 194, 221, 243, 314, 244, 328, 108, 296, 194, 348, 175, 32, 330, 194, 254, 74, 303, 244, 289, 32, 196, 243, 330, 108, 329, 108, 311, 194, 303, 1, 255, 175, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_dcb_kernel(\n    x_ptr,\n    dout_ptr,\n    cb_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    seq_idx_ptr,\n    dcb_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    nheads,\n    nheads_per_program,\n    ngroups,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_n,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    stride_dcb_batch,\n    stride_dcb_chunk,\n    stride_dcb_split,\n    stride_dcb_group,\n    stride_dcb_csize_m,\n    stride_dcb_csize_n,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize_m,\n    stride_ddA_cs_csize_n,\n    HAS_DDA_CS: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_sg = tl.program_id(axis=2)\n    pid_s = pid_sg // ngroups\n    pid_g = pid_sg - pid_s * ngroups\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head\n    )\n    dt_ptr += (\n        pid_b * stride_dt_batch\n        + pid_c * stride_dt_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head\n    )\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_head\n    )\n    if HAS_DDA_CS:\n        cb_ptr += (\n            pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_g * stride_cb_head\n        )\n        ddA_cumsum_ptr += (\n            pid_b * stride_ddA_cs_batch\n            + pid_c * stride_ddA_cs_chunk\n            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n            * stride_ddA_cs_head\n            + pid_m * stride_ddA_cs_csize_m\n        )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim\n    )\n    x_ptrs = x_ptr + (\n        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim\n    )\n    dt_ptrs = dt_ptr + offs_n * stride_dt_csize\n    if HAS_DDA_CS:\n        cb_ptrs = cb_ptr + (\n            offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n\n        )\n        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n\n\n    if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n        dcb_ptr += (\n            pid_b * stride_dcb_batch\n            + pid_c * stride_dcb_chunk\n            + pid_g * stride_dcb_group\n            + pid_s * stride_dcb_split\n        )\n        dcb_ptrs = dcb_ptr + (\n            offs_m[:, None] * stride_dcb_csize_m + offs_n[None, :] * stride_dcb_csize_n\n        )\n        tl.store(\n            dcb_ptrs,\n            tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=dcb_ptr.dtype.element_ty),\n            mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),\n        )\n        return\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    chunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_DDA_CS:\n        cb = tl.load(\n            cb_ptrs,\n            mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),\n            other=0.0,\n        ).to(tl.float32)\n    nheads_iter = min(\n        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program\n    )\n    for h in range(nheads_iter):\n        dout = tl.load(\n            dout_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n            other=0.0,\n        )\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit_n),\n            other=0.0,\n        )\n        dcb = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)\n        dcb *= dt_n\n        dA_cs_m = tl.load(\n            dA_cumsum_ptr + offs_m * stride_dA_cs_csize,\n            mask=offs_m < chunk_size_limit,\n            other=0.0,\n        ).to(tl.float32)\n        dA_cs_n = tl.load(\n            dA_cumsum_ptr + offs_n * stride_dA_cs_csize,\n            mask=offs_n < chunk_size_limit,\n            other=0.0,\n        ).to(tl.float32)\n        dcb *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        if HAS_DDA_CS:\n            tl.static_assert(\n                not HAS_SEQ_IDX, \"HAS_SEQ_IDX not supported with HAS_DDA_CS yet\"\n            )\n            ddA_cs = dcb * cb\n            mask = offs_m[:, None] >= offs_n[None, :] + 1\n            ddA_cs = tl.where(mask, ddA_cs, 0.0)\n            ddA_cs = tl.cumsum(ddA_cs, axis=1)\n            ddA_cs = tl.where(mask, ddA_cs, 0.0)\n            ddA_cs = tl.sum(ddA_cs, axis=0)\n            tl.store(\n                ddA_cumsum_ptrs + stride_ddA_cs_csize_n,\n                ddA_cs,\n                mask=offs_n < chunk_size - 1,\n            )\n            tl.store(ddA_cumsum_ptr, 0.0)\n        acc += dcb\n        dout_ptrs += stride_dout_head\n        x_ptrs += stride_x_head\n        dt_ptrs += stride_dt_head\n        dA_cumsum_ptr += stride_dA_cs_head\n        if HAS_DDA_CS:\n            ddA_cumsum_ptr += stride_ddA_cs_head\n            ddA_cumsum_ptrs += stride_ddA_cs_head\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_SEQ_IDX:\n        seq_idx_m = tl.load(\n            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit,\n            other=-1,\n        )\n        seq_idx_n = tl.load(\n            seq_idx_ptr + offs_n * stride_seq_idx_seqlen,\n            mask=offs_n < chunk_size_limit,\n            other=-2,\n        )\n        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)\n    mask = offs_m[:, None] >= offs_n[None, :]\n    acc = tl.where(mask, acc, 0.0)\n    dcb_ptr += (\n        pid_b * stride_dcb_batch\n        + pid_c * stride_dcb_chunk\n        + pid_g * stride_dcb_group\n        + pid_s * stride_dcb_split\n    )\n    dcb_ptrs = dcb_ptr + (\n        offs_m[:, None] * stride_dcb_csize_m + offs_n[None, :] * stride_dcb_csize_n\n    )\n    tl.store(\n        dcb_ptrs,\n        acc,\n        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 62, 6, 108, 297, 62, 6, 108, 298, 62, 6, 108, 299, 62, 6, 108, 300, 62, 6, 175, 62, 32, -1, 301, 194, 171, 243, 302, 194, 348, 175, 32, 303, 194, 301, 48, 257, 32, 304, 194, 301, 4, 303, 244, 257, 32, 305, 194, 171, 243, 302, 194, 349, 175, 32, 306, 194, 305, 48, 261, 32, 307, 194, 305, 4, 306, 244, 261, 32, 308, 194, 64, 243, 255, 108, 299, 175, 32, 309, 194, 171, 243, 302, 194, 350, 175, 48, 308, 32, 310, 194, 171, 243, 302, 194, 350, 175, 226, 308, 32, 247, 172, 304, 244, 262, 74, 303, 244, 255, 244, 263, 74, 243, 307, 244, 243, 259, 48, 261, 175, 74, 306, 244, 260, 175, 244, 264, 32, 248, 172, 304, 244, 266, 74, 303, 244, 255, 244, 267, 74, 243, 307, 244, 243, 259, 48, 261, 175, 74, 306, 244, 260, 175, 244, 268, 32, 250, 172, 304, 244, 275, 74, 303, 244, 276, 74, 243, 307, 244, 243, 259, 48, 261, 175, 74, 306, 244, 260, 175, 244, 277, 32, 251, 172, 304, 244, 279, 74, 303, 244, 280, 74, 243, 307, 244, 243, 259, 48, 261, 175, 74, 306, 244, 260, 175, 244, 281, 32, 182, 296, 62, 32, 249, 172, 304, 244, 270, 74, 303, 244, 271, 74, 307, 244, 272, 32, 254, 172, 304, 244, 291, 74, 303, 244, 292, 74, 243, 307, 244, 243, 259, 48, 261, 175, 74, 306, 244, 260, 175, 244, 293, 74, 309, 244, 294, 32, 185, 32, 182, 297, 62, 32, 252, 172, 304, 244, 283, 74, 303, 244, 255, 244, 284, 32, 185, 32, 311, 194, 309, 244, 298, 74, 76, 243, 350, 108, 298, 175, 32, 312, 194, 310, 244, 299, 74, 76, 243, 350, 108, 299, 175, 32, 313, 194, 76, 243, 350, 108, 300, 175, 32, 314, 194, 248, 74, 243, 311, 230, 62, 108, 202, 27, 244, 267, 74, 313, 230, 202, 108, 62, 27, 244, 269, 175, 32, 315, 194, 247, 74, 243, 312, 230, 202, 108, 62, 27, 244, 263, 74, 313, 230, 62, 108, 202, 27, 244, 265, 175, 32, 316, 194, 250, 74, 312, 244, 278, 32, 182, 296, 62, 32, 317, 194, 249, 74, 243, 311, 230, 62, 108, 202, 27, 244, 273, 74, 312, 230, 202, 108, 62, 27, 244, 274, 175, 32, 318, 194, 254, 74, 312, 244, 295, 32, 185, 32, 182, 310, 244, 299, 144, 243, 309, 74, 348, 175, 244, 298, 62, 32, 253, 172, 304, 244, 285, 74, 303, 244, 286, 74, 307, 244, 288, 74, 306, 244, 287, 32, 319, 194, 253, 74, 243, 311, 230, 62, 108, 202, 27, 244, 289, 74, 312, 230, 202, 108, 62, 27, 244, 290, 175, 32, 10, 243, 319, 108, 176, 243, 243, 298, 108, 299, 175, 108, 91, 194, 253, 82, 91, 82, 114, 175, 108, 320, 194, 243, 311, 230, 62, 108, 202, 27, 1, 255, 175, 173, 243, 312, 230, 202, 108, 62, 27, 1, 255, 175, 175, 32, 231, 32, 185, 32, 321, 194, 39, 243, 255, 108, 258, 4, 303, 244, 255, 175, 32, 322, 194, 39, 243, 321, 108, 243, 309, 74, 348, 175, 244, 298, 175, 32, 323, 194, 176, 243, 243, 298, 108, 299, 175, 108, 91, 194, 145, 175, 32, 182, 296, 62, 32, 324, 194, 55, 243, 317, 108, 320, 194, 243, 311, 230, 62, 108, 202, 27, 1, 255, 175, 173, 243, 312, 230, 202, 108, 62, 27, 1, 255, 175, 108, 325, 194, 350, 175, 82, 326, 243, 145, 175, 32, 185, 32, 327, 194, 39, 243, 260, 108, 259, 48, 261, 4, 306, 244, 260, 175, 32, 135, 328, 157, 5, 243, 327, 175, 62, 32, 329, 194, 55, 243, 314, 108, 320, 194, 243, 311, 230, 62, 108, 202, 27, 1, 321, 175, 173, 243, 313, 230, 202, 108, 62, 27, 1, 256, 175, 108, 325, 194, 350, 175, 32, 330, 194, 55, 243, 315, 108, 320, 194, 243, 313, 230, 62, 108, 202, 27, 1, 256, 175, 173, 243, 312, 230, 202, 108, 62, 27, 1, 322, 175, 108, 325, 194, 350, 175, 32, 331, 194, 15, 243, 329, 108, 330, 175, 32, 332, 194, 55, 243, 316, 108, 320, 194, 312, 1, 255, 108, 325, 194, 350, 175, 82, 326, 243, 145, 175, 32, 331, 24, 332, 32, 333, 194, 55, 243, 251, 74, 311, 244, 282, 108, 320, 194, 311, 1, 321, 108, 325, 194, 350, 175, 82, 326, 243, 145, 175, 32, 334, 194, 55, 243, 251, 74, 312, 244, 282, 108, 320, 194, 312, 1, 321, 108, 325, 194, 350, 175, 82, 326, 243, 145, 175, 32, 331, 24, 107, 243, 333, 230, 62, 108, 202, 27, 4, 334, 230, 202, 108, 62, 27, 175, 32, 182, 296, 62, 32, 83, 243, 63, 297, 108, 351, 175, 32, 335, 194, 331, 244, 324, 32, 320, 194, 311, 230, 62, 108, 202, 27, 144, 312, 230, 202, 108, 62, 27, 74, 348, 32, 335, 194, 205, 243, 320, 108, 335, 108, 350, 175, 32, 335, 194, 85, 243, 335, 108, 302, 194, 348, 175, 32, 335, 194, 205, 243, 320, 108, 335, 108, 350, 175, 32, 335, 194, 221, 243, 335, 108, 302, 194, 350, 175, 32, 10, 243, 318, 74, 295, 108, 335, 108, 320, 194, 312, 1, 255, 4, 348, 175, 32, 10, 243, 254, 108, 350, 175, 32, 185, 32, 323, 172, 331, 32, 314, 172, 268, 32, 315, 172, 264, 32, 316, 172, 277, 32, 251, 172, 281, 32, 182, 296, 62, 32, 254, 172, 293, 32, 318, 172, 293, 32, 185, 32, 78, 32, 311, 194, 309, 244, 298, 74, 76, 243, 350, 108, 298, 175, 32, 312, 194, 310, 244, 299, 74, 76, 243, 350, 108, 299, 175, 32, 182, 297, 62, 32, 336, 194, 55, 243, 252, 74, 311, 244, 284, 108, 320, 194, 311, 1, 321, 108, 325, 194, 4, 348, 175, 32, 337, 194, 55, 243, 252, 74, 312, 244, 284, 108, 320, 194, 312, 1, 321, 108, 325, 194, 4, 349, 175, 32, 323, 194, 205, 243, 336, 230, 62, 108, 202, 27, 77, 337, 230, 202, 108, 62, 27, 108, 323, 108, 350, 175, 32, 185, 32, 320, 194, 311, 230, 62, 108, 202, 27, 144, 312, 230, 202, 108, 62, 27, 32, 323, 194, 205, 243, 320, 108, 323, 108, 350, 175, 32, 253, 172, 304, 244, 285, 74, 303, 244, 286, 74, 307, 244, 288, 74, 306, 244, 287, 32, 319, 194, 253, 74, 243, 311, 230, 62, 108, 202, 27, 244, 289, 74, 312, 230, 202, 108, 62, 27, 244, 290, 175, 32, 10, 243, 319, 108, 323, 108, 320, 194, 243, 311, 230, 62, 108, 202, 27, 1, 255, 175, 173, 243, 312, 230, 202, 108, 62, 27, 1, 255, 175, 175, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_ddAcs_unstable_kernel(\n    dout_ptr,\n    out_ptr,\n    dt_ptr,\n    ddt_ptr,\n    x_ptr,\n    D_ptr,\n    ddA_cumsum_ptr,\n    dD_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_out_batch,\n    stride_out_seqlen,\n    stride_out_head,\n    stride_out_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_ddt_batch,\n    stride_ddt_chunk,\n    stride_ddt_head,\n    stride_ddt_csize,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_D_head,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize,\n    stride_dD_batch,\n    stride_dD_chunk,\n    stride_dD_head,\n    stride_dD_csize,\n    stride_dD_hdim,\n    HAS_D: tl.constexpr,\n    D_HAS_HDIM: tl.constexpr,\n    SUBTRACT_DDTDT: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    out_ptr += (\n        pid_b * stride_out_batch\n        + pid_c * chunk_size * stride_out_seqlen\n        + pid_h * stride_out_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    ddt_ptr += (\n        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\n    )\n    ddA_cumsum_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_c * stride_ddA_cs_chunk\n        + pid_h * stride_ddA_cs_head\n    )\n    if HAS_D:\n        x_ptr += (\n            pid_b * stride_x_batch\n            + pid_c * chunk_size * stride_x_seqlen\n            + pid_h * stride_x_head\n        )\n        dD_ptr += (\n            pid_b * stride_dD_batch\n            + pid_c * stride_dD_chunk\n            + pid_h * stride_dD_head\n            + pid_m * stride_dD_csize\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim\n    )\n    out_ptrs = out_ptr + (\n        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim\n    )\n    if HAS_D:\n        x_ptrs = x_ptr + (\n            offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n        )\n        if D_HAS_HDIM:\n            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dout = tl.load(\n        dout_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    out = tl.load(\n        out_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    if HAS_D:\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n            other=0.0,\n        ).to(tl.float32)\n        if D_HAS_HDIM:\n            dD = tl.sum(dout * x, axis=0)\n            tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n            D = tl.load(\n                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0\n            ).to(tl.float32)\n        else:\n            dD = tl.sum(dout * x)\n            tl.store(dD_ptr, dD)\n            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n        out -= x * D\n    ddA_cs = tl.sum(dout * out, axis=1)\n    if SUBTRACT_DDTDT:\n        dt = tl.load(\n            dt_ptr + offs_m * stride_dt_csize, mask=offs_m < chunk_size, other=0.0\n        ).to(tl.float32)\n        ddt = tl.load(\n            ddt_ptr + offs_m * stride_ddt_csize, mask=offs_m < chunk_size, other=0.0\n        ).to(tl.float32)\n        ddA_cs -= dt * ddt\n    tl.store(\n        ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 63, 6, 108, 290, 63, 6, 108, 291, 63, 6, 108, 292, 63, 6, 108, 293, 63, 6, 175, 63, 32, -1, 294, 194, 171, 243, 295, 194, 348, 175, 32, 296, 194, 294, 48, 257, 32, 297, 194, 294, 4, 296, 244, 257, 32, 298, 194, 171, 243, 295, 194, 349, 175, 32, 299, 194, 171, 243, 295, 194, 350, 175, 32, 247, 172, 297, 244, 259, 74, 296, 244, 255, 244, 260, 74, 298, 244, 261, 32, 248, 172, 297, 244, 263, 74, 296, 244, 255, 244, 264, 74, 298, 244, 265, 32, 249, 172, 297, 244, 267, 74, 296, 244, 268, 74, 298, 244, 269, 32, 250, 172, 297, 244, 271, 74, 296, 244, 272, 74, 298, 244, 273, 32, 253, 172, 297, 244, 280, 74, 296, 244, 281, 74, 298, 244, 282, 32, 182, 289, 63, 32, 251, 172, 297, 244, 275, 74, 296, 244, 255, 244, 276, 74, 298, 244, 277, 32, 254, 172, 297, 244, 284, 74, 296, 244, 285, 74, 298, 244, 286, 74, 299, 244, 287, 32, 185, 32, 300, 194, 299, 244, 292, 74, 76, 243, 350, 108, 292, 175, 32, 301, 194, 76, 243, 350, 108, 293, 175, 32, 302, 194, 247, 74, 243, 300, 230, 63, 108, 202, 27, 244, 260, 74, 301, 230, 202, 108, 63, 27, 244, 262, 175, 32, 303, 194, 248, 74, 243, 300, 230, 63, 108, 202, 27, 244, 264, 74, 301, 230, 202, 108, 63, 27, 244, 266, 175, 32, 182, 289, 63, 32, 304, 194, 251, 74, 243, 300, 230, 63, 108, 202, 27, 244, 276, 74, 301, 230, 202, 108, 63, 27, 244, 278, 175, 32, 182, 290, 63, 32, 305, 194, 254, 74, 301, 244, 288, 32, 185, 32, 185, 32, 306, 194, 39, 243, 255, 108, 258, 4, 296, 244, 255, 175, 32, 307, 194, 57, 243, 302, 108, 308, 194, 243, 300, 230, 63, 108, 202, 27, 1, 306, 175, 173, 243, 301, 230, 202, 108, 63, 27, 1, 256, 175, 108, 309, 194, 350, 175, 82, 310, 243, 145, 175, 32, 14, 194, 57, 243, 303, 108, 308, 194, 243, 300, 230, 63, 108, 202, 27, 1, 306, 175, 173, 243, 301, 230, 202, 108, 63, 27, 1, 256, 175, 108, 309, 194, 350, 175, 82, 310, 243, 145, 175, 32, 182, 289, 63, 32, 311, 194, 57, 243, 304, 108, 308, 194, 243, 300, 230, 63, 108, 202, 27, 1, 306, 175, 173, 243, 301, 230, 202, 108, 63, 27, 1, 256, 175, 108, 309, 194, 350, 175, 82, 310, 243, 145, 175, 32, 182, 290, 63, 32, 312, 194, 221, 243, 307, 244, 311, 108, 295, 194, 350, 175, 32, 10, 243, 305, 108, 312, 108, 308, 194, 301, 1, 256, 175, 32, 313, 194, 57, 243, 252, 74, 298, 244, 279, 74, 301, 108, 308, 194, 301, 1, 256, 108, 309, 194, 350, 175, 82, 310, 243, 145, 175, 32, 185, 32, 30, 63, 32, 312, 194, 221, 243, 307, 244, 311, 175, 32, 10, 243, 254, 108, 312, 175, 32, 313, 194, 57, 243, 252, 74, 298, 244, 279, 175, 82, 310, 243, 145, 175, 32, 56, 32, 14, 2, 311, 244, 313, 32, 185, 32, 314, 194, 221, 243, 307, 244, 14, 108, 295, 194, 348, 175, 32, 182, 291, 63, 32, 315, 194, 57, 243, 249, 74, 300, 244, 270, 108, 308, 194, 300, 1, 255, 108, 309, 194, 350, 175, 82, 310, 243, 145, 175, 32, 316, 194, 57, 243, 250, 74, 300, 244, 274, 108, 308, 194, 300, 1, 255, 108, 309, 194, 350, 175, 82, 310, 243, 145, 175, 32, 314, 2, 315, 244, 316, 32, 185, 32, 10, 243, 253, 74, 300, 244, 283, 108, 314, 108, 308, 194, 300, 1, 255, 175, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_ddAcs_stable_kernel_old(\n    x_ptr,\n    dout_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    cb_ptr,\n    ddAcs_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_n,\n    stride_ddAcs_batch,\n    stride_ddAcs_chunk,\n    stride_ddAcs_head,\n    stride_ddAcs_csize_m,\n    stride_ddAcs_csize_n,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim\n    )\n    x_ptrs = x_ptr + (\n        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim\n    )\n    dt_ptrs = dt_ptr + offs_n * stride_dt_csize\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n\n    )\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    chunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)\n\n    dout = tl.load(\n        dout_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n        other=0.0,\n    )\n    x = tl.load(\n        x_ptrs,\n        mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit_n),\n        other=0.0,\n    )\n    acc = tl.dot(dout, x)\n    cb = tl.load(\n        cb_ptrs,\n        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),\n        other=0.0,\n    ).to(tl.float32)\n    acc *= cb\n    dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)\n    acc *= dt_n\n    dA_cs_m = tl.load(\n        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0\n    ).to(tl.float32)\n    dA_cs_n = tl.load(\n        dA_cumsum_ptr + offs_n * stride_dA_cs_csize, mask=offs_n < chunk_size, other=0.0\n    ).to(tl.float32)\n    acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n    mask = offs_m[:, None] >= offs_n[None, :] + 1\n    acc = tl.where(mask, acc, 0.0)\n    acc = tl.cumsum(acc, axis=1)\n    acc = tl.where(mask, acc, 0.0)\n    ddA_cs = tl.sum(acc, axis=0)\n    ddAcs_ptr += (\n        pid_b * stride_ddAcs_batch\n        + pid_c * stride_ddAcs_chunk\n        + pid_h * stride_ddAcs_head\n        + pid_m * stride_ddAcs_csize_m\n    )\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    ddAcs_ptrs = ddAcs_ptr + offs_n * stride_ddAcs_csize_n\n    tl.store(ddAcs_ptrs + stride_ddAcs_csize_n, ddA_cs, mask=offs_n < chunk_size - 1)\n    tl.store(ddAcs_ptr, 0.0)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 62, 6, 108, 285, 62, 6, 108, 286, 62, 6, 175, 62, 32, -1, 287, 194, 171, 243, 288, 194, 348, 175, 32, 289, 194, 287, 48, 255, 32, 290, 194, 287, 4, 289, 244, 255, 32, 291, 194, 171, 243, 288, 194, 349, 175, 32, 292, 194, 64, 243, 253, 108, 285, 175, 32, 293, 194, 171, 243, 288, 194, 350, 175, 48, 292, 32, 294, 194, 171, 243, 288, 194, 350, 175, 226, 292, 32, 247, 172, 290, 244, 258, 74, 289, 244, 253, 244, 259, 74, 291, 244, 260, 32, 248, 172, 290, 244, 262, 74, 289, 244, 253, 244, 263, 74, 291, 244, 264, 32, 249, 172, 290, 244, 266, 74, 289, 244, 267, 74, 291, 244, 268, 32, 250, 172, 290, 244, 270, 74, 289, 244, 271, 74, 291, 244, 272, 32, 251, 172, 290, 244, 274, 74, 289, 244, 275, 74, 291, 48, 257, 244, 276, 32, 295, 194, 293, 244, 284, 74, 76, 243, 350, 108, 284, 175, 32, 296, 194, 294, 244, 285, 74, 76, 243, 350, 108, 285, 175, 32, 297, 194, 76, 243, 350, 108, 286, 175, 32, 298, 194, 248, 74, 243, 295, 230, 62, 108, 202, 27, 244, 263, 74, 297, 230, 202, 108, 62, 27, 244, 265, 175, 32, 299, 194, 247, 74, 243, 296, 230, 202, 108, 62, 27, 244, 259, 74, 297, 230, 62, 108, 202, 27, 244, 261, 175, 32, 300, 194, 249, 74, 296, 244, 269, 32, 301, 194, 251, 74, 243, 295, 230, 62, 108, 202, 27, 244, 277, 74, 296, 230, 202, 108, 62, 27, 244, 278, 175, 32, 302, 194, 39, 243, 253, 108, 256, 4, 289, 244, 253, 175, 32, 303, 194, 39, 243, 302, 108, 243, 293, 74, 348, 175, 244, 284, 175, 32, 304, 194, 55, 243, 298, 108, 305, 194, 243, 295, 230, 62, 108, 202, 27, 1, 302, 175, 173, 243, 297, 230, 202, 108, 62, 27, 1, 254, 175, 108, 306, 194, 350, 175, 32, 307, 194, 55, 243, 299, 108, 305, 194, 243, 297, 230, 62, 108, 202, 27, 1, 254, 175, 173, 243, 296, 230, 202, 108, 62, 27, 1, 303, 175, 108, 306, 194, 350, 175, 32, 308, 194, 15, 243, 304, 108, 307, 175, 32, 309, 194, 55, 243, 301, 108, 305, 194, 243, 295, 230, 62, 108, 202, 27, 1, 253, 175, 173, 243, 296, 230, 202, 108, 62, 27, 1, 253, 175, 108, 306, 194, 350, 175, 82, 310, 243, 145, 175, 32, 308, 24, 309, 32, 311, 194, 55, 243, 300, 108, 305, 194, 296, 1, 253, 108, 306, 194, 350, 175, 82, 310, 243, 145, 175, 32, 308, 24, 311, 32, 312, 194, 55, 243, 250, 74, 295, 244, 273, 108, 305, 194, 295, 1, 253, 108, 306, 194, 350, 175, 82, 310, 243, 145, 175, 32, 313, 194, 55, 243, 250, 74, 296, 244, 273, 108, 305, 194, 296, 1, 253, 108, 306, 194, 350, 175, 82, 310, 243, 145, 175, 32, 308, 24, 107, 243, 312, 230, 62, 108, 202, 27, 4, 313, 230, 202, 108, 62, 27, 175, 32, 305, 194, 295, 230, 62, 108, 202, 27, 144, 296, 230, 202, 108, 62, 27, 74, 348, 32, 308, 194, 205, 243, 305, 108, 308, 108, 350, 175, 32, 308, 194, 85, 243, 308, 108, 288, 194, 348, 175, 32, 308, 194, 205, 243, 305, 108, 308, 108, 350, 175, 32, 314, 194, 221, 243, 308, 108, 288, 194, 350, 175, 32, 252, 172, 290, 244, 279, 74, 289, 244, 280, 74, 291, 244, 281, 74, 293, 244, 282, 32, 296, 194, 294, 244, 285, 74, 76, 243, 350, 108, 285, 175, 32, 315, 194, 252, 74, 296, 244, 283, 32, 10, 243, 315, 74, 283, 108, 314, 108, 305, 194, 296, 1, 253, 4, 348, 175, 32, 10, 243, 252, 108, 350, 175, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_ddAcs_stable_kernel(\n    x_ptr,\n    dout_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    cb_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    hdim,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_n,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize_m,\n    stride_ddA_cs_csize_n,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n    ddA_cumsum_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_c * stride_ddA_cs_chunk\n        + pid_h * stride_ddA_cs_head\n        + pid_m * stride_ddA_cs_csize_m\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim\n    )\n    x_ptrs = x_ptr + (\n        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim\n    )\n    dt_ptrs = dt_ptr + offs_n * stride_dt_csize\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n\n    )\n    ddAcs_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n\n    tl.store(ddA_cumsum_ptr, 0.0)\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    dout = tl.load(\n        dout_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n        other=0.0,\n    )\n    dA_cs_m = tl.load(\n        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0\n    ).to(tl.float32)\n\n    lo, hi = 0, (pid_m + 1) * BLOCK_SIZE_M\n\n    for start_n in range(lo, hi, BLOCK_SIZE_N):\n        start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)\n\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_k[:, None] < hdim)\n            & (offs_n[None, :] < chunk_size_limit - start_n),\n            other=0.0,\n        )\n        acc = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0).to(\n            tl.float32\n        )\n        acc *= dt_n\n\n        cb = tl.load(\n            cb_ptrs,\n            mask=(offs_m[:, None] < chunk_size)\n            & (offs_n[None, :] < chunk_size - start_n),\n            other=0.0,\n        ).to(tl.float32)\n        acc *= cb\n        dA_cs_n = tl.load(\n            dA_cumsum_ptr + (start_n + offs_n) * stride_dA_cs_csize,\n            mask=offs_n < chunk_size - start_n,\n            other=0.0,\n        ).to(tl.float32)\n        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        mask = offs_m[:, None] >= start_n + offs_n[None, :] + 1\n        acc = tl.where(mask, acc, 0.0)\n        rowsum_new = rowsum + tl.sum(acc, axis=1)\n        acc = rowsum[:, None] + tl.cumsum(acc, axis=1)\n        rowsum = rowsum_new\n        acc = tl.where(mask, acc, 0.0)\n\n        ddA_cs = tl.sum(acc, axis=0)\n        tl.store(\n            ddAcs_ptrs + stride_ddA_cs_csize_n,\n            ddA_cs,\n            mask=offs_n < chunk_size - start_n - 1,\n        )\n        x_ptrs += BLOCK_SIZE_N * stride_x_seqlen\n        dt_ptrs += BLOCK_SIZE_N * stride_dt_csize\n        cb_ptrs += BLOCK_SIZE_N * stride_cb_csize_n\n        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n\n\n    for start_n in range(hi, chunk_size, BLOCK_SIZE_N):\n        tl.store(\n            ddAcs_ptrs + stride_ddA_cs_csize_n,\n            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),\n            mask=offs_n < chunk_size - start_n - 1,\n        )\n        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 63, 6, 108, 285, 63, 6, 108, 286, 63, 6, 175, 63, 32, -1, 287, 194, 171, 243, 288, 194, 348, 175, 32, 289, 194, 287, 48, 255, 32, 290, 194, 287, 4, 289, 244, 255, 32, 291, 194, 171, 243, 288, 194, 349, 175, 32, 292, 194, 171, 243, 288, 194, 350, 175, 32, 247, 172, 290, 244, 258, 74, 289, 244, 253, 244, 259, 74, 291, 244, 260, 32, 248, 172, 290, 244, 262, 74, 289, 244, 253, 244, 263, 74, 291, 244, 264, 32, 249, 172, 290, 244, 266, 74, 289, 244, 267, 74, 291, 244, 268, 32, 250, 172, 290, 244, 270, 74, 289, 244, 271, 74, 291, 244, 272, 32, 251, 172, 290, 244, 274, 74, 289, 244, 275, 74, 291, 48, 257, 244, 276, 32, 252, 172, 290, 244, 279, 74, 289, 244, 280, 74, 291, 244, 281, 74, 292, 244, 282, 32, 293, 194, 292, 244, 284, 74, 76, 243, 350, 108, 284, 175, 32, 294, 194, 76, 243, 350, 108, 285, 175, 32, 295, 194, 76, 243, 350, 108, 286, 175, 32, 296, 194, 248, 74, 243, 293, 230, 63, 108, 202, 27, 244, 263, 74, 295, 230, 202, 108, 63, 27, 244, 265, 175, 32, 297, 194, 247, 74, 243, 294, 230, 202, 108, 63, 27, 244, 259, 74, 295, 230, 63, 108, 202, 27, 244, 261, 175, 32, 298, 194, 249, 74, 294, 244, 269, 32, 299, 194, 251, 74, 243, 293, 230, 63, 108, 202, 27, 244, 277, 74, 294, 230, 202, 108, 63, 27, 244, 278, 175, 32, 300, 194, 252, 74, 294, 244, 283, 32, 10, 243, 252, 108, 350, 175, 32, 301, 194, 39, 243, 253, 108, 256, 4, 289, 244, 253, 175, 32, 302, 194, 176, 243, 243, 284, 108, 175, 108, 91, 194, 145, 175, 32, 303, 194, 57, 243, 296, 108, 304, 194, 243, 293, 230, 63, 108, 202, 27, 1, 301, 175, 173, 243, 295, 230, 202, 108, 63, 27, 1, 254, 175, 108, 305, 194, 350, 175, 32, 306, 194, 57, 243, 250, 74, 293, 244, 273, 108, 304, 194, 293, 1, 253, 108, 305, 194, 350, 175, 82, 307, 243, 145, 175, 32, 308, 108, 309, 194, 243, 350, 108, 243, 292, 74, 348, 175, 244, 284, 175, 32, 135, 310, 157, 5, 243, 308, 108, 309, 108, 285, 175, 63, 32, 310, 194, 55, 243, 310, 108, 285, 175, 32, 311, 194, 57, 243, 297, 108, 304, 194, 243, 295, 230, 63, 108, 202, 27, 1, 254, 175, 173, 243, 294, 230, 202, 108, 63, 27, 1, 301, 4, 310, 175, 108, 305, 194, 350, 175, 32, 312, 194, 15, 243, 303, 108, 311, 175, 32, 313, 194, 57, 243, 298, 108, 304, 194, 294, 1, 253, 4, 310, 108, 305, 194, 350, 175, 82, 307, 243, 145, 175, 32, 312, 24, 313, 32, 314, 194, 57, 243, 299, 108, 304, 194, 243, 293, 230, 63, 108, 202, 27, 1, 253, 175, 173, 243, 294, 230, 202, 108, 63, 27, 1, 253, 4, 310, 175, 108, 305, 194, 350, 175, 82, 307, 243, 145, 175, 32, 312, 24, 314, 32, 315, 194, 57, 243, 250, 74, 243, 310, 74, 294, 175, 244, 273, 108, 304, 194, 294, 1, 253, 4, 310, 108, 305, 194, 350, 175, 82, 307, 243, 145, 175, 32, 312, 24, 107, 243, 306, 230, 63, 108, 202, 27, 4, 315, 230, 202, 108, 63, 27, 175, 32, 304, 194, 293, 230, 63, 108, 202, 27, 144, 310, 74, 294, 230, 202, 108, 63, 27, 74, 348, 32, 312, 194, 205, 243, 304, 108, 312, 108, 350, 175, 32, 316, 194, 302, 74, 221, 243, 312, 108, 288, 194, 348, 175, 32, 312, 194, 302, 230, 63, 108, 202, 27, 74, 85, 243, 312, 108, 288, 194, 348, 175, 32, 302, 194, 316, 32, 312, 194, 205, 243, 304, 108, 312, 108, 350, 175, 32, 317, 194, 221, 243, 312, 108, 288, 194, 350, 175, 32, 10, 243, 300, 74, 283, 108, 317, 108, 304, 194, 294, 1, 253, 4, 310, 4, 348, 175, 32, 297, 172, 285, 244, 259, 32, 298, 172, 285, 244, 269, 32, 299, 172, 285, 244, 278, 32, 300, 172, 285, 244, 283, 32, 78, 32, 135, 310, 157, 5, 243, 309, 108, 253, 108, 285, 175, 63, 32, 10, 243, 300, 74, 283, 108, 176, 243, 243, 285, 108, 175, 108, 91, 194, 145, 175, 108, 304, 194, 294, 1, 253, 4, 310, 4, 348, 175, 32, 300, 172, 285, 244, 283, 32, 78, 32, 3, 32]}, {"code": "def _chunk_scan_bwd_ddAcs_prev_kernel(\n    dout_ptr,\n    prev_states_ptr,\n    C_ptr,\n    dA_cumsum_ptr,\n    seq_idx_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    dstate,\n    hdim,\n    batch,\n    seqlen,\n    nchunks,\n    nheads_ngroups_ratio,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_prev_states_batch,\n    stride_prev_states_chunk,\n    stride_prev_states_head,\n    stride_prev_states_hdim,\n    stride_prev_states_dstate,\n    stride_C_batch,\n    stride_C_seqlen,\n    stride_C_head,\n    stride_C_dstate,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    prev_states_ptr += (\n        pid_b * stride_prev_states_batch\n        + pid_c * stride_prev_states_chunk\n        + pid_h * stride_prev_states_head\n    )\n    C_ptr += (\n        pid_b * stride_C_batch\n        + pid_c * chunk_size * stride_C_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_C_head\n    )\n    ddA_cumsum_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_c * stride_ddA_cs_chunk\n        + pid_h * stride_ddA_cs_head\n    )\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim\n    )\n    prev_states_ptrs = prev_states_ptr + (\n        offs_n[None, :] * stride_prev_states_dstate\n        + offs_k[:, None] * stride_prev_states_hdim\n    )\n    C_ptrs = C_ptr + (\n        offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate\n    )\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dout = tl.load(\n        dout_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n        other=0.0,\n    )\n    prev_states = tl.load(\n        prev_states_ptrs,\n        mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),\n        other=0.0,\n    )\n    prev_states = prev_states.to(dout_ptrs.dtype.element_ty)\n    acc = tl.dot(dout, prev_states)\n    c = tl.load(\n        C_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),\n        other=0.0,\n    ).to(tl.float32)\n    ddA_cs = tl.sum(acc * c, axis=1)\n    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(\n        tl.float32\n    )\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_m)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(\n            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0\n        )\n        seq_idx_m = tl.load(\n            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit,\n            other=-1,\n        )\n        scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n    ddA_cs *= scale\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 62, 6, 108, 284, 62, 6, 108, 285, 62, 6, 108, 286, 62, 6, 175, 62, 32, -1, 287, 194, 171, 243, 288, 194, 348, 175, 32, 289, 194, 287, 48, 256, 32, 290, 194, 287, 4, 289, 244, 256, 32, 291, 194, 171, 243, 288, 194, 349, 175, 32, 292, 194, 64, 243, 254, 108, 285, 175, 32, 293, 194, 171, 243, 288, 194, 350, 175, 48, 292, 32, 294, 194, 171, 243, 288, 194, 350, 175, 226, 292, 32, 247, 172, 290, 244, 260, 74, 289, 244, 253, 244, 261, 74, 291, 244, 262, 32, 248, 172, 290, 244, 264, 74, 289, 244, 265, 74, 291, 244, 266, 32, 249, 172, 290, 244, 269, 74, 289, 244, 253, 244, 270, 74, 291, 48, 259, 244, 271, 32, 252, 172, 290, 244, 279, 74, 289, 244, 280, 74, 291, 244, 281, 32, 250, 172, 290, 244, 273, 74, 289, 244, 274, 74, 291, 244, 275, 32, 182, 283, 62, 32, 251, 172, 290, 244, 277, 74, 289, 244, 253, 244, 278, 32, 185, 32, 295, 194, 293, 244, 284, 74, 76, 243, 350, 108, 284, 175, 32, 296, 194, 294, 244, 285, 74, 76, 243, 350, 108, 285, 175, 32, 297, 194, 76, 243, 350, 108, 286, 175, 32, 298, 194, 247, 74, 243, 295, 230, 62, 108, 202, 27, 244, 261, 74, 297, 230, 202, 108, 62, 27, 244, 263, 175, 32, 299, 194, 248, 74, 243, 296, 230, 202, 108, 62, 27, 244, 268, 74, 297, 230, 62, 108, 202, 27, 244, 267, 175, 32, 300, 194, 249, 74, 243, 295, 230, 62, 108, 202, 27, 244, 270, 74, 296, 230, 202, 108, 62, 27, 244, 272, 175, 32, 301, 194, 250, 74, 295, 244, 276, 32, 302, 194, 39, 243, 253, 108, 257, 4, 289, 244, 253, 175, 32, 303, 194, 55, 243, 298, 108, 304, 194, 243, 295, 230, 62, 108, 202, 27, 1, 302, 175, 173, 243, 297, 230, 202, 108, 62, 27, 1, 255, 175, 108, 305, 194, 350, 175, 32, 306, 194, 55, 243, 299, 108, 304, 194, 243, 297, 230, 62, 108, 202, 27, 1, 255, 175, 173, 243, 296, 230, 202, 108, 62, 27, 1, 254, 175, 108, 305, 194, 350, 175, 32, 306, 194, 306, 82, 307, 243, 298, 82, 91, 82, 114, 175, 32, 308, 194, 15, 243, 303, 108, 306, 175, 32, 309, 194, 55, 243, 300, 108, 304, 194, 243, 295, 230, 62, 108, 202, 27, 1, 302, 175, 173, 243, 296, 230, 202, 108, 62, 27, 1, 254, 175, 108, 305, 194, 350, 175, 82, 307, 243, 145, 175, 32, 310, 194, 221, 243, 308, 244, 309, 108, 288, 194, 348, 175, 32, 311, 194, 55, 243, 301, 108, 304, 194, 295, 1, 302, 108, 305, 194, 350, 175, 82, 307, 243, 145, 175, 32, 182, 63, 283, 62, 32, 312, 194, 107, 243, 311, 175, 32, 185, 32, 182, 283, 62, 32, 313, 194, 55, 243, 251, 4, 278, 108, 304, 194, 289, 144, 348, 108, 305, 194, 350, 175, 32, 314, 194, 55, 243, 251, 74, 295, 244, 278, 108, 304, 194, 295, 1, 302, 108, 305, 194, 4, 348, 175, 32, 312, 194, 205, 243, 314, 77, 313, 108, 107, 243, 311, 175, 108, 350, 175, 32, 185, 32, 310, 24, 312, 32, 296, 194, 294, 244, 285, 74, 76, 243, 350, 108, 285, 175, 32, 315, 194, 252, 74, 295, 244, 282, 32, 196, 243, 315, 108, 310, 108, 304, 194, 295, 1, 253, 175, 32, 3, 32]}, {"code": "def _chunk_cumsum_fwd_kernel(\n    dt_ptr,\n    A_ptr,\n    dt_bias_ptr,\n    dt_out_ptr,\n    dA_cumsum_ptr,\n    batch,\n    seqlen,\n    nheads,\n    chunk_size,\n    dt_min,\n    dt_max,\n    stride_dt_batch,\n    stride_dt_seqlen,\n    stride_dt_head,\n    stride_A_head,\n    stride_dt_bias_head,\n    stride_dt_out_batch,\n    stride_dt_out_chunk,\n    stride_dt_out_head,\n    stride_dt_out_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    DT_SOFTPLUS: tl.constexpr,\n    HAS_DT_BIAS: tl.constexpr,\n    BLOCK_SIZE_H: tl.constexpr,\n    BLOCK_SIZE_CHUNK: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=0)\n    pid_c = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\n    dt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk\n    dA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk\n\n    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\n    dt_ptrs = dt_ptr + (\n        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen\n    )\n    A_ptrs = A_ptr + offs_h * stride_A_head\n    dt_out_ptrs = dt_out_ptr + (\n        offs_h[:, None] * stride_dt_out_head + offs_c[None, :] * stride_dt_out_csize\n    )\n    dA_cs_ptrs = dA_cumsum_ptr + (\n        offs_h[:, None] * stride_dA_cs_head + offs_c[None, :] * stride_dA_cs_csize\n    )\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    dt = tl.load(\n        dt_ptrs,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),\n        other=0.0,\n    ).to(tl.float32)\n    if HAS_DT_BIAS:\n        dt_bias = tl.load(\n            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0\n        ).to(tl.float32)\n        dt += dt_bias[:, None]\n    if DT_SOFTPLUS:\n        dt = softplus(dt)\n\n    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\n    dt = tl.where(\n        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0\n    )\n    tl.store(\n        dt_out_ptrs,\n        dt,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),\n    )\n    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)\n    dA = dt * A[:, None]\n    dA_cs = tl.cumsum(dA, axis=1)\n    tl.store(\n        dA_cs_ptrs,\n        dA_cs,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 63, 6, 108, 272, 63, 6, 108, 273, 63, 6, 108, 274, 63, 6, 175, 63, 32, -1, 275, 194, 171, 243, 276, 194, 348, 175, 32, 277, 194, 171, 243, 276, 194, 349, 175, 32, 278, 194, 171, 243, 276, 194, 350, 175, 32, 247, 172, 275, 244, 258, 74, 277, 244, 255, 244, 259, 32, 250, 172, 275, 244, 263, 74, 277, 244, 264, 32, 251, 172, 275, 244, 267, 74, 277, 244, 268, 32, 279, 194, 278, 244, 273, 74, 76, 243, 348, 108, 273, 175, 32, 280, 194, 76, 243, 348, 108, 274, 175, 32, 281, 194, 247, 74, 243, 279, 230, 63, 108, 202, 27, 244, 260, 74, 280, 230, 202, 108, 63, 27, 244, 259, 175, 32, 282, 194, 248, 74, 279, 244, 261, 32, 283, 194, 250, 74, 243, 279, 230, 63, 108, 202, 27, 244, 265, 74, 280, 230, 202, 108, 63, 27, 244, 266, 175, 32, 284, 194, 251, 74, 243, 279, 230, 63, 108, 202, 27, 244, 269, 74, 280, 230, 202, 108, 63, 27, 244, 270, 175, 32, 285, 194, 39, 243, 255, 108, 253, 4, 277, 244, 255, 175, 32, 286, 194, 57, 243, 281, 108, 287, 194, 243, 279, 230, 63, 108, 202, 27, 1, 254, 175, 173, 243, 280, 230, 202, 108, 63, 27, 1, 285, 175, 108, 288, 194, 348, 175, 82, 289, 243, 145, 175, 32, 182, 272, 63, 32, 290, 194, 57, 243, 249, 74, 279, 244, 262, 108, 287, 194, 279, 1, 254, 108, 288, 194, 348, 175, 82, 289, 243, 145, 175, 32, 286, 172, 290, 230, 63, 108, 202, 27, 32, 185, 32, 182, 271, 63, 32, 286, 194, 291, 243, 286, 175, 32, 185, 32, 286, 194, 67, 243, 192, 243, 286, 108, 256, 175, 108, 257, 175, 32, 286, 194, 205, 243, 243, 279, 230, 63, 108, 202, 27, 1, 254, 175, 173, 243, 280, 230, 202, 108, 63, 27, 1, 285, 175, 108, 286, 108, 348, 175, 32, 10, 243, 283, 108, 286, 108, 287, 194, 243, 279, 230, 63, 108, 202, 27, 1, 254, 175, 173, 243, 280, 230, 202, 108, 63, 27, 1, 255, 175, 175, 32, 292, 194, 57, 243, 282, 108, 287, 194, 279, 1, 254, 108, 288, 194, 348, 175, 82, 289, 243, 145, 175, 32, 293, 194, 286, 244, 292, 230, 63, 108, 202, 27, 32, 294, 194, 85, 243, 293, 108, 276, 194, 349, 175, 32, 10, 243, 284, 108, 294, 108, 287, 194, 243, 279, 230, 63, 108, 202, 27, 1, 254, 175, 173, 243, 280, 230, 202, 108, 63, 27, 1, 255, 175, 175, 32, 3, 32]}, {"code": "def _chunk_cumsum_bwd_kernel(\n    ddA_ptr,\n    ddt_out_ptr,\n    dt_ptr,\n    A_ptr,\n    dt_bias_ptr,\n    ddt_ptr,\n    dA_ptr,\n    ddt_bias_ptr,\n    batch,\n    seqlen,\n    nheads,\n    chunk_size,\n    dt_min,\n    dt_max,\n    stride_ddA_batch,\n    stride_ddA_chunk,\n    stride_ddA_head,\n    stride_ddA_csize,\n    stride_ddt_out_batch,\n    stride_ddt_out_chunk,\n    stride_ddt_out_head,\n    stride_ddt_out_csize,\n    stride_dt_batch,\n    stride_dt_seqlen,\n    stride_dt_head,\n    stride_A_head,\n    stride_dt_bias_head,\n    stride_ddt_batch,\n    stride_ddt_seqlen,\n    stride_ddt_head,\n    stride_dA_head,\n    stride_ddt_bias_head,\n    DT_SOFTPLUS: tl.constexpr,\n    HAS_DT_BIAS: tl.constexpr,\n    BLOCK_SIZE_H: tl.constexpr,\n    BLOCK_SIZE_CHUNK: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=0)\n    pid_c = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    ddt_out_ptr += pid_b * stride_ddt_out_batch + pid_c * stride_ddt_out_chunk\n    ddA_ptr += pid_b * stride_ddA_batch + pid_c * stride_ddA_chunk\n    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\n    ddt_ptr += pid_b * stride_ddt_batch + pid_c * chunk_size * stride_ddt_seqlen\n\n    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\n    ddt_out_ptrs = ddt_out_ptr + (\n        offs_h[:, None] * stride_ddt_out_head + offs_c[None, :] * stride_ddt_out_csize\n    )\n    ddA_ptrs = ddA_ptr + (\n        offs_h[:, None] * stride_ddA_head + offs_c[None, :] * stride_ddA_csize\n    )\n    dt_ptrs = dt_ptr + (\n        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen\n    )\n    ddt_ptrs = ddt_ptr + (\n        offs_h[:, None] * stride_ddt_head + offs_c[None, :] * stride_ddt_seqlen\n    )\n    A_ptrs = A_ptr + offs_h * stride_A_head\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    ddA = tl.load(\n        ddA_ptrs,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),\n        other=0.0,\n    ).to(tl.float32)\n    ddt_out = tl.load(\n        ddt_out_ptrs,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),\n        other=0.0,\n    ).to(tl.float32)\n    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)\n    ddt = ddA * A[:, None] + ddt_out\n    dt = tl.load(\n        dt_ptrs,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),\n        other=0.0,\n    ).to(tl.float32)\n    if HAS_DT_BIAS:\n        dt_bias = tl.load(\n            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0\n        ).to(tl.float32)\n        dt += dt_bias[:, None]\n    if DT_SOFTPLUS:\n        dt_presoftplus = dt\n        dt = softplus(dt)\n    clamp_mask = (dt < dt_min) | (dt > dt_max)\n\n    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\n    dt = tl.where(\n        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0\n    )\n    ddt = tl.where(\n        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), ddt, 0.0\n    )\n    ddt = tl.where(clamp_mask, 0.0, ddt)\n    if DT_SOFTPLUS:\n        ddt = tl.where(dt_presoftplus <= 20.0, ddt * tl.sigmoid(dt_presoftplus), ddt)\n    tl.store(\n        ddt_ptrs,\n        ddt,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),\n    )\n    dA = tl.sum(ddA * dt, axis=1)\n    tl.atomic_add(dA_ptr + offs_h * stride_dA_head, dA, mask=offs_h < nheads)\n    if HAS_DT_BIAS:\n        ddt_bias = tl.sum(ddt, axis=1)\n        tl.atomic_add(\n            ddt_bias_ptr + offs_h * stride_ddt_bias_head, ddt_bias, mask=offs_h < nheads\n        )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 62, 6, 108, 280, 62, 6, 108, 281, 62, 6, 108, 282, 62, 6, 175, 62, 32, -1, 283, 194, 171, 243, 284, 194, 348, 175, 32, 285, 194, 171, 243, 284, 194, 349, 175, 32, 286, 194, 171, 243, 284, 194, 350, 175, 32, 248, 172, 283, 244, 265, 74, 285, 244, 266, 32, 247, 172, 283, 244, 261, 74, 285, 244, 262, 32, 249, 172, 283, 244, 269, 74, 285, 244, 258, 244, 270, 32, 252, 172, 283, 244, 274, 74, 285, 244, 258, 244, 275, 32, 287, 194, 286, 244, 281, 74, 76, 243, 348, 108, 281, 175, 32, 288, 194, 76, 243, 348, 108, 282, 175, 32, 289, 194, 248, 74, 243, 287, 230, 62, 108, 202, 27, 244, 267, 74, 288, 230, 202, 108, 62, 27, 244, 268, 175, 32, 290, 194, 247, 74, 243, 287, 230, 62, 108, 202, 27, 244, 263, 74, 288, 230, 202, 108, 62, 27, 244, 264, 175, 32, 291, 194, 249, 74, 243, 287, 230, 62, 108, 202, 27, 244, 271, 74, 288, 230, 202, 108, 62, 27, 244, 270, 175, 32, 292, 194, 252, 74, 243, 287, 230, 62, 108, 202, 27, 244, 276, 74, 288, 230, 202, 108, 62, 27, 244, 275, 175, 32, 293, 194, 250, 74, 287, 244, 272, 32, 294, 194, 39, 243, 258, 108, 256, 4, 285, 244, 258, 175, 32, 295, 194, 55, 243, 290, 108, 296, 194, 243, 287, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 288, 230, 202, 108, 62, 27, 1, 294, 175, 108, 297, 194, 348, 175, 82, 298, 243, 145, 175, 32, 299, 194, 55, 243, 289, 108, 296, 194, 243, 287, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 288, 230, 202, 108, 62, 27, 1, 294, 175, 108, 297, 194, 348, 175, 82, 298, 243, 145, 175, 32, 300, 194, 55, 243, 293, 108, 296, 194, 287, 1, 257, 108, 297, 194, 348, 175, 82, 298, 243, 145, 175, 32, 301, 194, 295, 244, 300, 230, 62, 108, 202, 27, 74, 299, 32, 302, 194, 55, 243, 291, 108, 296, 194, 243, 287, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 288, 230, 202, 108, 62, 27, 1, 294, 175, 108, 297, 194, 348, 175, 82, 298, 243, 145, 175, 32, 182, 280, 62, 32, 303, 194, 55, 243, 251, 74, 287, 244, 273, 108, 296, 194, 287, 1, 257, 108, 297, 194, 348, 175, 82, 298, 243, 145, 175, 32, 302, 172, 303, 230, 62, 108, 202, 27, 32, 185, 32, 182, 279, 62, 32, 304, 194, 302, 32, 302, 194, 305, 243, 302, 175, 32, 185, 32, 306, 194, 243, 302, 1, 259, 175, 152, 243, 302, 124, 260, 175, 32, 302, 194, 66, 243, 192, 243, 302, 108, 259, 175, 108, 260, 175, 32, 302, 194, 205, 243, 243, 287, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 288, 230, 202, 108, 62, 27, 1, 294, 175, 108, 302, 108, 348, 175, 32, 301, 194, 205, 243, 243, 287, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 288, 230, 202, 108, 62, 27, 1, 294, 175, 108, 301, 108, 348, 175, 32, 301, 194, 205, 243, 306, 108, 348, 108, 301, 175, 32, 182, 279, 62, 32, 301, 194, 205, 243, 304, 218, 351, 108, 301, 244, 206, 243, 304, 175, 108, 301, 175, 32, 185, 32, 10, 243, 292, 108, 301, 108, 296, 194, 243, 287, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 288, 230, 202, 108, 62, 27, 1, 294, 175, 175, 32, 307, 194, 221, 243, 295, 244, 302, 108, 284, 194, 349, 175, 32, 196, 243, 253, 74, 287, 244, 277, 108, 307, 108, 296, 194, 287, 1, 257, 175, 32, 182, 280, 62, 32, 308, 194, 221, 243, 301, 108, 284, 194, 349, 175, 32, 196, 243, 254, 74, 287, 244, 278, 108, 308, 108, 296, 194, 287, 1, 257, 175, 32, 185, 32, 3, 32]}, {"code": "def _chunk_state_fwd_kernel(\n    x_ptr,\n    b_ptr,\n    states_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    seq_idx_ptr,\n    hdim,\n    dstate,\n    chunk_size,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_b_batch,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_b_dstate,\n    stride_states_batch,\n    stride_states_chunk,\n    stride_states_head,\n    stride_states_hdim,\n    stride_states_dstate,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    HAS_SEQ_IDX: tl.constexpr,\n    REVERSE: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    b_ptr += (\n        pid_b * stride_b_batch\n        + pid_c * chunk_size * stride_b_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_b_head\n    )\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen\n    )\n    b_ptrs = b_ptr + (\n        offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen\n    )\n    dt_ptrs = dt_ptr + offs_k * stride_dt_csize\n    if not REVERSE:\n        dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(\n            tl.float32\n        )\n    else:\n        dA_cs_last = tl.load(dA_cumsum_ptr).to(tl.float32)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    if HAS_SEQ_IDX:\n        seq_idx_ptrs = seq_idx_ptr + offs_k * stride_seq_idx_seqlen\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    if HAS_SEQ_IDX:\n        seq_idx_last = tl.load(\n            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen\n        )\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k),\n            other=0.0,\n        )\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate),\n            other=0.0,\n        ).to(tl.float32)\n        dA_cs_k = tl.load(\n            dA_cumsum_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0\n        ).to(tl.float32)\n        if HAS_SEQ_IDX:\n            seq_idx_k = tl.load(\n                seq_idx_ptrs, mask=offs_k < chunk_size_limit - k, other=-1\n            )\n        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(\n            tl.float32\n        )\n        if not HAS_SEQ_IDX:\n            scale = tl.exp((dA_cs_last - dA_cs_k)) * dt_k\n        else:\n            scale = tl.where(\n                seq_idx_k == seq_idx_last, tl.exp((dA_cs_last - dA_cs_k)) * dt_k, 0.0\n            )\n        b *= scale[:, None]\n        b = b.to(x_ptr.dtype.element_ty)\n        acc += tl.dot(x, b)\n        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen\n        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n        if HAS_SEQ_IDX:\n            seq_idx_ptrs += BLOCK_SIZE_K * stride_seq_idx_seqlen\n    states = acc.to(states_ptr.dtype.element_ty)\n\n    states_ptr += (\n        pid_b * stride_states_batch\n        + pid_c * stride_states_chunk\n        + pid_h * stride_states_head\n    )\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    states_ptrs = states_ptr + (\n        offs_m[:, None] * stride_states_hdim + offs_n[None, :] * stride_states_dstate\n    )\n    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)\n    tl.store(states_ptrs, states, mask=c_mask)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 63, 6, 108, 283, 63, 6, 108, 284, 63, 6, 108, 285, 63, 6, 108, 286, 63, 6, 175, 63, 32, -1, 287, 194, 171, 243, 288, 194, 348, 175, 32, 289, 194, 287, 48, 256, 32, 290, 194, 287, 4, 289, 244, 256, 32, 291, 194, 171, 243, 288, 194, 349, 175, 32, 292, 194, 65, 243, 254, 108, 285, 175, 32, 293, 194, 171, 243, 288, 194, 350, 175, 48, 292, 32, 294, 194, 171, 243, 288, 194, 350, 175, 226, 292, 32, 248, 172, 290, 244, 263, 74, 289, 244, 255, 244, 264, 74, 291, 48, 258, 244, 265, 32, 247, 172, 290, 244, 259, 74, 289, 244, 255, 244, 260, 74, 291, 244, 261, 32, 250, 172, 290, 244, 272, 74, 289, 244, 273, 74, 291, 244, 274, 32, 251, 172, 290, 244, 276, 74, 289, 244, 277, 74, 291, 244, 278, 32, 182, 282, 63, 32, 252, 172, 290, 244, 280, 74, 289, 244, 255, 244, 281, 32, 185, 32, 295, 194, 293, 244, 284, 74, 76, 243, 350, 108, 284, 175, 32, 296, 194, 294, 244, 285, 74, 76, 243, 350, 108, 285, 175, 32, 297, 194, 76, 243, 350, 108, 286, 175, 32, 298, 194, 247, 74, 243, 295, 230, 63, 108, 202, 27, 244, 262, 74, 297, 230, 202, 108, 63, 27, 244, 260, 175, 32, 299, 194, 248, 74, 243, 296, 230, 202, 108, 63, 27, 244, 266, 74, 297, 230, 63, 108, 202, 27, 244, 264, 175, 32, 300, 194, 250, 74, 297, 244, 275, 32, 182, 64, 283, 63, 32, 301, 194, 57, 243, 251, 74, 243, 255, 4, 348, 175, 244, 279, 175, 82, 302, 243, 145, 175, 32, 185, 32, 30, 63, 32, 301, 194, 57, 243, 251, 175, 82, 302, 243, 145, 175, 32, 56, 32, 303, 194, 251, 74, 297, 244, 279, 32, 182, 282, 63, 32, 304, 194, 252, 74, 297, 244, 281, 32, 185, 32, 305, 194, 39, 243, 255, 108, 257, 4, 289, 244, 255, 175, 32, 182, 282, 63, 32, 306, 194, 57, 243, 252, 74, 243, 305, 4, 348, 175, 244, 281, 175, 32, 185, 32, 307, 194, 176, 243, 243, 284, 108, 285, 175, 108, 91, 194, 145, 175, 32, 135, 308, 157, 5, 243, 350, 108, 305, 108, 286, 175, 63, 32, 309, 194, 57, 243, 298, 108, 310, 194, 243, 295, 230, 63, 108, 202, 27, 1, 253, 175, 173, 243, 297, 230, 202, 108, 63, 27, 1, 305, 4, 308, 175, 108, 311, 194, 350, 175, 32, 312, 194, 57, 243, 299, 108, 310, 194, 243, 297, 230, 63, 108, 202, 27, 1, 305, 4, 308, 175, 173, 243, 296, 230, 202, 108, 63, 27, 1, 254, 175, 108, 311, 194, 350, 175, 82, 302, 243, 145, 175, 32, 313, 194, 57, 243, 303, 108, 310, 194, 297, 1, 305, 4, 308, 108, 311, 194, 350, 175, 82, 302, 243, 145, 175, 32, 182, 282, 63, 32, 314, 194, 57, 243, 304, 108, 310, 194, 297, 1, 305, 4, 308, 108, 311, 194, 4, 348, 175, 32, 185, 32, 315, 194, 57, 243, 300, 108, 310, 194, 297, 1, 305, 4, 308, 108, 311, 194, 350, 175, 82, 302, 243, 145, 175, 32, 182, 64, 282, 63, 32, 316, 194, 107, 243, 301, 4, 313, 175, 244, 315, 32, 185, 32, 30, 63, 32, 316, 194, 205, 243, 314, 77, 306, 108, 107, 243, 301, 4, 313, 175, 244, 315, 108, 350, 175, 32, 56, 32, 312, 24, 316, 230, 63, 108, 202, 27, 32, 312, 194, 312, 82, 302, 243, 247, 82, 91, 82, 114, 175, 32, 307, 172, 15, 243, 309, 108, 312, 175, 32, 298, 172, 286, 244, 260, 32, 299, 172, 286, 244, 264, 32, 300, 172, 286, 244, 275, 32, 303, 172, 286, 244, 279, 32, 182, 282, 63, 32, 304, 172, 286, 244, 281, 32, 185, 32, 78, 32, 317, 194, 307, 82, 302, 243, 249, 82, 91, 82, 114, 175, 32, 249, 172, 290, 244, 267, 74, 289, 244, 268, 74, 291, 244, 269, 32, 295, 194, 293, 244, 284, 74, 76, 243, 350, 108, 284, 175, 32, 296, 194, 294, 244, 285, 74, 76, 243, 350, 108, 285, 175, 32, 318, 194, 249, 74, 243, 295, 230, 63, 108, 202, 27, 244, 270, 74, 296, 230, 202, 108, 63, 27, 244, 271, 175, 32, 319, 194, 243, 295, 230, 63, 108, 202, 27, 1, 253, 175, 173, 243, 296, 230, 202, 108, 63, 27, 1, 254, 175, 32, 10, 243, 318, 108, 317, 108, 310, 194, 319, 175, 32, 3, 32]}, {"code": "def _chunk_state_bwd_dx_kernel(\n    x_ptr,\n    b_ptr,\n    dstates_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    dx_ptr,\n    ddt_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    hdim,\n    dstate,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_b_batch,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_b_dstate,\n    stride_dstates_batch,\n    stride_dstates_chunk,\n    stride_states_head,\n    stride_states_hdim,\n    stride_states_dstate,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_dx_batch,\n    stride_dx_seqlen,\n    stride_dx_head,\n    stride_dx_hdim,\n    stride_ddt_batch,\n    stride_ddt_chunk,\n    stride_ddt_head,\n    stride_ddt_csize,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_DSTATE: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    b_ptr += (\n        pid_b * stride_b_batch\n        + pid_c * chunk_size * stride_b_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_b_head\n    )\n    dstates_ptr += (\n        pid_b * stride_dstates_batch\n        + pid_c * stride_dstates_chunk\n        + pid_h * stride_states_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    ddt_ptr += (\n        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\n    )\n    ddA_cumsum_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_c * stride_ddA_cs_chunk\n        + pid_h * stride_ddA_cs_head\n    )\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    offs_k = tl.arange(\n        0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K\n    )\n    b_ptrs = b_ptr + (\n        offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate\n    )\n    dstates_ptrs = dstates_ptr + (\n        offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate\n    )\n    if BLOCK_SIZE_DSTATE <= 128:\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate),\n            other=0.0,\n        )\n        dstates = tl.load(\n            dstates_ptrs,\n            mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n        dstates = dstates.to(b_ptr.dtype.element_ty)\n        acc = tl.dot(b, dstates)\n    else:\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            b = tl.load(\n                b_ptrs,\n                mask=(offs_m[:, None] < chunk_size_limit)\n                & (offs_k[None, :] < dstate - k),\n                other=0.0,\n            )\n            dstates = tl.load(\n                dstates_ptrs,\n                mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim),\n                other=0.0,\n            )\n            dstates = dstates.to(b_ptr.dtype.element_ty)\n            acc += tl.dot(b, dstates)\n            b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(\n        tl.float32\n    )\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0).to(\n        tl.float32\n    )\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n    acc *= tl.exp(dA_cs_last - dA_cs_m)[:, None]\n\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n    )\n    x = tl.load(\n        x_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    ddt = tl.sum(acc * x, axis=1)\n    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\n    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)\n    ddA_cs = -(ddt * dt_m)\n    ddA_cs_last = -tl.sum(ddA_cs)\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)\n    tl.atomic_add(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize, ddA_cs_last)\n\n    dx = (acc * dt_m[:, None]).to(dx_ptr.dtype.element_ty)\n    dx_ptr += (\n        pid_b * stride_dx_batch\n        + pid_c * chunk_size * stride_dx_seqlen\n        + pid_h * stride_dx_head\n    )\n    dx_ptrs = dx_ptr + (\n        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim\n    )\n    tl.store(\n        dx_ptrs,\n        dx,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 62, 6, 108, 295, 62, 6, 108, 296, 62, 6, 108, 297, 62, 6, 175, 62, 32, -1, 298, 194, 171, 243, 299, 194, 348, 175, 32, 300, 194, 298, 48, 258, 32, 301, 194, 298, 4, 300, 244, 258, 32, 302, 194, 171, 243, 299, 194, 349, 175, 32, 303, 194, 64, 243, 256, 108, 295, 175, 32, 304, 194, 171, 243, 299, 194, 350, 175, 48, 303, 32, 305, 194, 171, 243, 299, 194, 350, 175, 226, 303, 32, 247, 172, 301, 244, 261, 74, 300, 244, 255, 244, 262, 74, 302, 244, 263, 32, 248, 172, 301, 244, 265, 74, 300, 244, 255, 244, 266, 74, 302, 48, 260, 244, 267, 32, 249, 172, 301, 244, 269, 74, 300, 244, 270, 74, 302, 244, 271, 32, 250, 172, 301, 244, 274, 74, 300, 244, 275, 74, 302, 244, 276, 32, 253, 172, 301, 244, 286, 74, 300, 244, 287, 74, 302, 244, 288, 32, 254, 172, 301, 244, 290, 74, 300, 244, 291, 74, 302, 244, 292, 32, 251, 172, 301, 244, 278, 74, 300, 244, 279, 74, 302, 244, 280, 32, 306, 194, 304, 244, 294, 74, 76, 243, 350, 108, 294, 175, 32, 307, 194, 305, 244, 295, 74, 76, 243, 350, 108, 295, 175, 32, 308, 194, 39, 243, 255, 108, 259, 4, 300, 244, 255, 175, 32, 309, 194, 76, 243, 350, 108, 297, 182, 297, 218, 348, 349, 351, 30, 296, 175, 32, 310, 194, 248, 74, 243, 306, 230, 62, 108, 202, 27, 244, 266, 74, 309, 230, 202, 108, 62, 27, 244, 268, 175, 32, 311, 194, 249, 74, 243, 307, 230, 202, 108, 62, 27, 244, 272, 74, 309, 230, 62, 108, 202, 27, 244, 273, 175, 32, 182, 297, 218, 348, 349, 351, 62, 32, 312, 194, 55, 243, 310, 108, 313, 194, 243, 306, 230, 62, 108, 202, 27, 1, 308, 175, 173, 243, 309, 230, 202, 108, 62, 27, 1, 257, 175, 108, 314, 194, 350, 175, 32, 315, 194, 55, 243, 311, 108, 313, 194, 243, 309, 230, 62, 108, 202, 27, 1, 257, 175, 173, 243, 307, 230, 202, 108, 62, 27, 1, 256, 175, 108, 314, 194, 350, 175, 32, 315, 194, 315, 82, 316, 243, 248, 82, 91, 82, 114, 175, 32, 317, 194, 15, 243, 312, 108, 315, 175, 32, 185, 32, 30, 62, 32, 317, 194, 176, 243, 243, 294, 108, 295, 175, 108, 91, 194, 145, 175, 32, 135, 318, 157, 5, 243, 350, 108, 257, 108, 296, 175, 62, 32, 312, 194, 55, 243, 310, 108, 313, 194, 243, 306, 230, 62, 108, 202, 27, 1, 308, 175, 173, 243, 309, 230, 202, 108, 62, 27, 1, 257, 4, 318, 175, 108, 314, 194, 350, 175, 32, 315, 194, 55, 243, 311, 108, 313, 194, 243, 309, 230, 62, 108, 202, 27, 1, 257, 4, 318, 175, 173, 243, 307, 230, 202, 108, 62, 27, 1, 256, 175, 108, 314, 194, 350, 175, 32, 315, 194, 315, 82, 316, 243, 248, 82, 91, 82, 114, 175, 32, 317, 172, 15, 243, 312, 108, 315, 175, 32, 310, 172, 296, 244, 268, 32, 311, 172, 296, 244, 273, 32, 78, 32, 56, 32, 306, 194, 304, 244, 294, 74, 76, 243, 350, 108, 294, 175, 32, 307, 194, 305, 244, 295, 74, 76, 243, 350, 108, 295, 175, 32, 319, 194, 55, 243, 251, 74, 243, 255, 4, 348, 175, 244, 281, 175, 82, 316, 243, 145, 175, 32, 320, 194, 250, 74, 306, 244, 277, 32, 321, 194, 251, 74, 306, 244, 281, 32, 322, 194, 55, 243, 321, 108, 313, 194, 306, 1, 255, 108, 314, 194, 350, 175, 82, 316, 243, 145, 175, 32, 323, 194, 55, 243, 320, 108, 313, 194, 306, 1, 255, 108, 314, 194, 350, 175, 82, 316, 243, 145, 175, 32, 317, 24, 107, 243, 319, 4, 322, 175, 230, 62, 108, 202, 27, 32, 324, 194, 247, 74, 243, 306, 230, 62, 108, 202, 27, 244, 262, 74, 307, 230, 202, 108, 62, 27, 244, 264, 175, 32, 325, 194, 55, 243, 324, 108, 313, 194, 243, 306, 230, 62, 108, 202, 27, 1, 308, 175, 173, 243, 307, 230, 202, 108, 62, 27, 1, 256, 175, 108, 314, 194, 350, 175, 82, 316, 243, 145, 175, 32, 326, 194, 221, 243, 317, 244, 325, 108, 299, 194, 348, 175, 32, 327, 194, 253, 74, 306, 244, 289, 32, 196, 243, 327, 108, 326, 108, 313, 194, 306, 1, 255, 175, 32, 328, 194, 4, 243, 326, 244, 323, 175, 32, 329, 194, 4, 221, 243, 328, 175, 32, 330, 194, 254, 74, 306, 244, 293, 32, 196, 243, 330, 108, 328, 108, 313, 194, 306, 1, 255, 175, 32, 196, 243, 254, 74, 243, 255, 4, 348, 175, 244, 293, 108, 329, 175, 32, 331, 194, 243, 317, 244, 323, 230, 62, 108, 202, 27, 175, 82, 316, 243, 252, 82, 91, 82, 114, 175, 32, 252, 172, 301, 244, 282, 74, 300, 244, 255, 244, 283, 74, 302, 244, 284, 32, 332, 194, 252, 74, 243, 306, 230, 62, 108, 202, 27, 244, 283, 74, 307, 230, 202, 108, 62, 27, 244, 285, 175, 32, 10, 243, 332, 108, 331, 108, 313, 194, 243, 306, 230, 62, 108, 202, 27, 1, 308, 175, 173, 243, 307, 230, 202, 108, 62, 27, 1, 256, 175, 175, 32, 3, 32]}, {"code": "def _chunk_state_bwd_db_kernel(\n    x_ptr,\n    dstates_ptr,\n    b_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    seq_idx_ptr,\n    db_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    dstate,\n    hdim,\n    batch,\n    seqlen,\n    nheads,\n    nheads_per_program,\n    ngroups,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_dstates_batch,\n    stride_dstates_chunk,\n    stride_states_head,\n    stride_states_hdim,\n    stride_states_dstate,\n    stride_b_batch,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_b_dstate,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    stride_db_batch,\n    stride_db_seqlen,\n    stride_db_split,\n    stride_db_group,\n    stride_db_dstate,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize,\n    HAS_DDA_CS: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_sg = tl.program_id(axis=2)\n    pid_s = pid_sg // ngroups\n    pid_g = pid_sg - pid_s * ngroups\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head\n    )\n    db_ptr += (\n        pid_b * stride_db_batch\n        + pid_c * chunk_size * stride_db_seqlen\n        + pid_g * stride_db_group\n        + pid_s * stride_db_split\n    )\n    dstates_ptr += (\n        pid_b * stride_dstates_batch\n        + pid_c * stride_dstates_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n        * stride_states_head\n    )\n    dt_ptr += (\n        pid_b * stride_dt_batch\n        + pid_c * stride_dt_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head\n    )\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_head\n    )\n    if HAS_DDA_CS:\n        b_ptr += (\n            pid_b * stride_b_batch\n            + pid_c * chunk_size * stride_b_seqlen\n            + pid_g * stride_b_head\n        )\n        ddA_cumsum_ptr += (\n            pid_b * stride_ddA_cs_batch\n            + pid_c * stride_ddA_cs_chunk\n            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)\n            * stride_ddA_cs_head\n        )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_seqlen + offs_k[None, :] * stride_x_hdim\n    )\n    dstates_ptrs = dstates_ptr + (\n        offs_n[None, :] * stride_states_dstate + offs_k[:, None] * stride_states_hdim\n    )\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n    if HAS_DDA_CS:\n        b_ptrs = b_ptr + (\n            offs_m[:, None] * stride_b_seqlen + offs_n[None, :] * stride_b_dstate\n        )\n        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_DDA_CS:\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),\n            other=0.0,\n        ).to(tl.float32)\n    if HAS_SEQ_IDX:\n        seq_idx_m = tl.load(\n            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit,\n            other=-1,\n        )\n        seq_idx_last = tl.load(\n            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen\n        )\n    nheads_iter = min(\n        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program\n    )\n    for h in range(nheads_iter):\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),\n            other=0.0,\n        )\n        dstates = tl.load(\n            dstates_ptrs,\n            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),\n            other=0.0,\n        )\n        dstates = dstates.to(x_ptrs.dtype.element_ty)\n        db = tl.dot(x, dstates)\n        dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(\n            tl.float32\n        )\n        dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0).to(\n            tl.float32\n        )\n        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n        if not HAS_SEQ_IDX:\n            scale = tl.exp(dA_cs_last - dA_cs_m)\n        else:\n            scale = tl.where(\n                seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0\n            )\n        db *= (scale * dt_m)[:, None]\n        if HAS_DDA_CS:\n\n            ddA_cs = tl.sum(db * b, axis=1)\n            tl.atomic_add(\n                ddA_cumsum_ptrs + stride_ddA_cs_csize,\n                ddA_cs,\n                mask=offs_m < chunk_size - 1,\n            )\n        acc += db\n        x_ptrs += stride_x_head\n        dstates_ptrs += stride_states_head\n        dt_ptrs += stride_dt_head\n        dA_cumsum_ptr += stride_dA_cs_head\n        dA_cumsum_ptrs += stride_dA_cs_head\n        if HAS_DDA_CS:\n            ddA_cumsum_ptrs += stride_ddA_cs_head\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    db_ptrs = db_ptr + (\n        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_dstate\n    )\n    tl.store(\n        db_ptrs,\n        acc,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 63, 6, 108, 296, 63, 6, 108, 297, 63, 6, 108, 298, 63, 6, 108, 299, 63, 6, 175, 63, 32, -1, 300, 194, 171, 243, 301, 194, 348, 175, 32, 302, 194, 300, 48, 258, 32, 303, 194, 300, 4, 302, 244, 258, 32, 304, 194, 171, 243, 301, 194, 349, 175, 32, 305, 194, 304, 48, 262, 32, 306, 194, 304, 4, 305, 244, 262, 32, 307, 194, 65, 243, 256, 108, 298, 175, 32, 308, 194, 171, 243, 301, 194, 350, 175, 48, 307, 32, 309, 194, 171, 243, 301, 194, 350, 175, 226, 307, 32, 247, 172, 303, 244, 263, 74, 302, 244, 255, 244, 264, 74, 243, 306, 244, 243, 260, 48, 262, 175, 74, 305, 244, 261, 175, 244, 265, 32, 253, 172, 303, 244, 286, 74, 302, 244, 255, 244, 287, 74, 306, 244, 289, 74, 305, 244, 288, 32, 248, 172, 303, 244, 267, 74, 302, 244, 268, 74, 243, 306, 244, 243, 260, 48, 262, 175, 74, 305, 244, 261, 175, 244, 269, 32, 250, 172, 303, 244, 276, 74, 302, 244, 277, 74, 243, 306, 244, 243, 260, 48, 262, 175, 74, 305, 244, 261, 175, 244, 278, 32, 251, 172, 303, 244, 280, 74, 302, 244, 281, 74, 243, 306, 244, 243, 260, 48, 262, 175, 74, 305, 244, 261, 175, 244, 282, 32, 182, 295, 63, 32, 249, 172, 303, 244, 272, 74, 302, 244, 255, 244, 273, 74, 306, 244, 274, 32, 254, 172, 303, 244, 291, 74, 302, 244, 292, 74, 243, 306, 244, 243, 260, 48, 262, 175, 74, 305, 244, 261, 175, 244, 293, 32, 185, 32, 182, 296, 63, 32, 252, 172, 303, 244, 284, 74, 302, 244, 255, 244, 285, 32, 185, 32, 310, 194, 308, 244, 297, 74, 76, 243, 350, 108, 297, 175, 32, 311, 194, 309, 244, 298, 74, 76, 243, 350, 108, 298, 175, 32, 312, 194, 76, 243, 350, 108, 299, 175, 32, 313, 194, 247, 74, 243, 310, 230, 63, 108, 202, 27, 244, 264, 74, 312, 230, 202, 108, 63, 27, 244, 266, 175, 32, 314, 194, 248, 74, 243, 311, 230, 202, 108, 63, 27, 244, 271, 74, 312, 230, 63, 108, 202, 27, 244, 270, 175, 32, 315, 194, 250, 74, 310, 244, 279, 32, 316, 194, 251, 74, 310, 244, 283, 32, 182, 295, 63, 32, 317, 194, 249, 74, 243, 310, 230, 63, 108, 202, 27, 244, 273, 74, 311, 230, 202, 108, 63, 27, 244, 275, 175, 32, 318, 194, 254, 74, 310, 244, 294, 32, 185, 32, 319, 194, 39, 243, 255, 108, 259, 4, 302, 244, 255, 175, 32, 320, 194, 176, 243, 243, 297, 108, 298, 175, 108, 91, 194, 145, 175, 32, 182, 295, 63, 32, 321, 194, 57, 243, 317, 108, 322, 194, 243, 310, 230, 63, 108, 202, 27, 1, 319, 175, 173, 243, 311, 230, 202, 108, 63, 27, 1, 256, 175, 108, 323, 194, 350, 175, 82, 324, 243, 145, 175, 32, 185, 32, 182, 296, 63, 32, 325, 194, 57, 243, 252, 74, 310, 244, 285, 108, 322, 194, 310, 1, 319, 108, 323, 194, 4, 348, 175, 32, 326, 194, 57, 243, 252, 74, 243, 319, 4, 348, 175, 244, 285, 175, 32, 185, 32, 327, 194, 39, 243, 261, 108, 260, 48, 262, 4, 305, 244, 261, 175, 32, 135, 328, 157, 5, 243, 327, 175, 63, 32, 329, 194, 57, 243, 313, 108, 322, 194, 243, 310, 230, 63, 108, 202, 27, 1, 319, 175, 173, 243, 312, 230, 202, 108, 63, 27, 1, 257, 175, 108, 323, 194, 350, 175, 32, 330, 194, 57, 243, 314, 108, 322, 194, 243, 312, 230, 63, 108, 202, 27, 1, 257, 175, 173, 243, 311, 230, 202, 108, 63, 27, 1, 256, 175, 108, 323, 194, 350, 175, 32, 330, 194, 330, 82, 324, 243, 313, 82, 91, 82, 114, 175, 32, 331, 194, 15, 243, 329, 108, 330, 175, 32, 332, 194, 57, 243, 251, 74, 243, 255, 4, 348, 175, 244, 283, 175, 82, 324, 243, 145, 175, 32, 333, 194, 57, 243, 316, 108, 322, 194, 310, 1, 255, 108, 323, 194, 350, 175, 82, 324, 243, 145, 175, 32, 334, 194, 57, 243, 315, 108, 322, 194, 310, 1, 255, 108, 323, 194, 350, 175, 82, 324, 243, 145, 175, 32, 182, 64, 296, 63, 32, 335, 194, 107, 243, 332, 4, 333, 175, 32, 185, 32, 30, 63, 32, 335, 194, 205, 243, 325, 77, 326, 108, 107, 243, 332, 4, 333, 175, 108, 350, 175, 32, 56, 32, 331, 24, 243, 335, 244, 334, 175, 230, 63, 108, 202, 27, 32, 182, 295, 63, 32, 336, 194, 221, 243, 331, 244, 321, 108, 301, 194, 348, 175, 32, 196, 243, 318, 74, 294, 108, 336, 108, 322, 194, 310, 1, 255, 4, 348, 175, 32, 185, 32, 320, 172, 331, 32, 313, 172, 265, 32, 314, 172, 269, 32, 315, 172, 278, 32, 251, 172, 282, 32, 316, 172, 282, 32, 182, 295, 63, 32, 318, 172, 293, 32, 185, 32, 78, 32, 310, 194, 308, 244, 297, 74, 76, 243, 350, 108, 297, 175, 32, 311, 194, 309, 244, 298, 74, 76, 243, 350, 108, 298, 175, 32, 337, 194, 253, 74, 243, 310, 230, 63, 108, 202, 27, 244, 287, 74, 311, 230, 202, 108, 63, 27, 244, 290, 175, 32, 10, 243, 337, 108, 320, 108, 322, 194, 243, 310, 230, 63, 108, 202, 27, 1, 319, 175, 173, 243, 311, 230, 202, 108, 63, 27, 1, 256, 175, 175, 32, 3, 32]}, {"code": "def _chunk_state_bwd_ddAcs_stable_kernel(\n    x_ptr,\n    b_ptr,\n    dstates_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    seq_idx_ptr,\n    ddA_cumsum_ptr,\n    chunk_size,\n    hdim,\n    dstate,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_b_batch,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_b_dstate,\n    stride_dstates_batch,\n    stride_dstates_chunk,\n    stride_states_head,\n    stride_states_hdim,\n    stride_states_dstate,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_ddA_cs_csize,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_DSTATE: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    b_ptr += (\n        pid_b * stride_b_batch\n        + pid_c * chunk_size * stride_b_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_b_head\n    )\n    dstates_ptr += (\n        pid_b * stride_dstates_batch\n        + pid_c * stride_dstates_chunk\n        + pid_h * stride_states_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    ddA_cumsum_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_c * stride_ddA_cs_chunk\n        + pid_h * stride_ddA_cs_head\n    )\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    offs_k = tl.arange(\n        0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K\n    )\n    b_ptrs = b_ptr + (\n        offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate\n    )\n    dstates_ptrs = dstates_ptr + (\n        offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate\n    )\n    if BLOCK_SIZE_DSTATE <= 128:\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate),\n            other=0.0,\n        )\n        dstates = tl.load(\n            dstates_ptrs,\n            mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n        dstates = dstates.to(b_ptr.dtype.element_ty)\n        acc = tl.dot(b, dstates)\n    else:\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            b = tl.load(\n                b_ptrs,\n                mask=(offs_m[:, None] < chunk_size_limit)\n                & (offs_k[None, :] < dstate - k),\n                other=0.0,\n            )\n            dstates = tl.load(\n                dstates_ptrs,\n                mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim),\n                other=0.0,\n            )\n            dstates = dstates.to(b_ptr.dtype.element_ty)\n            acc += tl.dot(b, dstates)\n            b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    dA_cs_m = tl.load(\n        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0\n    ).to(tl.float32)\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(\n        tl.float32\n    )\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_last - dA_cs_m)\n    else:\n        seq_idx_m = tl.load(\n            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit,\n            other=-1,\n        )\n        seq_idx_last = tl.load(\n            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen\n        )\n        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0)\n    acc *= scale[:, None]\n\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n    )\n    x = tl.load(\n        x_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n    ddt = tl.sum(acc * x, axis=1)\n\n    ddA_cs = ddt * dt_m\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n\n    tl.atomic_add(\n        ddA_cumsum_ptrs + stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size - 1\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 62, 6, 108, 288, 62, 6, 108, 289, 62, 6, 108, 290, 62, 6, 108, 291, 62, 6, 175, 62, 32, -1, 292, 194, 171, 243, 293, 194, 348, 175, 32, 294, 194, 292, 48, 257, 32, 295, 194, 292, 4, 294, 244, 257, 32, 296, 194, 171, 243, 293, 194, 349, 175, 32, 297, 194, 64, 243, 255, 108, 289, 175, 32, 298, 194, 171, 243, 293, 194, 350, 175, 48, 297, 32, 299, 194, 171, 243, 293, 194, 350, 175, 226, 297, 32, 247, 172, 295, 244, 260, 74, 294, 244, 254, 244, 261, 74, 296, 244, 262, 32, 248, 172, 295, 244, 264, 74, 294, 244, 254, 244, 265, 74, 296, 48, 259, 244, 266, 32, 249, 172, 295, 244, 268, 74, 294, 244, 269, 74, 296, 244, 270, 32, 250, 172, 295, 244, 273, 74, 294, 244, 274, 74, 296, 244, 275, 32, 253, 172, 295, 244, 283, 74, 294, 244, 284, 74, 296, 244, 285, 32, 251, 172, 295, 244, 277, 74, 294, 244, 278, 74, 296, 244, 279, 32, 182, 287, 62, 32, 252, 172, 295, 244, 281, 74, 294, 244, 254, 244, 282, 32, 185, 32, 300, 194, 298, 244, 288, 74, 76, 243, 350, 108, 288, 175, 32, 301, 194, 299, 244, 289, 74, 76, 243, 350, 108, 289, 175, 32, 302, 194, 39, 243, 254, 108, 258, 4, 294, 244, 254, 175, 32, 303, 194, 76, 243, 350, 108, 291, 182, 291, 218, 348, 349, 351, 30, 290, 175, 32, 304, 194, 248, 74, 243, 300, 230, 62, 108, 202, 27, 244, 265, 74, 303, 230, 202, 108, 62, 27, 244, 267, 175, 32, 305, 194, 249, 74, 243, 301, 230, 202, 108, 62, 27, 244, 271, 74, 303, 230, 62, 108, 202, 27, 244, 272, 175, 32, 182, 291, 218, 348, 349, 351, 62, 32, 306, 194, 55, 243, 304, 108, 307, 194, 243, 300, 230, 62, 108, 202, 27, 1, 302, 175, 173, 243, 303, 230, 202, 108, 62, 27, 1, 256, 175, 108, 308, 194, 350, 175, 32, 309, 194, 55, 243, 305, 108, 307, 194, 243, 303, 230, 62, 108, 202, 27, 1, 256, 175, 173, 243, 301, 230, 202, 108, 62, 27, 1, 255, 175, 108, 308, 194, 350, 175, 32, 309, 194, 309, 82, 310, 243, 248, 82, 91, 82, 114, 175, 32, 311, 194, 15, 243, 306, 108, 309, 175, 32, 185, 32, 30, 62, 32, 311, 194, 176, 243, 243, 288, 108, 289, 175, 108, 91, 194, 145, 175, 32, 135, 312, 157, 5, 243, 350, 108, 256, 108, 290, 175, 62, 32, 306, 194, 55, 243, 304, 108, 307, 194, 243, 300, 230, 62, 108, 202, 27, 1, 302, 175, 173, 243, 303, 230, 202, 108, 62, 27, 1, 256, 4, 312, 175, 108, 308, 194, 350, 175, 32, 309, 194, 55, 243, 305, 108, 307, 194, 243, 303, 230, 62, 108, 202, 27, 1, 256, 4, 312, 175, 173, 243, 301, 230, 202, 108, 62, 27, 1, 255, 175, 108, 308, 194, 350, 175, 32, 309, 194, 309, 82, 310, 243, 248, 82, 91, 82, 114, 175, 32, 311, 172, 15, 243, 306, 108, 309, 175, 32, 304, 172, 290, 244, 267, 32, 305, 172, 290, 244, 272, 32, 78, 32, 56, 32, 300, 194, 298, 244, 288, 74, 76, 243, 350, 108, 288, 175, 32, 301, 194, 299, 244, 289, 74, 76, 243, 350, 108, 289, 175, 32, 313, 194, 55, 243, 251, 74, 300, 244, 280, 108, 307, 194, 300, 1, 254, 108, 308, 194, 350, 175, 82, 310, 243, 145, 175, 32, 314, 194, 55, 243, 251, 74, 243, 254, 4, 348, 175, 244, 280, 175, 82, 310, 243, 145, 175, 32, 182, 63, 287, 62, 32, 315, 194, 107, 243, 314, 4, 313, 175, 32, 185, 32, 30, 62, 32, 316, 194, 55, 243, 252, 74, 300, 244, 282, 108, 307, 194, 300, 1, 302, 108, 308, 194, 4, 348, 175, 32, 317, 194, 55, 243, 252, 74, 243, 302, 4, 348, 175, 244, 282, 175, 32, 315, 194, 205, 243, 316, 77, 317, 108, 107, 243, 314, 4, 313, 175, 108, 350, 175, 32, 56, 32, 311, 24, 315, 230, 62, 108, 202, 27, 32, 318, 194, 247, 74, 243, 300, 230, 62, 108, 202, 27, 244, 261, 74, 301, 230, 202, 108, 62, 27, 244, 263, 175, 32, 319, 194, 55, 243, 318, 108, 307, 194, 243, 300, 230, 62, 108, 202, 27, 1, 302, 175, 173, 243, 301, 230, 202, 108, 62, 27, 1, 255, 175, 108, 308, 194, 350, 175, 82, 310, 243, 145, 175, 32, 320, 194, 250, 74, 300, 244, 276, 32, 321, 194, 55, 243, 320, 108, 307, 194, 300, 1, 254, 108, 308, 194, 350, 175, 82, 310, 243, 145, 175, 32, 322, 194, 221, 243, 311, 244, 319, 108, 293, 194, 348, 175, 32, 323, 194, 322, 244, 321, 32, 324, 194, 253, 74, 300, 244, 286, 32, 196, 243, 324, 74, 286, 108, 323, 108, 307, 194, 300, 1, 254, 4, 348, 175, 32, 3, 32]}, {"code": "def _chunk_state_varlen_kernel(\n    x_ptr,\n    b_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    chunk_states_ptr,\n    cu_seqlens_ptr,\n    states_ptr,\n    hdim,\n    dstate,\n    chunk_size,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_b_dstate,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_chunk_states_chunk,\n    stride_chunk_states_head,\n    stride_chunk_states_hdim,\n    stride_chunk_states_dstate,\n    stride_states_batch,\n    stride_states_head,\n    stride_states_hdim,\n    stride_states_dstate,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    end_idx = tl.load(cu_seqlens_ptr + pid_b + 1)\n    pid_c = (end_idx - 1) // chunk_size\n    b_ptr += (\n        pid_c * chunk_size * stride_b_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_b_head\n    )\n    x_ptr += pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\n    dt_ptr += pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\n    chunk_states_ptr += (\n        pid_c * stride_chunk_states_chunk + pid_h * stride_chunk_states_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen\n    )\n    b_ptrs = b_ptr + (\n        offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen\n    )\n    dt_ptrs = dt_ptr + offs_k * stride_dt_csize\n    dA_cs_last = tl.load(\n        dA_cumsum_ptr + (end_idx - pid_c * chunk_size - 1) * stride_dA_cs_csize\n    ).to(tl.float32)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n\n    chunk_size_limit = end_idx - pid_c * chunk_size\n    start_idx = tl.load(cu_seqlens_ptr + pid_b)\n    start_idx_cur = tl.maximum(start_idx - pid_c * chunk_size, 0)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n        x = tl.load(\n            x_ptrs,\n            mask=(offs_m[:, None] < hdim)\n            & (offs_k[None, :] < chunk_size_limit - k)\n            & (offs_k[None, :] >= start_idx_cur - k),\n            other=0.0,\n        )\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_k[:, None] < chunk_size_limit - k)\n            & (offs_n[None, :] < dstate)\n            & (offs_k[:, None] >= start_idx_cur - k),\n            other=0.0,\n        ).to(tl.float32)\n        dA_cs_k = tl.load(\n            dA_cumsum_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0\n        ).to(tl.float32)\n        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(\n            tl.float32\n        )\n        scale = tl.where(\n            (offs_k >= start_idx_cur - k) & (offs_k < chunk_size_limit - k),\n            tl.exp((dA_cs_last - dA_cs_k)) * dt_k,\n            0.0,\n        )\n        b *= scale[:, None]\n        b = b.to(x_ptr.dtype.element_ty)\n        acc += tl.dot(x, b)\n        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen\n        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n\n    if start_idx < pid_c * chunk_size:\n        chunk_states_ptrs = chunk_states_ptr + (\n            offs_m[:, None] * stride_chunk_states_hdim\n            + offs_n[None, :] * stride_chunk_states_dstate\n        )\n        chunk_states = tl.load(\n            chunk_states_ptrs,\n            mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),\n            other=0.0,\n        ).to(tl.float32)\n\n        scale = tl.exp(dA_cs_last)\n        acc += chunk_states * scale\n\n    states = acc.to(states_ptr.dtype.element_ty)\n\n    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    states_ptrs = states_ptr + (\n        offs_m[:, None] * stride_states_hdim + offs_n[None, :] * stride_states_dstate\n    )\n    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)\n    tl.store(states_ptrs, states, mask=c_mask)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 63, 6, 108, 280, 63, 6, 108, 281, 63, 6, 175, 63, 32, -1, 282, 194, 171, 243, 283, 194, 348, 175, 32, 284, 194, 171, 243, 283, 194, 349, 175, 32, 285, 194, 65, 243, 255, 108, 280, 175, 32, 286, 194, 171, 243, 283, 194, 350, 175, 48, 285, 32, 287, 194, 171, 243, 283, 194, 350, 175, 226, 285, 32, 288, 194, 57, 243, 252, 74, 282, 74, 348, 175, 32, 289, 194, 243, 288, 4, 348, 175, 48, 256, 32, 248, 172, 289, 244, 256, 244, 262, 74, 284, 48, 258, 244, 263, 32, 247, 172, 289, 244, 256, 244, 259, 74, 284, 244, 260, 32, 249, 172, 289, 244, 265, 74, 284, 244, 266, 32, 250, 172, 289, 244, 268, 74, 284, 244, 269, 32, 251, 172, 289, 244, 271, 74, 284, 244, 272, 32, 290, 194, 286, 244, 279, 74, 76, 243, 350, 108, 279, 175, 32, 291, 194, 287, 244, 280, 74, 76, 243, 350, 108, 280, 175, 32, 292, 194, 76, 243, 350, 108, 281, 175, 32, 293, 194, 247, 74, 243, 290, 230, 63, 108, 202, 27, 244, 261, 74, 292, 230, 202, 108, 63, 27, 244, 259, 175, 32, 294, 194, 248, 74, 243, 291, 230, 202, 108, 63, 27, 244, 264, 74, 292, 230, 63, 108, 202, 27, 244, 262, 175, 32, 295, 194, 249, 74, 292, 244, 267, 32, 296, 194, 57, 243, 250, 74, 243, 288, 4, 289, 244, 256, 4, 348, 175, 244, 270, 175, 82, 297, 243, 145, 175, 32, 298, 194, 250, 74, 292, 244, 270, 32, 299, 194, 288, 4, 289, 244, 256, 32, 300, 194, 57, 243, 252, 74, 282, 175, 32, 301, 194, 192, 243, 300, 4, 289, 244, 256, 108, 350, 175, 32, 302, 194, 176, 243, 243, 279, 108, 280, 175, 108, 91, 194, 145, 175, 32, 135, 303, 157, 5, 243, 350, 108, 299, 108, 281, 175, 63, 32, 304, 194, 57, 243, 293, 108, 305, 194, 243, 290, 230, 63, 108, 202, 27, 1, 254, 175, 173, 243, 292, 230, 202, 108, 63, 27, 1, 299, 4, 303, 175, 173, 243, 292, 230, 202, 108, 63, 27, 144, 301, 4, 303, 175, 108, 306, 194, 350, 175, 32, 307, 194, 57, 243, 294, 108, 305, 194, 243, 292, 230, 63, 108, 202, 27, 1, 299, 4, 303, 175, 173, 243, 291, 230, 202, 108, 63, 27, 1, 255, 175, 173, 243, 292, 230, 63, 108, 202, 27, 144, 301, 4, 303, 175, 108, 306, 194, 350, 175, 82, 297, 243, 145, 175, 32, 308, 194, 57, 243, 298, 108, 305, 194, 292, 1, 299, 4, 303, 108, 306, 194, 350, 175, 82, 297, 243, 145, 175, 32, 309, 194, 57, 243, 295, 108, 305, 194, 292, 1, 299, 4, 303, 108, 306, 194, 350, 175, 82, 297, 243, 145, 175, 32, 310, 194, 205, 243, 243, 292, 144, 301, 4, 303, 175, 173, 243, 292, 1, 299, 4, 303, 175, 108, 107, 243, 296, 4, 308, 175, 244, 309, 108, 350, 175, 32, 307, 24, 310, 230, 63, 108, 202, 27, 32, 307, 194, 307, 82, 297, 243, 247, 82, 91, 82, 114, 175, 32, 302, 172, 15, 243, 304, 108, 307, 175, 32, 293, 172, 281, 244, 259, 32, 294, 172, 281, 244, 262, 32, 295, 172, 281, 244, 267, 32, 298, 172, 281, 244, 270, 32, 78, 32, 182, 300, 1, 289, 244, 256, 63, 32, 311, 194, 251, 74, 243, 290, 230, 63, 108, 202, 27, 244, 273, 74, 291, 230, 202, 108, 63, 27, 244, 274, 175, 32, 312, 194, 57, 243, 311, 108, 305, 194, 243, 290, 230, 63, 108, 202, 27, 1, 254, 175, 173, 243, 291, 230, 202, 108, 63, 27, 1, 255, 175, 108, 306, 194, 350, 175, 82, 297, 243, 145, 175, 32, 310, 194, 107, 243, 296, 175, 32, 302, 172, 312, 244, 310, 32, 185, 32, 313, 194, 302, 82, 297, 243, 253, 82, 91, 82, 114, 175, 32, 253, 172, 282, 244, 275, 74, 284, 244, 276, 32, 290, 194, 286, 244, 279, 74, 76, 243, 350, 108, 279, 175, 32, 291, 194, 287, 244, 280, 74, 76, 243, 350, 108, 280, 175, 32, 314, 194, 253, 74, 243, 290, 230, 63, 108, 202, 27, 244, 277, 74, 291, 230, 202, 108, 63, 27, 244, 278, 175, 32, 315, 194, 243, 290, 230, 63, 108, 202, 27, 1, 254, 175, 173, 243, 291, 230, 202, 108, 63, 27, 1, 255, 175, 32, 10, 243, 314, 108, 313, 108, 305, 194, 315, 175, 32, 3, 32]}, {"code": "def _chunk_scan_chunk_state_bwd_dx_kernel(\n    x_ptr,\n    cb_ptr,\n    dout_ptr,\n    dt_ptr,\n    dA_cumsum_ptr,\n    seq_idx_ptr,\n    D_ptr,\n    b_ptr,\n    dstates_ptr,\n    dx_ptr,\n    ddt_ptr,\n    dD_ptr,\n    chunk_size,\n    hdim,\n    dstate,\n    batch,\n    seqlen,\n    nheads_ngroups_ratio,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_head,\n    stride_x_hdim,\n    stride_cb_batch,\n    stride_cb_chunk,\n    stride_cb_head,\n    stride_cb_csize_m,\n    stride_cb_csize_k,\n    stride_dout_batch,\n    stride_dout_seqlen,\n    stride_dout_head,\n    stride_dout_hdim,\n    stride_dt_batch,\n    stride_dt_chunk,\n    stride_dt_head,\n    stride_dt_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    stride_D_head,\n    stride_b_batch,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_b_dstate,\n    stride_dstates_batch,\n    stride_dstates_chunk,\n    stride_dstates_head,\n    stride_dstates_hdim,\n    stride_dstates_dstate,\n    stride_dx_batch,\n    stride_dx_seqlen,\n    stride_dx_head,\n    stride_dx_hdim,\n    stride_ddt_batch,\n    stride_ddt_chunk,\n    stride_ddt_head,\n    stride_ddt_csize,\n    stride_dD_batch,\n    stride_dD_chunk,\n    stride_dD_head,\n    stride_dD_csize,\n    stride_dD_hdim,\n    HAS_D: tl.constexpr,\n    D_HAS_HDIM: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_DSTATE: tl.constexpr,\n    IS_TRITON_22: tl.constexpr,\n):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (\n        pid_b * stride_x_batch\n        + pid_c * chunk_size * stride_x_seqlen\n        + pid_h * stride_x_head\n    )\n    cb_ptr += (\n        pid_b * stride_cb_batch\n        + pid_c * stride_cb_chunk\n        + (pid_h // nheads_ngroups_ratio) * stride_cb_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_c * chunk_size * stride_dout_seqlen\n        + pid_h * stride_dout_head\n    )\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    ddt_ptr += (\n        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\n    )\n    dA_cumsum_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_c * stride_dA_cs_chunk\n        + pid_h * stride_dA_cs_head\n    )\n    b_ptr += (\n        pid_b * stride_b_batch\n        + pid_c * chunk_size * stride_b_seqlen\n        + (pid_h // nheads_ngroups_ratio) * stride_b_head\n    )\n    dstates_ptr += (\n        pid_b * stride_dstates_batch\n        + pid_c * stride_dstates_chunk\n        + pid_h * stride_dstates_head\n    )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    dA_cs_m = tl.load(\n        dA_cumsum_ptr + offs_m * stride_dA_cs_csize,\n        mask=offs_m < chunk_size_limit,\n        other=0.0,\n    ).to(tl.float32)\n\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(\n        tl.float32\n    )\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_last - dA_cs_m)\n    else:\n        seq_idx_m = tl.load(\n            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit,\n            other=-1,\n        )\n        seq_idx_last = tl.load(\n            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen\n        )\n        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0)\n\n    offs_dstate = tl.arange(\n        0,\n        (\n            BLOCK_SIZE_DSTATE\n            if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128\n            else BLOCK_SIZE_K\n        ),\n    )\n    b_ptrs = b_ptr + (\n        offs_m[:, None] * stride_b_seqlen + offs_dstate[None, :] * stride_b_dstate\n    )\n    dstates_ptrs = dstates_ptr + (\n        offs_n[None, :] * stride_dstates_hdim\n        + offs_dstate[:, None] * stride_dstates_dstate\n    )\n    if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128:\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_dstate[None, :] < dstate),\n            other=0.0,\n        )\n        dstates = tl.load(\n            dstates_ptrs,\n            mask=(offs_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n        dstates = dstates.to(b_ptr.dtype.element_ty)\n        acc = tl.dot(b, dstates) * scale[:, None]\n    else:\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            b = tl.load(\n                b_ptrs,\n                mask=(offs_m[:, None] < chunk_size_limit)\n                & (offs_dstate[None, :] < dstate - k),\n                other=0.0,\n            )\n            dstates = tl.load(\n                dstates_ptrs,\n                mask=(offs_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim),\n                other=0.0,\n            )\n            dstates = dstates.to(b_ptr.dtype.element_ty)\n            acc += tl.dot(b, dstates)\n            b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n            dstates_ptrs += BLOCK_SIZE_K * stride_dstates_dstate\n        acc *= scale[:, None]\n\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    cb_ptrs = cb_ptr + (\n        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k\n    )\n    dout_ptrs = dout_ptr + (\n        offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim\n    )\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    K_MAX = chunk_size_limit\n    K_MIN = pid_m * BLOCK_SIZE_M\n    cb_ptrs += K_MIN * stride_cb_csize_k\n    dout_ptrs += K_MIN * stride_dout_seqlen\n    dA_cumsum_ptrs += K_MIN * stride_dA_cs_csize\n    for k in range(K_MIN, K_MAX, BLOCK_SIZE_K):\n        k = tl.multiple_of(k, BLOCK_SIZE_K)\n\n        cb = tl.load(\n            cb_ptrs,\n            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k),\n            other=0.0,\n        )\n        dout = tl.load(\n            dout_ptrs,\n            mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim),\n            other=0.0,\n        )\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0).to(\n            tl.float32\n        )\n        cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])\n\n        mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)\n        cb = tl.where(mask, cb, 0.0)\n        cb = cb.to(dout_ptr.dtype.element_ty)\n        acc += tl.dot(cb, dout)\n        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\n    dx = acc * dt_m[:, None]\n    dx_ptr += (\n        pid_b * stride_dx_batch\n        + pid_c * chunk_size * stride_dx_seqlen\n        + pid_h * stride_dx_head\n    )\n    dx_ptrs = dx_ptr + (\n        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim\n    )\n    if HAS_D:\n        dout_res_ptrs = dout_ptr + (\n            offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim\n        )\n        dout_res = tl.load(\n            dout_res_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n            other=0.0,\n        ).to(tl.float32)\n        if D_HAS_HDIM:\n            D = tl.load(\n                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0\n            ).to(tl.float32)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n        dx += dout_res * D\n    tl.store(\n        dx_ptrs,\n        dx,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n    )\n\n    x_ptrs = x_ptr + (\n        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim\n    )\n    x = tl.load(\n        x_ptrs,\n        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),\n        other=0.0,\n    ).to(tl.float32)\n    if HAS_D:\n        dD_ptr += (\n            pid_b * stride_dD_batch\n            + pid_c * stride_dD_chunk\n            + pid_h * stride_dD_head\n            + pid_m * stride_dD_csize\n        )\n        if D_HAS_HDIM:\n            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n            dD = tl.sum(dout_res * x, axis=0)\n            tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n        else:\n            dD = tl.sum(dout_res * x)\n            tl.store(dD_ptr, dD)\n    ddt = tl.sum(acc * x, axis=1)\n    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\n    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 108, 301, 108, 302, 108, 303, 108, 304, 108, 305, 108, 306, 108, 307, 108, 308, 108, 309, 108, 310, 108, 311, 62, 6, 108, 312, 62, 6, 108, 313, 62, 6, 108, 314, 62, 6, 108, 315, 62, 6, 108, 316, 62, 6, 108, 317, 62, 6, 108, 318, 62, 6, 175, 62, 32, -1, 319, 194, 171, 243, 320, 194, 348, 175, 32, 321, 194, 319, 48, 262, 32, 322, 194, 319, 4, 321, 244, 262, 32, 323, 194, 171, 243, 320, 194, 349, 175, 32, 324, 194, 64, 243, 260, 108, 315, 175, 32, 325, 194, 171, 243, 320, 194, 350, 175, 48, 324, 32, 326, 194, 171, 243, 320, 194, 350, 175, 226, 324, 32, 247, 172, 322, 244, 265, 74, 321, 244, 259, 244, 266, 74, 323, 244, 267, 32, 248, 172, 322, 244, 269, 74, 321, 244, 270, 74, 323, 48, 264, 244, 271, 32, 249, 172, 322, 244, 274, 74, 321, 244, 259, 244, 275, 74, 323, 244, 276, 32, 250, 172, 322, 244, 278, 74, 321, 244, 279, 74, 323, 244, 280, 32, 257, 172, 322, 244, 302, 74, 321, 244, 303, 74, 323, 244, 304, 32, 251, 172, 322, 244, 282, 74, 321, 244, 283, 74, 323, 244, 284, 32, 254, 172, 322, 244, 289, 74, 321, 244, 259, 244, 290, 74, 323, 48, 264, 244, 291, 32, 255, 172, 322, 244, 293, 74, 321, 244, 294, 74, 323, 244, 295, 32, 182, 313, 62, 32, 252, 172, 322, 244, 286, 74, 321, 244, 259, 244, 287, 32, 185, 32, 327, 194, 325, 244, 314, 74, 76, 243, 350, 108, 314, 175, 32, 328, 194, 326, 244, 315, 74, 76, 243, 350, 108, 315, 175, 32, 329, 194, 39, 243, 259, 108, 263, 4, 321, 244, 259, 175, 32, 330, 194, 176, 243, 243, 314, 108, 315, 175, 108, 91, 194, 145, 175, 32, 331, 194, 55, 243, 251, 74, 327, 244, 285, 108, 332, 194, 327, 1, 329, 108, 333, 194, 350, 175, 82, 334, 243, 145, 175, 32, 335, 194, 55, 243, 251, 74, 243, 259, 4, 348, 175, 244, 285, 175, 82, 334, 243, 145, 175, 32, 182, 63, 313, 62, 32, 336, 194, 107, 243, 335, 4, 331, 175, 32, 185, 32, 30, 62, 32, 337, 194, 55, 243, 252, 74, 327, 244, 287, 108, 332, 194, 327, 1, 329, 108, 333, 194, 4, 348, 175, 32, 338, 194, 55, 243, 252, 74, 243, 329, 4, 348, 175, 244, 287, 175, 32, 336, 194, 205, 243, 337, 77, 338, 108, 107, 243, 335, 4, 331, 175, 108, 350, 175, 32, 56, 32, 339, 194, 76, 243, 350, 108, 317, 182, 318, 102, 317, 218, 348, 349, 351, 30, 316, 175, 32, 340, 194, 254, 74, 243, 327, 230, 62, 108, 202, 27, 244, 290, 74, 339, 230, 202, 108, 62, 27, 244, 292, 175, 32, 341, 194, 255, 74, 243, 328, 230, 202, 108, 62, 27, 244, 296, 74, 339, 230, 62, 108, 202, 27, 244, 297, 175, 32, 182, 318, 102, 317, 218, 348, 349, 351, 62, 32, 342, 194, 55, 243, 340, 108, 332, 194, 243, 327, 230, 62, 108, 202, 27, 1, 329, 175, 173, 243, 339, 230, 202, 108, 62, 27, 1, 261, 175, 108, 333, 194, 350, 175, 32, 343, 194, 55, 243, 341, 108, 332, 194, 243, 339, 230, 62, 108, 202, 27, 1, 261, 175, 173, 243, 328, 230, 202, 108, 62, 27, 1, 260, 175, 108, 333, 194, 350, 175, 32, 343, 194, 343, 82, 334, 243, 254, 82, 91, 82, 114, 175, 32, 330, 194, 15, 243, 342, 108, 343, 175, 244, 336, 230, 62, 108, 202, 27, 32, 185, 32, 30, 62, 32, 135, 344, 157, 5, 243, 350, 108, 261, 108, 316, 175, 62, 32, 342, 194, 55, 243, 340, 108, 332, 194, 243, 327, 230, 62, 108, 202, 27, 1, 329, 175, 173, 243, 339, 230, 202, 108, 62, 27, 1, 261, 4, 344, 175, 108, 333, 194, 350, 175, 32, 343, 194, 55, 243, 341, 108, 332, 194, 243, 339, 230, 62, 108, 202, 27, 1, 261, 4, 344, 175, 173, 243, 328, 230, 202, 108, 62, 27, 1, 260, 175, 108, 333, 194, 350, 175, 32, 343, 194, 343, 82, 334, 243, 254, 82, 91, 82, 114, 175, 32, 330, 172, 15, 243, 342, 108, 343, 175, 32, 340, 172, 316, 244, 292, 32, 341, 172, 316, 244, 297, 32, 78, 32, 330, 24, 336, 230, 62, 108, 202, 27, 32, 56, 32, 345, 194, 76, 243, 350, 108, 316, 175, 32, 346, 194, 248, 74, 243, 327, 230, 62, 108, 202, 27, 244, 272, 74, 345, 230, 202, 108, 62, 27, 244, 273, 175, 32, 352, 194, 249, 74, 243, 345, 230, 62, 108, 202, 27, 244, 275, 74, 328, 230, 202, 108, 62, 27, 244, 277, 175, 32, 353, 194, 251, 74, 345, 244, 285, 32, 354, 194, 329, 32, 355, 194, 325, 244, 314, 32, 346, 172, 355, 244, 273, 32, 352, 172, 355, 244, 275, 32, 353, 172, 355, 244, 285, 32, 135, 344, 157, 5, 243, 355, 108, 354, 108, 316, 175, 62, 32, 344, 194, 57, 243, 344, 108, 316, 175, 32, 356, 194, 55, 243, 346, 108, 332, 194, 243, 327, 230, 62, 108, 202, 27, 1, 259, 175, 173, 243, 345, 230, 202, 108, 62, 27, 1, 354, 4, 344, 175, 108, 333, 194, 350, 175, 32, 357, 194, 55, 243, 352, 108, 332, 194, 243, 345, 230, 62, 108, 202, 27, 1, 354, 4, 344, 175, 173, 243, 328, 230, 202, 108, 62, 27, 1, 260, 175, 108, 333, 194, 350, 175, 32, 358, 194, 55, 243, 353, 108, 332, 194, 345, 1, 354, 4, 344, 108, 333, 194, 350, 175, 82, 334, 243, 145, 175, 32, 356, 24, 107, 243, 358, 230, 202, 108, 62, 27, 4, 331, 230, 62, 108, 202, 27, 175, 32, 332, 194, 243, 344, 74, 345, 230, 202, 108, 62, 27, 144, 327, 230, 62, 108, 202, 27, 175, 173, 243, 344, 74, 345, 230, 202, 108, 62, 27, 1, 354, 175, 32, 356, 194, 205, 243, 332, 108, 356, 108, 350, 175, 32, 356, 194, 356, 82, 334, 243, 249, 82, 91, 82, 114, 175, 32, 330, 172, 15, 243, 356, 108, 357, 175, 32, 346, 172, 316, 244, 273, 32, 352, 172, 316, 244, 275, 32, 353, 172, 316, 244, 285, 32, 78, 32, 327, 194, 325, 244, 314, 74, 76, 243, 350, 108, 314, 175, 32, 328, 194, 326, 244, 315, 74, 76, 243, 350, 108, 315, 175, 32, 359, 194, 250, 74, 327, 244, 281, 32, 360, 194, 55, 243, 359, 108, 332, 194, 327, 1, 329, 108, 333, 194, 350, 175, 82, 334, 243, 145, 175, 32, 361, 194, 330, 244, 360, 230, 62, 108, 202, 27, 32, 256, 172, 322, 244, 298, 74, 321, 244, 259, 244, 299, 74, 323, 244, 300, 32, 362, 194, 256, 74, 243, 327, 230, 62, 108, 202, 27, 244, 299, 74, 328, 230, 202, 108, 62, 27, 244, 301, 175, 32, 182, 311, 62, 32, 363, 194, 249, 74, 243, 327, 230, 62, 108, 202, 27, 244, 275, 74, 328, 230, 202, 108, 62, 27, 244, 277, 175, 32, 364, 194, 55, 243, 363, 108, 332, 194, 243, 327, 230, 62, 108, 202, 27, 1, 329, 175, 173, 243, 328, 230, 202, 108, 62, 27, 1, 260, 175, 108, 333, 194, 350, 175, 82, 334, 243, 145, 175, 32, 182, 312, 62, 32, 365, 194, 55, 243, 253, 74, 323, 244, 288, 74, 328, 108, 332, 194, 328, 1, 260, 108, 333, 194, 350, 175, 82, 334, 243, 145, 175, 32, 185, 32, 30, 62, 32, 365, 194, 55, 243, 253, 74, 323, 244, 288, 175, 82, 334, 243, 145, 175, 32, 56, 32, 361, 172, 364, 244, 365, 32, 185, 32, 10, 243, 362, 108, 361, 108, 332, 194, 243, 327, 230, 62, 108, 202, 27, 1, 329, 175, 173, 243, 328, 230, 202, 108, 62, 27, 1, 260, 175, 175, 32, 366, 194, 247, 74, 243, 327, 230, 62, 108, 202, 27, 244, 266, 74, 328, 230, 202, 108, 62, 27, 244, 268, 175, 32, 367, 194, 55, 243, 366, 108, 332, 194, 243, 327, 230, 62, 108, 202, 27, 1, 329, 175, 173, 243, 328, 230, 202, 108, 62, 27, 1, 260, 175, 108, 333, 194, 350, 175, 82, 334, 243, 145, 175, 32, 182, 311, 62, 32, 258, 172, 322, 244, 306, 74, 321, 244, 307, 74, 323, 244, 308, 74, 325, 244, 309, 32, 182, 312, 62, 32, 368, 194, 258, 74, 328, 244, 310, 32, 369, 194, 221, 243, 364, 244, 367, 108, 320, 194, 350, 175, 32, 10, 243, 368, 108, 369, 108, 332, 194, 328, 1, 260, 175, 32, 185, 32, 30, 62, 32, 369, 194, 221, 243, 364, 244, 367, 175, 32, 10, 243, 258, 108, 369, 175, 32, 56, 32, 185, 32, 370, 194, 221, 243, 330, 244, 367, 108, 320, 194, 348, 175, 32, 371, 194, 257, 74, 327, 244, 305, 32, 196, 243, 371, 108, 370, 108, 332, 194, 327, 1, 259, 175, 32, 3, 32]}, {"code": "def _state_passing_fwd_kernel(\n    states_ptr,\n    out_ptr,\n    final_states_ptr,\n    dA_cs_ptr,\n    initstates_ptr,\n    seq_idx_ptr,\n    dim,\n    nchunks,\n    seqlen,\n    chunk_size,\n    stride_states_batch,\n    stride_states_chunk,\n    stride_states_head,\n    stride_states_dim,\n    stride_out_batch,\n    stride_out_chunk,\n    stride_out_head,\n    stride_out_dim,\n    stride_final_states_batch,\n    stride_final_states_head,\n    stride_final_states_dim,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_initstates_batch,\n    stride_initstates_head,\n    stride_initstates_dim,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    HAS_INITSTATES: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head\n    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head\n    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\n    final_states_ptr += (\n        pid_b * stride_final_states_batch + pid_h * stride_final_states_head\n    )\n    if HAS_INITSTATES:\n        initstates_ptr += (\n            pid_b * stride_initstates_batch + pid_h * stride_initstates_head\n        )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += pid_b * stride_seq_idx_batch\n\n    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    states_ptrs = states_ptr + offs_m * stride_states_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    final_states_ptrs = final_states_ptr + offs_m * stride_final_states_dim\n\n    if not HAS_INITSTATES:\n        states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    else:\n        initstates_ptrs = initstates_ptr + offs_m * stride_initstates_dim\n        states = tl.load(initstates_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    tl.store(out_ptrs, states, mask=offs_m < dim)\n    out_ptrs += stride_out_chunk\n    seq_idx = 0\n    for c in range(nchunks):\n        new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)\n        scale = tl.exp(dA_cs)\n        if HAS_SEQ_IDX:\n            seq_idx_new = tl.load(\n                seq_idx_ptr\n                + (min((c + 1) * chunk_size, seqlen) - 1) * stride_seq_idx_seqlen\n            )\n            scale = tl.where(seq_idx_new == seq_idx, scale, 0.0)\n            seq_idx = seq_idx_new\n        states = scale * states + new_states\n        if c < nchunks - 1:\n            tl.store(out_ptrs, states, mask=offs_m < dim)\n        else:\n            tl.store(final_states_ptrs, states, mask=offs_m < dim)\n        states_ptrs += stride_states_chunk\n        dA_cs_ptr += stride_dA_cs_chunk\n        out_ptrs += stride_out_chunk", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 170, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 63, 6, 108, 276, 63, 6, 108, 277, 63, 6, 175, 63, 32, -1, 278, 194, 171, 243, 279, 194, 348, 175, 32, 280, 194, 171, 243, 279, 194, 349, 175, 32, 281, 194, 171, 243, 279, 194, 350, 175, 32, 247, 172, 278, 244, 256, 74, 280, 244, 258, 32, 250, 172, 278, 244, 267, 74, 280, 244, 269, 32, 248, 172, 278, 244, 260, 74, 280, 244, 262, 32, 249, 172, 278, 244, 264, 74, 280, 244, 265, 32, 182, 275, 63, 32, 251, 172, 278, 244, 270, 74, 280, 244, 271, 32, 185, 32, 182, 276, 63, 32, 252, 172, 278, 244, 273, 32, 185, 32, 282, 194, 281, 244, 277, 74, 76, 243, 350, 108, 277, 175, 32, 283, 194, 247, 74, 282, 244, 259, 32, 284, 194, 248, 74, 282, 244, 263, 32, 285, 194, 249, 74, 282, 244, 266, 32, 182, 64, 275, 63, 32, 286, 194, 176, 243, 243, 277, 108, 175, 108, 91, 194, 145, 175, 32, 185, 32, 30, 63, 32, 287, 194, 251, 74, 282, 244, 272, 32, 286, 194, 57, 243, 287, 108, 288, 194, 282, 1, 170, 108, 289, 194, 350, 175, 82, 290, 243, 145, 175, 32, 56, 32, 10, 243, 284, 108, 286, 108, 288, 194, 282, 1, 170, 175, 32, 284, 172, 261, 32, 291, 194, 350, 32, 135, 292, 157, 5, 243, 253, 175, 63, 32, 293, 194, 57, 243, 283, 108, 288, 194, 282, 1, 170, 108, 289, 194, 350, 175, 82, 290, 243, 145, 175, 32, 294, 194, 57, 243, 250, 175, 82, 290, 243, 145, 175, 32, 295, 194, 107, 243, 294, 175, 32, 182, 276, 63, 32, 296, 194, 57, 243, 252, 74, 243, 39, 243, 243, 292, 74, 348, 175, 244, 255, 108, 254, 175, 4, 348, 175, 244, 274, 175, 32, 295, 194, 205, 243, 296, 77, 291, 108, 295, 108, 350, 175, 32, 291, 194, 296, 32, 185, 32, 286, 194, 295, 244, 286, 74, 293, 32, 182, 292, 1, 253, 4, 348, 63, 32, 10, 243, 284, 108, 286, 108, 288, 194, 282, 1, 170, 175, 32, 185, 32, 30, 63, 32, 10, 243, 285, 108, 286, 108, 288, 194, 282, 1, 170, 175, 32, 56, 32, 283, 172, 257, 32, 250, 172, 268, 32, 284, 172, 261, 32, 78, 32, 3, 32]}, {"code": "def _state_passing_bwd_kernel(\n    dout_ptr,\n    out_ptr,\n    dA_cs_ptr,\n    dfinal_states_ptr,\n    seq_idx_ptr,\n    dstates_ptr,\n    ddA_cs_ptr,\n    dinitstates_ptr,\n    states_converted_ptr,\n    dim,\n    nchunks,\n    seqlen,\n    chunk_size,\n    stride_dout_batch,\n    stride_dout_chunk,\n    stride_dout_head,\n    stride_dout_dim,\n    stride_out_batch,\n    stride_out_chunk,\n    stride_out_head,\n    stride_out_dim,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dfinal_states_batch,\n    stride_dfinal_states_head,\n    stride_dfinal_states_dim,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    stride_dstates_batch,\n    stride_dstates_chunk,\n    stride_dstates_head,\n    stride_dstates_dim,\n    stride_ddA_cs_batch,\n    stride_ddA_cs_chunk,\n    stride_ddA_cs_head,\n    stride_dinitstates_batch,\n    stride_dinitstates_head,\n    stride_dinitstates_dim,\n    CONVERT_STATES: tl.constexpr,\n    HAS_DFINAL_STATES: tl.constexpr,\n    HAS_DINITSTATES: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    dstates_ptr += (\n        pid_b * stride_dstates_batch\n        + pid_h * stride_dstates_head\n        + (nchunks - 1) * stride_dstates_chunk\n    )\n    dA_cs_ptr += (\n        pid_b * stride_dA_cs_batch\n        + pid_h * stride_dA_cs_head\n        + (nchunks - 1) * stride_dA_cs_chunk\n    )\n    ddA_cs_ptr += (\n        pid_b * stride_ddA_cs_batch\n        + pid_h * stride_ddA_cs_head\n        + (nchunks - 1) * stride_ddA_cs_chunk\n        + pid_m\n    )\n    out_ptr += (\n        pid_b * stride_out_batch\n        + pid_h * stride_out_head\n        + (nchunks - 1) * stride_out_chunk\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch\n        + pid_h * stride_dout_head\n        + (nchunks - 1) * stride_dout_chunk\n    )\n    if CONVERT_STATES:\n        states_converted_ptr += (\n            pid_b * stride_out_batch\n            + pid_h * stride_out_head\n            + (nchunks - 1) * stride_out_chunk\n        )\n    if HAS_DFINAL_STATES:\n        dfinal_states_ptr += (\n            pid_b * stride_dfinal_states_batch + pid_h * stride_dfinal_states_head\n        )\n    if HAS_DINITSTATES:\n        dinitstates_ptr += (\n            pid_b * stride_dinitstates_batch + pid_h * stride_dinitstates_head\n        )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += pid_b * stride_seq_idx_batch\n\n    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    dstates_ptrs = dstates_ptr + offs_m * stride_dstates_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    dout_ptrs = dout_ptr + offs_m * stride_dout_dim\n    if CONVERT_STATES:\n        states_converted_ptrs = states_converted_ptr + offs_m * stride_out_dim\n\n    if HAS_DFINAL_STATES:\n        dstates = tl.load(\n            dfinal_states_ptr + offs_m * stride_dfinal_states_dim,\n            mask=offs_m < dim,\n            other=0.0,\n        ).to(tl.float32)\n    else:\n        dstates = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    tl.store(dstates_ptrs, dstates, mask=offs_m < dim)\n    if HAS_SEQ_IDX:\n        seq_idx = tl.load(seq_idx_ptr + (seqlen - 1) * stride_seq_idx_seqlen)\n    dstates_ptrs -= stride_dstates_chunk\n    for c in range(nchunks - 1):\n        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)\n        scale = tl.exp(dA_cs)\n        if HAS_SEQ_IDX:\n            seq_idx_new = tl.load(\n                seq_idx_ptr\n                + (((nchunks - c - 1) * chunk_size - 1) * stride_seq_idx_seqlen)\n            )\n            scale = tl.where(seq_idx_new == seq_idx, scale, 0.0)\n            seq_idx = seq_idx_new\n        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        if CONVERT_STATES:\n            tl.store(states_converted_ptrs, out, mask=offs_m < dim)\n        ddA = tl.sum(out * dstates) * scale\n        tl.store(ddA_cs_ptr, ddA)\n        dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        dstates = scale * dstates + dout\n        tl.store(dstates_ptrs, dstates, mask=offs_m < dim)\n        dout_ptrs -= stride_dout_chunk\n        dstates_ptrs -= stride_dstates_chunk\n        dA_cs_ptr -= stride_dA_cs_chunk\n        ddA_cs_ptr -= stride_ddA_cs_chunk\n        out_ptrs -= stride_out_chunk\n        if CONVERT_STATES:\n            states_converted_ptrs -= stride_out_chunk\n    if CONVERT_STATES:\n        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        tl.store(states_converted_ptrs, out, mask=offs_m < dim)\n    if not HAS_DINITSTATES:\n        tl.store(ddA_cs_ptr, 0.0)\n    else:\n        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)\n        scale = tl.exp(dA_cs)\n        if HAS_SEQ_IDX:\n            scale = tl.where(seq_idx == 0, scale, 0.0)\n        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        ddA = tl.sum(out * dstates) * scale\n        tl.store(ddA_cs_ptr, ddA)\n        dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        dstates = scale * dstates + dout\n        tl.store(\n            dinitstates_ptr + offs_m * stride_dinitstates_dim,\n            dstates,\n            mask=offs_m < dim,\n        )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 170, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 62, 6, 108, 286, 62, 6, 108, 287, 62, 6, 108, 288, 62, 6, 108, 289, 62, 6, 175, 62, 32, -1, 290, 194, 171, 243, 291, 194, 348, 175, 32, 292, 194, 171, 243, 291, 194, 349, 175, 32, 293, 194, 171, 243, 291, 194, 350, 175, 32, 252, 172, 290, 244, 275, 74, 292, 244, 277, 74, 243, 256, 4, 348, 175, 244, 276, 32, 249, 172, 290, 244, 267, 74, 292, 244, 269, 74, 243, 256, 4, 348, 175, 244, 268, 32, 253, 172, 290, 244, 279, 74, 292, 244, 281, 74, 243, 256, 4, 348, 175, 244, 280, 74, 293, 32, 248, 172, 290, 244, 263, 74, 292, 244, 265, 74, 243, 256, 4, 348, 175, 244, 264, 32, 247, 172, 290, 244, 259, 74, 292, 244, 261, 74, 243, 256, 4, 348, 175, 244, 260, 32, 182, 285, 62, 32, 255, 172, 290, 244, 263, 74, 292, 244, 265, 74, 243, 256, 4, 348, 175, 244, 264, 32, 185, 32, 182, 286, 62, 32, 250, 172, 290, 244, 270, 74, 292, 244, 271, 32, 185, 32, 182, 287, 62, 32, 254, 172, 290, 244, 282, 74, 292, 244, 283, 32, 185, 32, 182, 288, 62, 32, 251, 172, 290, 244, 273, 32, 185, 32, 294, 194, 293, 244, 289, 74, 76, 243, 350, 108, 289, 175, 32, 295, 194, 252, 74, 294, 244, 278, 32, 296, 194, 248, 74, 294, 244, 266, 32, 297, 194, 247, 74, 294, 244, 262, 32, 182, 285, 62, 32, 298, 194, 255, 74, 294, 244, 266, 32, 185, 32, 182, 286, 62, 32, 299, 194, 55, 243, 250, 74, 294, 244, 272, 108, 300, 194, 294, 1, 170, 108, 301, 194, 350, 175, 82, 302, 243, 145, 175, 32, 185, 32, 30, 62, 32, 299, 194, 176, 243, 243, 289, 108, 175, 108, 91, 194, 145, 175, 32, 56, 32, 10, 243, 295, 108, 299, 108, 300, 194, 294, 1, 170, 175, 32, 182, 288, 62, 32, 303, 194, 55, 243, 251, 74, 243, 257, 4, 348, 175, 244, 274, 175, 32, 185, 32, 295, 2, 276, 32, 135, 304, 157, 5, 243, 256, 4, 348, 175, 62, 32, 305, 194, 55, 243, 249, 175, 82, 302, 243, 145, 175, 32, 306, 194, 107, 243, 305, 175, 32, 182, 288, 62, 32, 307, 194, 55, 243, 251, 74, 243, 243, 256, 4, 304, 4, 348, 175, 244, 258, 4, 348, 175, 244, 274, 175, 32, 306, 194, 205, 243, 307, 77, 303, 108, 306, 108, 350, 175, 32, 303, 194, 307, 32, 185, 32, 14, 194, 55, 243, 296, 108, 300, 194, 294, 1, 170, 108, 301, 194, 350, 175, 82, 302, 243, 145, 175, 32, 182, 285, 62, 32, 10, 243, 298, 108, 14, 108, 300, 194, 294, 1, 170, 175, 32, 185, 32, 308, 194, 221, 243, 14, 244, 299, 175, 244, 306, 32, 10, 243, 253, 108, 308, 175, 32, 309, 194, 55, 243, 297, 108, 300, 194, 294, 1, 170, 108, 301, 194, 350, 175, 82, 302, 243, 145, 175, 32, 299, 194, 306, 244, 299, 74, 309, 32, 10, 243, 295, 108, 299, 108, 300, 194, 294, 1, 170, 175, 32, 297, 2, 260, 32, 295, 2, 276, 32, 249, 2, 268, 32, 253, 2, 280, 32, 296, 2, 264, 32, 182, 285, 62, 32, 298, 2, 264, 32, 185, 32, 78, 32, 182, 285, 62, 32, 14, 194, 55, 243, 296, 108, 300, 194, 294, 1, 170, 108, 301, 194, 350, 175, 82, 302, 243, 145, 175, 32, 10, 243, 298, 108, 14, 108, 300, 194, 294, 1, 170, 175, 32, 185, 32, 182, 63, 287, 62, 32, 10, 243, 253, 108, 350, 175, 32, 185, 32, 30, 62, 32, 305, 194, 55, 243, 249, 175, 82, 302, 243, 145, 175, 32, 306, 194, 107, 243, 305, 175, 32, 182, 288, 62, 32, 306, 194, 205, 243, 303, 77, 350, 108, 306, 108, 350, 175, 32, 185, 32, 14, 194, 55, 243, 296, 108, 300, 194, 294, 1, 170, 108, 301, 194, 350, 175, 82, 302, 243, 145, 175, 32, 308, 194, 221, 243, 14, 244, 299, 175, 244, 306, 32, 10, 243, 253, 108, 308, 175, 32, 309, 194, 55, 243, 297, 108, 300, 194, 294, 1, 170, 108, 301, 194, 350, 175, 82, 302, 243, 145, 175, 32, 299, 194, 306, 244, 299, 74, 309, 32, 10, 243, 254, 74, 294, 244, 284, 108, 299, 108, 300, 194, 294, 1, 170, 175, 32, 56, 32, 3, 32]}, {"code": "def tuned_attn_fwd(\n    Q,\n    K,\n    V,\n    sm_scale,\n    M,\n    Out,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_on,\n    seqlen_q,\n    seqlen_k,\n    dropout_p,\n    philox_seed,\n    philox_offset_base,\n    encoded_softmax,\n    STAGE: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    pre_load_v: tl.constexpr,\n    ENABLE_DROPOUT: tl.constexpr,\n    RETURN_ENCODED_SOFTMAX: tl.constexpr,\n):\n    bare_attn_fwd(\n        Q,\n        K,\n        V,\n        sm_scale,\n        M,\n        Out,\n        stride_qz,\n        stride_qh,\n        stride_qm,\n        stride_qk,\n        stride_kz,\n        stride_kh,\n        stride_kn,\n        stride_kk,\n        stride_vz,\n        stride_vh,\n        stride_vk,\n        stride_vn,\n        stride_oz,\n        stride_oh,\n        stride_om,\n        stride_on,\n        seqlen_q,\n        seqlen_k,\n        dropout_p,\n        philox_seed,\n        philox_offset_base,\n        encoded_softmax,\n        STAGE,\n        BLOCK_M,\n        BLOCK_DMODEL,\n        BLOCK_N,\n        pre_load_v,\n        ENABLE_DROPOUT,\n        RETURN_ENCODED_SOFTMAX,\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 63, 6, 108, 276, 63, 6, 108, 277, 63, 6, 108, 278, 63, 6, 108, 279, 63, 6, 108, 280, 63, 6, 108, 281, 63, 6, 175, 63, 32, -1, 282, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 175, 32, 3, 32]}, {"code": "def tuned_attn_fwd(\n    Q,\n    K,\n    V,\n    B,\n    sm_scale,\n    M,\n    Out,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_bz,\n    stride_bh,\n    stride_bm,\n    stride_bn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_on,\n    num_head_q,\n    num_head_k,\n    cu_seqlens_q,\n    cu_seqlens_k,\n    num_seqlens,\n    max_seqlen_q,\n    max_seqlen_k,\n    head_dim,\n    dropout_p,\n    philox_seed_ptr,\n    philox_offset1,\n    philox_offset2,\n    philox_seed_output,\n    philox_offset_output,\n    encoded_softmax,\n    CAUSAL_TYPE: tl.constexpr,\n    Window_left,\n    Window_right,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    pre_load_v: tl.constexpr,\n    ENABLE_DROPOUT: tl.constexpr,\n    RETURN_ENCODED_SOFTMAX: tl.constexpr,\n    PADDED_HEAD: tl.constexpr,\n    BIAS_TYPE: tl.constexpr,\n):\n    bare_attn_fwd(\n        Q,\n        K,\n        V,\n        B,\n        sm_scale,\n        M,\n        Out,\n        stride_qz,\n        stride_qh,\n        stride_qm,\n        stride_qk,\n        stride_kz,\n        stride_kh,\n        stride_kn,\n        stride_kk,\n        stride_vz,\n        stride_vh,\n        stride_vk,\n        stride_vn,\n        stride_bz,\n        stride_bh,\n        stride_bm,\n        stride_bn,\n        stride_oz,\n        stride_oh,\n        stride_om,\n        stride_on,\n        num_head_q,\n        num_head_k,\n        cu_seqlens_q,\n        cu_seqlens_k,\n        num_seqlens,\n        max_seqlen_q,\n        max_seqlen_k,\n        head_dim,\n        dropout_p,\n        philox_seed_ptr,\n        philox_offset1,\n        philox_offset2,\n        philox_seed_output,\n        philox_offset_output,\n        encoded_softmax,\n        CAUSAL_TYPE,\n        Window_left,\n        Window_right,\n        BLOCK_M,\n        BLOCK_DMODEL,\n        BLOCK_N,\n        pre_load_v,\n        ENABLE_DROPOUT,\n        RETURN_ENCODED_SOFTMAX,\n        PADDED_HEAD,\n        BIAS_TYPE=BIAS_TYPE,\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 62, 6, 108, 290, 108, 291, 108, 292, 62, 6, 108, 293, 62, 6, 108, 294, 62, 6, 108, 295, 62, 6, 108, 296, 62, 6, 108, 297, 62, 6, 108, 298, 62, 6, 108, 299, 62, 6, 175, 62, 32, -1, 300, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 194, 299, 175, 32, 3, 32]}, {"code": "def tuned_attn_bwd(\n    Q,\n    K,\n    V,\n    B,\n    sm_scale,\n    Out,\n    DO,\n    DK,\n    DV,\n    DQ,\n    DB,\n    L,\n    D,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_bz,\n    stride_bh,\n    stride_bm,\n    stride_bn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_ok,\n    stride_dkz,\n    stride_dkh,\n    stride_dkn,\n    stride_dkk,\n    stride_dvz,\n    stride_dvh,\n    stride_dvk,\n    stride_dvn,\n    stride_dqz,\n    stride_dqh,\n    stride_dqm,\n    stride_dqk,\n    stride_dbz,\n    stride_dbh,\n    stride_dbm,\n    stride_dbn,\n    num_head_q,\n    num_head_k,\n    cu_seqlens_q,\n    cu_seqlens_k,\n    num_seqlens,\n    max_seqlen_q,\n    max_seqlen_k,\n    head_dim,\n    dropout_p,\n    philox_seed_ptr,\n    philox_offset1,\n    philox_offset2,\n    BLOCK_DMODEL: tl.constexpr,\n    CAUSAL: tl.constexpr,\n    ENABLE_DROPOUT: tl.constexpr,\n    PADDED_HEAD: tl.constexpr,\n    BIAS_TYPE: tl.constexpr,\n    BLOCK_M1: tl.constexpr,\n    BLOCK_N1: tl.constexpr,\n    BLOCK_M2: tl.constexpr,\n    BLOCK_N2: tl.constexpr,\n    BLK_SLICE_FACTOR: tl.constexpr,\n):\n    bare_attn_bwd(\n        Q,\n        K,\n        V,\n        B,\n        sm_scale,\n        Out,\n        DO,\n        DK,\n        DV,\n        DQ,\n        DB,\n        L,\n        D,\n        stride_qz,\n        stride_qh,\n        stride_qm,\n        stride_qk,\n        stride_kz,\n        stride_kh,\n        stride_kn,\n        stride_kk,\n        stride_vz,\n        stride_vh,\n        stride_vk,\n        stride_vn,\n        stride_bz,\n        stride_bh,\n        stride_bm,\n        stride_bn,\n        stride_oz,\n        stride_oh,\n        stride_om,\n        stride_ok,\n        stride_dkz,\n        stride_dkh,\n        stride_dkn,\n        stride_dkk,\n        stride_dvz,\n        stride_dvh,\n        stride_dvk,\n        stride_dvn,\n        stride_dqz,\n        stride_dqh,\n        stride_dqm,\n        stride_dqk,\n        stride_dbz,\n        stride_dbh,\n        stride_dbm,\n        stride_dbn,\n        num_head_q,\n        num_head_k,\n        cu_seqlens_q,\n        cu_seqlens_k,\n        num_seqlens,\n        max_seqlen_q,\n        max_seqlen_k,\n        head_dim,\n        dropout_p,\n        philox_seed_ptr,\n        philox_offset_base,\n        BLOCK_DMODEL,\n        CAUSAL,\n        ENABLE_DROPOUT,\n        PADDED_HEAD,\n        BIAS_TYPE,\n        BLOCK_M1,\n        BLOCK_N1,\n        BLOCK_M2,\n        BLOCK_N2,\n        BLK_SLICE_FACTOR,\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 108, 301, 108, 302, 108, 303, 108, 304, 108, 305, 108, 306, 108, 307, 108, 308, 63, 6, 108, 309, 63, 6, 108, 310, 63, 6, 108, 311, 63, 6, 108, 312, 63, 6, 108, 313, 63, 6, 108, 314, 63, 6, 108, 315, 63, 6, 108, 316, 63, 6, 108, 317, 63, 6, 175, 63, 32, -1, 318, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 108, 301, 108, 302, 108, 303, 108, 304, 108, 305, 108, 319, 108, 308, 108, 309, 108, 310, 108, 311, 108, 312, 108, 313, 108, 314, 108, 315, 108, 316, 108, 317, 175, 32, 3, 32]}, {"code": "def sized_tuned_bwd_kernel_dk_dv(\n    Q,\n    K,\n    V,\n    B,\n    sm_scale,\n    Out,\n    DO,\n    DK,\n    DV,\n    L,\n    D,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_bz,\n    stride_bh,\n    stride_bm,\n    stride_bn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_ok,\n    stride_dkz,\n    stride_dkh,\n    stride_dkn,\n    stride_dkk,\n    stride_dvz,\n    stride_dvh,\n    stride_dvk,\n    stride_dvn,\n    cu_seqlens_q,\n    cu_seqlens_k,\n    num_seqlens,\n    max_seqlen_q,\n    max_seqlen_k,\n    head_dim,\n    dropout_p,\n    philox_seed,\n    philox_offset_base,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    CAUSAL: tl.constexpr,\n    ENABLE_DROPOUT: tl.constexpr,\n    PADDED_HEAD: tl.constexpr,\n    BIAS_TYPE: tl.constexpr,\n):\n    bare_bwd_kernel_dk_dv(\n        Q,\n        K,\n        V,\n        B,\n        sm_scale,\n        Out,\n        DO,\n        DK,\n        DV,\n        L,\n        D,\n        stride_qz,\n        stride_qh,\n        stride_qm,\n        stride_qk,\n        stride_kz,\n        stride_kh,\n        stride_kn,\n        stride_kk,\n        stride_vz,\n        stride_vh,\n        stride_vk,\n        stride_vn,\n        stride_bz,\n        stride_bh,\n        stride_bm,\n        stride_bn,\n        stride_oz,\n        stride_oh,\n        stride_om,\n        stride_ok,\n        stride_dkz,\n        stride_dkh,\n        stride_dkn,\n        stride_dkk,\n        stride_dvz,\n        stride_dvh,\n        stride_dvk,\n        stride_dvn,\n        cu_seqlens_q,\n        cu_seqlens_k,\n        num_seqlens,\n        max_seqlen_q,\n        max_seqlen_k,\n        head_dim,\n        dropout_p,\n        philox_seed,\n        philox_offset_base,\n        BLOCK_M,\n        BLOCK_DMODEL,\n        BLOCK_N,\n        CAUSAL,\n        ENABLE_DROPOUT,\n        PADDED_HEAD=PADDED_HEAD,\n        BIAS_TYPE=BIAS_TYPE,\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 62, 6, 108, 296, 62, 6, 108, 297, 62, 6, 108, 298, 62, 6, 108, 299, 62, 6, 108, 300, 62, 6, 108, 301, 62, 6, 175, 62, 32, -1, 302, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 194, 300, 108, 301, 194, 301, 175, 32, 3, 32]}, {"code": "def sized_tuned_bwd_kernel_dq(\n    Q,\n    K,\n    V,\n    B,\n    sm_scale,\n    Out,\n    DO,\n    DQ,\n    DB,\n    L,\n    D,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_bz,\n    stride_bh,\n    stride_bm,\n    stride_bn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_ok,\n    stride_dqz,\n    stride_dqh,\n    stride_dqm,\n    stride_dqk,\n    stride_dbz,\n    stride_dbh,\n    stride_dbm,\n    stride_dbn,\n    cu_seqlens_q,\n    cu_seqlens_k,\n    num_seqlens,\n    max_seqlen_q,\n    max_seqlen_k,\n    head_dim,\n    dropout_p,\n    philox_seed,\n    philox_offset_base,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    CAUSAL: tl.constexpr,\n    ENABLE_DROPOUT: tl.constexpr,\n    PADDED_HEAD: tl.constexpr,\n    BIAS_TYPE: tl.constexpr,\n):\n    bare_bwd_kernel_dq(\n        Q,\n        K,\n        V,\n        B,\n        sm_scale,\n        Out,\n        DO,\n        DQ,\n        DB,\n        L,\n        D,\n        stride_qz,\n        stride_qh,\n        stride_qm,\n        stride_qk,\n        stride_kz,\n        stride_kh,\n        stride_kn,\n        stride_kk,\n        stride_vz,\n        stride_vh,\n        stride_vk,\n        stride_vn,\n        stride_bz,\n        stride_bh,\n        stride_bm,\n        stride_bn,\n        stride_oz,\n        stride_oh,\n        stride_om,\n        stride_ok,\n        stride_dqz,\n        stride_dqh,\n        stride_dqm,\n        stride_dqk,\n        stride_dbz,\n        stride_dbh,\n        stride_dbm,\n        stride_dbn,\n        cu_seqlens_q,\n        cu_seqlens_k,\n        num_seqlens,\n        max_seqlen_q,\n        max_seqlen_k,\n        head_dim,\n        dropout_p,\n        philox_seed,\n        philox_offset_base,\n        BLOCK_M,\n        BLOCK_DMODEL,\n        BLOCK_N,\n        CAUSAL,\n        ENABLE_DROPOUT,\n        PADDED_HEAD=PADDED_HEAD,\n        BIAS_TYPE=BIAS_TYPE,\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 63, 6, 108, 296, 63, 6, 108, 297, 63, 6, 108, 298, 63, 6, 108, 299, 63, 6, 108, 300, 63, 6, 108, 301, 63, 6, 175, 63, 32, -1, 302, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 194, 300, 108, 301, 194, 301, 175, 32, 3, 32]}, {"code": "def tuned_bwd_kernel_dk_dv(\n    Q,\n    K,\n    V,\n    B,\n    sm_scale,\n    Out,\n    DO,\n    DK,\n    DV,\n    L,\n    D,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_bz,\n    stride_bh,\n    stride_bm,\n    stride_bn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_ok,\n    stride_dkz,\n    stride_dkh,\n    stride_dkn,\n    stride_dkk,\n    stride_dvz,\n    stride_dvh,\n    stride_dvk,\n    stride_dvn,\n    cu_seqlens_q,\n    cu_seqlens_k,\n    num_seqlens,\n    max_seqlen_q,\n    max_seqlen_k,\n    head_dim,\n    dropout_p,\n    philox_seed,\n    philox_offset_base,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    CAUSAL: tl.constexpr,\n    ENABLE_DROPOUT: tl.constexpr,\n    PADDED_HEAD: tl.constexpr,\n    BIAS_TYPE: tl.constexpr,\n):\n    bare_bwd_kernel_dk_dv(\n        Q,\n        K,\n        V,\n        B,\n        sm_scale,\n        Out,\n        DO,\n        DK,\n        DV,\n        L,\n        D,\n        stride_qz,\n        stride_qh,\n        stride_qm,\n        stride_qk,\n        stride_kz,\n        stride_kh,\n        stride_kn,\n        stride_kk,\n        stride_vz,\n        stride_vh,\n        stride_vk,\n        stride_vn,\n        stride_bz,\n        stride_bh,\n        stride_bm,\n        stride_bn,\n        stride_oz,\n        stride_oh,\n        stride_om,\n        stride_ok,\n        stride_dkz,\n        stride_dkh,\n        stride_dkn,\n        stride_dkk,\n        stride_dvz,\n        stride_dvh,\n        stride_dvk,\n        stride_dvn,\n        cu_seqlens_q,\n        cu_seqlens_k,\n        num_seqlens,\n        max_seqlen_q,\n        max_seqlen_k,\n        head_dim,\n        dropout_p,\n        philox_seed,\n        philox_offset_base,\n        BLOCK_M,\n        BLOCK_DMODEL,\n        BLOCK_N,\n        CAUSAL,\n        ENABLE_DROPOUT,\n        PADDED_HEAD=PADDED_HEAD,\n        BIAS_TYPE=BIAS_TYPE,\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 62, 6, 108, 296, 62, 6, 108, 297, 62, 6, 108, 298, 62, 6, 108, 299, 62, 6, 108, 300, 62, 6, 108, 301, 62, 6, 175, 62, 32, -1, 302, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 194, 300, 108, 301, 194, 301, 175, 32, 3, 32]}, {"code": "def tuned_bwd_kernel_dq(\n    Q,\n    K,\n    V,\n    B,\n    sm_scale,\n    Out,\n    DO,\n    DQ,\n    DB,\n    L,\n    D,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_bz,\n    stride_bh,\n    stride_bm,\n    stride_bn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_ok,\n    stride_dqz,\n    stride_dqh,\n    stride_dqm,\n    stride_dqk,\n    stride_dbz,\n    stride_dbh,\n    stride_dbm,\n    stride_dbn,\n    cu_seqlens_q,\n    cu_seqlens_k,\n    num_seqlens,\n    max_seqlen_q,\n    max_seqlen_k,\n    head_dim,\n    dropout_p,\n    philox_seed,\n    philox_offset_base,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    CAUSAL: tl.constexpr,\n    ENABLE_DROPOUT: tl.constexpr,\n    PADDED_HEAD: tl.constexpr,\n    BIAS_TYPE: tl.constexpr,\n):\n    bare_bwd_kernel_dq(\n        Q,\n        K,\n        V,\n        B,\n        sm_scale,\n        Out,\n        DO,\n        DQ,\n        DB,\n        L,\n        D,\n        stride_qz,\n        stride_qh,\n        stride_qm,\n        stride_qk,\n        stride_kz,\n        stride_kh,\n        stride_kn,\n        stride_kk,\n        stride_vz,\n        stride_vh,\n        stride_vk,\n        stride_vn,\n        stride_bz,\n        stride_bh,\n        stride_bm,\n        stride_bn,\n        stride_oz,\n        stride_oh,\n        stride_om,\n        stride_ok,\n        stride_dqz,\n        stride_dqh,\n        stride_dqm,\n        stride_dqk,\n        stride_dbz,\n        stride_dbh,\n        stride_dbm,\n        stride_dbn,\n        cu_seqlens_q,\n        cu_seqlens_k,\n        num_seqlens,\n        max_seqlen_q,\n        max_seqlen_k,\n        head_dim,\n        dropout_p,\n        philox_seed,\n        philox_offset_base,\n        BLOCK_M,\n        BLOCK_DMODEL,\n        BLOCK_N,\n        CAUSAL,\n        ENABLE_DROPOUT,\n        PADDED_HEAD=PADDED_HEAD,\n        BIAS_TYPE=BIAS_TYPE,\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 63, 6, 108, 296, 63, 6, 108, 297, 63, 6, 108, 298, 63, 6, 108, 299, 63, 6, 108, 300, 63, 6, 108, 301, 63, 6, 175, 63, 32, -1, 302, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 108, 279, 108, 280, 108, 281, 108, 282, 108, 283, 108, 284, 108, 285, 108, 286, 108, 287, 108, 288, 108, 289, 108, 290, 108, 291, 108, 292, 108, 293, 108, 294, 108, 295, 108, 296, 108, 297, 108, 298, 108, 299, 108, 300, 194, 300, 108, 301, 194, 301, 175, 32, 3, 32]}, {"code": "def _attn_fwd(\n    Q,\n    K,\n    V,\n    sm_scale,\n    M,\n    Out,\n    L,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_on,\n    Z,\n    H,\n    H_KV,\n    N_CTX,\n    ROUND_CTX,\n    NKV_CTX,\n    sliding_window_offset,\n    sliding_window_size,\n    IS_EVEN_M: tl.constexpr,\n    IS_EVEN_N: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    END: tl.constexpr,\n    INIT: tl.constexpr,\n    SLIDING_WINDOW: tl.constexpr,\n    COMPLEMENT_SLIDING_WINDOW: tl.constexpr,\n):\n\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H // H_KV)\n    q_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    k_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64) * stride_kh\n    v_offset = off_z.to(tl.int64) * stride_vz + off_hkv.to(tl.int64) * stride_vh\n    o_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh\n\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + v_offset,\n        shape=(NKV_CTX, BLOCK_DMODEL),\n        strides=(stride_vk, stride_vn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + k_offset,\n        shape=(BLOCK_DMODEL, NKV_CTX),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1),\n    )\n    O_block_ptr = tl.make_block_ptr(\n        base=Out + o_offset,\n        shape=(ROUND_CTX, BLOCK_DMODEL),\n        strides=(stride_om, stride_on),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\n    m_ptrs = M + off_hz * ROUND_CTX + offs_m\n    l_ptrs = L + off_hz * ROUND_CTX + offs_m\n    if INIT:\n        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n        l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    else:\n\n        m_i = tl.load(m_ptrs).to(tl.float32)\n        l_i = tl.load(l_ptrs).to(tl.float32)\n        acc = tl.load(O_block_ptr).to(tl.float32)\n\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634\n\n    if IS_EVEN_M:\n        q = tl.load(Q_block_ptr)\n    else:\n        q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n\n    acc, l_i, m_i = _attn_fwd_inner(\n        acc,\n        l_i,\n        m_i,\n        q,\n        K_block_ptr,\n        V_block_ptr,\n        start_m,\n        qk_scale,\n        NKV_CTX,\n        sliding_window_offset,\n        sliding_window_size,\n        BLOCK_M,\n        BLOCK_DMODEL,\n        BLOCK_N,\n        SLIDING_WINDOW,\n        IS_EVEN_M,\n        IS_EVEN_N,\n        COMPLEMENT_SLIDING_WINDOW,\n    )\n\n    if END:\n        m_i += tl.math.log2(l_i)\n        acc = acc / l_i[:, None]\n    else:\n        tl.store(l_ptrs, l_i)\n\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty))", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 108, 273, 108, 274, 108, 275, 108, 276, 108, 277, 108, 278, 62, 6, 108, 279, 62, 6, 108, 280, 62, 6, 108, 281, 62, 6, 108, 282, 62, 6, 108, 283, 62, 6, 108, 284, 62, 6, 108, 285, 62, 6, 108, 286, 62, 6, 175, 62, 32, -1, 287, 194, 171, 243, 348, 175, 32, 288, 194, 171, 243, 349, 175, 32, 289, 194, 288, 48, 271, 32, 290, 194, 288, 226, 271, 32, 291, 194, 290, 48, 243, 271, 48, 272, 175, 32, 292, 194, 289, 82, 293, 243, 181, 175, 244, 254, 74, 290, 82, 293, 243, 181, 175, 244, 255, 32, 294, 194, 289, 82, 293, 243, 181, 175, 244, 258, 74, 291, 82, 293, 243, 181, 175, 244, 259, 32, 295, 194, 289, 82, 293, 243, 181, 175, 244, 262, 74, 291, 82, 293, 243, 181, 175, 244, 263, 32, 296, 194, 289, 82, 293, 243, 181, 175, 244, 266, 74, 290, 82, 293, 243, 181, 175, 244, 267, 32, 297, 194, 219, 243, 298, 194, 247, 74, 292, 108, 123, 194, 243, 273, 108, 281, 175, 108, 299, 194, 243, 256, 108, 257, 175, 108, 300, 194, 243, 287, 244, 280, 108, 348, 175, 108, 301, 194, 243, 280, 108, 281, 175, 108, 302, 194, 243, 349, 108, 348, 175, 175, 32, 303, 194, 219, 243, 298, 194, 249, 74, 295, 108, 123, 194, 243, 275, 108, 281, 175, 108, 299, 194, 243, 264, 108, 265, 175, 108, 300, 194, 243, 348, 108, 348, 175, 108, 301, 194, 243, 282, 108, 281, 175, 108, 302, 194, 243, 349, 108, 348, 175, 175, 32, 304, 194, 219, 243, 298, 194, 248, 74, 294, 108, 123, 194, 243, 281, 108, 275, 175, 108, 299, 194, 243, 261, 108, 260, 175, 108, 300, 194, 243, 348, 108, 348, 175, 108, 301, 194, 243, 281, 108, 282, 175, 108, 302, 194, 243, 348, 108, 349, 175, 175, 32, 305, 194, 219, 243, 298, 194, 252, 74, 296, 108, 123, 194, 243, 274, 108, 281, 175, 108, 299, 194, 243, 268, 108, 269, 175, 108, 300, 194, 243, 287, 244, 280, 108, 348, 175, 108, 301, 194, 243, 280, 108, 281, 175, 108, 302, 194, 243, 349, 108, 348, 175, 175, 32, 306, 194, 287, 244, 280, 74, 76, 243, 348, 108, 280, 175, 32, 307, 194, 251, 74, 288, 244, 274, 74, 306, 32, 308, 194, 253, 74, 288, 244, 274, 74, 306, 32, 182, 284, 62, 32, 309, 194, 176, 243, 230, 280, 27, 108, 91, 194, 145, 175, 4, 310, 243, 350, 175, 32, 311, 194, 176, 243, 230, 280, 27, 108, 91, 194, 145, 175, 74, 349, 32, 312, 194, 176, 243, 230, 280, 108, 281, 27, 108, 91, 194, 145, 175, 32, 185, 32, 30, 62, 32, 309, 194, 55, 243, 307, 175, 82, 293, 243, 145, 175, 32, 311, 194, 55, 243, 308, 175, 82, 293, 243, 145, 175, 32, 312, 194, 55, 243, 305, 175, 82, 293, 243, 145, 175, 32, 56, 32, 313, 194, 250, 32, 313, 24, 351, 32, 182, 278, 62, 32, 314, 194, 55, 243, 297, 175, 32, 185, 32, 30, 62, 32, 314, 194, 55, 243, 297, 108, 315, 194, 243, 348, 108, 349, 175, 108, 316, 194, 352, 175, 32, 56, 32, 312, 108, 311, 108, 309, 194, 317, 243, 312, 108, 311, 108, 309, 108, 314, 108, 304, 108, 303, 108, 287, 108, 313, 108, 275, 108, 276, 108, 277, 108, 280, 108, 281, 108, 282, 108, 285, 108, 278, 108, 279, 108, 286, 175, 32, 182, 283, 62, 32, 309, 172, 111, 243, 311, 175, 32, 312, 194, 312, 42, 311, 230, 62, 108, 202, 27, 32, 185, 32, 30, 62, 32, 10, 243, 308, 108, 311, 175, 32, 56, 32, 10, 243, 307, 108, 309, 175, 32, 10, 243, 305, 108, 312, 82, 293, 243, 252, 82, 187, 82, 114, 175, 175, 32, 3, 32]}, {"code": "def _score_kernel(\n    Q,\n    K,\n    M,\n    sm_scale,\n    Out,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_oz,\n    stride_oh,\n    stride_on,\n    Z,\n    H,\n    H_KV,\n    N_CTX,\n    ROUND_CTX,\n    NKV_CTX,\n    sliding_window_offset,\n    sliding_window_size,\n    SLIDING_WINDOW: tl.constexpr,\n    COMPLEMENT_SLIDING_WINDOW: tl.constexpr,\n    IS_EVEN_M: tl.constexpr,\n    IS_EVEN_N: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_n = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H // H_KV)\n    q_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    k_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64) * stride_kh\n    m_ptrs = M + off_hz * ROUND_CTX + tl.arange(0, BLOCK_M)\n    o = tl.zeros([BLOCK_M], dtype=tl.float32)\n\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + k_offset,\n        shape=(BLOCK_DMODEL, NKV_CTX),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, start_n * BLOCK_N),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1),\n    )\n\n    if IS_EVEN_N:\n        k = tl.load(K_block_ptr)\n    else:\n        k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n\n    lo = 0\n    hi = ROUND_CTX\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634\n\n    for start_m in range(lo, hi, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        if IS_EVEN_M:\n            q = tl.load(Q_block_ptr)\n        else:\n            q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n\n        m = tl.load(m_ptrs)\n\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n\n        if SLIDING_WINDOW:\n            dist = (\n                tl.arange(0, BLOCK_M)[:, None]\n                - tl.arange(0, BLOCK_N)[None, :]\n                + start_m\n                - start_n * BLOCK_N\n                + sliding_window_offset\n            )\n\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = dist >= sliding_window_size\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n\n        qk = qk - m[:, None]\n        p = tl.math.exp2(qk)\n\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n\n        if not IS_EVEN_N:\n            p = tl.where(((tl.arange(0, BLOCK_M) + start_m) < N_CTX)[:, None], p, 0)\n\n        o += tl.sum(p, axis=0)\n\n        Q_block_ptr = tl.advance(Q_block_ptr, offsets=(BLOCK_M, 0))\n        m_ptrs = m_ptrs + BLOCK_M\n\n    o_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh\n    o_range = tl.arange(0, BLOCK_N) + start_n * BLOCK_N\n    o_ptrs = Out + o_offset + o_range\n    tl.store(o_ptrs, o.to(Out.type.element_ty), mask=o_range < NKV_CTX)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 63, 6, 108, 272, 63, 6, 108, 273, 63, 6, 108, 274, 63, 6, 108, 275, 63, 6, 108, 276, 63, 6, 108, 277, 63, 6, 175, 63, 32, -1, 278, 194, 171, 243, 348, 175, 32, 279, 194, 171, 243, 349, 175, 32, 280, 194, 279, 48, 264, 32, 281, 194, 279, 226, 264, 32, 282, 194, 281, 48, 243, 264, 48, 265, 175, 32, 283, 194, 280, 82, 284, 243, 181, 175, 244, 252, 74, 281, 82, 284, 243, 181, 175, 244, 253, 32, 285, 194, 280, 82, 284, 243, 181, 175, 244, 256, 74, 282, 82, 284, 243, 181, 175, 244, 257, 32, 286, 194, 249, 74, 279, 244, 267, 74, 76, 243, 348, 108, 275, 175, 32, 287, 194, 176, 243, 230, 275, 27, 108, 91, 194, 145, 175, 32, 288, 194, 219, 243, 289, 194, 247, 74, 283, 108, 123, 194, 243, 266, 108, 276, 175, 108, 290, 194, 243, 254, 108, 255, 175, 108, 291, 194, 243, 348, 108, 348, 175, 108, 292, 194, 243, 275, 108, 276, 175, 108, 293, 194, 243, 349, 108, 348, 175, 175, 32, 294, 194, 219, 243, 289, 194, 248, 74, 285, 108, 123, 194, 243, 276, 108, 268, 175, 108, 290, 194, 243, 259, 108, 258, 175, 108, 291, 194, 243, 348, 108, 278, 244, 277, 175, 108, 292, 194, 243, 276, 108, 277, 175, 108, 293, 194, 243, 348, 108, 349, 175, 175, 32, 182, 274, 63, 32, 295, 194, 57, 243, 294, 175, 32, 185, 32, 30, 63, 32, 295, 194, 57, 243, 294, 108, 296, 194, 243, 348, 108, 349, 175, 108, 297, 194, 350, 175, 32, 56, 32, 298, 194, 348, 32, 299, 194, 267, 32, 300, 194, 250, 32, 300, 24, 351, 32, 135, 301, 157, 5, 243, 298, 108, 299, 108, 275, 175, 63, 32, 301, 194, 55, 243, 301, 108, 275, 175, 32, 182, 273, 63, 32, 302, 194, 57, 243, 288, 175, 32, 185, 32, 30, 63, 32, 302, 194, 57, 243, 288, 108, 296, 194, 243, 348, 108, 349, 175, 108, 297, 194, 350, 175, 32, 56, 32, 303, 194, 57, 243, 286, 175, 32, 304, 194, 176, 243, 230, 275, 108, 277, 27, 108, 91, 194, 145, 175, 32, 304, 172, 15, 243, 302, 108, 295, 175, 32, 304, 194, 304, 244, 300, 32, 182, 271, 63, 32, 305, 194, 76, 243, 348, 108, 275, 175, 230, 63, 108, 202, 27, 4, 76, 243, 348, 108, 277, 175, 230, 202, 108, 63, 27, 74, 301, 4, 278, 244, 277, 74, 269, 32, 182, 272, 63, 32, 306, 194, 305, 144, 270, 32, 185, 32, 30, 63, 32, 306, 194, 243, 305, 144, 348, 175, 173, 243, 305, 1, 270, 175, 32, 56, 32, 185, 32, 304, 194, 304, 4, 303, 230, 63, 108, 202, 27, 32, 307, 194, 155, 243, 304, 175, 32, 182, 271, 63, 32, 307, 194, 205, 243, 306, 108, 307, 108, 348, 175, 32, 185, 32, 182, 64, 274, 63, 32, 307, 194, 205, 243, 243, 76, 243, 348, 108, 275, 175, 74, 301, 1, 266, 175, 230, 63, 108, 202, 27, 108, 307, 108, 348, 175, 32, 185, 32, 287, 172, 221, 243, 307, 108, 308, 194, 348, 175, 32, 288, 194, 143, 243, 288, 108, 291, 194, 243, 275, 108, 348, 175, 175, 32, 286, 194, 286, 74, 275, 32, 78, 32, 309, 194, 280, 82, 284, 243, 181, 175, 244, 260, 74, 281, 82, 284, 243, 181, 175, 244, 261, 32, 310, 194, 76, 243, 348, 108, 277, 175, 74, 278, 244, 277, 32, 311, 194, 251, 74, 309, 74, 310, 32, 10, 243, 311, 108, 287, 82, 284, 243, 251, 82, 187, 82, 114, 175, 108, 306, 194, 310, 1, 268, 175, 32, 3, 32]}, {"code": "def _bmm_chunk_bwd_kernel(\n    a_ptr,\n    dout_ptr,\n    db_ptr,\n    res_ptr,\n    seqlen,\n    chunk_size,\n    K,\n    ngroups,\n    stride_a_batch,\n    stride_a_seqlen,\n    stride_a_head,\n    stride_ak,\n    stride_dout_batch,\n    stride_dout_chunk,\n    stride_dout_head,\n    stride_dout_csize_m,\n    stride_dout_csize_n,\n    stride_db_batch,\n    stride_db_seqlen,\n    stride_db_head,\n    stride_db_k,\n    stride_res_batch,\n    stride_res_seqlen,\n    stride_res_head,\n    stride_res_k,\n    dot_dtype: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_CS: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(K, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n\n    a_ptr += (\n        pid_b * stride_a_batch\n        + pid_c * chunk_size * stride_a_seqlen\n        + pid_h * stride_a_head\n    )\n    dout_ptr += (\n        pid_b * stride_dout_batch + pid_c * stride_dout_chunk + pid_h * stride_dout_head\n    )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_cs = tl.arange(0, BLOCK_SIZE_CS)\n    dout_ptrs = dout_ptr + (\n        offs_m[:, None] * stride_dout_csize_n + offs_cs[None, :] * stride_dout_csize_m\n    )\n    a_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] * stride_ak)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):\n        dout = tl.load(\n            dout_ptrs,\n            mask=(offs_m[:, None] < chunk_size)\n            & (offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS),\n            other=0.0,\n        ).to(dot_dtype)\n        a = tl.load(\n            a_ptrs,\n            mask=(offs_cs[:, None] < chunk_size_limit - cs * BLOCK_SIZE_CS)\n            & (offs_n[None, :] < K),\n            other=0.0,\n        ).to(dot_dtype)\n        acc += tl.dot(dout, a)\n        dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m\n        a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_RESIDUAL:\n        res_ptr += (\n            pid_b * stride_res_batch\n            + pid_c * chunk_size * stride_res_seqlen\n            + pid_h * stride_res_head\n        )\n        res_ptrs = res_ptr + (\n            offs_m[:, None] * stride_res_seqlen + offs_n[None, :] * stride_res_k\n        )\n        res = tl.load(\n            res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)\n        ).to(tl.float32)\n        acc += res\n    db = acc.to(db_ptr.dtype.element_ty)\n\n    db_ptr += (\n        pid_b * stride_db_batch\n        + pid_c * chunk_size * stride_db_seqlen\n        + pid_h * stride_db_head\n    )\n    db_ptrs = db_ptr + (\n        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_k\n    )\n    tl.store(\n        db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 108, 272, 62, 6, 108, 273, 62, 6, 108, 274, 62, 6, 108, 275, 62, 6, 108, 276, 62, 6, 175, 62, 32, -1, 277, 194, 171, 243, 278, 194, 348, 175, 32, 279, 194, 171, 243, 278, 194, 349, 175, 32, 280, 194, 279, 48, 254, 32, 281, 194, 279, 4, 280, 244, 254, 32, 282, 194, 64, 243, 253, 108, 275, 175, 32, 283, 194, 171, 243, 278, 194, 350, 175, 48, 282, 32, 284, 194, 171, 243, 278, 194, 350, 175, 226, 282, 32, 247, 172, 277, 244, 255, 74, 280, 244, 252, 244, 256, 74, 281, 244, 257, 32, 248, 172, 277, 244, 259, 74, 280, 244, 260, 74, 281, 244, 261, 32, 285, 194, 283, 244, 274, 74, 76, 243, 350, 108, 274, 175, 32, 286, 194, 284, 244, 275, 74, 76, 243, 350, 108, 275, 175, 32, 287, 194, 76, 243, 350, 108, 276, 175, 32, 288, 194, 248, 74, 243, 285, 230, 62, 108, 202, 27, 244, 263, 74, 287, 230, 202, 108, 62, 27, 244, 262, 175, 32, 289, 194, 247, 74, 243, 287, 230, 62, 108, 202, 27, 244, 256, 74, 286, 230, 202, 108, 62, 27, 244, 258, 175, 32, 290, 194, 39, 243, 252, 108, 251, 4, 280, 244, 252, 175, 32, 291, 194, 176, 243, 243, 274, 108, 275, 175, 108, 91, 194, 145, 175, 32, 135, 292, 157, 5, 243, 350, 108, 64, 243, 290, 108, 276, 175, 175, 62, 32, 293, 194, 55, 243, 288, 108, 294, 194, 243, 285, 230, 62, 108, 202, 27, 1, 252, 175, 173, 243, 287, 230, 202, 108, 62, 27, 1, 290, 4, 292, 244, 276, 175, 108, 295, 194, 350, 175, 82, 296, 243, 272, 175, 32, 297, 194, 55, 243, 289, 108, 294, 194, 243, 287, 230, 62, 108, 202, 27, 1, 290, 4, 292, 244, 276, 175, 173, 243, 286, 230, 202, 108, 62, 27, 1, 253, 175, 108, 295, 194, 350, 175, 82, 296, 243, 272, 175, 32, 291, 172, 15, 243, 293, 108, 297, 175, 32, 288, 172, 276, 244, 262, 32, 289, 172, 276, 244, 256, 32, 78, 32, 285, 194, 283, 244, 274, 74, 76, 243, 350, 108, 274, 175, 32, 286, 194, 284, 244, 275, 74, 76, 243, 350, 108, 275, 175, 32, 182, 273, 62, 32, 250, 172, 277, 244, 268, 74, 280, 244, 252, 244, 269, 74, 281, 244, 270, 32, 298, 194, 250, 74, 243, 285, 230, 62, 108, 202, 27, 244, 269, 74, 286, 230, 202, 108, 62, 27, 244, 271, 175, 32, 299, 194, 55, 243, 298, 108, 294, 194, 243, 285, 230, 62, 108, 202, 27, 1, 290, 175, 173, 243, 286, 230, 202, 108, 62, 27, 1, 253, 175, 175, 82, 296, 243, 145, 175, 32, 291, 172, 299, 32, 185, 32, 300, 194, 291, 82, 296, 243, 249, 82, 91, 82, 114, 175, 32, 249, 172, 277, 244, 264, 74, 280, 244, 252, 244, 265, 74, 281, 244, 266, 32, 301, 194, 249, 74, 243, 285, 230, 62, 108, 202, 27, 244, 265, 74, 286, 230, 202, 108, 62, 27, 244, 267, 175, 32, 10, 243, 301, 108, 300, 108, 294, 194, 243, 285, 230, 62, 108, 202, 27, 1, 290, 175, 173, 243, 286, 230, 202, 108, 62, 27, 1, 253, 175, 175, 32, 3, 32]}, {"code": "def _bmm_chunk_fwd_kernel(\n    a_ptr,\n    b_ptr,\n    out_ptr,\n    seq_idx_ptr,\n    seqlen,\n    chunk_size,\n    K,\n    ngroups,\n    stride_a_batch,\n    stride_a_seqlen,\n    stride_a_head,\n    stride_ak,\n    stride_b_batch,\n    stride_b_seqlen,\n    stride_b_head,\n    stride_bk,\n    stride_out_batch,\n    stride_out_chunk,\n    stride_out_head,\n    stride_outm,\n    stride_outn,\n    stride_seq_idx_batch,\n    stride_seq_idx_seqlen,\n    IS_CAUSAL: tl.constexpr,\n    dot_dtype: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    if IS_CAUSAL:\n        if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n            return\n    a_ptr += (\n        pid_b * stride_a_batch\n        + pid_c * chunk_size * stride_a_seqlen\n        + pid_h * stride_a_head\n    )\n    b_ptr += (\n        pid_b * stride_b_batch\n        + pid_c * chunk_size * stride_b_seqlen\n        + pid_h * stride_b_head\n    )\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (\n            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n        )\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_b_seqlen)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(\n            a_ptrs,\n            mask=(offs_m[:, None] < chunk_size_limit)\n            & (offs_k[None, :] < K - k * BLOCK_SIZE_K),\n            other=0.0,\n        ).to(dot_dtype)\n        b = tl.load(\n            b_ptrs,\n            mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K)\n            & (offs_n[None, :] < chunk_size_limit),\n            other=0.0,\n        ).to(dot_dtype)\n        acc += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_SEQ_IDX:\n        chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n        seq_idx_m = tl.load(\n            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit,\n            other=-1,\n        )\n        seq_idx_n = tl.load(\n            seq_idx_ptr + offs_n * stride_seq_idx_seqlen,\n            mask=offs_n < chunk_size_limit,\n            other=-2,\n        )\n        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)\n    out = acc.to(out_ptr.dtype.element_ty)\n\n    out_ptr += (\n        pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h * stride_out_head\n    )\n    out_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] * stride_outn)\n    tl.store(\n        out_ptrs,\n        out,\n        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 63, 6, 108, 271, 63, 6, 108, 272, 63, 6, 108, 273, 63, 6, 108, 274, 63, 6, 108, 275, 63, 6, 175, 63, 32, -1, 276, 194, 171, 243, 277, 194, 348, 175, 32, 278, 194, 171, 243, 277, 194, 349, 175, 32, 279, 194, 278, 48, 254, 32, 280, 194, 278, 4, 279, 244, 254, 32, 281, 194, 65, 243, 252, 108, 274, 175, 32, 282, 194, 171, 243, 277, 194, 350, 175, 48, 281, 32, 283, 194, 171, 243, 277, 194, 350, 175, 226, 281, 32, 182, 270, 63, 32, 182, 283, 244, 274, 144, 243, 282, 74, 348, 175, 244, 273, 63, 32, 231, 32, 185, 32, 185, 32, 247, 172, 276, 244, 255, 74, 279, 244, 252, 244, 256, 74, 280, 244, 257, 32, 248, 172, 276, 244, 259, 74, 279, 244, 252, 244, 260, 74, 280, 244, 261, 32, 182, 272, 63, 32, 250, 172, 276, 244, 268, 74, 279, 244, 252, 244, 269, 32, 185, 32, 284, 194, 282, 244, 273, 74, 76, 243, 350, 108, 273, 175, 32, 285, 194, 283, 244, 274, 74, 76, 243, 350, 108, 274, 175, 32, 286, 194, 76, 243, 350, 108, 275, 175, 32, 287, 194, 247, 74, 243, 284, 230, 63, 108, 202, 27, 244, 256, 74, 286, 230, 202, 108, 63, 27, 244, 258, 175, 32, 288, 194, 248, 74, 243, 286, 230, 63, 108, 202, 27, 244, 262, 74, 285, 230, 202, 108, 63, 27, 244, 260, 175, 32, 289, 194, 39, 243, 252, 108, 251, 4, 279, 244, 252, 175, 32, 290, 194, 176, 243, 243, 273, 108, 274, 175, 108, 91, 194, 145, 175, 32, 135, 291, 157, 5, 243, 350, 108, 65, 243, 253, 108, 275, 175, 175, 63, 32, 292, 194, 57, 243, 287, 108, 293, 194, 243, 284, 230, 63, 108, 202, 27, 1, 289, 175, 173, 243, 286, 230, 202, 108, 63, 27, 1, 253, 4, 291, 244, 275, 175, 108, 294, 194, 350, 175, 82, 295, 243, 271, 175, 32, 296, 194, 57, 243, 288, 108, 293, 194, 243, 286, 230, 63, 108, 202, 27, 1, 253, 4, 291, 244, 275, 175, 173, 243, 285, 230, 202, 108, 63, 27, 1, 289, 175, 108, 294, 194, 350, 175, 82, 295, 243, 271, 175, 32, 290, 172, 15, 243, 292, 108, 296, 175, 32, 287, 172, 275, 244, 258, 32, 288, 172, 275, 244, 262, 32, 78, 32, 284, 194, 282, 244, 273, 74, 76, 243, 350, 108, 273, 175, 32, 285, 194, 283, 244, 274, 74, 76, 243, 350, 108, 274, 175, 32, 182, 272, 63, 32, 289, 194, 39, 243, 252, 108, 251, 4, 279, 244, 252, 175, 32, 297, 194, 57, 243, 250, 74, 284, 244, 269, 108, 293, 194, 284, 1, 289, 108, 294, 194, 4, 348, 175, 32, 298, 194, 57, 243, 250, 74, 285, 244, 269, 108, 293, 194, 285, 1, 289, 108, 294, 194, 4, 349, 175, 32, 290, 194, 205, 243, 297, 230, 63, 108, 202, 27, 77, 298, 230, 202, 108, 63, 27, 108, 290, 108, 350, 175, 32, 185, 32, 14, 194, 290, 82, 295, 243, 249, 82, 91, 82, 114, 175, 32, 249, 172, 276, 244, 263, 74, 279, 244, 264, 74, 280, 244, 265, 32, 299, 194, 249, 74, 243, 266, 244, 284, 230, 63, 108, 202, 27, 74, 285, 230, 202, 108, 63, 27, 244, 267, 175, 32, 10, 243, 299, 108, 14, 108, 293, 194, 243, 284, 230, 63, 108, 202, 27, 1, 252, 175, 173, 243, 285, 230, 202, 108, 63, 27, 1, 252, 175, 175, 32, 3, 32]}, {"code": "def bmm_kernel(\n    A,\n    B,\n    O,\n    M,\n    N,\n    K,\n    TILE_M: tl.constexpr,\n    TILE_N: tl.constexpr,\n    TILE_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    DIVISIBLE_M: tl.constexpr,\n    DIVISIBLE_N: tl.constexpr,\n    DIVISIBLE_K: tl.constexpr,\n):\n\n    pid_b = tl.program_id(2)\n    A += pid_b * M * K\n    B += pid_b * K * N\n    O += pid_b * M * N\n\n    pidx = tl.program_id(0)\n    pidy = tl.program_id(1)\n\n    if GROUP_M == 1:\n        pid_m, pid_n = pidx, pidy\n    else:\n\n        gridx = tl.num_programs(0)\n        gridy = tl.num_programs(1)\n        pid = pidx + pidy * gridx\n\n        num_CTA_per_group = gridy * GROUP_M\n\n        group_id = pid // num_CTA_per_group\n        inner_group_id = pid % num_CTA_per_group\n        if (group_id * GROUP_M + GROUP_M) > gridx:\n            GROUP_SIZE = gridx % GROUP_M\n        else:\n            GROUP_SIZE = GROUP_M\n        pid_m = group_id * GROUP_M + inner_group_id % GROUP_SIZE\n        pid_n = inner_group_id // GROUP_SIZE\n\n    offs_m = pid_m * TILE_M + tl.arange(0, TILE_M)\n    offs_n = pid_n * TILE_N + tl.arange(0, TILE_N)\n    offs_k = tl.arange(0, TILE_K)\n\n    if not DIVISIBLE_M:\n        mask_m = offs_m < M\n    if not DIVISIBLE_N:\n        mask_n = offs_n < N\n\n    a_ptrs = A + offs_m[:, None] * K + offs_k[None, :]\n    b_ptrs = B + offs_k[:, None] * N + offs_n[None, :]\n    o_ptrs = O + offs_m[:, None] * N + offs_n[None, :]\n\n    num_iters = tl.cdiv(K, TILE_K)\n    o = tl.zeros((TILE_M, TILE_N), dtype=tl.float32)\n    for _ in range(num_iters):\n        if DIVISIBLE_K:\n            if DIVISIBLE_M:\n                mask_a = None\n            else:\n                mask_a = mask_m[:, None]\n            if DIVISIBLE_N:\n                mask_b = None\n            else:\n                mask_b = mask_n[None, :]\n        else:\n            mask_k = offs_k < K\n            if DIVISIBLE_M:\n                mask_a = mask_k[None, :]\n            else:\n                mask_a = mask_m[:, None] & mask_k[None, :]\n            if DIVISIBLE_N:\n                mask_b = mask_k[:, None]\n            else:\n                mask_b = mask_k[:, None] & mask_n[None, :]\n\n        a = tl.load(a_ptrs, mask_a)\n        b = tl.load(b_ptrs, mask_b)\n\n        offs_k += TILE_K\n        a_ptrs += TILE_K\n        b_ptrs += TILE_K * N\n\n        o += tl.dot(a, b, allow_tf32=False)\n\n    if DIVISIBLE_M and DIVISIBLE_N:\n        mask_c = None\n    elif DIVISIBLE_M and not DIVISIBLE_N:\n        mask_c = mask_n[None, :]\n    elif not DIVISIBLE_M and DIVISIBLE_N:\n        mask_c = mask_m[:, None]\n    else:\n        mask_c = mask_m[:, None] & mask_n[None, :]\n    tl.store(o_ptrs, o, mask_c)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 62, 6, 108, 254, 62, 6, 108, 255, 62, 6, 108, 256, 62, 6, 108, 257, 62, 6, 108, 258, 62, 6, 108, 259, 62, 6, 175, 62, 32, -1, 260, 194, 171, 243, 348, 175, 32, 247, 172, 260, 244, 250, 244, 252, 32, 248, 172, 260, 244, 252, 244, 251, 32, 249, 172, 260, 244, 250, 244, 251, 32, 261, 194, 171, 243, 349, 175, 32, 262, 194, 171, 243, 350, 175, 32, 182, 256, 77, 350, 62, 32, 263, 108, 264, 194, 243, 261, 108, 262, 175, 32, 185, 32, 30, 62, 32, 265, 194, 138, 243, 349, 175, 32, 266, 194, 138, 243, 350, 175, 32, 267, 194, 261, 74, 262, 244, 265, 32, 268, 194, 266, 244, 256, 32, 269, 194, 267, 48, 268, 32, 270, 194, 267, 226, 268, 32, 182, 269, 244, 256, 74, 256, 124, 265, 62, 32, 271, 194, 265, 226, 256, 32, 185, 32, 30, 62, 32, 271, 194, 256, 32, 56, 32, 263, 194, 269, 244, 256, 74, 270, 226, 271, 32, 264, 194, 270, 48, 271, 32, 56, 32, 272, 194, 263, 244, 253, 74, 76, 243, 349, 108, 253, 175, 32, 273, 194, 264, 244, 254, 74, 76, 243, 349, 108, 254, 175, 32, 274, 194, 76, 243, 349, 108, 255, 175, 32, 182, 63, 257, 62, 32, 275, 194, 272, 1, 250, 32, 185, 32, 182, 63, 258, 62, 32, 276, 194, 273, 1, 251, 32, 185, 32, 277, 194, 247, 74, 272, 230, 62, 108, 202, 27, 244, 252, 74, 274, 230, 202, 108, 62, 27, 32, 278, 194, 248, 74, 274, 230, 62, 108, 202, 27, 244, 251, 74, 273, 230, 202, 108, 62, 27, 32, 279, 194, 249, 74, 272, 230, 62, 108, 202, 27, 244, 251, 74, 273, 230, 202, 108, 62, 27, 32, 280, 194, 64, 243, 252, 108, 255, 175, 32, 281, 194, 176, 243, 243, 253, 108, 254, 175, 108, 91, 194, 145, 175, 32, 135, 282, 157, 5, 243, 280, 175, 62, 32, 182, 259, 62, 32, 182, 257, 62, 32, 283, 194, 202, 32, 185, 32, 30, 62, 32, 283, 194, 275, 230, 62, 108, 202, 27, 32, 56, 32, 182, 258, 62, 32, 284, 194, 202, 32, 185, 32, 30, 62, 32, 284, 194, 276, 230, 202, 108, 62, 27, 32, 56, 32, 185, 32, 30, 62, 32, 285, 194, 274, 1, 252, 32, 182, 257, 62, 32, 283, 194, 285, 230, 202, 108, 62, 27, 32, 185, 32, 30, 62, 32, 283, 194, 275, 230, 62, 108, 202, 27, 173, 285, 230, 202, 108, 62, 27, 32, 56, 32, 182, 258, 62, 32, 284, 194, 285, 230, 62, 108, 202, 27, 32, 185, 32, 30, 62, 32, 284, 194, 285, 230, 62, 108, 202, 27, 173, 276, 230, 202, 108, 62, 27, 32, 56, 32, 56, 32, 286, 194, 55, 243, 277, 108, 283, 175, 32, 287, 194, 55, 243, 278, 108, 284, 175, 32, 274, 172, 255, 32, 277, 172, 255, 32, 278, 172, 255, 244, 251, 32, 281, 172, 15, 243, 286, 108, 287, 108, 288, 194, 59, 175, 32, 78, 32, 182, 257, 102, 258, 62, 32, 289, 194, 202, 32, 65, 32, 37, 257, 102, 243, 63, 258, 175, 62, 32, 289, 194, 276, 230, 202, 108, 62, 27, 32, 65, 32, 37, 63, 257, 102, 258, 62, 32, 289, 194, 275, 230, 62, 108, 202, 27, 32, 185, 32, 30, 62, 32, 289, 194, 275, 230, 62, 108, 202, 27, 173, 276, 230, 202, 108, 62, 27, 32, 56, 32, 10, 243, 279, 108, 281, 108, 289, 175, 32, 3, 32]}, {"code": "def chunk_simple_gla_bwd_kernel_dqkg(\n    q,\n    k,\n    v,\n    h,\n    g,\n    do,\n    dh,\n    dq,\n    dk,\n    dg,\n    s_k_h,\n    s_k_t,\n    s_v_h,\n    s_v_t,\n    s_h_h,\n    s_h_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n):\n\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n\n    p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    last_idx = min(i_t * BT + BT, T) - 1\n    b_g_last = tl.load(g + i_bh * T + last_idx)\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    b_dg_last = tl.zeros(\n        [\n            1,\n        ],\n        dtype=tl.float32,\n    )\n    b_dg = tl.zeros(\n        [\n            BT,\n        ],\n        dtype=tl.float32,\n    )\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n        )\n        p_h = tl.make_block_ptr(\n            h + i_bh * s_h_h,\n            (V, NT * K),\n            (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK),\n            (BV, BK),\n            (0, 1),\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * s_v_h,\n            (T, V),\n            (s_v_t, 1),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dh = tl.make_block_ptr(\n            dh + i_bh * s_h_h,\n            (V, NT * K),\n            (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK),\n            (BV, BK),\n            (0, 1),\n        )\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        b_dg_last += tl.sum(b_h * b_dh)\n        b_ds += tl.dot(b_do, tl.trans(b_v))\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\n\n    p_q = tl.make_block_ptr(\n        q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_k = tl.make_block_ptr(\n        k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_dg_last *= tl.exp(b_g_last)\n    b_dq = b_dq * tl.exp(b_g)[:, None] * scale\n    b_dk = b_dk * tl.exp(-b_g + b_g_last)[:, None]\n    b_dg_last += tl.sum(b_dk * b_k)\n    b_ds = tl.where(\n        o_i[:, None] >= o_i[None, :],\n        b_ds * scale * tl.exp(b_g[:, None] - b_g[None, :]),\n        0,\n    )\n    b_ds = b_ds.to(b_k.dtype)\n\n    b_dq += tl.dot(b_ds, b_k)\n    b_dk += tl.dot(tl.trans(b_ds), b_q)\n    b_dg += tl.sum(b_q * b_dq - b_k * b_dk, axis=1)\n\n    b_dg = tl.where(o_i < min(BT, T - i_t * BT) - 1, b_dg, b_dg + b_dg_last)\n    p_dq = tl.make_block_ptr(\n        dq + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_dk = tl.make_block_ptr(\n        dk + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n    )\n    p_dg = tl.make_block_ptr(\n        dg + (i_k * n_bh + i_bh) * T, (T,), (1,), (i_t * BT,), (BT,), (0,)\n    )\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 63, 6, 108, 265, 63, 6, 108, 266, 63, 6, 108, 267, 63, 6, 108, 268, 63, 6, 108, 269, 63, 6, 108, 270, 63, 6, 175, 63, 32, -1, 271, 108, 272, 108, 273, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 108, 171, 243, 350, 175, 175, 32, 274, 194, 138, 243, 350, 175, 32, 275, 194, 76, 243, 348, 108, 267, 175, 32, 276, 194, 219, 243, 251, 74, 273, 244, 264, 108, 243, 264, 108, 175, 108, 243, 349, 108, 175, 108, 243, 272, 244, 267, 108, 175, 108, 243, 267, 108, 175, 108, 243, 348, 108, 175, 175, 32, 277, 194, 57, 243, 276, 108, 278, 194, 243, 348, 108, 175, 175, 32, 279, 194, 39, 243, 272, 244, 267, 74, 267, 108, 264, 175, 4, 349, 32, 280, 194, 57, 243, 251, 74, 273, 244, 264, 74, 279, 175, 32, 281, 194, 176, 243, 230, 267, 108, 268, 27, 108, 91, 194, 145, 175, 32, 282, 194, 176, 243, 230, 267, 108, 268, 27, 108, 91, 194, 145, 175, 32, 283, 194, 176, 243, 230, 267, 108, 267, 27, 108, 91, 194, 145, 175, 32, 284, 194, 176, 243, 230, 349, 27, 108, 91, 194, 145, 175, 32, 285, 194, 176, 243, 230, 267, 27, 108, 91, 194, 145, 175, 32, 135, 286, 157, 5, 243, 65, 243, 266, 108, 269, 175, 175, 63, 32, 287, 194, 219, 243, 249, 74, 273, 244, 259, 108, 243, 264, 108, 266, 175, 108, 243, 260, 108, 349, 175, 108, 243, 272, 244, 267, 108, 286, 244, 269, 175, 108, 243, 267, 108, 269, 175, 108, 243, 349, 108, 348, 175, 175, 32, 288, 194, 219, 243, 250, 74, 273, 244, 261, 108, 243, 266, 108, 270, 244, 265, 175, 108, 243, 349, 108, 262, 175, 108, 243, 286, 244, 269, 108, 272, 244, 265, 74, 271, 244, 268, 175, 108, 243, 269, 108, 268, 175, 108, 243, 348, 108, 349, 175, 175, 32, 289, 194, 219, 243, 252, 74, 273, 244, 259, 108, 243, 264, 108, 266, 175, 108, 243, 260, 108, 349, 175, 108, 243, 272, 244, 267, 108, 286, 244, 269, 175, 108, 243, 267, 108, 269, 175, 108, 243, 349, 108, 348, 175, 175, 32, 290, 194, 219, 243, 253, 74, 273, 244, 261, 108, 243, 266, 108, 270, 244, 265, 175, 108, 243, 349, 108, 262, 175, 108, 243, 286, 244, 269, 108, 272, 244, 265, 74, 271, 244, 268, 175, 108, 243, 269, 108, 268, 175, 108, 243, 348, 108, 349, 175, 175, 32, 291, 194, 57, 243, 287, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 292, 194, 57, 243, 289, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 293, 194, 57, 243, 288, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 294, 194, 57, 243, 290, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 284, 172, 221, 243, 293, 244, 294, 175, 32, 283, 172, 15, 243, 292, 108, 72, 243, 291, 175, 175, 32, 281, 172, 15, 243, 292, 108, 293, 82, 295, 243, 292, 82, 91, 175, 175, 32, 282, 172, 15, 243, 291, 108, 294, 82, 295, 243, 291, 82, 91, 175, 175, 32, 78, 32, 296, 194, 219, 243, 247, 74, 273, 244, 257, 108, 243, 264, 108, 265, 175, 108, 243, 258, 108, 349, 175, 108, 243, 272, 244, 267, 108, 271, 244, 268, 175, 108, 243, 267, 108, 268, 175, 108, 243, 349, 108, 348, 175, 175, 32, 297, 194, 219, 243, 248, 74, 273, 244, 257, 108, 243, 264, 108, 265, 175, 108, 243, 258, 108, 349, 175, 108, 243, 272, 244, 267, 108, 271, 244, 268, 175, 108, 243, 267, 108, 268, 175, 108, 243, 349, 108, 348, 175, 175, 32, 298, 194, 57, 243, 297, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 299, 194, 57, 243, 296, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 284, 24, 107, 243, 280, 175, 32, 281, 194, 281, 244, 107, 243, 277, 175, 230, 63, 108, 202, 27, 244, 263, 32, 282, 194, 282, 244, 107, 243, 4, 277, 74, 280, 175, 230, 63, 108, 202, 27, 32, 284, 172, 221, 243, 282, 244, 298, 175, 32, 283, 194, 205, 243, 275, 230, 63, 108, 202, 27, 144, 275, 230, 202, 108, 63, 27, 108, 283, 244, 263, 244, 107, 243, 277, 230, 63, 108, 202, 27, 4, 277, 230, 202, 108, 63, 27, 175, 108, 348, 175, 32, 283, 194, 283, 82, 295, 243, 298, 82, 91, 175, 32, 281, 172, 15, 243, 283, 108, 298, 175, 32, 282, 172, 15, 243, 72, 243, 283, 175, 108, 299, 175, 32, 285, 172, 221, 243, 299, 244, 281, 4, 298, 244, 282, 108, 300, 194, 349, 175, 32, 285, 194, 205, 243, 275, 1, 39, 243, 267, 108, 264, 4, 272, 244, 267, 175, 4, 349, 108, 285, 108, 285, 74, 284, 175, 32, 301, 194, 219, 243, 254, 74, 273, 244, 257, 108, 243, 264, 108, 265, 175, 108, 243, 258, 108, 349, 175, 108, 243, 272, 244, 267, 108, 271, 244, 268, 175, 108, 243, 267, 108, 268, 175, 108, 243, 349, 108, 348, 175, 175, 32, 302, 194, 219, 243, 255, 74, 273, 244, 257, 108, 243, 264, 108, 265, 175, 108, 243, 258, 108, 349, 175, 108, 243, 272, 244, 267, 108, 271, 244, 268, 175, 108, 243, 267, 108, 268, 175, 108, 243, 349, 108, 348, 175, 175, 32, 303, 194, 219, 243, 256, 74, 243, 271, 244, 274, 74, 273, 175, 244, 264, 108, 243, 264, 108, 175, 108, 243, 349, 108, 175, 108, 243, 272, 244, 267, 108, 175, 108, 243, 267, 108, 175, 108, 243, 348, 108, 175, 175, 32, 10, 243, 301, 108, 281, 82, 295, 243, 301, 82, 91, 82, 114, 175, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 10, 243, 302, 108, 282, 82, 295, 243, 302, 82, 91, 82, 114, 175, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 10, 243, 303, 108, 285, 82, 295, 243, 303, 82, 91, 82, 114, 175, 108, 278, 194, 243, 348, 108, 175, 175, 32, 3, 32]}, {"code": "def chunk_global_cumsum_scalar_kernel(\n    s,\n    o,\n    T: tl.constexpr,\n    BT: tl.constexpr,\n):\n    i_bh = tl.program_id(0)\n    b_z = tl.zeros([], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\n        b_o = tl.cumsum(b_s, axis=0) + b_z[None]\n        b_zz = tl.sum(b_s, axis=0)\n        b_z += b_zz\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 62, 6, 108, 250, 62, 6, 175, 62, 32, -1, 251, 194, 171, 243, 348, 175, 32, 252, 194, 176, 243, 230, 27, 108, 91, 194, 145, 175, 32, 135, 253, 157, 5, 243, 64, 243, 249, 108, 250, 175, 175, 62, 32, 254, 194, 219, 243, 247, 74, 251, 244, 249, 108, 243, 249, 108, 175, 108, 243, 349, 108, 175, 108, 243, 253, 244, 250, 108, 175, 108, 243, 250, 108, 175, 108, 243, 348, 108, 175, 175, 32, 255, 194, 219, 243, 248, 74, 251, 244, 249, 108, 243, 249, 108, 175, 108, 243, 349, 108, 175, 108, 243, 253, 244, 250, 108, 175, 108, 243, 250, 108, 175, 108, 243, 348, 108, 175, 175, 32, 256, 194, 55, 243, 254, 108, 257, 194, 243, 348, 108, 175, 175, 82, 258, 243, 145, 175, 32, 259, 194, 85, 243, 256, 108, 260, 194, 348, 175, 74, 252, 230, 202, 27, 32, 261, 194, 221, 243, 256, 108, 260, 194, 348, 175, 32, 252, 172, 261, 32, 10, 243, 255, 108, 259, 82, 258, 243, 255, 82, 91, 82, 114, 175, 108, 257, 194, 243, 348, 108, 175, 175, 32, 78, 32, 3, 32]}, {"code": "def chunk_global_cumsum_vector_kernel(\n    s,\n    z,\n    s_s_h,\n    s_s_t,\n    s_s_d,\n    T: tl.constexpr,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(\n            s + i_bh * s_s_h,\n            (T, S),\n            (s_s_t, s_s_d),\n            (i_t * BT, i_s * BS),\n            (BT, BS),\n            (1, 0),\n        )\n        p_z = tl.make_block_ptr(\n            z + i_bh * s_s_h,\n            (T, S),\n            (s_s_t, s_s_d),\n            (i_t * BT, i_s * BS),\n            (BT, BS),\n            (1, 0),\n        )\n\n        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 63, 6, 108, 253, 63, 6, 108, 254, 63, 6, 108, 255, 63, 6, 175, 63, 32, -1, 256, 108, 257, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 175, 32, 258, 194, 76, 243, 348, 108, 254, 175, 32, 259, 194, 205, 243, 258, 230, 63, 108, 202, 27, 144, 258, 230, 202, 108, 63, 27, 108, 349, 108, 348, 175, 32, 260, 194, 176, 243, 230, 255, 27, 108, 91, 194, 145, 175, 32, 135, 261, 157, 5, 243, 65, 243, 252, 108, 254, 175, 175, 63, 32, 262, 194, 219, 243, 247, 74, 257, 244, 249, 108, 243, 252, 108, 253, 175, 108, 243, 250, 108, 251, 175, 108, 243, 261, 244, 254, 108, 256, 244, 255, 175, 108, 243, 254, 108, 255, 175, 108, 243, 349, 108, 348, 175, 175, 32, 263, 194, 219, 243, 248, 74, 257, 244, 249, 108, 243, 252, 108, 253, 175, 108, 243, 250, 108, 251, 175, 108, 243, 261, 244, 254, 108, 256, 244, 255, 175, 108, 243, 254, 108, 255, 175, 108, 243, 349, 108, 348, 175, 175, 32, 264, 194, 57, 243, 262, 108, 265, 194, 243, 348, 108, 349, 175, 175, 82, 266, 243, 145, 175, 32, 267, 194, 260, 230, 202, 108, 63, 27, 74, 15, 243, 259, 108, 264, 108, 268, 194, 59, 175, 32, 10, 243, 263, 108, 267, 82, 266, 243, 263, 82, 91, 82, 114, 175, 108, 265, 194, 243, 348, 108, 349, 175, 175, 32, 182, 261, 144, 348, 63, 32, 260, 172, 221, 243, 264, 108, 348, 175, 32, 185, 32, 78, 32, 3, 32]}, {"code": "def chunk_delta_rule_fwd_kernel_h(\n    k,\n    v,\n    d,\n    v_new,\n    h,\n    initial_state,\n    final_state,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(\n            initial_state + i_bh * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT):\n        p_h = tl.make_block_ptr(\n            h + i_bh * s_h_h + i_t * K * V,\n            (K, V),\n            (s_h_t, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_h_cumsum = tl.zeros([BK, BV], dtype=tl.float32)\n        for i_c in range(tl.cdiv(BT, BC)):\n            p_k = tl.make_block_ptr(\n                k + i_bh * s_qk_h,\n                (K, T),\n                (s_qk_d, s_qk_t),\n                (i_k * BK, i_t * BT + i_c * BC),\n                (BK, BC),\n                (0, 1),\n            )\n            p_d = tl.make_block_ptr(\n                d + i_bh * s_qk_h,\n                (T, K),\n                (s_qk_t, s_qk_d),\n                (i_t * BT + i_c * BC, i_k * BK),\n                (BC, BK),\n                (1, 0),\n            )\n            p_v = tl.make_block_ptr(\n                v + i_bh * s_vo_h,\n                (T, V),\n                (s_vo_t, s_vo_d),\n                (i_t * BT + i_c * BC, i_v * BV),\n                (BC, BV),\n                (1, 0),\n            )\n            p_v_new = tl.make_block_ptr(\n                v_new + i_bh * s_vo_h,\n                (T, V),\n                (s_vo_t, s_vo_d),\n                (i_t * BT + i_c * BC, i_v * BV),\n                (BC, BV),\n                (1, 0),\n            )\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_d = tl.load(p_d, boundary_check=(0, 1))\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n            b_v -= tl.dot(b_d, b_h.to(b_k.dtype), allow_tf32=False)\n            tl.store(p_v_new, b_v.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))\n            b_h_cumsum += tl.dot(b_k, b_v.to(b_k.dtype), allow_tf32=False)\n        b_h += b_h_cumsum\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(\n            final_state + i_bh * K * V,\n            (K, V),\n            (V, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 108, 266, 62, 6, 108, 267, 62, 6, 108, 268, 62, 6, 108, 269, 62, 6, 108, 270, 62, 6, 108, 271, 62, 6, 108, 272, 62, 6, 175, 62, 32, -1, 273, 108, 274, 108, 275, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 108, 171, 243, 350, 175, 175, 32, 276, 194, 176, 243, 230, 268, 108, 269, 27, 108, 91, 194, 145, 175, 32, 182, 271, 62, 32, 277, 194, 219, 243, 252, 74, 275, 244, 264, 244, 265, 108, 243, 264, 108, 265, 175, 108, 243, 265, 108, 349, 175, 108, 243, 273, 244, 268, 108, 274, 244, 269, 175, 108, 243, 268, 108, 269, 175, 108, 243, 349, 108, 348, 175, 175, 32, 276, 194, 55, 243, 277, 108, 278, 194, 243, 348, 108, 349, 175, 175, 82, 279, 243, 145, 175, 32, 185, 32, 135, 280, 157, 5, 243, 270, 175, 62, 32, 281, 194, 219, 243, 251, 74, 275, 244, 260, 74, 280, 244, 264, 244, 265, 108, 243, 264, 108, 265, 175, 108, 243, 261, 108, 349, 175, 108, 243, 273, 244, 268, 108, 274, 244, 269, 175, 108, 243, 268, 108, 269, 175, 108, 243, 349, 108, 348, 175, 175, 32, 10, 243, 281, 108, 276, 82, 279, 243, 281, 82, 91, 82, 114, 175, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 282, 194, 176, 243, 230, 268, 108, 269, 27, 108, 91, 194, 145, 175, 32, 135, 283, 157, 5, 243, 64, 243, 266, 108, 267, 175, 175, 62, 32, 284, 194, 219, 243, 247, 74, 275, 244, 254, 108, 243, 264, 108, 263, 175, 108, 243, 256, 108, 255, 175, 108, 243, 273, 244, 268, 108, 280, 244, 266, 74, 283, 244, 267, 175, 108, 243, 268, 108, 267, 175, 108, 243, 348, 108, 349, 175, 175, 32, 285, 194, 219, 243, 249, 74, 275, 244, 254, 108, 243, 263, 108, 264, 175, 108, 243, 255, 108, 256, 175, 108, 243, 280, 244, 266, 74, 283, 244, 267, 108, 273, 244, 268, 175, 108, 243, 267, 108, 268, 175, 108, 243, 349, 108, 348, 175, 175, 32, 286, 194, 219, 243, 248, 74, 275, 244, 257, 108, 243, 263, 108, 265, 175, 108, 243, 258, 108, 259, 175, 108, 243, 280, 244, 266, 74, 283, 244, 267, 108, 274, 244, 269, 175, 108, 243, 267, 108, 269, 175, 108, 243, 349, 108, 348, 175, 175, 32, 287, 194, 219, 243, 250, 74, 275, 244, 257, 108, 243, 263, 108, 265, 175, 108, 243, 258, 108, 259, 175, 108, 243, 280, 244, 266, 74, 283, 244, 267, 108, 274, 244, 269, 175, 108, 243, 267, 108, 269, 175, 108, 243, 349, 108, 348, 175, 175, 32, 288, 194, 55, 243, 284, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 289, 194, 55, 243, 285, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 290, 194, 55, 243, 286, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 290, 2, 15, 243, 289, 108, 276, 82, 279, 243, 288, 82, 91, 175, 108, 291, 194, 59, 175, 32, 10, 243, 287, 108, 290, 82, 279, 243, 287, 82, 91, 82, 114, 175, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 282, 172, 15, 243, 288, 108, 290, 82, 279, 243, 288, 82, 91, 175, 108, 291, 194, 59, 175, 32, 78, 32, 276, 172, 282, 32, 78, 32, 182, 272, 62, 32, 292, 194, 219, 243, 253, 74, 275, 244, 264, 244, 265, 108, 243, 264, 108, 265, 175, 108, 243, 265, 108, 349, 175, 108, 243, 273, 244, 268, 108, 274, 244, 269, 175, 108, 243, 268, 108, 269, 175, 108, 243, 349, 108, 348, 175, 175, 32, 10, 243, 292, 108, 276, 82, 279, 243, 292, 82, 91, 82, 114, 175, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 185, 32, 3, 32]}, {"code": "def chunk_gated_abc_fwd_kernel_cum(\n    s,\n    o,\n    s_s_h,\n    s_s_t,\n    s_s_d,\n    T: tl.constexpr,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0).to(tl.float32)\n\n    p_s = tl.make_block_ptr(\n        s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0)\n    )\n    p_o = tl.make_block_ptr(\n        o + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0)\n    )\n\n    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n    b_o = tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 63, 6, 108, 253, 63, 6, 108, 254, 63, 6, 108, 255, 63, 6, 175, 63, 32, -1, 256, 108, 257, 108, 258, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 108, 171, 243, 350, 175, 175, 32, 259, 194, 76, 243, 348, 108, 254, 175, 32, 260, 194, 205, 243, 259, 230, 63, 108, 202, 27, 144, 259, 230, 202, 108, 63, 27, 108, 349, 108, 348, 175, 82, 261, 243, 145, 175, 32, 262, 194, 219, 243, 247, 74, 258, 244, 249, 108, 243, 252, 108, 253, 175, 108, 243, 250, 108, 251, 175, 108, 243, 257, 244, 254, 108, 256, 244, 255, 175, 108, 243, 254, 108, 255, 175, 108, 243, 349, 108, 348, 175, 175, 32, 263, 194, 219, 243, 248, 74, 258, 244, 249, 108, 243, 252, 108, 253, 175, 108, 243, 250, 108, 251, 175, 108, 243, 257, 244, 254, 108, 256, 244, 255, 175, 108, 243, 254, 108, 255, 175, 108, 243, 349, 108, 348, 175, 175, 32, 264, 194, 57, 243, 262, 108, 265, 194, 243, 348, 108, 349, 175, 175, 82, 261, 243, 145, 175, 32, 266, 194, 15, 243, 260, 108, 264, 108, 267, 194, 59, 175, 32, 10, 243, 263, 108, 266, 82, 261, 243, 263, 82, 91, 82, 114, 175, 108, 265, 194, 243, 348, 108, 349, 175, 175, 32, 3, 32]}, {"code": "def chunk_gla_fwd_A_kernel_intra_sub_inter(\n    q,\n    k,\n    g,\n    A,\n    s_k_h,\n    s_k_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr,\n):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_i, i_j = i_c // NC, i_c % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    if i_i <= i_j:\n        return\n\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        m_k = o_k < K\n\n        p_q = tl.make_block_ptr(\n            q + i_bh * s_k_h,\n            (T, K),\n            (s_k_t, 1),\n            (i_t * BT + i_i * BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_g = tl.make_block_ptr(\n            g + i_bh * s_k_h,\n            (T, K),\n            (s_k_t, 1),\n            (i_t * BT + i_i * BC, i_k * BK),\n            (BC, BK),\n            (1, 0),\n        )\n        p_k = tl.make_block_ptr(\n            k + i_bh * s_k_h,\n            (K, T),\n            (1, s_k_t),\n            (i_k * BK, i_t * BT + i_j * BC),\n            (BK, BC),\n            (0, 1),\n        )\n        p_gk = tl.make_block_ptr(\n            g + i_bh * s_k_h,\n            (K, T),\n            (1, s_k_t),\n            (i_k * BK, i_t * BT + i_j * BC),\n            (BK, BC),\n            (0, 1),\n        )\n        p_gn = tl.max_contiguous(\n            tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + i_i * BC) * K + o_k, BK), BK\n        )\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g - b_gn[None, :]) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)\n        b_A += tl.dot(b_qg, b_kg)\n\n    p_A = tl.make_block_ptr(\n        A + i_bh * T * BT,\n        (T, BT),\n        (BT, 1),\n        (i_t * BT + i_i * BC, i_j * BC),\n        (BC, BC),\n        (1, 0),\n    )\n    tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 62, 6, 108, 255, 62, 6, 108, 256, 62, 6, 108, 257, 62, 6, 108, 258, 62, 6, 108, 259, 62, 6, 175, 62, 32, -1, 260, 108, 261, 108, 262, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 108, 171, 243, 350, 175, 175, 32, 263, 108, 264, 194, 243, 261, 48, 259, 108, 261, 226, 259, 175, 32, 182, 260, 244, 256, 74, 263, 244, 257, 144, 254, 62, 32, 231, 32, 185, 32, 182, 263, 218, 264, 62, 32, 231, 32, 185, 32, 265, 194, 176, 243, 230, 257, 108, 257, 27, 108, 91, 194, 145, 175, 32, 135, 266, 157, 5, 243, 64, 243, 255, 108, 258, 175, 175, 62, 32, 267, 194, 266, 244, 258, 74, 76, 243, 348, 108, 258, 175, 32, 268, 194, 267, 1, 255, 32, 269, 194, 219, 243, 247, 74, 262, 244, 251, 108, 243, 254, 108, 255, 175, 108, 243, 252, 108, 349, 175, 108, 243, 260, 244, 256, 74, 263, 244, 257, 108, 266, 244, 258, 175, 108, 243, 257, 108, 258, 175, 108, 243, 349, 108, 348, 175, 175, 32, 270, 194, 219, 243, 249, 74, 262, 244, 251, 108, 243, 254, 108, 255, 175, 108, 243, 252, 108, 349, 175, 108, 243, 260, 244, 256, 74, 263, 244, 257, 108, 266, 244, 258, 175, 108, 243, 257, 108, 258, 175, 108, 243, 349, 108, 348, 175, 175, 32, 271, 194, 219, 243, 248, 74, 262, 244, 251, 108, 243, 255, 108, 254, 175, 108, 243, 349, 108, 252, 175, 108, 243, 266, 244, 258, 108, 260, 244, 256, 74, 264, 244, 257, 175, 108, 243, 258, 108, 257, 175, 108, 243, 348, 108, 349, 175, 175, 32, 272, 194, 219, 243, 249, 74, 262, 244, 251, 108, 243, 255, 108, 254, 175, 108, 243, 349, 108, 252, 175, 108, 243, 266, 244, 258, 108, 260, 244, 256, 74, 264, 244, 257, 175, 108, 243, 258, 108, 257, 175, 108, 243, 348, 108, 349, 175, 175, 32, 273, 194, 211, 243, 57, 243, 249, 74, 262, 244, 251, 74, 243, 260, 244, 256, 74, 263, 244, 257, 175, 244, 255, 74, 267, 108, 258, 175, 108, 258, 175, 32, 274, 194, 55, 243, 273, 108, 275, 194, 268, 108, 276, 194, 348, 175, 32, 277, 194, 55, 243, 269, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 279, 194, 55, 243, 270, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 280, 194, 277, 244, 107, 243, 279, 4, 274, 230, 202, 108, 62, 27, 175, 244, 253, 32, 281, 194, 55, 243, 271, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 282, 194, 55, 243, 272, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 283, 194, 281, 244, 107, 243, 274, 230, 62, 108, 202, 27, 4, 282, 175, 32, 265, 172, 15, 243, 280, 108, 283, 175, 32, 78, 32, 284, 194, 219, 243, 250, 74, 262, 244, 254, 244, 256, 108, 243, 254, 108, 256, 175, 108, 243, 256, 108, 349, 175, 108, 243, 260, 244, 256, 74, 263, 244, 257, 108, 264, 244, 257, 175, 108, 243, 257, 108, 257, 175, 108, 243, 349, 108, 348, 175, 175, 32, 10, 243, 284, 108, 265, 82, 285, 243, 250, 82, 91, 82, 114, 175, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 3, 32]}, {"code": "def chunk_gla_fwd_A_kernel_intra_sub_intra(\n    q,\n    k,\n    g,\n    A,\n    s_k_h,\n    s_k_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n):\n    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_j = i_i\n    if i_t * BT + i_i * BC >= T:\n        return\n\n    o_i = tl.arange(0, BC)\n    o_k = tl.arange(0, BK)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_j * BC\n    m_k = o_k < K\n    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n\n    p_q = tl.make_block_ptr(\n        q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0)\n    )\n    p_g = tl.make_block_ptr(\n        g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0)\n    )\n\n    p_k = tl.max_contiguous(\n        tl.multiple_of(k + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK\n    )\n    p_gk = tl.max_contiguous(\n        tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)\n        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n        b_A = tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.0)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += K\n        p_gk += K", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 63, 6, 108, 255, 63, 6, 108, 256, 63, 6, 108, 257, 63, 6, 108, 258, 63, 6, 175, 63, 32, -1, 259, 108, 260, 108, 261, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 108, 171, 243, 350, 175, 175, 32, 262, 194, 260, 32, 182, 259, 244, 256, 74, 260, 244, 257, 144, 254, 63, 32, 231, 32, 185, 32, 263, 194, 76, 243, 348, 108, 257, 175, 32, 264, 194, 76, 243, 348, 108, 258, 175, 32, 265, 194, 261, 244, 254, 244, 256, 74, 243, 259, 244, 256, 74, 260, 244, 257, 74, 76, 243, 348, 108, 257, 175, 175, 244, 256, 74, 262, 244, 257, 32, 266, 194, 264, 1, 255, 32, 267, 194, 259, 244, 256, 74, 260, 244, 257, 74, 76, 243, 348, 108, 257, 175, 1, 254, 32, 268, 194, 219, 243, 247, 74, 261, 244, 251, 108, 243, 254, 108, 255, 175, 108, 243, 252, 108, 349, 175, 108, 243, 259, 244, 256, 74, 260, 244, 257, 108, 348, 175, 108, 243, 257, 108, 258, 175, 108, 243, 349, 108, 348, 175, 175, 32, 269, 194, 219, 243, 249, 74, 261, 244, 251, 108, 243, 254, 108, 255, 175, 108, 243, 252, 108, 349, 175, 108, 243, 259, 244, 256, 74, 260, 244, 257, 108, 348, 175, 108, 243, 257, 108, 258, 175, 108, 243, 349, 108, 348, 175, 175, 32, 270, 194, 211, 243, 55, 243, 248, 74, 261, 244, 251, 74, 243, 259, 244, 256, 74, 262, 244, 257, 175, 244, 255, 74, 264, 108, 258, 175, 108, 258, 175, 32, 271, 194, 211, 243, 55, 243, 249, 74, 261, 244, 251, 74, 243, 259, 244, 256, 74, 262, 244, 257, 175, 244, 255, 74, 264, 108, 258, 175, 108, 258, 175, 32, 272, 194, 57, 243, 268, 108, 273, 194, 243, 348, 108, 349, 175, 175, 32, 274, 194, 57, 243, 269, 108, 273, 194, 243, 348, 108, 349, 175, 175, 32, 135, 275, 157, 5, 243, 348, 108, 39, 243, 257, 108, 254, 4, 259, 244, 256, 4, 260, 244, 257, 175, 175, 63, 32, 276, 194, 57, 243, 270, 108, 277, 194, 266, 108, 278, 194, 348, 175, 82, 279, 243, 145, 175, 32, 280, 194, 57, 243, 271, 108, 277, 194, 266, 108, 278, 194, 348, 175, 82, 279, 243, 145, 175, 32, 281, 194, 221, 243, 272, 244, 276, 230, 202, 108, 63, 27, 244, 107, 243, 274, 4, 280, 230, 202, 108, 63, 27, 175, 108, 349, 175, 32, 281, 194, 205, 243, 263, 144, 275, 108, 281, 244, 253, 108, 348, 175, 32, 10, 243, 250, 74, 265, 74, 275, 108, 281, 108, 277, 194, 267, 175, 32, 270, 172, 255, 32, 271, 172, 255, 32, 78, 32, 3, 32]}, {"code": "def chunk_gla_fwd_A_kernel_intra_sub_intra_split(\n    q,\n    k,\n    g,\n    A,\n    s_k_h,\n    s_k_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr,\n):\n    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_tc // NC, i_tc % NC\n    i_j = i_i\n    n_bh = tl.num_programs(2)\n    if i_t * BT + i_i * BC >= T:\n        return\n\n    o_i = tl.arange(0, BC)\n    o_k = i_k * BK + tl.arange(0, BK)\n    o_A = (i_bh + i_k * n_bh) * T * BC + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BC\n    m_k = o_k < K\n    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n\n    p_q = tl.make_block_ptr(\n        q + i_bh * s_k_h,\n        (T, K),\n        (s_k_t, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    p_g = tl.make_block_ptr(\n        g + i_bh * s_k_h,\n        (T, K),\n        (s_k_t, 1),\n        (i_t * BT + i_i * BC, i_k * BK),\n        (BC, BK),\n        (1, 0),\n    )\n    p_k = tl.max_contiguous(\n        tl.multiple_of(k + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK\n    )\n    p_gk = tl.max_contiguous(\n        tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK\n    )\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_A = tl.zeros([BC], dtype=tl.float32)\n        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)\n        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n        b_A += tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.0)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += K\n        p_gk += K", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 62, 6, 108, 255, 62, 6, 108, 256, 62, 6, 108, 257, 62, 6, 108, 258, 62, 6, 108, 259, 62, 6, 175, 62, 32, -1, 260, 108, 261, 108, 262, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 108, 171, 243, 350, 175, 175, 32, 263, 108, 264, 194, 243, 261, 48, 259, 108, 261, 226, 259, 175, 32, 265, 194, 264, 32, 266, 194, 138, 243, 350, 175, 32, 182, 263, 244, 256, 74, 264, 244, 257, 144, 254, 62, 32, 231, 32, 185, 32, 267, 194, 76, 243, 348, 108, 257, 175, 32, 268, 194, 260, 244, 258, 74, 76, 243, 348, 108, 258, 175, 32, 269, 194, 243, 262, 74, 260, 244, 266, 175, 244, 254, 244, 257, 74, 243, 263, 244, 256, 74, 264, 244, 257, 74, 76, 243, 348, 108, 257, 175, 175, 244, 257, 32, 270, 194, 268, 1, 255, 32, 271, 194, 263, 244, 256, 74, 264, 244, 257, 74, 76, 243, 348, 108, 257, 175, 1, 254, 32, 272, 194, 219, 243, 247, 74, 262, 244, 251, 108, 243, 254, 108, 255, 175, 108, 243, 252, 108, 349, 175, 108, 243, 263, 244, 256, 74, 264, 244, 257, 108, 260, 244, 258, 175, 108, 243, 257, 108, 258, 175, 108, 243, 349, 108, 348, 175, 175, 32, 273, 194, 219, 243, 249, 74, 262, 244, 251, 108, 243, 254, 108, 255, 175, 108, 243, 252, 108, 349, 175, 108, 243, 263, 244, 256, 74, 264, 244, 257, 108, 260, 244, 258, 175, 108, 243, 257, 108, 258, 175, 108, 243, 349, 108, 348, 175, 175, 32, 274, 194, 211, 243, 57, 243, 248, 74, 262, 244, 251, 74, 243, 263, 244, 256, 74, 265, 244, 257, 175, 244, 255, 74, 268, 108, 258, 175, 108, 258, 175, 32, 275, 194, 211, 243, 57, 243, 249, 74, 262, 244, 251, 74, 243, 263, 244, 256, 74, 265, 244, 257, 175, 244, 255, 74, 268, 108, 258, 175, 108, 258, 175, 32, 276, 194, 55, 243, 272, 108, 277, 194, 243, 348, 108, 349, 175, 175, 32, 278, 194, 55, 243, 273, 108, 277, 194, 243, 348, 108, 349, 175, 175, 32, 135, 279, 157, 5, 243, 348, 108, 39, 243, 257, 108, 254, 4, 263, 244, 256, 4, 264, 244, 257, 175, 175, 62, 32, 280, 194, 176, 243, 230, 257, 27, 108, 91, 194, 145, 175, 32, 281, 194, 55, 243, 274, 108, 282, 194, 270, 108, 283, 194, 348, 175, 82, 284, 243, 145, 175, 32, 285, 194, 55, 243, 275, 108, 282, 194, 270, 108, 283, 194, 348, 175, 82, 284, 243, 145, 175, 32, 280, 172, 221, 243, 276, 244, 281, 230, 202, 108, 62, 27, 244, 107, 243, 278, 4, 285, 230, 202, 108, 62, 27, 175, 108, 349, 175, 32, 280, 194, 205, 243, 267, 144, 279, 108, 280, 244, 253, 108, 348, 175, 32, 10, 243, 250, 74, 269, 74, 279, 108, 280, 108, 282, 194, 271, 175, 32, 274, 172, 255, 32, 275, 172, 255, 32, 78, 32, 3, 32]}, {"code": "def chunk_gla_fwd_A_kernel_intra_sub_intra_merge(\n    A, A2, T: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, NK: tl.constexpr\n):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    if i_t * BT + i_c * BC >= T:\n        return\n    n_bh = tl.num_programs(2)\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(0, NK):\n        p_A = tl.make_block_ptr(\n            A + (i_bh + i_k * n_bh) * T * BC,\n            (T, BC),\n            (BC, 1),\n            (i_t * BT + i_c * BC, 0),\n            (BC, BC),\n            (1, 0),\n        )\n        b_A += tl.load(p_A, boundary_check=(0, 1))\n    p_A2 = tl.make_block_ptr(\n        A2 + i_bh * T * BT,\n        (T, BT),\n        (BT, 1),\n        (i_t * BT + i_c * BC, i_c * BC),\n        (BC, BC),\n        (1, 0),\n    )\n    tl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 63, 6, 108, 250, 63, 6, 108, 251, 63, 6, 108, 252, 63, 6, 175, 63, 32, -1, 253, 108, 254, 108, 255, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 108, 171, 243, 350, 175, 175, 32, 182, 253, 244, 250, 74, 254, 244, 251, 144, 249, 63, 32, 231, 32, 185, 32, 256, 194, 138, 243, 350, 175, 32, 257, 194, 176, 243, 230, 251, 108, 251, 27, 108, 91, 194, 145, 175, 32, 135, 258, 157, 5, 243, 348, 108, 252, 175, 63, 32, 259, 194, 219, 243, 247, 74, 243, 255, 74, 258, 244, 256, 175, 244, 249, 244, 251, 108, 243, 249, 108, 251, 175, 108, 243, 251, 108, 349, 175, 108, 243, 253, 244, 250, 74, 254, 244, 251, 108, 348, 175, 108, 243, 251, 108, 251, 175, 108, 243, 349, 108, 348, 175, 175, 32, 257, 172, 57, 243, 259, 108, 260, 194, 243, 348, 108, 349, 175, 175, 32, 78, 32, 261, 194, 219, 243, 248, 74, 255, 244, 249, 244, 250, 108, 243, 249, 108, 250, 175, 108, 243, 250, 108, 349, 175, 108, 243, 253, 244, 250, 74, 254, 244, 251, 108, 254, 244, 251, 175, 108, 243, 251, 108, 251, 175, 108, 243, 349, 108, 348, 175, 175, 32, 10, 243, 261, 108, 257, 82, 262, 243, 248, 82, 91, 82, 114, 175, 108, 260, 194, 243, 348, 108, 349, 175, 175, 32, 3, 32]}, {"code": "def chunk_gla_fwd_kernel_o(\n    q,\n    v,\n    g,\n    h,\n    o,\n    A,\n    s_k_h,\n    s_k_t,\n    s_v_h,\n    s_v_t,\n    s_h_h,\n    s_h_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(\n            q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_g = tl.make_block_ptr(\n            g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_h = tl.make_block_ptr(\n            h + i_bh * s_h_h + i_t * K * V,\n            (K, V),\n            (s_h_t, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = (b_q * tl.exp(b_g)).to(b_q.dtype)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h.to(b_qg.dtype))\n\n    p_v = tl.make_block_ptr(\n        v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_o = tl.make_block_ptr(\n        o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    p_A = tl.make_block_ptr(\n        A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)\n    )\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_A = tl.where(m_s, b_A, 0.0).to(b_v.dtype)\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 62, 6, 108, 261, 62, 6, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 175, 62, 32, -1, 266, 108, 267, 108, 268, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 108, 171, 243, 350, 175, 175, 32, 269, 194, 76, 243, 348, 108, 263, 175, 230, 62, 108, 202, 27, 144, 76, 243, 348, 108, 263, 175, 230, 202, 108, 62, 27, 32, 270, 194, 176, 243, 230, 263, 108, 265, 27, 108, 91, 194, 145, 175, 32, 135, 271, 157, 5, 243, 64, 243, 261, 108, 264, 175, 175, 62, 32, 272, 194, 219, 243, 247, 74, 268, 244, 253, 108, 243, 260, 108, 261, 175, 108, 243, 254, 108, 349, 175, 108, 243, 267, 244, 263, 108, 271, 244, 264, 175, 108, 243, 263, 108, 264, 175, 108, 243, 349, 108, 348, 175, 175, 32, 273, 194, 219, 243, 249, 74, 268, 244, 253, 108, 243, 260, 108, 261, 175, 108, 243, 254, 108, 349, 175, 108, 243, 267, 244, 263, 108, 271, 244, 264, 175, 108, 243, 263, 108, 264, 175, 108, 243, 349, 108, 348, 175, 175, 32, 274, 194, 219, 243, 250, 74, 268, 244, 257, 74, 267, 244, 261, 244, 262, 108, 243, 261, 108, 262, 175, 108, 243, 258, 108, 349, 175, 108, 243, 271, 244, 264, 108, 266, 244, 265, 175, 108, 243, 264, 108, 265, 175, 108, 243, 349, 108, 348, 175, 175, 32, 275, 194, 55, 243, 272, 108, 276, 194, 243, 348, 108, 349, 175, 175, 32, 275, 194, 243, 275, 244, 259, 175, 82, 277, 243, 275, 82, 91, 175, 32, 278, 194, 55, 243, 273, 108, 276, 194, 243, 348, 108, 349, 175, 175, 32, 279, 194, 243, 275, 244, 107, 243, 278, 175, 175, 82, 277, 243, 275, 82, 91, 175, 32, 280, 194, 55, 243, 274, 108, 276, 194, 243, 348, 108, 349, 175, 175, 32, 182, 271, 144, 348, 62, 32, 270, 172, 15, 243, 279, 108, 280, 82, 277, 243, 279, 82, 91, 175, 175, 32, 185, 32, 78, 32, 281, 194, 219, 243, 248, 74, 268, 244, 255, 108, 243, 260, 108, 262, 175, 108, 243, 256, 108, 349, 175, 108, 243, 267, 244, 263, 108, 266, 244, 265, 175, 108, 243, 263, 108, 265, 175, 108, 243, 349, 108, 348, 175, 175, 32, 282, 194, 219, 243, 251, 74, 268, 244, 255, 108, 243, 260, 108, 262, 175, 108, 243, 256, 108, 349, 175, 108, 243, 267, 244, 263, 108, 266, 244, 265, 175, 108, 243, 263, 108, 265, 175, 108, 243, 349, 108, 348, 175, 175, 32, 283, 194, 219, 243, 252, 74, 268, 244, 260, 244, 263, 108, 243, 260, 108, 263, 175, 108, 243, 263, 108, 349, 175, 108, 243, 267, 244, 263, 108, 348, 175, 108, 243, 263, 108, 263, 175, 108, 243, 349, 108, 348, 175, 175, 32, 284, 194, 55, 243, 281, 108, 276, 194, 243, 348, 108, 349, 175, 175, 32, 285, 194, 55, 243, 283, 108, 276, 194, 243, 348, 108, 349, 175, 175, 32, 285, 194, 205, 243, 269, 108, 285, 108, 348, 175, 82, 277, 243, 284, 82, 91, 175, 32, 270, 172, 15, 243, 285, 108, 284, 108, 286, 194, 59, 175, 32, 10, 243, 282, 108, 270, 82, 277, 243, 282, 82, 91, 82, 114, 175, 108, 276, 194, 243, 348, 108, 349, 175, 175, 32, 3, 32]}, {"code": "def chunk_simple_gla_fwd_kernel_o(\n    q,\n    k,\n    v,\n    h,\n    g,\n    o,\n    s_k_h,\n    s_k_t,\n    s_v_h,\n    s_v_t,\n    s_h_h,\n    s_h_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(\n            q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)\n        )\n        p_k = tl.make_block_ptr(\n            k + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1)\n        )\n        p_h = tl.make_block_ptr(\n            h + i_bh * s_h_h + i_t * K * V,\n            (K, V),\n            (s_h_t, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n\n    p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_o = b_o * tl.exp(b_g)[:, None]\n    b_s = b_s * tl.exp(b_g[:, None] - b_g[None, :])\n    b_s = tl.where(m_s, b_s, 0)\n\n    p_v = tl.make_block_ptr(\n        v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale\n    p_o = tl.make_block_ptr(\n        o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)\n    )\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 63, 6, 108, 261, 63, 6, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 108, 265, 63, 6, 175, 63, 32, -1, 266, 108, 267, 108, 268, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 108, 171, 243, 350, 175, 175, 32, 269, 194, 76, 243, 348, 108, 263, 175, 32, 270, 194, 269, 230, 63, 108, 202, 27, 144, 269, 230, 202, 108, 63, 27, 32, 271, 194, 176, 243, 230, 263, 108, 265, 27, 108, 91, 194, 145, 175, 32, 272, 194, 176, 243, 230, 263, 108, 263, 27, 108, 91, 194, 145, 175, 32, 135, 273, 157, 5, 243, 65, 243, 261, 108, 264, 175, 175, 63, 32, 274, 194, 219, 243, 247, 74, 268, 244, 253, 108, 243, 260, 108, 261, 175, 108, 243, 254, 108, 349, 175, 108, 243, 267, 244, 263, 108, 273, 244, 264, 175, 108, 243, 263, 108, 264, 175, 108, 243, 349, 108, 348, 175, 175, 32, 275, 194, 219, 243, 248, 74, 268, 244, 253, 108, 243, 261, 108, 260, 175, 108, 243, 349, 108, 254, 175, 108, 243, 273, 244, 264, 108, 267, 244, 263, 175, 108, 243, 264, 108, 263, 175, 108, 243, 348, 108, 349, 175, 175, 32, 276, 194, 219, 243, 250, 74, 268, 244, 257, 74, 267, 244, 261, 244, 262, 108, 243, 261, 108, 262, 175, 108, 243, 258, 108, 349, 175, 108, 243, 273, 244, 264, 108, 266, 244, 265, 175, 108, 243, 264, 108, 265, 175, 108, 243, 349, 108, 348, 175, 175, 32, 277, 194, 57, 243, 274, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 279, 194, 57, 243, 275, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 280, 194, 57, 243, 276, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 271, 172, 15, 243, 277, 108, 280, 108, 281, 194, 59, 175, 32, 272, 172, 15, 243, 277, 108, 279, 108, 281, 194, 59, 175, 32, 78, 32, 282, 194, 219, 243, 251, 74, 268, 244, 260, 108, 243, 260, 108, 175, 108, 243, 349, 108, 175, 108, 243, 267, 244, 263, 108, 175, 108, 243, 263, 108, 175, 108, 243, 348, 108, 175, 175, 32, 283, 194, 57, 243, 282, 108, 278, 194, 243, 348, 108, 175, 175, 32, 271, 194, 271, 244, 107, 243, 283, 175, 230, 63, 108, 202, 27, 32, 272, 194, 272, 244, 107, 243, 283, 230, 63, 108, 202, 27, 4, 283, 230, 202, 108, 63, 27, 175, 32, 272, 194, 205, 243, 270, 108, 272, 108, 348, 175, 32, 284, 194, 219, 243, 249, 74, 268, 244, 255, 108, 243, 260, 108, 262, 175, 108, 243, 256, 108, 349, 175, 108, 243, 267, 244, 263, 108, 266, 244, 265, 175, 108, 243, 263, 108, 265, 175, 108, 243, 349, 108, 348, 175, 175, 32, 285, 194, 57, 243, 284, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 271, 194, 243, 271, 74, 15, 243, 272, 82, 286, 243, 285, 82, 91, 175, 108, 285, 108, 281, 194, 59, 175, 175, 244, 259, 32, 287, 194, 219, 243, 252, 74, 268, 244, 255, 108, 243, 260, 108, 262, 175, 108, 243, 256, 108, 349, 175, 108, 243, 267, 244, 263, 108, 266, 244, 265, 175, 108, 243, 263, 108, 265, 175, 108, 243, 349, 108, 348, 175, 175, 32, 10, 243, 287, 108, 271, 82, 286, 243, 287, 82, 91, 82, 114, 175, 108, 278, 194, 243, 348, 108, 349, 175, 175, 32, 3, 32]}, {"code": "def chunk_retention_fwd_kernel_h(\n    k,\n    v,\n    h,\n    h0,\n    ht,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n):\n\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(\n            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(\n            k + i_bh * s_qk_h,\n            (K, T),\n            (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_v = tl.make_block_ptr(\n            v + i_bh * s_vo_h,\n            (T, V),\n            (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_h = tl.make_block_ptr(\n            h + i_bh * s_h_h + i_t * K * V,\n            (K, V),\n            (s_h_t, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if i_t == NT - 1 and (T % BT) != 0:\n            d_b = tl.math.exp2((T % BT) * b_b)\n            d_i = tl.math.exp2(((T % BT) - o_i - 1) * b_b)\n        b_h = d_b * b_h + tl.dot(\n            b_k, (b_v * d_i[:, None]).to(b_k.dtype), allow_tf32=False\n        )\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(\n            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)\n        )\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 62, 6, 108, 261, 62, 6, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 108, 266, 62, 6, 108, 267, 62, 6, 108, 268, 62, 6, 108, 269, 62, 6, 175, 62, 32, -1, 270, 108, 271, 108, 272, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 108, 171, 243, 350, 175, 175, 32, 273, 194, 272, 226, 260, 32, 274, 194, 111, 243, 349, 4, 155, 243, 4, 351, 4, 273, 244, 349, 175, 175, 32, 275, 194, 76, 243, 348, 108, 264, 175, 32, 276, 108, 277, 194, 243, 155, 243, 264, 244, 274, 175, 108, 155, 243, 243, 264, 4, 275, 4, 349, 175, 244, 274, 175, 175, 32, 278, 194, 176, 243, 230, 265, 108, 266, 27, 108, 91, 194, 145, 175, 32, 182, 268, 62, 32, 279, 194, 219, 243, 250, 74, 272, 244, 262, 244, 263, 108, 243, 262, 108, 263, 175, 108, 243, 263, 108, 349, 175, 108, 243, 270, 244, 265, 108, 271, 244, 266, 175, 108, 243, 265, 108, 266, 175, 108, 243, 349, 108, 348, 175, 175, 32, 278, 194, 55, 243, 279, 108, 280, 194, 243, 348, 108, 349, 175, 175, 82, 281, 243, 145, 175, 32, 185, 32, 135, 282, 157, 5, 243, 267, 175, 62, 32, 283, 194, 219, 243, 247, 74, 272, 244, 252, 108, 243, 262, 108, 261, 175, 108, 243, 254, 108, 253, 175, 108, 243, 270, 244, 265, 108, 282, 244, 264, 175, 108, 243, 265, 108, 264, 175, 108, 243, 348, 108, 349, 175, 175, 32, 284, 194, 219, 243, 248, 74, 272, 244, 255, 108, 243, 261, 108, 263, 175, 108, 243, 256, 108, 257, 175, 108, 243, 282, 244, 264, 108, 271, 244, 266, 175, 108, 243, 264, 108, 266, 175, 108, 243, 349, 108, 348, 175, 175, 32, 285, 194, 219, 243, 249, 74, 272, 244, 258, 74, 282, 244, 262, 244, 263, 108, 243, 262, 108, 263, 175, 108, 243, 259, 108, 349, 175, 108, 243, 270, 244, 265, 108, 271, 244, 266, 175, 108, 243, 265, 108, 266, 175, 108, 243, 349, 108, 348, 175, 175, 32, 10, 243, 285, 108, 278, 82, 281, 243, 285, 82, 91, 82, 114, 175, 108, 280, 194, 243, 348, 108, 349, 175, 175, 32, 286, 194, 55, 243, 283, 108, 280, 194, 243, 348, 108, 349, 175, 175, 32, 287, 194, 55, 243, 284, 108, 280, 194, 243, 348, 108, 349, 175, 175, 32, 182, 282, 77, 267, 4, 349, 102, 261, 226, 264, 186, 348, 62, 32, 276, 194, 155, 243, 261, 226, 264, 244, 274, 175, 32, 277, 194, 155, 243, 243, 261, 226, 264, 4, 275, 4, 349, 175, 244, 274, 175, 32, 185, 32, 278, 194, 276, 244, 278, 74, 15, 243, 286, 108, 243, 287, 244, 277, 230, 62, 108, 202, 27, 175, 82, 281, 243, 286, 82, 91, 175, 108, 288, 194, 59, 175, 32, 78, 32, 182, 269, 62, 32, 289, 194, 219, 243, 251, 74, 272, 244, 262, 244, 263, 108, 243, 262, 108, 263, 175, 108, 243, 263, 108, 349, 175, 108, 243, 270, 244, 265, 108, 271, 244, 266, 175, 108, 243, 265, 108, 266, 175, 108, 243, 349, 108, 348, 175, 175, 32, 10, 243, 289, 108, 278, 82, 281, 243, 289, 82, 91, 82, 114, 175, 108, 280, 194, 243, 348, 108, 349, 175, 175, 32, 185, 32, 3, 32]}, {"code": "def chunk_retention_fwd_kernel_o(\n    q,\n    k,\n    v,\n    h,\n    o,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    scale,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_i = tl.math.exp2((o_i + 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(\n            q + i_bh * s_qk_h,\n            (T, K),\n            (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK),\n            (BT, BK),\n            (1, 0),\n        )\n        p_k = tl.make_block_ptr(\n            k + i_bh * s_qk_h,\n            (K, T),\n            (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_h = tl.make_block_ptr(\n            h + i_bh * s_h_h + i_t * K * V,\n            (K, V),\n            (s_h_t, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot((b_q * d_i[:, None]).to(b_q.dtype), b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_s *= d_s\n    p_v = tl.make_block_ptr(\n        v + i_bh * s_vo_h,\n        (T, V),\n        (s_vo_t, s_vo_d),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale\n    p_o = tl.make_block_ptr(\n        o + i_bh * s_vo_h,\n        (T, V),\n        (s_vo_t, s_vo_d),\n        (i_t * BT, i_v * BV),\n        (BT, BV),\n        (1, 0),\n    )\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 63, 6, 108, 262, 63, 6, 108, 263, 63, 6, 108, 264, 63, 6, 108, 265, 63, 6, 108, 266, 63, 6, 108, 267, 63, 6, 175, 63, 32, -1, 268, 108, 269, 108, 270, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 108, 171, 243, 350, 175, 175, 32, 271, 194, 270, 226, 261, 32, 272, 194, 111, 243, 349, 4, 155, 243, 4, 351, 4, 271, 244, 349, 175, 175, 32, 273, 194, 76, 243, 348, 108, 265, 175, 32, 274, 194, 155, 243, 243, 273, 74, 349, 175, 244, 272, 175, 32, 275, 194, 273, 230, 63, 108, 202, 27, 144, 273, 230, 202, 108, 63, 27, 32, 276, 194, 205, 243, 275, 108, 155, 243, 243, 273, 230, 63, 108, 202, 27, 4, 273, 230, 202, 108, 63, 27, 175, 244, 272, 175, 108, 348, 175, 32, 277, 194, 176, 243, 230, 265, 108, 267, 27, 108, 91, 194, 145, 175, 32, 278, 194, 176, 243, 230, 265, 108, 265, 27, 108, 91, 194, 145, 175, 32, 135, 279, 157, 5, 243, 65, 243, 263, 108, 266, 175, 175, 63, 32, 280, 194, 219, 243, 247, 74, 270, 244, 252, 108, 243, 262, 108, 263, 175, 108, 243, 253, 108, 254, 175, 108, 243, 269, 244, 265, 108, 279, 244, 266, 175, 108, 243, 265, 108, 266, 175, 108, 243, 349, 108, 348, 175, 175, 32, 281, 194, 219, 243, 248, 74, 270, 244, 252, 108, 243, 263, 108, 262, 175, 108, 243, 254, 108, 253, 175, 108, 243, 279, 244, 266, 108, 269, 244, 265, 175, 108, 243, 266, 108, 265, 175, 108, 243, 348, 108, 349, 175, 175, 32, 282, 194, 219, 243, 250, 74, 270, 244, 258, 74, 269, 244, 263, 244, 264, 108, 243, 263, 108, 264, 175, 108, 243, 259, 108, 349, 175, 108, 243, 279, 244, 266, 108, 268, 244, 267, 175, 108, 243, 266, 108, 267, 175, 108, 243, 349, 108, 348, 175, 175, 32, 283, 194, 57, 243, 280, 108, 284, 194, 243, 348, 108, 349, 175, 175, 32, 285, 194, 57, 243, 281, 108, 284, 194, 243, 348, 108, 349, 175, 175, 32, 286, 194, 57, 243, 282, 108, 284, 194, 243, 348, 108, 349, 175, 175, 32, 277, 172, 15, 243, 243, 283, 244, 274, 230, 63, 108, 202, 27, 175, 82, 287, 243, 283, 82, 91, 175, 108, 286, 108, 288, 194, 59, 175, 32, 278, 172, 15, 243, 283, 108, 285, 108, 288, 194, 59, 175, 32, 78, 32, 278, 24, 276, 32, 289, 194, 219, 243, 249, 74, 270, 244, 255, 108, 243, 262, 108, 264, 175, 108, 243, 256, 108, 257, 175, 108, 243, 269, 244, 265, 108, 268, 244, 267, 175, 108, 243, 265, 108, 267, 175, 108, 243, 349, 108, 348, 175, 175, 32, 290, 194, 57, 243, 289, 108, 284, 194, 243, 348, 108, 349, 175, 175, 32, 277, 194, 243, 277, 74, 15, 243, 278, 82, 287, 243, 290, 82, 91, 175, 108, 290, 108, 288, 194, 59, 175, 175, 244, 260, 32, 291, 194, 219, 243, 251, 74, 270, 244, 255, 108, 243, 262, 108, 264, 175, 108, 243, 256, 108, 257, 175, 108, 243, 269, 244, 265, 108, 268, 244, 267, 175, 108, 243, 265, 108, 267, 175, 108, 243, 349, 108, 348, 175, 175, 32, 10, 243, 291, 108, 277, 82, 287, 243, 291, 82, 91, 82, 114, 175, 108, 284, 194, 243, 348, 108, 349, 175, 175, 32, 3, 32]}, {"code": "def chunk_retention_bwd_kernel_dh(\n    q,\n    do,\n    dh,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    scale,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n):\n\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(\n            q + i_bh * s_qk_h,\n            (K, T),\n            (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT),\n            (BK, BT),\n            (0, 1),\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * s_vo_h,\n            (T, V),\n            (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dh = tl.make_block_ptr(\n            dh + i_bh * s_h_h + i_t * K * V,\n            (K, V),\n            (s_h_t, 1),\n            (i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = d_b * b_dh + tl.dot(\n            b_q, (b_do * d_i[:, None]).to(b_q.dtype), allow_tf32=False\n        )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 62, 6, 108, 260, 62, 6, 108, 261, 62, 6, 108, 262, 62, 6, 108, 263, 62, 6, 108, 264, 62, 6, 108, 265, 62, 6, 108, 266, 62, 6, 175, 62, 32, -1, 267, 108, 268, 108, 269, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 108, 171, 243, 350, 175, 175, 32, 270, 194, 269, 226, 259, 32, 271, 194, 111, 243, 349, 4, 155, 243, 4, 351, 4, 270, 244, 349, 175, 175, 32, 272, 194, 76, 243, 348, 108, 263, 175, 32, 273, 108, 274, 194, 243, 155, 243, 263, 244, 271, 175, 108, 155, 243, 243, 272, 74, 349, 175, 244, 271, 175, 175, 32, 275, 194, 176, 243, 230, 264, 108, 265, 27, 108, 91, 194, 145, 175, 32, 135, 276, 157, 5, 243, 266, 4, 349, 108, 4, 349, 108, 4, 349, 175, 62, 32, 277, 194, 219, 243, 247, 74, 269, 244, 250, 108, 243, 261, 108, 260, 175, 108, 243, 252, 108, 251, 175, 108, 243, 267, 244, 264, 108, 276, 244, 263, 175, 108, 243, 264, 108, 263, 175, 108, 243, 348, 108, 349, 175, 175, 32, 278, 194, 219, 243, 248, 74, 269, 244, 253, 108, 243, 260, 108, 262, 175, 108, 243, 254, 108, 255, 175, 108, 243, 276, 244, 263, 108, 268, 244, 265, 175, 108, 243, 263, 108, 265, 175, 108, 243, 349, 108, 348, 175, 175, 32, 279, 194, 219, 243, 249, 74, 269, 244, 256, 74, 276, 244, 261, 244, 262, 108, 243, 261, 108, 262, 175, 108, 243, 257, 108, 349, 175, 108, 243, 267, 244, 264, 108, 268, 244, 265, 175, 108, 243, 264, 108, 265, 175, 108, 243, 349, 108, 348, 175, 175, 32, 10, 243, 279, 108, 275, 82, 280, 243, 279, 82, 91, 82, 114, 175, 108, 281, 194, 243, 348, 108, 349, 175, 175, 32, 282, 194, 55, 243, 277, 108, 281, 194, 243, 348, 108, 349, 175, 175, 32, 282, 194, 243, 282, 244, 258, 175, 82, 280, 243, 282, 82, 91, 175, 32, 283, 194, 55, 243, 278, 108, 281, 194, 243, 348, 108, 349, 175, 175, 32, 275, 194, 273, 244, 275, 74, 15, 243, 282, 108, 243, 283, 244, 274, 230, 62, 108, 202, 27, 175, 82, 280, 243, 282, 82, 91, 175, 108, 284, 194, 59, 175, 32, 78, 32, 3, 32]}, {"code": "def chunk_retention_bwd_kernel_dqkv(\n    q,\n    k,\n    v,\n    h,\n    do,\n    dh,\n    dq,\n    dk,\n    dv,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    scale,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n):\n\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    n_bh = tl.num_programs(2)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    d_q = (d_q * scale).to(d_q.dtype)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0) * scale\n    p_q = tl.make_block_ptr(\n        q + i_bh * s_qk_h,\n        (K, T),\n        (s_qk_d, s_qk_t),\n        (i_k * BK, i_t * BT),\n        (BK, BT),\n        (0, 1),\n    )\n    p_k = tl.make_block_ptr(\n        k + i_bh * s_qk_h,\n        (T, K),\n        (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * tl.trans(d_s)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(\n            v + i_bh * s_vo_h,\n            (T, V),\n            (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_h = tl.make_block_ptr(\n            h + i_bh * s_h_h,\n            (V, NT * K),\n            (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK),\n            (BV, BK),\n            (0, 1),\n        )\n        p_do = tl.make_block_ptr(\n            do + i_bh * s_vo_h,\n            (T, V),\n            (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        p_dh = tl.make_block_ptr(\n            dh + i_bh * s_h_h,\n            (NT * K, V),\n            (s_h_t, 1),\n            (i_t * K + i_k * BK, i_v * BV),\n            (BK, BV),\n            (1, 0),\n        )\n        p_dv = tl.make_block_ptr(\n            dv + (i_k * n_bh + i_bh) * s_vo_h,\n            (T, V),\n            (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV),\n            (BT, BV),\n            (1, 0),\n        )\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None] + tl.dot(\n            b_s.to(b_q.dtype), b_do, allow_tf32=False\n        )\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    b_ds = (b_ds * d_s).to(b_q.dtype)\n    b_dq = b_dq * d_q[:, None] + tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk = b_dk * d_k[:, None] + tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    p_dq = tl.make_block_ptr(\n        dq + i_bh * s_qk_h,\n        (T, K),\n        (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    p_dk = tl.make_block_ptr(\n        dk + i_bh * s_qk_h,\n        (T, K),\n        (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK),\n        (BT, BK),\n        (1, 0),\n    )\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 63, 6, 108, 266, 63, 6, 108, 267, 63, 6, 108, 268, 63, 6, 108, 269, 63, 6, 108, 270, 63, 6, 108, 271, 63, 6, 108, 272, 63, 6, 175, 63, 32, -1, 273, 108, 274, 108, 275, 194, 243, 171, 243, 348, 175, 108, 171, 243, 349, 175, 108, 171, 243, 350, 175, 175, 32, 276, 194, 275, 226, 265, 32, 277, 194, 138, 243, 350, 175, 32, 278, 194, 111, 243, 349, 4, 155, 243, 4, 351, 4, 276, 244, 349, 175, 175, 32, 279, 194, 76, 243, 348, 108, 269, 175, 32, 280, 108, 281, 194, 243, 155, 243, 243, 279, 74, 349, 175, 244, 278, 175, 108, 155, 243, 243, 269, 4, 279, 4, 349, 175, 244, 278, 175, 175, 32, 280, 194, 243, 280, 244, 264, 175, 82, 282, 243, 280, 82, 91, 175, 32, 283, 194, 279, 230, 63, 108, 202, 27, 144, 279, 230, 202, 108, 63, 27, 32, 284, 194, 205, 243, 283, 108, 155, 243, 243, 279, 230, 63, 108, 202, 27, 4, 279, 230, 202, 108, 63, 27, 175, 244, 278, 175, 108, 348, 175, 244, 264, 32, 285, 194, 219, 243, 247, 74, 275, 244, 256, 108, 243, 267, 108, 266, 175, 108, 243, 258, 108, 257, 175, 108, 243, 273, 244, 270, 108, 274, 244, 269, 175, 108, 243, 270, 108, 269, 175, 108, 243, 348, 108, 349, 175, 175, 32, 286, 194, 219, 243, 248, 74, 275, 244, 256, 108, 243, 266, 108, 267, 175, 108, 243, 257, 108, 258, 175, 108, 243, 274, 244, 269, 108, 273, 244, 270, 175, 108, 243, 269, 108, 270, 175, 108, 243, 349, 108, 348, 175, 175, 32, 287, 194, 57, 243, 285, 108, 288, 194, 243, 348, 108, 349, 175, 175, 32, 289, 194, 57, 243, 286, 108, 288, 194, 243, 348, 108, 349, 175, 175, 32, 290, 194, 15, 243, 289, 108, 287, 108, 291, 194, 59, 175, 244, 72, 243, 284, 175, 32, 292, 194, 176, 243, 230, 269, 108, 270, 27, 108, 91, 194, 145, 175, 32, 293, 194, 176, 243, 230, 269, 108, 270, 27, 108, 91, 194, 145, 175, 32, 294, 194, 176, 243, 230, 269, 108, 269, 27, 108, 91, 194, 145, 175, 32, 135, 295, 157, 5, 243, 65, 243, 268, 108, 271, 175, 175, 63, 32, 296, 194, 219, 243, 249, 74, 275, 244, 259, 108, 243, 266, 108, 268, 175, 108, 243, 260, 108, 261, 175, 108, 243, 274, 244, 269, 108, 295, 244, 271, 175, 108, 243, 269, 108, 271, 175, 108, 243, 349, 108, 348, 175, 175, 32, 297, 194, 219, 243, 250, 74, 275, 244, 262, 108, 243, 268, 108, 272, 244, 267, 175, 108, 243, 349, 108, 263, 175, 108, 243, 295, 244, 271, 108, 274, 244, 267, 74, 273, 244, 270, 175, 108, 243, 271, 108, 270, 175, 108, 243, 348, 108, 349, 175, 175, 32, 298, 194, 219, 243, 251, 74, 275, 244, 259, 108, 243, 266, 108, 268, 175, 108, 243, 260, 108, 261, 175, 108, 243, 274, 244, 269, 108, 295, 244, 271, 175, 108, 243, 269, 108, 271, 175, 108, 243, 349, 108, 348, 175, 175, 32, 299, 194, 219, 243, 252, 74, 275, 244, 262, 108, 243, 272, 244, 267, 108, 268, 175, 108, 243, 263, 108, 349, 175, 108, 243, 274, 244, 267, 74, 273, 244, 270, 108, 295, 244, 271, 175, 108, 243, 270, 108, 271, 175, 108, 243, 349, 108, 348, 175, 175, 32, 300, 194, 219, 243, 255, 74, 243, 273, 244, 277, 74, 275, 175, 244, 259, 108, 243, 266, 108, 268, 175, 108, 243, 260, 108, 261, 175, 108, 243, 274, 244, 269, 108, 295, 244, 271, 175, 108, 243, 269, 108, 271, 175, 108, 243, 349, 108, 348, 175, 175, 32, 301, 194, 57, 243, 296, 108, 288, 194, 243, 348, 108, 349, 175, 175, 32, 302, 194, 57, 243, 298, 108, 288, 194, 243, 348, 108, 349, 175, 175, 32, 303, 194, 57, 243, 297, 108, 288, 194, 243, 348, 108, 349, 175, 175, 32, 304, 194, 57, 243, 299, 108, 288, 194, 243, 348, 108, 349, 175, 175, 32, 294, 172, 15, 243, 302, 108, 72, 243, 301, 175, 108, 291, 194, 59, 175, 32, 292, 172, 15, 243, 302, 108, 303, 108, 291, 194, 59, 175, 32, 293, 172, 15, 243, 301, 108, 72, 243, 304, 175, 108, 291, 194, 59, 175, 32, 305, 194, 15, 243, 289, 108, 304, 108, 291, 194, 59, 175, 244, 281, 230, 63, 108, 202, 27, 74, 15, 243, 290, 82, 282, 243, 287, 82, 91, 175, 108, 302, 108, 291, 194, 59, 175, 32, 10, 243, 300, 108, 305, 82, 282, 243, 300, 82, 91, 82, 114, 175, 108, 288, 194, 243, 348, 108, 349, 175, 175, 32, 78, 32, 294, 194, 243, 294, 244, 284, 175, 82, 282, 243, 287, 82, 91, 175, 32, 292, 194, 292, 244, 280, 230, 63, 108, 202, 27, 74, 15, 243, 294, 108, 289, 108, 291, 194, 59, 175, 32, 293, 194, 293, 244, 281, 230, 63, 108, 202, 27, 74, 72, 243, 15, 243, 287, 108, 294, 108, 291, 194, 59, 175, 175, 32, 306, 194, 219, 243, 253, 74, 275, 244, 256, 108, 243, 266, 108, 267, 175, 108, 243, 257, 108, 258, 175, 108, 243, 274, 244, 269, 108, 273, 244, 270, 175, 108, 243, 269, 108, 270, 175, 108, 243, 349, 108, 348, 175, 175, 32, 307, 194, 219, 243, 254, 74, 275, 244, 256, 108, 243, 266, 108, 267, 175, 108, 243, 257, 108, 258, 175, 108, 243, 274, 244, 269, 108, 273, 244, 270, 175, 108, 243, 269, 108, 270, 175, 108, 243, 349, 108, 348, 175, 175, 32, 10, 243, 306, 108, 292, 82, 282, 243, 306, 82, 91, 82, 114, 175, 108, 288, 194, 243, 348, 108, 349, 175, 175, 32, 10, 243, 307, 108, 293, 82, 282, 243, 307, 82, 91, 82, 114, 175, 108, 288, 194, 243, 348, 108, 349, 175, 175, 32, 3, 32]}, {"code": "def _chunk_cumsum_fwd_kernel(\n    dt_ptr,\n    A_ptr,\n    dt_bias_ptr,\n    dt_out_ptr,\n    dA_cumsum_ptr,\n    batch,\n    seqlen,\n    nheads,\n    chunk_size,\n    dt_min,\n    dt_max,\n    stride_dt_batch,\n    stride_dt_seqlen,\n    stride_dt_head,\n    stride_A_head,\n    stride_dt_bias_head,\n    stride_dt_out_batch,\n    stride_dt_out_chunk,\n    stride_dt_out_head,\n    stride_dt_out_csize,\n    stride_dA_cs_batch,\n    stride_dA_cs_chunk,\n    stride_dA_cs_head,\n    stride_dA_cs_csize,\n    DT_SOFTPLUS: tl.constexpr,\n    HAS_DT_BIAS: tl.constexpr,\n    BLOCK_SIZE_H: tl.constexpr,\n    BLOCK_SIZE_CHUNK: tl.constexpr,\n):\n\n    pid_b = tl.program_id(axis=0)\n    pid_c = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\n    dt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk\n    dA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk\n\n    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\n    dt_ptrs = dt_ptr + (\n        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen\n    )\n    A_ptrs = A_ptr + offs_h * stride_A_head\n    dt_out_ptrs = dt_out_ptr + (\n        offs_h[:, None] * stride_dt_out_head + offs_c[None, :] * stride_dt_out_csize\n    )\n    dA_cs_ptrs = dA_cumsum_ptr + (\n        offs_h[:, None] * stride_dA_cs_head + offs_c[None, :] * stride_dA_cs_csize\n    )\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    dt = tl.load(\n        dt_ptrs,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),\n        other=0.0,\n    ).to(tl.float32)\n    if HAS_DT_BIAS:\n        dt_bias = tl.load(\n            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0\n        ).to(tl.float32)\n        dt += dt_bias[:, None]\n    if DT_SOFTPLUS:\n        dt = tl.where(dt <= 20.0, tl.log(1 + tl.exp(dt)), dt)\n\n    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\n    dt = tl.where(\n        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0\n    )\n    tl.store(\n        dt_out_ptrs,\n        dt,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),\n    )\n    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)\n    dA = dt * A[:, None]\n    dA_cs = tl.cumsum(dA, axis=1)\n    tl.store(\n        dA_cs_ptrs,\n        dA_cs,\n        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 108, 263, 108, 264, 108, 265, 108, 266, 108, 267, 108, 268, 108, 269, 108, 270, 108, 271, 62, 6, 108, 272, 62, 6, 108, 273, 62, 6, 108, 274, 62, 6, 175, 62, 32, -1, 275, 194, 171, 243, 276, 194, 348, 175, 32, 277, 194, 171, 243, 276, 194, 349, 175, 32, 278, 194, 171, 243, 276, 194, 350, 175, 32, 247, 172, 275, 244, 258, 74, 277, 244, 255, 244, 259, 32, 250, 172, 275, 244, 263, 74, 277, 244, 264, 32, 251, 172, 275, 244, 267, 74, 277, 244, 268, 32, 279, 194, 278, 244, 273, 74, 76, 243, 348, 108, 273, 175, 32, 280, 194, 76, 243, 348, 108, 274, 175, 32, 281, 194, 247, 74, 243, 279, 230, 62, 108, 202, 27, 244, 260, 74, 280, 230, 202, 108, 62, 27, 244, 259, 175, 32, 282, 194, 248, 74, 279, 244, 261, 32, 283, 194, 250, 74, 243, 279, 230, 62, 108, 202, 27, 244, 265, 74, 280, 230, 202, 108, 62, 27, 244, 266, 175, 32, 284, 194, 251, 74, 243, 279, 230, 62, 108, 202, 27, 244, 269, 74, 280, 230, 202, 108, 62, 27, 244, 270, 175, 32, 285, 194, 39, 243, 255, 108, 253, 4, 277, 244, 255, 175, 32, 286, 194, 55, 243, 281, 108, 287, 194, 243, 279, 230, 62, 108, 202, 27, 1, 254, 175, 173, 243, 280, 230, 202, 108, 62, 27, 1, 285, 175, 108, 288, 194, 348, 175, 82, 289, 243, 145, 175, 32, 182, 272, 62, 32, 290, 194, 55, 243, 249, 74, 279, 244, 262, 108, 287, 194, 279, 1, 254, 108, 288, 194, 348, 175, 82, 289, 243, 145, 175, 32, 286, 172, 290, 230, 62, 108, 202, 27, 32, 185, 32, 182, 271, 62, 32, 286, 194, 205, 243, 286, 218, 351, 108, 50, 243, 349, 74, 107, 243, 286, 175, 175, 108, 286, 175, 32, 185, 32, 286, 194, 66, 243, 192, 243, 286, 108, 256, 175, 108, 257, 175, 32, 286, 194, 205, 243, 243, 279, 230, 62, 108, 202, 27, 1, 254, 175, 173, 243, 280, 230, 202, 108, 62, 27, 1, 285, 175, 108, 286, 108, 348, 175, 32, 10, 243, 283, 108, 286, 108, 287, 194, 243, 279, 230, 62, 108, 202, 27, 1, 254, 175, 173, 243, 280, 230, 202, 108, 62, 27, 1, 255, 175, 175, 32, 291, 194, 55, 243, 282, 108, 287, 194, 279, 1, 254, 108, 288, 194, 348, 175, 82, 289, 243, 145, 175, 32, 292, 194, 286, 244, 291, 230, 62, 108, 202, 27, 32, 293, 194, 85, 243, 292, 108, 276, 194, 349, 175, 32, 10, 243, 284, 108, 293, 108, 287, 194, 243, 279, 230, 62, 108, 202, 27, 1, 254, 175, 173, 243, 280, 230, 202, 108, 62, 27, 1, 255, 175, 175, 32, 3, 32]}, {"code": "def cross_entropy_fwd_kernel(\n    loss_ptr,\n    lse_ptr,\n    logits_ptr,\n    labels_ptr,\n    smoothing,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,\n    n_cols,\n    n_rows,\n    logits_row_stride,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n    SPLIT: tl.constexpr,\n):\n\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(\n        logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")\n    ).to(tl.float32)\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignored_index:\n        loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(\n            n_cols, (col_block_idx + 1) * BLOCK_SIZE\n        ):\n            logits_label = tl.load(logits_ptr + label_idx)\n            if HAS_SMOOTHING:\n                loss = (\n                    (lse if not SPLIT else 0.0)\n                    - smoothing * sum_logits / total_classes\n                    - (1 - smoothing) * logits_label\n                )\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        else:\n            if HAS_SMOOTHING:\n                loss = smoothing * (\n                    (lse if not SPLIT else 0.0) - sum_logits / total_classes\n                )\n            else:\n                loss = 0.0\n        if not SPLIT:\n            loss += lse_square_scale * lse * lse\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 63, 6, 108, 260, 63, 6, 108, 261, 63, 6, 175, 63, 32, -1, 262, 194, 171, 243, 348, 175, 32, 263, 194, 171, 243, 349, 175, 32, 249, 194, 249, 74, 262, 244, 258, 82, 264, 243, 181, 175, 32, 265, 194, 263, 244, 259, 74, 76, 243, 348, 108, 259, 175, 32, 266, 194, 57, 243, 250, 74, 262, 175, 32, 267, 194, 57, 243, 249, 74, 265, 108, 268, 194, 265, 1, 256, 108, 269, 194, 4, 270, 243, 350, 175, 175, 82, 264, 243, 145, 175, 32, 271, 194, 12, 243, 267, 108, 348, 175, 32, 182, 260, 63, 32, 272, 194, 221, 243, 205, 243, 265, 1, 256, 108, 267, 108, 348, 175, 108, 348, 175, 32, 185, 32, 273, 194, 50, 243, 221, 243, 107, 243, 267, 4, 271, 175, 108, 348, 175, 175, 74, 271, 32, 10, 243, 248, 74, 263, 244, 257, 74, 262, 108, 273, 175, 32, 182, 266, 77, 253, 63, 32, 274, 194, 348, 32, 185, 32, 30, 63, 32, 266, 2, 255, 32, 182, 266, 144, 263, 244, 259, 102, 266, 1, 39, 243, 256, 108, 243, 263, 74, 349, 175, 244, 259, 175, 63, 32, 275, 194, 57, 243, 249, 74, 266, 175, 32, 182, 260, 63, 32, 274, 194, 243, 273, 182, 64, 261, 30, 348, 175, 4, 251, 244, 272, 42, 254, 4, 243, 349, 4, 251, 175, 244, 275, 32, 185, 32, 30, 63, 32, 274, 194, 243, 273, 182, 64, 261, 30, 348, 175, 4, 275, 32, 56, 32, 66, 32, 37, 260, 63, 32, 274, 194, 251, 244, 243, 243, 273, 182, 64, 261, 30, 348, 175, 4, 272, 42, 254, 175, 32, 185, 32, 30, 63, 32, 274, 194, 348, 32, 56, 32, 182, 64, 261, 63, 32, 274, 172, 252, 244, 273, 244, 273, 32, 185, 32, 56, 32, 10, 243, 247, 74, 263, 244, 257, 74, 262, 108, 274, 175, 32, 3, 32]}, {"code": "def cross_entropy_bwd_kernel(\n    dlogits_ptr,\n    dloss_ptr,\n    logits_ptr,\n    lse_ptr,\n    labels_ptr,\n    smoothing,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,\n    n_cols,\n    logits_row_stride,\n    dlogits_row_stride,\n    dloss_row_stride,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n):\n\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignored_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(\n        logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")\n    ).to(tl.float32)\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_negative = smoothing / total_classes\n        probs = (\n            tl.where(col_offsets == label_idx, probs - (1 - smoothing), probs)\n            - smooth_negative\n        )\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, dloss * probs, mask=col_offsets < n_cols)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 62, 6, 108, 262, 62, 6, 175, 62, 32, -1, 263, 194, 171, 243, 348, 175, 32, 264, 194, 171, 243, 349, 175, 32, 249, 194, 249, 74, 263, 244, 258, 82, 265, 243, 181, 175, 32, 247, 194, 247, 74, 263, 244, 259, 82, 265, 243, 181, 175, 32, 266, 194, 264, 244, 261, 74, 76, 243, 348, 108, 261, 175, 32, 267, 194, 55, 243, 251, 74, 263, 175, 32, 182, 267, 186, 254, 62, 32, 268, 194, 55, 243, 248, 74, 263, 244, 260, 175, 32, 185, 32, 30, 62, 32, 268, 194, 348, 32, 56, 32, 269, 194, 55, 243, 249, 74, 266, 108, 270, 194, 266, 1, 257, 108, 271, 194, 4, 272, 243, 350, 175, 175, 82, 265, 243, 145, 175, 32, 273, 194, 55, 243, 250, 74, 263, 175, 32, 274, 194, 107, 243, 269, 4, 273, 175, 32, 274, 172, 351, 244, 253, 244, 273, 244, 274, 32, 267, 2, 256, 32, 182, 262, 62, 32, 275, 194, 252, 42, 255, 32, 274, 194, 205, 243, 266, 77, 267, 108, 274, 4, 243, 349, 4, 252, 175, 108, 274, 175, 4, 275, 32, 185, 32, 30, 62, 32, 274, 194, 205, 243, 266, 77, 267, 108, 274, 4, 349, 108, 274, 175, 32, 56, 32, 10, 243, 247, 74, 266, 108, 268, 244, 274, 108, 270, 194, 266, 1, 257, 175, 32, 3, 32]}, {"code": "def cross_entropy_fwd_kernel(\n    loss_ptr,\n    lse_ptr,\n    z_loss_ptr,\n    logits_ptr,\n    labels_ptr,\n    smoothing,\n    logit_scale,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,\n    n_cols,\n    n_rows,\n    logits_row_stride,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n    SPLIT: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = (\n        tl.load(\n            logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")\n        ).to(tl.float32)\n        * logit_scale\n    )\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignored_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(\n            n_cols, (col_block_idx + 1) * BLOCK_SIZE\n        ):\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (\n                    (lse if not SPLIT else 0.0)\n                    - smoothing * sum_logits / total_classes\n                    - (1 - smoothing) * logits_label\n                )\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        else:\n            if HAS_SMOOTHING:\n                loss = smoothing * (\n                    (lse if not SPLIT else 0.0) - sum_logits / total_classes\n                )\n            else:\n                loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 63, 6, 108, 262, 63, 6, 108, 263, 63, 6, 175, 63, 32, -1, 264, 194, 171, 243, 348, 175, 32, 265, 194, 171, 243, 349, 175, 32, 250, 194, 250, 74, 264, 244, 260, 82, 266, 243, 181, 175, 32, 267, 194, 265, 244, 261, 74, 76, 243, 348, 108, 261, 175, 32, 268, 194, 57, 243, 251, 74, 264, 175, 32, 269, 194, 57, 243, 250, 74, 267, 108, 270, 194, 267, 1, 258, 108, 271, 194, 4, 272, 243, 350, 175, 175, 82, 266, 243, 145, 175, 244, 253, 32, 273, 194, 12, 243, 269, 108, 348, 175, 32, 182, 262, 63, 32, 274, 194, 221, 243, 205, 243, 267, 1, 258, 108, 269, 108, 348, 175, 108, 348, 175, 32, 185, 32, 275, 194, 50, 243, 221, 243, 107, 243, 269, 4, 273, 175, 108, 348, 175, 175, 74, 273, 32, 10, 243, 248, 74, 265, 244, 259, 74, 264, 108, 275, 175, 32, 182, 268, 77, 255, 63, 32, 276, 194, 348, 32, 277, 194, 348, 32, 185, 32, 30, 63, 32, 268, 2, 257, 32, 182, 268, 144, 265, 244, 261, 102, 268, 1, 39, 243, 258, 108, 243, 265, 74, 349, 175, 244, 261, 175, 63, 32, 278, 194, 57, 243, 250, 74, 268, 175, 244, 253, 32, 182, 262, 63, 32, 276, 194, 243, 275, 182, 64, 263, 30, 348, 175, 4, 252, 244, 274, 42, 256, 4, 243, 349, 4, 252, 175, 244, 278, 32, 185, 32, 30, 63, 32, 276, 194, 243, 275, 182, 64, 263, 30, 348, 175, 4, 278, 32, 56, 32, 66, 32, 37, 262, 63, 32, 276, 194, 252, 244, 243, 243, 275, 182, 64, 263, 30, 348, 175, 4, 274, 42, 256, 175, 32, 185, 32, 30, 63, 32, 276, 194, 348, 32, 56, 32, 182, 64, 263, 63, 32, 277, 194, 254, 244, 275, 244, 275, 32, 276, 172, 277, 32, 185, 32, 30, 63, 32, 277, 194, 348, 32, 56, 32, 56, 32, 10, 243, 247, 74, 265, 244, 259, 74, 264, 108, 276, 175, 32, 182, 64, 263, 63, 32, 10, 243, 249, 74, 265, 244, 259, 74, 264, 108, 277, 175, 32, 185, 32, 3, 32]}, {"code": "def cross_entropy_bwd_kernel(\n    dlogits_ptr,\n    dloss_ptr,\n    logits_ptr,\n    lse_ptr,\n    labels_ptr,\n    smoothing,\n    logit_scale,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,\n    n_cols,\n    logits_row_stride,\n    dlogits_row_stride,\n    dloss_row_stride,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignored_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = (\n        tl.load(\n            logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")\n        ).to(tl.float32)\n        * logit_scale\n    )\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_positive = 1.0 - smoothing\n        smooth_negative = smoothing / total_classes\n        probs = (\n            tl.where(col_offsets == label_idx, probs - (1 - smoothing), probs)\n            - smooth_negative\n        )\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(\n        dlogits_ptr + col_offsets,\n        (dloss * logit_scale) * probs,\n        mask=col_offsets < n_cols,\n    )", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 108, 260, 108, 261, 108, 262, 62, 6, 108, 263, 62, 6, 175, 62, 32, -1, 264, 194, 171, 243, 348, 175, 32, 265, 194, 171, 243, 349, 175, 32, 249, 194, 249, 74, 264, 244, 259, 82, 266, 243, 181, 175, 32, 247, 194, 247, 74, 264, 244, 260, 82, 266, 243, 181, 175, 32, 267, 194, 265, 244, 262, 74, 76, 243, 348, 108, 262, 175, 32, 268, 194, 55, 243, 251, 74, 264, 175, 32, 182, 268, 186, 255, 62, 32, 269, 194, 55, 243, 248, 74, 264, 244, 261, 175, 32, 185, 32, 30, 62, 32, 269, 194, 348, 32, 56, 32, 270, 194, 55, 243, 249, 74, 267, 108, 271, 194, 267, 1, 258, 108, 272, 194, 4, 273, 243, 350, 175, 175, 82, 266, 243, 145, 175, 244, 253, 32, 274, 194, 55, 243, 250, 74, 264, 175, 32, 275, 194, 107, 243, 270, 4, 274, 175, 32, 275, 172, 351, 244, 254, 244, 274, 244, 275, 32, 268, 2, 257, 32, 182, 263, 62, 32, 276, 194, 349, 4, 252, 32, 277, 194, 252, 42, 256, 32, 275, 194, 205, 243, 267, 77, 268, 108, 275, 4, 243, 349, 4, 252, 175, 108, 275, 175, 4, 277, 32, 185, 32, 30, 62, 32, 275, 194, 205, 243, 267, 77, 268, 108, 275, 4, 349, 108, 275, 175, 32, 56, 32, 10, 243, 247, 74, 267, 108, 269, 244, 253, 244, 275, 108, 271, 194, 267, 1, 258, 175, 32, 3, 32]}, {"code": "def dequantize_kernel(\n    b_ptr,\n    b_scale_ptr,\n    fpb_ptr,\n    K,\n    N,\n    stride_bk,\n    stride_bn,\n    stride_fpbk,\n    stride_fpbn,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    b_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_bk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]\n    ) * stride_bn\n    fpb_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_fpbk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]\n    ) * stride_fpbn\n    bs_offs = n_block_idx * BLOCK_SIZE_N + offs_n[None, :]\n    n_mask = n_block_idx * BLOCK_SIZE_N + offs_n[None, :] < N\n    mask = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None] < K) & n_mask\n    int_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=n_mask, other=0.0)\n    tl.store(fpb_ptr + fpb_offs, int_b * scale_b, mask=mask)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 63, 6, 108, 257, 63, 6, 175, 63, 32, -1, 258, 194, 171, 243, 259, 194, 348, 175, 32, 260, 194, 171, 243, 259, 194, 349, 175, 32, 261, 194, 76, 243, 348, 108, 257, 175, 32, 262, 194, 76, 243, 348, 108, 256, 175, 32, 263, 194, 243, 258, 244, 257, 74, 261, 230, 63, 108, 202, 27, 175, 244, 252, 74, 243, 260, 244, 256, 74, 262, 230, 202, 108, 63, 27, 175, 244, 253, 32, 264, 194, 243, 258, 244, 257, 74, 261, 230, 63, 108, 202, 27, 175, 244, 254, 74, 243, 260, 244, 256, 74, 262, 230, 202, 108, 63, 27, 175, 244, 255, 32, 265, 194, 260, 244, 256, 74, 262, 230, 202, 108, 63, 27, 32, 266, 194, 260, 244, 256, 74, 262, 230, 202, 108, 63, 27, 1, 251, 32, 267, 194, 243, 258, 244, 257, 74, 261, 230, 63, 108, 202, 27, 1, 250, 175, 173, 266, 32, 268, 194, 57, 243, 247, 74, 263, 108, 267, 194, 267, 108, 269, 194, 348, 175, 32, 270, 194, 57, 243, 248, 74, 265, 108, 267, 194, 266, 108, 269, 194, 348, 175, 32, 10, 243, 249, 74, 264, 108, 268, 244, 270, 108, 267, 194, 267, 175, 32, 3, 32]}, {"code": "def _cross_entropy_forward(\n    logits_ptr,\n    logits_row_stride,\n    loss_ptr,\n    logsumexp_ptr,\n    labels_ptr,\n    VOCAB_SIZE: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    DO_SOFTCAPPING: tl.constexpr,\n    SOFTCAP: tl.constexpr,\n    DO_LOGIT_SCALING: tl.constexpr,\n    LOGIT_SCALE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    logits_ptr += row_idx * logits_row_stride.to(tl.int64)\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx\n    labels_ptr += row_idx\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n\n    label_idx = tl.load(labels_ptr).to(tl.int32)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float(\"inf\"))\n\n    if DO_LOGIT_SCALING:\n        logits = LOGIT_SCALE * logits\n    if DO_SOFTCAPPING:\n        logits = SOFTCAP * triton_tanh(logits / SOFTCAP)\n\n    logits = logits.to(tl.float32)\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n\n    if label_idx != -100:\n        x = tl.load(logits_ptr + label_idx)\n        if DO_LOGIT_SCALING:\n            x = LOGIT_SCALE * x\n        if DO_SOFTCAPPING:\n            x = SOFTCAP * triton_tanh(x / SOFTCAP)\n        loss = logsumexp - x.to(tl.float32)\n    else:\n        loss = 0.0\n    tl.store(logsumexp_ptr, logsumexp)\n    tl.store(loss_ptr, loss)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 62, 6, 108, 253, 62, 6, 108, 254, 62, 6, 108, 255, 62, 6, 108, 256, 62, 6, 108, 257, 62, 6, 175, 62, 32, -1, 258, 194, 171, 243, 348, 175, 32, 247, 172, 258, 244, 248, 82, 259, 243, 181, 175, 32, 249, 172, 258, 32, 250, 172, 258, 32, 251, 172, 258, 32, 260, 194, 76, 243, 348, 108, 253, 175, 32, 261, 194, 260, 1, 252, 32, 262, 194, 55, 243, 251, 175, 82, 259, 243, 246, 175, 32, 263, 194, 55, 243, 247, 74, 260, 108, 261, 194, 261, 108, 264, 194, 4, 265, 243, 349, 175, 175, 32, 182, 256, 62, 32, 263, 194, 257, 244, 263, 32, 185, 32, 182, 254, 62, 32, 263, 194, 255, 244, 266, 243, 263, 42, 255, 175, 32, 185, 32, 263, 194, 263, 82, 259, 243, 145, 175, 32, 267, 194, 12, 243, 263, 108, 348, 175, 32, 268, 194, 267, 74, 50, 243, 221, 243, 107, 243, 263, 4, 267, 175, 108, 348, 175, 175, 32, 182, 262, 186, 4, 350, 348, 348, 62, 32, 269, 194, 55, 243, 247, 74, 262, 175, 32, 182, 256, 62, 32, 269, 194, 257, 244, 269, 32, 185, 32, 182, 254, 62, 32, 269, 194, 255, 244, 266, 243, 269, 42, 255, 175, 32, 185, 32, 270, 194, 268, 4, 269, 82, 259, 243, 145, 175, 32, 185, 32, 30, 62, 32, 270, 194, 348, 32, 56, 32, 10, 243, 250, 108, 268, 175, 32, 10, 243, 249, 108, 270, 175, 32, 3, 32]}, {"code": "def _chunked_cross_entropy_forward(\n    logits_ptr,\n    logits_row_stride,\n    loss_ptr,\n    logsumexp_ptr,\n    labels_ptr,\n    VOCAB_SIZE: tl.constexpr,\n    N_CHUNKS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    DO_SOFTCAPPING: tl.constexpr,\n    SOFTCAP: tl.constexpr,\n    DO_LOGIT_SCALING: tl.constexpr,\n    LOGIT_SCALE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    chunk_idx = tl.program_id(1)\n    logits_ptr += row_idx * logits_row_stride.to(tl.int64)\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx * N_CHUNKS + chunk_idx\n    labels_ptr += row_idx\n\n    col_offsets = chunk_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n\n    label_idx = tl.load(labels_ptr).to(tl.int32)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float(\"inf\"))\n\n    if DO_LOGIT_SCALING:\n        logits = LOGIT_SCALE * logits\n    if DO_SOFTCAPPING:\n        logits = SOFTCAP * triton_tanh(logits / SOFTCAP)\n\n    logits = logits.to(tl.float32)\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n\n    if chunk_idx == 0:\n        if label_idx != -100:\n            x = tl.load(logits_ptr + label_idx).to(tl.float32)\n            if DO_LOGIT_SCALING:\n                x = LOGIT_SCALE * x\n            if DO_SOFTCAPPING:\n                x = SOFTCAP * triton_tanh(x / SOFTCAP)\n            loss = -1.0 * x.to(tl.float32)\n        else:\n            loss = 0.0\n        tl.store(loss_ptr, loss)\n        tl.store(logsumexp_ptr, logsumexp)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 63, 6, 108, 253, 63, 6, 108, 254, 63, 6, 108, 255, 63, 6, 108, 256, 63, 6, 108, 257, 63, 6, 108, 258, 63, 6, 175, 63, 32, -1, 259, 194, 171, 243, 348, 175, 32, 260, 194, 171, 243, 349, 175, 32, 247, 172, 259, 244, 248, 82, 261, 243, 181, 175, 32, 249, 172, 259, 32, 250, 172, 259, 244, 253, 74, 260, 32, 251, 172, 259, 32, 262, 194, 260, 244, 254, 74, 76, 243, 348, 108, 254, 175, 32, 263, 194, 262, 1, 252, 32, 264, 194, 57, 243, 251, 175, 82, 261, 243, 62, 175, 32, 265, 194, 57, 243, 247, 74, 262, 108, 263, 194, 263, 108, 266, 194, 4, 267, 243, 350, 175, 175, 32, 182, 257, 63, 32, 265, 194, 258, 244, 265, 32, 185, 32, 182, 255, 63, 32, 265, 194, 256, 244, 268, 243, 265, 42, 256, 175, 32, 185, 32, 265, 194, 265, 82, 261, 243, 145, 175, 32, 269, 194, 12, 243, 265, 108, 348, 175, 32, 270, 194, 269, 74, 50, 243, 221, 243, 107, 243, 265, 4, 269, 175, 108, 348, 175, 175, 32, 182, 260, 77, 348, 63, 32, 182, 264, 186, 4, 349, 348, 348, 63, 32, 271, 194, 57, 243, 247, 74, 264, 175, 82, 261, 243, 145, 175, 32, 182, 257, 63, 32, 271, 194, 258, 244, 271, 32, 185, 32, 182, 255, 63, 32, 271, 194, 256, 244, 268, 243, 271, 42, 256, 175, 32, 185, 32, 272, 194, 4, 349, 244, 271, 82, 261, 243, 145, 175, 32, 185, 32, 30, 63, 32, 272, 194, 348, 32, 56, 32, 10, 243, 249, 108, 272, 175, 32, 10, 243, 250, 108, 270, 175, 32, 185, 32, 3, 32]}, {"code": "def _cross_entropy_backward(\n    logits_ptr,\n    logits_row_stride,\n    dloss_ptr,\n    dloss_row_stride,\n    logsumexp_ptr,\n    labels_ptr,\n    VOCAB_SIZE: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    DO_SOFTCAPPING: tl.constexpr,\n    SOFTCAP: tl.constexpr,\n    DO_LOGIT_SCALING: tl.constexpr,\n    LOGIT_SCALE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    block_idx = tl.program_id(1)\n\n    logits_ptr += row_idx * logits_row_stride.to(tl.int64)\n    dloss_ptr += row_idx * dloss_row_stride\n    col_offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr + row_idx).to(tl.int32)\n\n    if label_idx != -100:\n        dloss = tl.load(dloss_ptr)\n    else:\n        dloss = 0.0\n\n    x = tl.load(logits_ptr + col_offsets, mask=mask, other=-float(\"inf\"))\n\n    if DO_LOGIT_SCALING:\n        x = x * LOGIT_SCALE\n\n    if DO_SOFTCAPPING:\n        partial = triton_tanh(x / SOFTCAP)\n        x = SOFTCAP * partial\n\n    logsumexp = tl.load(logsumexp_ptr + row_idx)\n    y = tl.exp(x.to(tl.float32) - logsumexp)\n    y = tl.where(\n        col_offsets == label_idx,\n        y - 1.0,\n        y,\n    )\n\n    if DO_LOGIT_SCALING:\n        y = y * LOGIT_SCALE\n\n    if DO_SOFTCAPPING:\n        y = y * (1.0 - partial * partial)\n\n    tl.store(logits_ptr + col_offsets, dloss * y, mask=mask)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 62, 6, 108, 254, 62, 6, 108, 255, 62, 6, 108, 256, 62, 6, 108, 257, 62, 6, 108, 258, 62, 6, 175, 62, 32, -1, 259, 194, 171, 243, 348, 175, 32, 260, 194, 171, 243, 349, 175, 32, 247, 172, 259, 244, 248, 82, 261, 243, 181, 175, 32, 249, 172, 259, 244, 250, 32, 262, 194, 260, 244, 254, 74, 76, 243, 348, 108, 254, 175, 32, 263, 194, 262, 1, 253, 32, 264, 194, 55, 243, 252, 74, 259, 175, 82, 261, 243, 246, 175, 32, 182, 264, 186, 4, 349, 348, 348, 62, 32, 265, 194, 55, 243, 249, 175, 32, 185, 32, 30, 62, 32, 265, 194, 348, 32, 56, 32, 266, 194, 55, 243, 247, 74, 262, 108, 263, 194, 263, 108, 267, 194, 4, 268, 243, 350, 175, 175, 32, 182, 257, 62, 32, 266, 194, 266, 244, 258, 32, 185, 32, 182, 255, 62, 32, 269, 194, 270, 243, 266, 42, 256, 175, 32, 266, 194, 256, 244, 269, 32, 185, 32, 271, 194, 55, 243, 251, 74, 259, 175, 32, 272, 194, 107, 243, 266, 82, 261, 243, 145, 175, 4, 271, 175, 32, 272, 194, 205, 243, 262, 77, 264, 108, 272, 4, 349, 108, 272, 175, 32, 182, 257, 62, 32, 272, 194, 272, 244, 258, 32, 185, 32, 182, 255, 62, 32, 272, 194, 272, 244, 243, 349, 4, 269, 244, 269, 175, 32, 185, 32, 10, 243, 247, 74, 262, 108, 265, 244, 272, 108, 263, 194, 263, 175, 32, 3, 32]}, {"code": "def _rms_layernorm_backward(\n    dY,\n    dY_row_stride,\n    X,\n    X_row_stride,\n    W,\n    W_row_stride,\n    r,\n    r_row_stride,\n    dW,\n    dW_row_stride,\n    n_cols,\n    eps,\n    GEMMA: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    dY += row_idx * dY_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n\n    dY_row = tl.load(dY + col_offsets, mask=mask, other=0).to(tl.float32)\n    X_row = tl.load(X + col_offsets, mask=mask, other=0).to(tl.float32)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0).to(tl.float32)\n\n    inv_var = tl.load(r).to(tl.float32)\n    normed = X_row * inv_var\n\n    if GEMMA:\n        dY_W = dY_row * (W_row + 1.0)\n    else:\n        dY_W = dY_row * W_row\n\n    rowsum_dY_normed = tl.sum(dY_W * normed, axis=0)\n    output = inv_var / n_cols * (n_cols * dY_W - normed * rowsum_dY_normed)\n    tl.store(dY + col_offsets, output, mask=mask)", "encoded": [29, 347, 243, 247, 108, 248, 108, 249, 108, 250, 108, 251, 108, 252, 108, 253, 108, 254, 108, 255, 108, 256, 108, 257, 108, 258, 108, 259, 63, 6, 108, 260, 63, 6, 175, 63, 32, -1, 261, 194, 171, 243, 348, 175, 32, 262, 194, 76, 243, 348, 108, 260, 175, 32, 263, 194, 262, 1, 257, 32, 247, 172, 261, 244, 248, 32, 249, 172, 261, 244, 250, 32, 253, 172, 261, 244, 254, 32, 264, 194, 57, 243, 247, 74, 262, 108, 263, 194, 263, 108, 265, 194, 348, 175, 82, 266, 243, 145, 175, 32, 267, 194, 57, 243, 249, 74, 262, 108, 263, 194, 263, 108, 265, 194, 348, 175, 82, 266, 243, 145, 175, 32, 268, 194, 57, 243, 251, 74, 262, 108, 263, 194, 263, 108, 265, 194, 348, 175, 82, 266, 243, 145, 175, 32, 269, 194, 57, 243, 253, 175, 82, 266, 243, 145, 175, 32, 270, 194, 267, 244, 269, 32, 182, 259, 63, 32, 271, 194, 264, 244, 243, 268, 74, 349, 175, 32, 185, 32, 30, 63, 32, 271, 194, 264, 244, 268, 32, 56, 32, 272, 194, 221, 243, 271, 244, 270, 108, 273, 194, 348, 175, 32, 274, 194, 269, 42, 257, 244, 243, 257, 244, 271, 4, 270, 244, 272, 175, 32, 10, 243, 247, 74, 262, 108, 274, 108, 263, 194, 263, 175, 32, 3, 32]}, {"code": "def triton_f4_to_scaled_bf16_kernel(\n    x_ptr,\n    s_ptr,\n    output_ptr,\n    n_elements_in,\n    mx_block_size: tl.constexpr,\n    sign_mask_f4: tl.constexpr,\n    mantissa_mask_f4: tl.constexpr,\n    mbits_f4_e2m1: tl.constexpr,\n    ebits_f4_e2m1: tl.constexpr,\n    f4_e2m1_exp_bias: tl.constexpr,\n    mbits_f32: tl.constexpr,\n    ebits_f32: tl.constexpr,\n    f32_exp_bias: tl.constexpr,\n    zero_bits_f32: tl.constexpr,\n    zero_point_five_bits_f32: tl.constexpr,\n    e8m0_exponent_bias: tl.constexpr,\n    e8m0_exponent_nan_val: tl.constexpr,\n    BLOCK_SIZE_IN: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    n_elements_out = n_elements_in * 2\n    n_elements_s = n_elements_out // 32\n\n    BLOCK_SIZE_S: tl.constexpr = BLOCK_SIZE_IN // 16\n    BLOCK_SIZE_OUT: tl.constexpr = BLOCK_SIZE_IN * 2\n\n    block_start_in = pid * BLOCK_SIZE_IN\n    offsets_in = block_start_in + tl.arange(0, BLOCK_SIZE_IN)\n    mask_in = offsets_in < n_elements_in\n\n    x_packed = tl.load(x_ptr + offsets_in, mask=mask_in)\n    output = _fp4_packed_to_bf16(\n        x_packed,\n        sign_mask_f4,\n        mantissa_mask_f4,\n        mbits_f4_e2m1,\n        ebits_f4_e2m1,\n        f4_e2m1_exp_bias,\n        mbits_f32,\n        ebits_f32,\n        f32_exp_bias,\n        zero_bits_f32,\n        zero_point_five_bits_f32,\n    )\n\n    block_start_s = pid * BLOCK_SIZE_S\n    offsets_s = block_start_s + tl.arange(0, BLOCK_SIZE_S)\n    mask_s = offsets_s < n_elements_s\n    s = tl.load(s_ptr + offsets_s, mask=mask_s)\n\n    s_offset = s.to(tl.int16) - e8m0_exponent_bias\n    s_fp = tl.extra.cuda.libdevice.pow(2.0, s_offset).to(tl.bfloat16)\n    s_fp = tl.where(s != e8m0_exponent_nan_val, s_fp, float(\"nan\"))\n\n    output = tl.reshape(output, (BLOCK_SIZE_OUT // mx_block_size, mx_block_size))\n    s_fp = tl.reshape(s_fp, (BLOCK_SIZE_S // 1, 1))\n    output = output * s_fp\n    output = tl.reshape(output, (BLOCK_SIZE_OUT,))\n\n    block_start_out = pid * BLOCK_SIZE_OUT\n    offsets_out = block_start_out + tl.arange(0, BLOCK_SIZE_OUT)\n    mask_out = offsets_out < n_elements_out\n\n    tl.store(output_ptr + offsets_out, output, mask=mask_out)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 62, 6, 109, 253, 62, 6, 109, 254, 62, 6, 109, 255, 62, 6, 109, 256, 62, 6, 109, 257, 62, 6, 109, 258, 62, 6, 109, 259, 62, 6, 109, 260, 62, 6, 109, 261, 62, 6, 109, 262, 62, 6, 109, 263, 62, 6, 109, 264, 62, 6, 109, 265, 62, 6, 176, 62, 32, -1, 266, 195, 172, 244, 267, 195, 349, 176, 32, 268, 195, 251, 245, 350, 32, 269, 195, 268, 48, 351, 350, 32, 270, 62, 6, 195, 265, 48, 352, 353, 32, 271, 62, 6, 195, 265, 245, 350, 32, 272, 195, 266, 245, 265, 32, 273, 195, 272, 74, 76, 244, 349, 109, 265, 176, 32, 274, 195, 273, 1, 251, 32, 275, 195, 55, 244, 248, 74, 273, 109, 276, 195, 274, 176, 32, 277, 195, 278, 244, 275, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 109, 261, 109, 262, 176, 32, 279, 195, 266, 245, 270, 32, 280, 195, 279, 74, 76, 244, 349, 109, 270, 176, 32, 281, 195, 280, 1, 269, 32, 282, 195, 55, 244, 249, 74, 280, 109, 276, 195, 281, 176, 32, 283, 195, 282, 82, 284, 244, 17, 176, 4, 263, 32, 285, 195, 85, 244, 350, 109, 283, 176, 82, 284, 244, 234, 176, 32, 285, 195, 206, 244, 282, 187, 264, 109, 285, 109, 286, 244, 354, 176, 176, 32, 277, 195, 23, 244, 277, 109, 244, 271, 48, 252, 109, 252, 176, 176, 32, 285, 195, 23, 244, 285, 109, 244, 270, 48, 352, 109, 352, 176, 176, 32, 277, 195, 277, 245, 285, 32, 277, 195, 23, 244, 277, 109, 244, 271, 109, 176, 176, 32, 287, 195, 266, 245, 271, 32, 288, 195, 287, 74, 76, 244, 349, 109, 271, 176, 32, 289, 195, 288, 1, 268, 32, 10, 244, 250, 74, 288, 109, 277, 109, 276, 195, 289, 176, 32, 3, 32]}, {"code": "def triton_red_fused_native_layer_norm_0(\n    in_out_ptr0,\n    in_ptr0,\n    in_ptr1,\n    in_ptr2,\n    out_ptr0,\n    out_ptr1,\n    xnumel,\n    rnumel,\n    XBLOCK: tl.constexpr,\n    RBLOCK: tl.constexpr,\n):\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex\n    tmp3_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    tmp3_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    tmp3_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp0 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"\n        ).to(tl.float32)\n        tmp1 = tmp0.to(tl.float32)\n        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(\n            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0\n        )\n        tmp3_mean = tl.where(rmask, tmp3_mean_next, tmp3_mean)\n        tmp3_m2 = tl.where(rmask, tmp3_m2_next, tmp3_m2)\n        tmp3_weight = tl.where(rmask, tmp3_weight_next, tmp3_weight)\n    tmp3_tmp, tmp4_tmp, tmp5_tmp = triton_helpers.welford(\n        tmp3_mean, tmp3_m2, tmp3_weight, 1\n    )\n    tmp3 = tmp3_tmp[:, None]\n    tmp4 = tmp4_tmp[:, None]\n    tmp5 = tmp5_tmp[:, None]\n    tl.store(out_ptr0 + (x0), tmp3, None)\n    tmp6 = rnumel\n    tmp7 = tmp4 / tmp6\n    tmp8 = 1e-05\n    tmp9 = tmp7 + tmp8\n    tmp10 = libdevice.rsqrt(tmp9)\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + (x0), tmp10, None)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp11 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"\n        ).to(tl.float32)\n        tmp15 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp18 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp12 = tmp11.to(tl.float32)\n        tmp13 = tmp12 - tmp3\n        tmp14 = tmp13 * tmp10\n        tmp16 = tmp15.to(tl.float32)\n        tmp17 = tmp14 * tmp16\n        tmp19 = tmp18.to(tl.float32)\n        tmp20 = tmp17 + tmp19\n        tmp21 = tmp20.to(tl.float32)\n        tl.store(out_ptr1 + (r1 + (rnumel * x0)), tmp21, rmask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 63, 6, 109, 257, 63, 6, 176, 63, 32, -1, 258, 195, 172, 244, 349, 176, 245, 256, 32, 259, 195, 258, 74, 76, 244, 349, 109, 256, 176, 231, 63, 109, 203, 27, 32, 260, 195, 259, 1, 254, 32, 261, 195, 76, 244, 349, 109, 257, 176, 231, 203, 109, 63, 27, 32, 262, 195, 259, 32, 263, 195, 177, 244, 231, 256, 109, 257, 27, 109, 146, 176, 32, 264, 195, 177, 244, 231, 256, 109, 257, 27, 109, 146, 176, 32, 265, 195, 177, 244, 231, 256, 109, 257, 27, 109, 146, 176, 32, 136, 266, 158, 5, 244, 349, 109, 255, 109, 257, 176, 63, 32, 267, 195, 266, 74, 261, 32, 268, 195, 267, 1, 255, 32, 269, 195, 267, 32, 270, 195, 57, 244, 249, 74, 244, 269, 74, 255, 245, 262, 176, 109, 268, 109, 271, 195, 350, 176, 82, 272, 244, 146, 176, 32, 273, 195, 270, 82, 272, 244, 146, 176, 32, 274, 195, 194, 244, 273, 109, 231, 256, 109, 257, 27, 176, 32, 275, 109, 276, 109, 277, 195, 127, 82, 278, 244, 274, 109, 263, 109, 264, 109, 265, 109, 266, 77, 349, 176, 32, 263, 195, 206, 244, 268, 109, 275, 109, 263, 176, 32, 264, 195, 206, 244, 268, 109, 276, 109, 264, 176, 32, 265, 195, 206, 244, 268, 109, 277, 109, 265, 176, 32, 78, 32, 279, 109, 280, 109, 281, 195, 127, 82, 282, 244, 263, 109, 264, 109, 265, 109, 351, 176, 32, 283, 195, 279, 231, 63, 109, 203, 27, 32, 284, 195, 280, 231, 63, 109, 203, 27, 32, 285, 195, 281, 231, 63, 109, 203, 27, 32, 10, 244, 252, 74, 262, 109, 283, 109, 203, 176, 32, 286, 195, 255, 32, 287, 195, 284, 42, 286, 32, 288, 195, 352, 32, 289, 195, 287, 74, 288, 32, 290, 195, 16, 82, 291, 244, 289, 176, 32, 51, 244, 176, 32, 10, 244, 248, 74, 262, 109, 290, 109, 203, 176, 32, 136, 266, 158, 5, 244, 349, 109, 255, 109, 257, 176, 63, 32, 267, 195, 266, 74, 261, 32, 268, 195, 267, 1, 255, 32, 269, 195, 267, 32, 292, 195, 57, 244, 249, 74, 244, 269, 74, 255, 245, 262, 176, 109, 268, 109, 271, 195, 353, 176, 82, 272, 244, 146, 176, 32, 293, 195, 57, 244, 250, 74, 269, 109, 268, 109, 271, 195, 350, 176, 82, 272, 244, 146, 176, 32, 294, 195, 57, 244, 251, 74, 269, 109, 268, 109, 271, 195, 350, 176, 82, 272, 244, 146, 176, 32, 295, 195, 292, 82, 272, 244, 146, 176, 32, 296, 195, 295, 4, 283, 32, 297, 195, 296, 245, 290, 32, 298, 195, 293, 82, 272, 244, 146, 176, 32, 299, 195, 297, 245, 298, 32, 300, 195, 294, 82, 272, 244, 146, 176, 32, 301, 195, 299, 74, 300, 32, 302, 195, 301, 82, 272, 244, 146, 176, 32, 10, 244, 253, 74, 244, 269, 74, 255, 245, 262, 176, 109, 302, 109, 268, 176, 32, 78, 32, 3, 32]}, {"code": "def fused_recurrent_hgrn_fwd_kernel(\n    x,\n    g,\n    o,\n    h0,\n    ht,\n    T: tl.constexpr,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr,\n):\n    i_d, i_bh = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n\n    p_x = x + i_bh * T * D + o_d\n    p_g = g + i_bh * T * D + o_d\n    p_o = o + i_bh * T * D + o_d\n\n    b_h = tl.zeros([BD], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * D + o_d\n        b_h += tl.load(p_h0, mask=mask, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_x = tl.load(p_x, mask=mask, other=0).to(tl.float32)\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_h = b_g * b_h + b_x\n        tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask)\n\n        p_x += D\n        p_g += D\n        p_o += D\n\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * D + o_d\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 62, 6, 109, 254, 62, 6, 109, 255, 62, 6, 109, 256, 62, 6, 109, 257, 62, 6, 176, 62, 32, -1, 258, 109, 259, 195, 244, 172, 244, 349, 176, 109, 172, 244, 350, 176, 176, 32, 260, 195, 258, 245, 255, 74, 76, 244, 349, 109, 255, 176, 32, 261, 195, 260, 1, 254, 32, 262, 195, 248, 74, 259, 245, 253, 245, 254, 74, 260, 32, 263, 195, 249, 74, 259, 245, 253, 245, 254, 74, 260, 32, 264, 195, 250, 74, 259, 245, 253, 245, 254, 74, 260, 32, 265, 195, 177, 244, 231, 255, 27, 109, 92, 195, 146, 176, 32, 183, 256, 62, 32, 266, 195, 251, 74, 259, 245, 254, 74, 260, 32, 265, 173, 55, 244, 266, 109, 261, 195, 261, 109, 267, 195, 349, 176, 82, 268, 244, 146, 176, 32, 186, 32, 136, 269, 158, 5, 244, 349, 109, 253, 176, 62, 32, 270, 195, 55, 244, 262, 109, 261, 195, 261, 109, 267, 195, 349, 176, 82, 268, 244, 146, 176, 32, 271, 195, 55, 244, 263, 109, 261, 195, 261, 109, 267, 195, 349, 176, 82, 268, 244, 146, 176, 32, 265, 195, 271, 245, 265, 74, 270, 32, 10, 244, 264, 109, 265, 82, 268, 244, 264, 82, 92, 82, 115, 176, 109, 261, 195, 261, 176, 32, 262, 173, 254, 32, 263, 173, 254, 32, 264, 173, 254, 32, 78, 32, 183, 257, 62, 32, 272, 195, 252, 74, 259, 245, 254, 74, 260, 32, 10, 244, 272, 109, 265, 82, 268, 244, 272, 82, 92, 82, 115, 176, 109, 261, 195, 261, 176, 32, 186, 32, 3, 32]}, {"code": "def fused_recurrent_hgrn_bwd_kernel(\n    g,\n    o,\n    dx,\n    dg,\n    do,\n    h0,\n    T: tl.constexpr,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n):\n    i_d, i_bh = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n\n    p_g = g + (i_bh * T + T - 1) * D + o_d\n    p_o = o + (i_bh * T + T - 2) * D + o_d\n    p_dx = dx + (i_bh * T + T - 1) * D + o_d\n    p_dg = dg + (i_bh * T + T - 1) * D + o_d\n    p_do = do + (i_bh * T + T - 1) * D + o_d\n\n    b_dh = tl.zeros([BD], dtype=tl.float32)\n    for i in range(T - 1, -1, -1):\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)\n        if i > 0:\n            b_o = tl.load(p_o, mask=mask, other=0).to(tl.float32)\n        elif USE_INITIAL_STATE:\n            b_o = tl.load(h0 + i_bh * D + o_d, mask=mask, other=0).to(tl.float32)\n        else:\n            b_o = tl.zeros([BD], dtype=tl.float32)\n\n        b_dh = b_dh + b_do\n        b_dx = b_dh\n        b_dg = b_dh * b_o\n        b_dh = b_dh * b_g\n        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), mask=mask)\n\n        p_g -= D\n        p_o -= D\n        p_dx -= D\n        p_dg -= D\n        p_do -= D", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 63, 6, 109, 255, 63, 6, 109, 256, 63, 6, 109, 257, 63, 6, 176, 63, 32, -1, 258, 109, 259, 195, 244, 172, 244, 349, 176, 109, 172, 244, 350, 176, 176, 32, 260, 195, 258, 245, 256, 74, 76, 244, 349, 109, 256, 176, 32, 261, 195, 260, 1, 255, 32, 262, 195, 248, 74, 244, 259, 245, 254, 74, 254, 4, 350, 176, 245, 255, 74, 260, 32, 263, 195, 249, 74, 244, 259, 245, 254, 74, 254, 4, 351, 176, 245, 255, 74, 260, 32, 264, 195, 250, 74, 244, 259, 245, 254, 74, 254, 4, 350, 176, 245, 255, 74, 260, 32, 265, 195, 251, 74, 244, 259, 245, 254, 74, 254, 4, 350, 176, 245, 255, 74, 260, 32, 266, 195, 252, 74, 244, 259, 245, 254, 74, 254, 4, 350, 176, 245, 255, 74, 260, 32, 267, 195, 177, 244, 231, 256, 27, 109, 92, 195, 146, 176, 32, 136, 268, 158, 5, 244, 254, 4, 350, 109, 4, 350, 109, 4, 350, 176, 63, 32, 269, 195, 57, 244, 262, 109, 261, 195, 261, 109, 270, 195, 349, 176, 82, 271, 244, 146, 176, 32, 272, 195, 57, 244, 266, 109, 261, 195, 261, 109, 270, 195, 349, 176, 82, 271, 244, 146, 176, 32, 183, 268, 125, 349, 63, 32, 273, 195, 57, 244, 263, 109, 261, 195, 261, 109, 270, 195, 349, 176, 82, 271, 244, 146, 176, 32, 66, 32, 37, 257, 63, 32, 273, 195, 57, 244, 253, 74, 259, 245, 255, 74, 260, 109, 261, 195, 261, 109, 270, 195, 349, 176, 82, 271, 244, 146, 176, 32, 186, 32, 30, 63, 32, 273, 195, 177, 244, 231, 256, 27, 109, 92, 195, 146, 176, 32, 56, 32, 267, 195, 267, 74, 272, 32, 274, 195, 267, 32, 275, 195, 267, 245, 273, 32, 267, 195, 267, 245, 269, 32, 10, 244, 264, 109, 274, 82, 271, 244, 264, 82, 92, 82, 115, 176, 109, 261, 195, 261, 176, 32, 10, 244, 265, 109, 275, 82, 271, 244, 265, 82, 92, 82, 115, 176, 109, 261, 195, 261, 176, 32, 262, 2, 255, 32, 263, 2, 255, 32, 264, 2, 255, 32, 265, 2, 255, 32, 266, 2, 255, 32, 78, 32, 3, 32]}, {"code": "def matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    bs_ptr,\n    bzp_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    stride_bsk,\n    stride_bsn,\n    stride_bzpk,\n    stride_bzpn,\n    group_size,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    SPLIT_K: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n\n    b_ptrs = b_ptr + (offs_k[:, None] // 8) * stride_bk + offs_bn[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n\n        bs_ptrs = (\n            bs_ptr\n            + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size)\n            * stride_bsk\n            + offs_bn[None, :] * stride_bsn\n        )\n\n        bzp_ptrs = (\n            bzp_ptr\n            + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size)\n            * stride_bzpk\n            + (offs_bn[None, :] // 8) * stride_bzpn\n        )\n        b_shift_bits = (offs_k[:, None] % 8) * 4\n        bzp_shift_bits = (offs_bn[None, :] % 8) * 4\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        bs = tl.load(bs_ptrs)\n        bzp = tl.load(bzp_ptrs)\n\n        int_b = (b >> b_shift_bits) & 0xF\n        int_bzp = (bzp >> bzp_shift_bits) & 0xF\n        b = ((int_b - int_bzp) * bs).to(a.dtype)\n        accumulator += tl.dot(a, b.to(a.dtype))\n\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk // 8\n\n    c = accumulator.to(c_ptr.dtype.element_ty)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 109, 261, 109, 262, 109, 263, 109, 264, 109, 265, 109, 266, 109, 267, 62, 6, 109, 268, 62, 6, 109, 269, 62, 6, 109, 270, 62, 6, 109, 271, 62, 6, 176, 62, 32, -1, 272, 195, 172, 244, 273, 195, 349, 176, 32, 274, 195, 172, 244, 273, 195, 350, 176, 32, 275, 195, 64, 244, 253, 109, 267, 176, 32, 276, 195, 64, 244, 254, 109, 268, 176, 32, 277, 195, 64, 244, 255, 109, 269, 176, 32, 278, 195, 270, 245, 276, 32, 279, 195, 272, 48, 278, 32, 280, 195, 279, 245, 270, 32, 281, 195, 39, 244, 275, 4, 280, 109, 270, 176, 32, 282, 195, 280, 74, 272, 227, 281, 32, 283, 195, 272, 227, 278, 48, 281, 32, 284, 195, 244, 282, 245, 267, 74, 76, 244, 349, 109, 267, 176, 176, 227, 253, 32, 285, 195, 244, 283, 245, 268, 74, 76, 244, 349, 109, 268, 176, 176, 227, 254, 32, 286, 195, 274, 245, 269, 74, 76, 244, 349, 109, 269, 176, 32, 287, 195, 248, 74, 284, 231, 62, 109, 203, 27, 245, 256, 74, 286, 231, 203, 109, 62, 27, 245, 257, 32, 288, 195, 249, 74, 286, 231, 62, 109, 203, 27, 48, 351, 245, 258, 74, 285, 231, 203, 109, 62, 27, 245, 259, 32, 289, 195, 177, 244, 244, 267, 109, 268, 176, 109, 92, 195, 146, 176, 32, 136, 290, 158, 5, 244, 349, 109, 64, 244, 255, 109, 269, 245, 271, 176, 176, 62, 32, 291, 195, 251, 74, 244, 286, 231, 62, 109, 203, 27, 74, 290, 245, 269, 245, 271, 176, 48, 266, 245, 262, 74, 285, 231, 203, 109, 62, 27, 245, 263, 32, 292, 195, 252, 74, 244, 286, 231, 62, 109, 203, 27, 74, 290, 245, 269, 245, 271, 176, 48, 266, 245, 264, 74, 285, 231, 203, 109, 62, 27, 48, 351, 245, 265, 32, 293, 195, 286, 231, 62, 109, 203, 27, 227, 351, 245, 352, 32, 294, 195, 285, 231, 203, 109, 62, 27, 227, 351, 245, 352, 32, 295, 195, 55, 244, 287, 176, 32, 296, 195, 55, 244, 288, 176, 32, 297, 195, 55, 244, 291, 176, 32, 298, 195, 55, 244, 292, 176, 32, 299, 195, 296, 128, 293, 174, 350, 353, 32, 300, 195, 298, 128, 294, 174, 350, 353, 32, 296, 195, 244, 244, 299, 4, 300, 176, 245, 297, 176, 82, 301, 244, 295, 82, 92, 176, 32, 289, 173, 15, 244, 295, 109, 296, 82, 301, 244, 295, 82, 92, 176, 176, 32, 287, 173, 269, 245, 271, 245, 257, 32, 288, 173, 269, 245, 271, 245, 258, 48, 351, 32, 78, 32, 302, 195, 289, 82, 301, 244, 250, 82, 92, 82, 115, 176, 32, 303, 195, 282, 245, 267, 74, 76, 244, 349, 109, 267, 176, 32, 304, 195, 283, 245, 268, 74, 76, 244, 349, 109, 268, 176, 32, 305, 195, 250, 74, 260, 245, 303, 231, 62, 109, 203, 27, 74, 261, 245, 304, 231, 203, 109, 62, 27, 32, 306, 195, 244, 303, 231, 62, 109, 203, 27, 1, 253, 176, 174, 244, 304, 231, 203, 109, 62, 27, 1, 254, 176, 32, 183, 271, 77, 350, 62, 32, 10, 244, 305, 109, 302, 109, 307, 195, 306, 176, 32, 186, 32, 30, 62, 32, 197, 244, 305, 109, 302, 109, 307, 195, 306, 176, 32, 56, 32, 3, 32]}, {"code": "def _int8_matmul_rowwise_dequantize(\n    A,\n    B,\n    C,\n    bias,\n    state_x_ptr,\n    state_w_ptr,\n    M,\n    N,\n    K,\n    divfactor,\n    has_bias: tl.constexpr,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    SPLIT_K: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    ACC_TYPE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (pid % group_size)\n    pid_n = (pid % width) // (group_size)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    w_factor = tl.load(state_w_ptr + rbn)[None, :]\n    x_factor = tl.load(state_x_ptr + ram)[:, None]\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            k_remaining = K - k * (BLOCK_K * SPLIT_K)\n            a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.0)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    acc = w_factor * (x_factor * (acc * divfactor))\n    acc = acc.to(C.dtype.element_ty)\n    if has_bias:\n        bias = tl.load(bias + rn).to(C.dtype.element_ty)\n        acc = acc + bias[None, :]\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 63, 6, 109, 259, 109, 260, 109, 261, 109, 262, 109, 263, 109, 264, 109, 265, 63, 6, 109, 266, 63, 6, 109, 267, 63, 6, 109, 268, 63, 6, 109, 269, 63, 6, 109, 270, 63, 6, 109, 271, 63, 6, 176, 63, 32, -1, 272, 195, 172, 244, 349, 176, 32, 273, 195, 172, 244, 350, 176, 32, 274, 195, 65, 244, 254, 109, 265, 176, 32, 275, 195, 65, 244, 255, 109, 266, 176, 32, 276, 195, 268, 245, 275, 32, 277, 195, 272, 48, 276, 32, 278, 195, 39, 244, 274, 4, 277, 245, 268, 109, 268, 176, 32, 279, 195, 277, 245, 268, 74, 272, 227, 278, 32, 280, 195, 272, 227, 276, 48, 278, 32, 281, 195, 279, 245, 265, 74, 76, 244, 349, 109, 265, 176, 32, 282, 195, 280, 245, 266, 74, 76, 244, 349, 109, 266, 176, 32, 283, 195, 212, 244, 55, 244, 281, 227, 254, 109, 265, 176, 109, 265, 176, 32, 284, 195, 212, 244, 55, 244, 282, 227, 255, 109, 266, 176, 109, 266, 176, 32, 285, 195, 273, 245, 267, 74, 76, 244, 349, 109, 267, 176, 32, 248, 195, 248, 74, 244, 283, 231, 63, 109, 203, 27, 245, 259, 74, 285, 231, 203, 109, 63, 27, 245, 260, 176, 32, 249, 195, 249, 74, 244, 285, 231, 63, 109, 203, 27, 245, 261, 74, 284, 231, 203, 109, 63, 27, 245, 262, 176, 32, 281, 195, 279, 245, 265, 74, 76, 244, 349, 109, 265, 176, 32, 282, 195, 280, 245, 266, 74, 76, 244, 349, 109, 266, 176, 32, 286, 195, 57, 244, 253, 74, 284, 176, 231, 203, 109, 63, 27, 32, 287, 195, 57, 244, 252, 74, 283, 176, 231, 63, 109, 203, 27, 32, 288, 195, 177, 244, 244, 265, 109, 266, 176, 109, 92, 195, 62, 176, 32, 136, 289, 158, 5, 244, 349, 109, 65, 244, 256, 109, 267, 245, 269, 176, 176, 63, 32, 183, 270, 63, 32, 290, 195, 57, 244, 248, 176, 32, 291, 195, 57, 244, 249, 176, 32, 186, 32, 30, 63, 32, 292, 195, 256, 4, 289, 245, 244, 267, 245, 269, 176, 32, 290, 195, 57, 244, 248, 109, 293, 195, 285, 231, 203, 109, 63, 27, 1, 292, 109, 294, 195, 349, 176, 32, 291, 195, 57, 244, 249, 109, 293, 195, 285, 231, 63, 109, 203, 27, 1, 292, 109, 294, 195, 349, 176, 32, 56, 32, 288, 173, 15, 244, 290, 109, 291, 176, 32, 248, 173, 267, 245, 269, 245, 260, 32, 249, 173, 267, 245, 269, 245, 261, 32, 78, 32, 288, 195, 286, 245, 244, 287, 245, 244, 288, 245, 257, 176, 176, 32, 288, 195, 288, 82, 295, 244, 250, 82, 92, 82, 115, 176, 32, 183, 258, 63, 32, 251, 195, 57, 244, 251, 74, 282, 176, 82, 295, 244, 250, 82, 92, 82, 115, 176, 32, 288, 195, 288, 74, 251, 231, 203, 109, 63, 27, 32, 186, 32, 250, 195, 250, 74, 244, 281, 231, 63, 109, 203, 27, 245, 263, 74, 282, 231, 203, 109, 63, 27, 245, 264, 176, 32, 293, 195, 244, 281, 1, 254, 176, 231, 63, 109, 203, 27, 174, 244, 282, 1, 255, 176, 231, 203, 109, 63, 27, 32, 183, 269, 77, 350, 63, 32, 10, 244, 250, 109, 288, 109, 293, 195, 293, 176, 32, 186, 32, 30, 63, 32, 197, 244, 250, 109, 288, 109, 293, 195, 293, 176, 32, 56, 32, 3, 32]}, {"code": "def matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K: tl.constexpr,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    tl.static_assert(\n        K % (4 * BLOCK_SIZE_K) == 0,\n        \"K / 4 must be divisible by BLOCK_SIZE_K => K divisible by 4*BLOCK_SIZE_K\",\n    )\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    for i in range(4):\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n        for j in range(0, tl.cdiv(K // 4, BLOCK_SIZE_K)):\n            k = i * tl.cdiv(K // 4, BLOCK_SIZE_K) + j\n            a = tl.load(\n                a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0\n            ).to(tl.int8)\n            b_uint8 = tl.load(b_ptrs, mask=offs_k[:, None] < K, other=0)\n            mask = 3 << (2 * i)\n            b = ((b_uint8 & mask) >> (2 * i)).to(tl.int8)\n            tensor_full = tl.full((1,), 1, dtype=tl.int8)\n            accumulator += tl.dot(a, (b - tensor_full), out_dtype=tl.int32)\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 62, 6, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 62, 6, 109, 261, 62, 6, 109, 262, 62, 6, 109, 263, 62, 6, 176, 62, 32, -1, 83, 244, 253, 227, 244, 349, 245, 262, 176, 77, 350, 109, 351, 176, 32, 264, 195, 172, 244, 265, 195, 350, 176, 32, 266, 195, 64, 244, 251, 109, 260, 176, 32, 267, 195, 64, 244, 252, 109, 261, 176, 32, 268, 195, 263, 245, 267, 32, 269, 195, 264, 48, 268, 32, 270, 195, 269, 245, 263, 32, 271, 195, 39, 244, 266, 4, 270, 109, 263, 176, 32, 272, 195, 270, 74, 264, 227, 268, 227, 271, 32, 273, 195, 264, 227, 268, 48, 271, 32, 274, 195, 244, 272, 245, 260, 74, 76, 244, 350, 109, 260, 176, 176, 227, 251, 32, 275, 195, 244, 273, 245, 261, 74, 76, 244, 350, 109, 261, 176, 176, 227, 252, 32, 276, 195, 76, 244, 350, 109, 262, 176, 32, 277, 195, 248, 74, 244, 274, 231, 62, 109, 203, 27, 245, 254, 74, 276, 231, 203, 109, 62, 27, 245, 255, 176, 32, 278, 195, 249, 74, 244, 276, 231, 62, 109, 203, 27, 245, 256, 74, 275, 231, 203, 109, 62, 27, 245, 257, 176, 32, 279, 195, 177, 244, 244, 260, 109, 261, 176, 109, 92, 195, 247, 176, 32, 136, 280, 158, 5, 244, 349, 176, 62, 32, 278, 195, 249, 74, 244, 276, 231, 62, 109, 203, 27, 245, 256, 74, 275, 231, 203, 109, 62, 27, 245, 257, 176, 32, 136, 281, 158, 5, 244, 350, 109, 64, 244, 253, 48, 349, 109, 262, 176, 176, 62, 32, 282, 195, 280, 245, 64, 244, 253, 48, 349, 109, 262, 176, 74, 281, 32, 283, 195, 55, 244, 277, 109, 284, 195, 276, 231, 203, 109, 62, 27, 1, 253, 4, 282, 245, 262, 109, 285, 195, 350, 176, 82, 286, 244, 141, 176, 32, 287, 195, 55, 244, 278, 109, 284, 195, 276, 231, 62, 109, 203, 27, 1, 253, 109, 285, 195, 350, 176, 32, 284, 195, 352, 137, 353, 245, 280, 32, 288, 195, 244, 244, 287, 174, 284, 176, 128, 353, 245, 280, 176, 82, 286, 244, 141, 176, 32, 289, 195, 238, 244, 244, 354, 109, 176, 109, 354, 109, 92, 195, 141, 176, 32, 279, 173, 15, 244, 283, 109, 288, 4, 289, 109, 290, 195, 247, 176, 32, 277, 173, 262, 245, 255, 32, 278, 173, 262, 245, 256, 32, 78, 32, 78, 32, 291, 195, 279, 32, 292, 195, 272, 245, 260, 74, 76, 244, 350, 109, 260, 176, 32, 293, 195, 273, 245, 261, 74, 76, 244, 350, 109, 261, 176, 32, 294, 195, 250, 74, 258, 245, 292, 231, 62, 109, 203, 27, 74, 259, 245, 293, 231, 203, 109, 62, 27, 32, 295, 195, 244, 292, 231, 62, 109, 203, 27, 1, 251, 176, 174, 244, 293, 231, 203, 109, 62, 27, 1, 252, 176, 32, 10, 244, 294, 109, 291, 109, 284, 195, 295, 176, 32, 3, 32]}, {"code": "def quantize_int8_perrow_kernel(\n    fpa_ptr,\n    a_ptr,\n    as_ptr,\n    M,\n    K,\n    stride_fpam,\n    stride_fpak,\n    stride_am,\n    stride_ak,\n    stride_asm,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :] * stride_fpak\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    a_max = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        a_max = tl.maximum(a_max, tl.max(tl.abs(fpa), axis=1))\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n    a_scale = a_max / 127.0\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :] * stride_fpak\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        inta = (fpa / a_scale[:, None]).to(tl.int8)\n        tl.store(a_ptrs, inta, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K)\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n    as_offs = pid_m * BLOCK_SIZE_M * stride_asm + tl.arange(0, BLOCK_SIZE_M)\n    tl.store(as_ptr + as_offs, a_scale)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 63, 6, 109, 259, 63, 6, 176, 63, 32, -1, 260, 195, 172, 244, 261, 195, 349, 176, 32, 262, 195, 76, 244, 349, 109, 259, 176, 32, 263, 195, 244, 260, 245, 258, 74, 76, 244, 349, 109, 258, 176, 176, 227, 251, 32, 264, 195, 248, 74, 263, 231, 63, 109, 203, 27, 245, 253, 74, 262, 231, 203, 109, 63, 27, 245, 254, 32, 265, 195, 249, 74, 263, 231, 63, 109, 203, 27, 245, 255, 74, 262, 231, 203, 109, 63, 27, 245, 256, 32, 266, 195, 177, 244, 244, 258, 109, 176, 109, 92, 195, 146, 176, 32, 136, 267, 158, 5, 244, 349, 109, 65, 244, 252, 109, 259, 176, 176, 63, 32, 268, 195, 57, 244, 264, 109, 269, 195, 262, 231, 203, 109, 63, 27, 1, 252, 4, 267, 245, 259, 109, 270, 195, 349, 176, 32, 266, 195, 193, 244, 266, 109, 12, 244, 101, 244, 268, 176, 109, 261, 195, 350, 176, 176, 32, 264, 173, 259, 245, 254, 32, 78, 32, 271, 195, 266, 42, 351, 32, 264, 195, 248, 74, 263, 231, 63, 109, 203, 27, 245, 253, 74, 262, 231, 203, 109, 63, 27, 245, 254, 32, 136, 267, 158, 5, 244, 349, 109, 65, 244, 252, 109, 259, 176, 176, 63, 32, 268, 195, 57, 244, 264, 109, 269, 195, 262, 231, 203, 109, 63, 27, 1, 252, 4, 267, 245, 259, 109, 270, 195, 349, 176, 32, 272, 195, 244, 268, 42, 271, 231, 63, 109, 203, 27, 176, 82, 273, 244, 141, 176, 32, 10, 244, 265, 109, 272, 109, 269, 195, 262, 231, 203, 109, 63, 27, 1, 252, 4, 267, 245, 259, 176, 32, 264, 173, 259, 245, 254, 32, 265, 173, 259, 245, 256, 32, 78, 32, 274, 195, 260, 245, 258, 245, 257, 74, 76, 244, 349, 109, 258, 176, 32, 10, 244, 250, 74, 274, 109, 271, 176, 32, 3, 32]}, {"code": "def matmul_kernel(\n    a_ptr,\n    as_ptr,\n    b_ptr,\n    bs_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_asm,\n    stride_bk,\n    stride_bn,\n    stride_bsn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    SPLIT_K: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    as_ptrs = as_ptr + offs_am * stride_asm\n    bs_ptrs = bs_ptr + offs_bn * stride_bsn\n    a_scale = tl.load(as_ptrs, mask=offs_am < M, other=0.0)\n    b_scale = tl.load(bs_ptrs, mask=offs_bn < N, other=0.0)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n\n        a = tl.load(\n            a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0\n        )\n        b = tl.load(\n            b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0\n        )\n\n        accumulator += tl.dot(a, b)\n\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n\n    c = (accumulator.to(tl.float32) * a_scale[:, None] * b_scale[None, :]).to(\n        tl.float16\n    )\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 109, 261, 109, 262, 109, 263, 109, 264, 62, 6, 109, 265, 62, 6, 109, 266, 62, 6, 109, 267, 62, 6, 109, 268, 62, 6, 176, 62, 32, -1, 269, 195, 172, 244, 270, 195, 349, 176, 32, 271, 195, 172, 244, 270, 195, 350, 176, 32, 272, 195, 64, 244, 253, 109, 264, 176, 32, 273, 195, 64, 244, 254, 109, 265, 176, 32, 274, 195, 267, 245, 273, 32, 275, 195, 269, 48, 274, 32, 276, 195, 275, 245, 267, 32, 277, 195, 39, 244, 272, 4, 276, 109, 267, 176, 32, 278, 195, 276, 74, 269, 227, 277, 32, 279, 195, 269, 227, 274, 48, 277, 32, 280, 195, 244, 278, 245, 264, 74, 76, 244, 349, 109, 264, 176, 176, 227, 253, 32, 281, 195, 244, 279, 245, 265, 74, 76, 244, 349, 109, 265, 176, 176, 227, 254, 32, 282, 195, 271, 245, 266, 74, 76, 244, 349, 109, 266, 176, 32, 283, 195, 248, 74, 244, 280, 231, 62, 109, 203, 27, 245, 256, 74, 282, 231, 203, 109, 62, 27, 245, 257, 176, 32, 284, 195, 250, 74, 244, 282, 231, 62, 109, 203, 27, 245, 259, 74, 281, 231, 203, 109, 62, 27, 245, 260, 176, 32, 285, 195, 249, 74, 280, 245, 258, 32, 286, 195, 251, 74, 281, 245, 261, 32, 287, 195, 55, 244, 285, 109, 288, 195, 280, 1, 253, 109, 289, 195, 349, 176, 32, 290, 195, 55, 244, 286, 109, 288, 195, 281, 1, 254, 109, 289, 195, 349, 176, 32, 291, 195, 177, 244, 244, 264, 109, 265, 176, 109, 92, 195, 247, 176, 32, 136, 292, 158, 5, 244, 349, 109, 64, 244, 255, 109, 266, 245, 268, 176, 176, 62, 32, 293, 195, 55, 244, 283, 109, 288, 195, 282, 231, 203, 109, 62, 27, 1, 255, 4, 292, 245, 266, 245, 268, 109, 289, 195, 349, 176, 32, 294, 195, 55, 244, 284, 109, 288, 195, 282, 231, 62, 109, 203, 27, 1, 255, 4, 292, 245, 266, 245, 268, 109, 289, 195, 349, 176, 32, 291, 173, 15, 244, 293, 109, 294, 176, 32, 283, 173, 266, 245, 268, 245, 257, 32, 284, 173, 266, 245, 268, 245, 259, 32, 78, 32, 295, 195, 244, 291, 82, 296, 244, 146, 176, 245, 287, 231, 62, 109, 203, 27, 245, 290, 231, 203, 109, 62, 27, 176, 82, 296, 244, 21, 176, 32, 297, 195, 278, 245, 264, 74, 76, 244, 349, 109, 264, 176, 32, 298, 195, 279, 245, 265, 74, 76, 244, 349, 109, 265, 176, 32, 299, 195, 252, 74, 262, 245, 297, 231, 62, 109, 203, 27, 74, 263, 245, 298, 231, 203, 109, 62, 27, 32, 300, 195, 244, 297, 231, 62, 109, 203, 27, 1, 253, 176, 174, 244, 298, 231, 203, 109, 62, 27, 1, 254, 176, 32, 183, 268, 77, 350, 62, 32, 10, 244, 299, 109, 295, 109, 288, 195, 300, 176, 32, 186, 32, 30, 62, 32, 197, 244, 299, 109, 295, 109, 288, 195, 300, 176, 32, 56, 32, 3, 32]}, {"code": "def _softmax(\n    Y,\n    X,\n    M,\n    stride_ym,\n    stride_yn,\n    stride_xm,\n    stride_xn,\n    stride_m,\n    K,\n    LOG: tl.constexpr,\n    MASK_TYPE: tl.constexpr,\n    CAUSAL: tl.constexpr,\n    DEPTH: tl.constexpr,\n    IS_FP16: tl.constexpr,\n):\n\n    m = tl.program_id(0)\n    n = tl.program_id(1)\n    k = tl.arange(0, DEPTH)\n    x_ptrs = X + m * stride_xm + n * stride_xn + k\n    io_mask = k < K\n    if CAUSAL:\n        io_mask = io_mask & (k <= n)\n    x = tl.load(x_ptrs, mask=io_mask, other=float(\"-inf\"))\n    if CAUSAL:\n        off = float(\"-inf\")\n        off = off.to(x.dtype)\n        x = tl.where(k > n, off, x)\n    if MASK_TYPE is not None:\n        if MASK_TYPE == \"qk\":\n            mask_ptrs = M + n * stride_m + k\n        elif MASK_TYPE == \"bk\":\n            mask_ptrs = M + m * stride_m + k\n        add_mask = tl.load(mask_ptrs, io_mask, other=float(\"-inf\"))\n        x += add_mask\n    z = x - tl.max(x, axis=0)\n    if IS_FP16:\n        z = z.to(tl.float32)\n    num = tl.exp(z)\n    denom = tl.sum(num, axis=0)\n    if LOG:\n        y = z - tl.log(denom)\n    else:\n        y = num / denom\n    y_ptrs = Y + m * stride_ym + n * stride_yn + k\n    tl.store(y_ptrs, y, mask=k < K)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 63, 6, 109, 258, 63, 6, 109, 259, 63, 6, 109, 260, 63, 6, 109, 261, 63, 6, 176, 63, 32, -1, 262, 195, 172, 244, 349, 176, 32, 263, 195, 172, 244, 350, 176, 32, 264, 195, 76, 244, 349, 109, 260, 176, 32, 265, 195, 249, 74, 262, 245, 253, 74, 263, 245, 254, 74, 264, 32, 266, 195, 264, 1, 256, 32, 183, 259, 63, 32, 266, 195, 266, 174, 244, 264, 219, 263, 176, 32, 186, 32, 267, 195, 57, 244, 265, 109, 268, 195, 266, 109, 269, 195, 270, 244, 351, 176, 176, 32, 183, 259, 63, 32, 271, 195, 270, 244, 351, 176, 32, 271, 195, 271, 82, 272, 244, 267, 82, 92, 176, 32, 267, 195, 206, 244, 264, 125, 263, 109, 271, 109, 267, 176, 32, 186, 32, 183, 258, 111, 64, 203, 63, 32, 183, 258, 77, 352, 63, 32, 273, 195, 250, 74, 263, 245, 255, 74, 264, 32, 66, 32, 37, 258, 77, 353, 63, 32, 273, 195, 250, 74, 262, 245, 255, 74, 264, 32, 186, 32, 274, 195, 57, 244, 273, 109, 266, 109, 269, 195, 270, 244, 351, 176, 176, 32, 267, 173, 274, 32, 186, 32, 275, 195, 267, 4, 12, 244, 267, 109, 276, 195, 349, 176, 32, 183, 261, 63, 32, 275, 195, 275, 82, 272, 244, 146, 176, 32, 186, 32, 277, 195, 108, 244, 275, 176, 32, 278, 195, 222, 244, 277, 109, 276, 195, 349, 176, 32, 183, 257, 63, 32, 279, 195, 275, 4, 50, 244, 278, 176, 32, 186, 32, 30, 63, 32, 279, 195, 277, 42, 278, 32, 56, 32, 280, 195, 248, 74, 262, 245, 251, 74, 263, 245, 252, 74, 264, 32, 10, 244, 280, 109, 279, 109, 268, 195, 264, 1, 256, 176, 32, 3, 32]}, {"code": "def _softmax_backward(\n    GradIn,\n    GradOut,\n    Out,\n    stride_bm,\n    stride_bn,\n    stride_gm,\n    stride_gn,\n    stride_om,\n    stride_on,\n    K,\n    LOG: tl.constexpr,\n    CAUSAL: tl.constexpr,\n    DEPTH: tl.constexpr,\n    IS_FP16: tl.constexpr,\n):\n\n    m = tl.program_id(0)\n    n = tl.program_id(1)\n    k = tl.arange(0, DEPTH)\n    grad_out_ptrs = GradOut + m * stride_gm + n * stride_gn + k\n    out_ptrs = Out + m * stride_om + n * stride_on + k\n    io_mask = k < K\n    if CAUSAL:\n        io_mask = io_mask & (k <= n)\n    g = tl.load(grad_out_ptrs, mask=io_mask, other=float(0))\n    o = tl.load(out_ptrs, mask=io_mask, other=float(0))\n    if CAUSAL:\n        zero = float(0)\n        zero = zero.to(g.dtype)\n        g = tl.where(k > n, zero, g)\n        o = tl.where(k > n, zero, o)\n    if LOG:\n        s = tl.sum(g, 0)\n        if IS_FP16:\n            o = o.to(tl.float32)\n        grad_in = g - tl.exp(o) * s\n    else:\n        s = tl.sum(g * o, 0)\n        grad_in = o * (g - s)\n    grad_in_ptrs = GradIn + m * stride_bm + n * stride_bn + k\n    tl.store(grad_in_ptrs, grad_in, mask=k < K)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 62, 6, 109, 259, 62, 6, 109, 260, 62, 6, 109, 261, 62, 6, 176, 62, 32, -1, 262, 195, 172, 244, 349, 176, 32, 263, 195, 172, 244, 350, 176, 32, 264, 195, 76, 244, 349, 109, 260, 176, 32, 265, 195, 249, 74, 262, 245, 253, 74, 263, 245, 254, 74, 264, 32, 266, 195, 250, 74, 262, 245, 255, 74, 263, 245, 256, 74, 264, 32, 267, 195, 264, 1, 257, 32, 183, 259, 62, 32, 267, 195, 267, 174, 244, 264, 219, 263, 176, 32, 186, 32, 268, 195, 55, 244, 265, 109, 269, 195, 267, 109, 270, 195, 271, 244, 349, 176, 176, 32, 272, 195, 55, 244, 266, 109, 269, 195, 267, 109, 270, 195, 271, 244, 349, 176, 176, 32, 183, 259, 62, 32, 273, 195, 271, 244, 349, 176, 32, 273, 195, 273, 82, 274, 244, 268, 82, 92, 176, 32, 268, 195, 206, 244, 264, 125, 263, 109, 273, 109, 268, 176, 32, 272, 195, 206, 244, 264, 125, 263, 109, 273, 109, 272, 176, 32, 186, 32, 183, 258, 62, 32, 275, 195, 222, 244, 268, 109, 349, 176, 32, 183, 261, 62, 32, 272, 195, 272, 82, 274, 244, 146, 176, 32, 186, 32, 276, 195, 268, 4, 108, 244, 272, 176, 245, 275, 32, 186, 32, 30, 62, 32, 275, 195, 222, 244, 268, 245, 272, 109, 349, 176, 32, 276, 195, 272, 245, 244, 268, 4, 275, 176, 32, 56, 32, 277, 195, 248, 74, 262, 245, 251, 74, 263, 245, 252, 74, 264, 32, 10, 244, 277, 109, 276, 109, 269, 195, 264, 1, 257, 176, 32, 3, 32]}, {"code": "def _layer_norm_fwd_1pass_kernel(\n    X,\n    Y,\n    W,\n    B,\n    RESIDUAL,\n    X1,\n    W1,\n    B1,\n    Y1,\n    RESIDUAL_OUT,\n    ROWSCALE,\n    SEEDS,\n    DROPOUT_MASK,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_y_row,\n    stride_res_row,\n    stride_res_out_row,\n    stride_x1_row,\n    stride_y1_row,\n    M,\n    N,\n    eps,\n    dropout_p,\n    IS_RMS_NORM: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    STORE_RESIDUAL_OUT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_DROPOUT: tl.constexpr,\n    STORE_DROPOUT_MASK: tl.constexpr,\n    HAS_ROWSCALE: tl.constexpr,\n    HAS_X1: tl.constexpr,\n    HAS_W1: tl.constexpr,\n    HAS_B1: tl.constexpr,\n):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    if HAS_X1:\n        X1 += row * stride_x1_row\n    if HAS_W1:\n        Y1 += row * stride_y1_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_ROWSCALE:\n        rowscale = tl.load(ROWSCALE + row).to(tl.float32)\n        x *= rowscale\n    if HAS_DROPOUT:\n        keep_mask = (\n            tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n        )\n        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)\n        if STORE_DROPOUT_MASK:\n            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)\n    if HAS_X1:\n        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)\n        if HAS_ROWSCALE:\n            rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)\n            x1 *= rowscale\n        if HAS_DROPOUT:\n            keep_mask = (\n                tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)\n                > dropout_p\n            )\n            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)\n            if STORE_DROPOUT_MASK:\n                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask, mask=cols < N)\n        x += x1\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n    if HAS_W1:\n        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)\n        if HAS_B1:\n            b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)\n        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1\n        tl.store(Y1 + cols, y1, mask=mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 109, 261, 109, 262, 109, 263, 109, 264, 109, 265, 109, 266, 109, 267, 109, 268, 109, 269, 109, 270, 109, 271, 109, 272, 109, 273, 63, 6, 109, 274, 63, 6, 109, 275, 63, 6, 109, 276, 63, 6, 109, 277, 63, 6, 109, 278, 63, 6, 109, 279, 63, 6, 109, 280, 63, 6, 109, 281, 63, 6, 109, 282, 63, 6, 109, 283, 63, 6, 176, 63, 32, -1, 284, 195, 172, 244, 349, 176, 32, 248, 173, 284, 245, 263, 32, 249, 173, 284, 245, 264, 32, 183, 275, 63, 32, 252, 173, 284, 245, 265, 32, 186, 32, 183, 276, 63, 32, 257, 173, 284, 245, 266, 32, 186, 32, 183, 281, 63, 32, 253, 173, 284, 245, 267, 32, 186, 32, 183, 282, 63, 32, 256, 173, 284, 245, 268, 32, 186, 32, 285, 195, 76, 244, 349, 109, 274, 176, 32, 286, 195, 57, 244, 248, 74, 285, 109, 287, 195, 285, 1, 270, 109, 288, 195, 349, 176, 82, 289, 244, 146, 176, 32, 183, 280, 63, 32, 290, 195, 57, 244, 258, 74, 284, 176, 82, 289, 244, 146, 176, 32, 286, 24, 290, 32, 186, 32, 183, 278, 63, 32, 291, 195, 69, 244, 57, 244, 259, 74, 284, 176, 82, 289, 244, 166, 176, 109, 285, 109, 292, 195, 350, 176, 125, 272, 32, 286, 195, 206, 244, 291, 109, 286, 42, 244, 351, 4, 272, 176, 109, 349, 176, 32, 183, 279, 63, 32, 10, 244, 260, 74, 284, 245, 270, 74, 285, 109, 291, 109, 287, 195, 285, 1, 270, 176, 32, 186, 32, 186, 32, 183, 281, 63, 32, 293, 195, 57, 244, 253, 74, 285, 109, 287, 195, 285, 1, 270, 109, 288, 195, 349, 176, 82, 289, 244, 146, 176, 32, 183, 280, 63, 32, 290, 195, 57, 244, 258, 74, 269, 74, 284, 176, 82, 289, 244, 146, 176, 32, 293, 24, 290, 32, 186, 32, 183, 278, 63, 32, 291, 195, 69, 244, 57, 244, 259, 74, 269, 74, 284, 176, 82, 289, 244, 166, 176, 109, 285, 109, 292, 195, 350, 176, 125, 272, 32, 293, 195, 206, 244, 291, 109, 293, 42, 244, 351, 4, 272, 176, 109, 349, 176, 32, 183, 279, 63, 32, 10, 244, 260, 74, 244, 269, 74, 284, 176, 245, 270, 74, 285, 109, 291, 109, 287, 195, 285, 1, 270, 176, 32, 186, 32, 186, 32, 286, 173, 293, 32, 186, 32, 183, 275, 63, 32, 294, 195, 57, 244, 252, 74, 285, 109, 287, 195, 285, 1, 270, 109, 288, 195, 349, 176, 82, 289, 244, 146, 176, 32, 286, 173, 294, 32, 186, 32, 183, 276, 63, 32, 10, 244, 257, 74, 285, 109, 286, 109, 287, 195, 285, 1, 270, 176, 32, 186, 32, 183, 64, 273, 63, 32, 295, 195, 222, 244, 286, 109, 296, 195, 349, 176, 42, 270, 32, 10, 244, 261, 74, 284, 109, 295, 176, 32, 297, 195, 206, 244, 285, 1, 270, 109, 286, 4, 295, 109, 349, 176, 32, 298, 195, 222, 244, 297, 245, 297, 109, 296, 195, 349, 176, 42, 270, 32, 186, 32, 30, 63, 32, 297, 195, 206, 244, 285, 1, 270, 109, 286, 109, 349, 176, 32, 298, 195, 222, 244, 297, 245, 297, 109, 296, 195, 349, 176, 42, 270, 32, 56, 32, 299, 195, 351, 42, 131, 244, 298, 74, 271, 176, 32, 10, 244, 262, 74, 284, 109, 299, 176, 32, 287, 195, 285, 1, 270, 32, 300, 195, 57, 244, 250, 74, 285, 109, 287, 195, 287, 176, 82, 289, 244, 146, 176, 32, 183, 277, 63, 32, 301, 195, 57, 244, 251, 74, 285, 109, 287, 195, 287, 176, 82, 289, 244, 146, 176, 32, 186, 32, 302, 195, 244, 286, 4, 295, 176, 245, 299, 183, 64, 273, 30, 286, 245, 299, 32, 303, 195, 302, 245, 300, 74, 301, 183, 277, 30, 302, 245, 300, 32, 10, 244, 249, 74, 285, 109, 303, 109, 287, 195, 287, 176, 32, 183, 282, 63, 32, 304, 195, 57, 244, 254, 74, 285, 109, 287, 195, 287, 176, 82, 289, 244, 146, 176, 32, 183, 283, 63, 32, 305, 195, 57, 244, 255, 74, 285, 109, 287, 195, 287, 176, 82, 289, 244, 146, 176, 32, 186, 32, 306, 195, 302, 245, 304, 74, 305, 183, 283, 30, 302, 245, 304, 32, 10, 244, 256, 74, 285, 109, 306, 109, 287, 195, 287, 176, 32, 186, 32, 3, 32]}, {"code": "def _layer_norm_fwd_1pass_kernel(\n    X,\n    Y,\n    W,\n    B,\n    RESIDUAL,\n    RESIDUAL_OUT,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_y_row,\n    stride_res_row,\n    stride_res_out_row,\n    N,\n    eps,\n    IS_RMS_NORM: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    STORE_RESIDUAL_OUT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 109, 261, 109, 262, 62, 6, 109, 263, 62, 6, 109, 264, 62, 6, 109, 265, 62, 6, 109, 266, 62, 6, 176, 62, 32, -1, 267, 195, 172, 244, 349, 176, 32, 248, 173, 267, 245, 256, 32, 249, 173, 267, 245, 257, 32, 183, 264, 62, 32, 252, 173, 267, 245, 258, 32, 186, 32, 183, 265, 62, 32, 253, 173, 267, 245, 259, 32, 186, 32, 268, 195, 76, 244, 349, 109, 263, 176, 32, 269, 195, 55, 244, 248, 74, 268, 109, 270, 195, 268, 1, 260, 109, 271, 195, 349, 176, 82, 272, 244, 146, 176, 32, 183, 264, 62, 32, 273, 195, 55, 244, 252, 74, 268, 109, 270, 195, 268, 1, 260, 109, 271, 195, 349, 176, 82, 272, 244, 146, 176, 32, 269, 173, 273, 32, 186, 32, 183, 265, 62, 32, 10, 244, 253, 74, 268, 109, 269, 109, 270, 195, 268, 1, 260, 176, 32, 186, 32, 183, 63, 262, 62, 32, 274, 195, 222, 244, 269, 109, 275, 195, 349, 176, 42, 260, 32, 10, 244, 254, 74, 267, 109, 274, 176, 32, 276, 195, 206, 244, 268, 1, 260, 109, 269, 4, 274, 109, 349, 176, 32, 277, 195, 222, 244, 276, 245, 276, 109, 275, 195, 349, 176, 42, 260, 32, 186, 32, 30, 62, 32, 276, 195, 206, 244, 268, 1, 260, 109, 269, 109, 349, 176, 32, 277, 195, 222, 244, 276, 245, 276, 109, 275, 195, 349, 176, 42, 260, 32, 56, 32, 278, 195, 350, 42, 131, 244, 277, 74, 261, 176, 32, 10, 244, 255, 74, 267, 109, 278, 176, 32, 270, 195, 268, 1, 260, 32, 279, 195, 55, 244, 250, 74, 268, 109, 270, 195, 270, 176, 82, 272, 244, 146, 176, 32, 183, 266, 62, 32, 280, 195, 55, 244, 251, 74, 268, 109, 270, 195, 270, 176, 82, 272, 244, 146, 176, 32, 186, 32, 281, 195, 244, 269, 4, 274, 176, 245, 278, 183, 63, 262, 30, 269, 245, 278, 32, 282, 195, 281, 245, 279, 74, 280, 183, 266, 30, 281, 245, 279, 32, 10, 244, 249, 74, 268, 109, 282, 109, 270, 195, 270, 176, 32, 3, 32]}, {"code": "def _layer_norm_bwd_kernel(\n    X,\n    W,\n    B,\n    Y,\n    DY,\n    DX,\n    DW,\n    DB,\n    DRESIDUAL,\n    DRESIDUAL_IN,\n    Mean,\n    Rstd,\n    stride_x_row,\n    stride_y_row,\n    stride_dy_row,\n    stride_dx_row,\n    stride_dres_row,\n    stride_dres_in_row,\n    M,\n    N,\n    eps,\n    rows_per_program,\n    IS_RMS_NORM: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    HAS_DRESIDUAL: tl.constexpr,\n    STORE_DRESIDUAL: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row) if Mean is not None else 0.0\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 109, 261, 109, 262, 109, 263, 109, 264, 109, 265, 109, 266, 109, 267, 109, 268, 109, 269, 109, 270, 63, 6, 109, 271, 63, 6, 109, 272, 63, 6, 109, 273, 63, 6, 109, 274, 63, 6, 109, 275, 63, 6, 176, 63, 32, -1, 276, 195, 172, 244, 349, 176, 32, 277, 195, 276, 245, 269, 32, 278, 195, 76, 244, 349, 109, 271, 176, 32, 279, 195, 278, 1, 267, 32, 248, 173, 277, 245, 260, 32, 183, 272, 63, 32, 256, 173, 277, 245, 264, 32, 186, 32, 183, 273, 63, 32, 257, 173, 277, 245, 265, 32, 186, 32, 252, 173, 277, 245, 262, 32, 253, 173, 277, 245, 263, 32, 183, 275, 63, 32, 251, 173, 277, 245, 261, 32, 186, 32, 280, 195, 57, 244, 249, 74, 278, 109, 279, 195, 279, 176, 82, 281, 244, 146, 176, 32, 183, 275, 103, 274, 63, 32, 282, 195, 57, 244, 250, 74, 278, 109, 279, 195, 279, 109, 283, 195, 349, 176, 82, 281, 244, 146, 176, 32, 186, 32, 284, 195, 177, 244, 244, 271, 109, 176, 109, 92, 195, 146, 176, 32, 183, 274, 63, 32, 285, 195, 177, 244, 244, 271, 109, 176, 109, 92, 195, 146, 176, 32, 186, 32, 286, 195, 39, 244, 244, 276, 74, 350, 176, 245, 269, 109, 266, 176, 32, 136, 287, 158, 5, 244, 277, 109, 286, 176, 63, 32, 288, 195, 57, 244, 248, 74, 278, 109, 279, 195, 279, 109, 283, 195, 349, 176, 82, 281, 244, 146, 176, 32, 289, 195, 57, 244, 252, 74, 278, 109, 279, 195, 279, 109, 283, 195, 349, 176, 82, 281, 244, 146, 176, 32, 183, 64, 270, 63, 32, 290, 195, 57, 244, 258, 74, 287, 176, 183, 258, 111, 64, 203, 30, 349, 32, 186, 32, 291, 195, 57, 244, 259, 74, 287, 176, 32, 292, 195, 244, 288, 4, 290, 176, 245, 291, 183, 64, 270, 30, 288, 245, 291, 32, 292, 195, 206, 244, 279, 109, 292, 109, 349, 176, 32, 183, 275, 63, 32, 293, 195, 292, 245, 280, 74, 282, 183, 274, 30, 292, 245, 280, 32, 10, 244, 251, 74, 278, 109, 293, 109, 279, 195, 279, 176, 32, 186, 32, 294, 195, 280, 245, 289, 32, 284, 173, 289, 245, 292, 32, 183, 274, 63, 32, 285, 173, 289, 32, 186, 32, 183, 64, 270, 63, 32, 295, 195, 222, 244, 292, 245, 294, 109, 296, 195, 349, 176, 42, 267, 32, 297, 195, 222, 244, 294, 109, 296, 195, 349, 176, 42, 267, 32, 298, 195, 244, 294, 4, 244, 292, 245, 295, 74, 297, 176, 176, 245, 291, 32, 186, 32, 30, 63, 32, 295, 195, 222, 244, 292, 245, 294, 109, 296, 195, 349, 176, 42, 267, 32, 298, 195, 244, 294, 4, 292, 245, 295, 176, 245, 291, 32, 56, 32, 183, 272, 63, 32, 299, 195, 57, 244, 256, 74, 278, 109, 279, 195, 279, 109, 283, 195, 349, 176, 82, 281, 244, 146, 176, 32, 298, 173, 299, 32, 186, 32, 183, 273, 63, 32, 10, 244, 257, 74, 278, 109, 298, 109, 279, 195, 279, 176, 32, 186, 32, 10, 244, 253, 74, 278, 109, 298, 109, 279, 195, 279, 176, 32, 248, 173, 260, 32, 183, 272, 63, 32, 256, 173, 264, 32, 186, 32, 183, 273, 63, 32, 257, 173, 265, 32, 186, 32, 183, 275, 63, 32, 251, 173, 261, 32, 186, 32, 252, 173, 262, 32, 253, 173, 263, 32, 78, 32, 10, 244, 254, 74, 276, 245, 267, 74, 278, 109, 284, 109, 279, 195, 279, 176, 32, 183, 274, 63, 32, 10, 244, 255, 74, 276, 245, 267, 74, 278, 109, 285, 109, 279, 195, 279, 176, 32, 186, 32, 3, 32]}, {"code": "def triton_red_fused_native_layer_norm_no_welford(\n    in_out_ptr0,\n    in_out_ptr1,\n    in_ptr0,\n    in_ptr1,\n    in_ptr2,\n    out_ptr0,\n    xnumel,\n    rnumel,\n    XBLOCK: tl.constexpr,\n    RBLOCK: tl.constexpr,\n):\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex\n    _tmp3 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp0 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"\n        ).to(tl.float32)\n        tmp1 = tmp0.to(tl.float32)\n        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n        tmp4 = _tmp3 + tmp2\n        _tmp3 = tmp4\n    tmp3 = tl.sum(_tmp3, 1)[:, None]\n    tmp5 = rnumel\n    tmp6 = tmp3 / tmp5\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + (x0), tmp6, None)\n    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp7 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"\n        ).to(tl.float32)\n        tmp8 = tmp7.to(tl.float32)\n        tmp9 = tmp8 - tmp6\n        tmp10 = tmp9 * tmp9\n        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])\n        tmp13 = _tmp12 + tmp11\n        _tmp12 = tmp13\n    tmp12 = tl.sum(_tmp12, 1)[:, None]\n    tmp14 = rnumel\n    tmp15 = tmp12 / tmp14\n    tmp16 = 1e-05\n    tmp17 = tmp15 + tmp16\n    tmp18 = libdevice.rsqrt(tmp17)\n    tl.debug_barrier()\n    tl.store(in_out_ptr1 + (x0), tmp18, None)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp19 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"\n        ).to(tl.float32)\n        tmp23 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp26 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp20 = tmp19.to(tl.float32)\n        tmp21 = tmp20 - tmp6\n        tmp22 = tmp21 * tmp18\n        tmp24 = tmp23.to(tl.float32)\n        tmp25 = tmp22 * tmp24\n        tmp27 = tmp26.to(tl.float32)\n        tmp28 = tmp25 + tmp27\n        tmp29 = tmp28.to(tl.float32)\n        tl.store(out_ptr0 + (r1 + (rnumel * x0)), tmp29, rmask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 62, 6, 109, 257, 62, 6, 176, 62, 32, -1, 258, 195, 172, 244, 349, 176, 245, 256, 32, 259, 195, 258, 74, 76, 244, 349, 109, 256, 176, 231, 62, 109, 203, 27, 32, 260, 195, 259, 1, 254, 32, 261, 195, 76, 244, 349, 109, 257, 176, 231, 203, 109, 62, 27, 32, 262, 195, 259, 32, 263, 195, 238, 244, 231, 256, 109, 257, 27, 109, 349, 109, 146, 176, 32, 136, 264, 158, 5, 244, 349, 109, 255, 109, 257, 176, 62, 32, 265, 195, 264, 74, 261, 32, 266, 195, 265, 1, 255, 32, 267, 195, 265, 32, 268, 195, 55, 244, 250, 74, 244, 267, 74, 255, 245, 262, 176, 109, 266, 109, 269, 195, 350, 176, 82, 270, 244, 146, 176, 32, 271, 195, 268, 82, 270, 244, 146, 176, 32, 272, 195, 194, 244, 271, 109, 231, 256, 109, 257, 27, 176, 32, 273, 195, 263, 74, 272, 32, 263, 195, 273, 32, 78, 32, 274, 195, 222, 244, 263, 109, 351, 176, 231, 62, 109, 203, 27, 32, 275, 195, 255, 32, 276, 195, 274, 42, 275, 32, 51, 244, 176, 32, 10, 244, 248, 74, 262, 109, 276, 109, 203, 176, 32, 277, 195, 238, 244, 231, 256, 109, 257, 27, 109, 349, 109, 146, 176, 32, 136, 264, 158, 5, 244, 349, 109, 255, 109, 257, 176, 62, 32, 265, 195, 264, 74, 261, 32, 266, 195, 265, 1, 255, 32, 267, 195, 265, 32, 278, 195, 55, 244, 250, 74, 244, 267, 74, 255, 245, 262, 176, 109, 266, 109, 269, 195, 350, 176, 82, 270, 244, 146, 176, 32, 279, 195, 278, 82, 270, 244, 146, 176, 32, 280, 195, 279, 4, 276, 32, 281, 195, 280, 245, 280, 32, 282, 195, 194, 244, 281, 109, 231, 256, 109, 257, 27, 176, 32, 283, 195, 277, 74, 282, 32, 277, 195, 283, 32, 78, 32, 284, 195, 222, 244, 277, 109, 351, 176, 231, 62, 109, 203, 27, 32, 285, 195, 255, 32, 286, 195, 284, 42, 285, 32, 287, 195, 352, 32, 288, 195, 286, 74, 287, 32, 289, 195, 16, 82, 290, 244, 288, 176, 32, 51, 244, 176, 32, 10, 244, 249, 74, 262, 109, 289, 109, 203, 176, 32, 136, 264, 158, 5, 244, 349, 109, 255, 109, 257, 176, 62, 32, 265, 195, 264, 74, 261, 32, 266, 195, 265, 1, 255, 32, 267, 195, 265, 32, 291, 195, 55, 244, 250, 74, 244, 267, 74, 255, 245, 262, 176, 109, 266, 109, 269, 195, 353, 176, 82, 270, 244, 146, 176, 32, 292, 195, 55, 244, 251, 74, 267, 109, 266, 109, 269, 195, 350, 176, 82, 270, 244, 146, 176, 32, 293, 195, 55, 244, 252, 74, 267, 109, 266, 109, 269, 195, 350, 176, 82, 270, 244, 146, 176, 32, 294, 195, 291, 82, 270, 244, 146, 176, 32, 295, 195, 294, 4, 276, 32, 296, 195, 295, 245, 289, 32, 297, 195, 292, 82, 270, 244, 146, 176, 32, 298, 195, 296, 245, 297, 32, 299, 195, 293, 82, 270, 244, 146, 176, 32, 300, 195, 298, 74, 299, 32, 301, 195, 300, 82, 270, 244, 146, 176, 32, 10, 244, 253, 74, 244, 267, 74, 255, 245, 262, 176, 109, 301, 109, 266, 176, 32, 78, 32, 3, 32]}, {"code": "def log_softmax_kernel(\n    output_ptr,\n    input_ptr,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offset = tl.arange(0, BLOCK_N)\n    offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n    mask = m_offset[:, None] < M and n_offset[None, :] < N\n    input_ptrs = input_ptr + offset\n    inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(tl.float32)\n    row_minus_max = inp - tl.max(inp, axis=1)[:, None]\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=1)[:, None]\n    softmax_output = tl.log(numerator / denominator)\n    output_ptrs = output_ptr + offset\n    tl.store(output_ptrs, softmax_output, mask=mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 63, 6, 109, 254, 63, 6, 176, 63, 32, -1, 255, 195, 172, 244, 349, 176, 32, 256, 195, 172, 244, 350, 176, 32, 257, 195, 255, 245, 253, 74, 76, 244, 349, 109, 253, 176, 32, 258, 195, 76, 244, 349, 109, 254, 176, 32, 259, 195, 257, 231, 63, 109, 203, 27, 245, 251, 245, 252, 74, 258, 231, 203, 109, 63, 27, 245, 252, 74, 256, 32, 260, 195, 257, 231, 63, 109, 203, 27, 1, 250, 103, 258, 231, 203, 109, 63, 27, 1, 251, 32, 261, 195, 249, 74, 259, 32, 262, 195, 57, 244, 261, 109, 260, 195, 260, 109, 263, 195, 4, 264, 244, 351, 176, 176, 82, 265, 244, 146, 176, 32, 266, 195, 262, 4, 12, 244, 262, 109, 267, 195, 350, 176, 231, 63, 109, 203, 27, 32, 268, 195, 108, 244, 266, 176, 32, 269, 195, 222, 244, 268, 109, 267, 195, 350, 176, 231, 63, 109, 203, 27, 32, 270, 195, 50, 244, 268, 42, 269, 176, 32, 271, 195, 248, 74, 259, 32, 10, 244, 271, 109, 270, 109, 260, 195, 260, 176, 32, 3, 32]}, {"code": "def log_softmax_backward_kernel(\n    out_ptr,\n    out_grad_ptr,\n    in_grad_ptr,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offset = tl.arange(0, BLOCK_N)\n\n    offsets = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n    mask = m_offset[:, None] < M and n_offset[None, :] < N\n    out_ptrs = out_ptr + offsets\n    out = tl.load(out_ptrs, mask=mask).to(tl.float32)\n    out_grad_ptrs = out_grad_ptr + offsets\n    out_grad = tl.load(out_grad_ptrs, mask=mask).to(tl.float32)\n\n    scale = tl.sum(out_grad, 1)\n    in_grad = out_grad - tl.exp(out.to(tl.float32)) * scale[:, None]\n\n    in_grad_ptrs = in_grad_ptr + offsets\n    tl.store(in_grad_ptrs, in_grad, mask=mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 62, 6, 109, 255, 62, 6, 176, 62, 32, -1, 256, 195, 172, 244, 349, 176, 32, 257, 195, 172, 244, 350, 176, 32, 258, 195, 256, 245, 254, 74, 76, 244, 349, 109, 254, 176, 32, 259, 195, 76, 244, 349, 109, 255, 176, 32, 260, 195, 258, 231, 62, 109, 203, 27, 245, 252, 245, 253, 74, 259, 231, 203, 109, 62, 27, 245, 253, 74, 257, 32, 261, 195, 258, 231, 62, 109, 203, 27, 1, 251, 103, 259, 231, 203, 109, 62, 27, 1, 252, 32, 262, 195, 248, 74, 260, 32, 14, 195, 55, 244, 262, 109, 261, 195, 261, 176, 82, 263, 244, 146, 176, 32, 264, 195, 249, 74, 260, 32, 265, 195, 55, 244, 264, 109, 261, 195, 261, 176, 82, 263, 244, 146, 176, 32, 266, 195, 222, 244, 265, 109, 350, 176, 32, 267, 195, 265, 4, 108, 244, 14, 82, 263, 244, 146, 176, 176, 245, 266, 231, 62, 109, 203, 27, 32, 268, 195, 250, 74, 260, 32, 10, 244, 268, 109, 267, 109, 261, 195, 261, 176, 32, 3, 32]}, {"code": "def logsumexp_fwd_kernel(\n    x, z, scale, D: tl.constexpr, B: tl.constexpr, HAS_SCALE: tl.constexpr\n):\n    i_n, i_d = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64)\n    o_d = i_d * B + tl.arange(0, B)\n    m_d = o_d < D\n\n    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float(\"inf\"))\n    if HAS_SCALE:\n        b_x = b_x * scale\n    b_m = tl.max(b_x, 0)\n    b_z = tl.log(tl.sum(tl.exp(b_x - b_m), 0)) + b_m\n    tl.store(z + i_n * tl.cdiv(D, B) + i_d, b_z)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 63, 6, 109, 252, 63, 6, 109, 253, 63, 6, 176, 63, 32, -1, 254, 109, 255, 195, 244, 172, 244, 349, 176, 82, 256, 244, 182, 176, 109, 172, 244, 350, 176, 82, 256, 244, 182, 176, 176, 32, 257, 195, 255, 245, 252, 74, 76, 244, 349, 109, 252, 176, 32, 258, 195, 257, 1, 251, 32, 259, 195, 57, 244, 248, 74, 254, 245, 251, 74, 257, 109, 260, 195, 258, 109, 261, 195, 4, 262, 244, 351, 176, 176, 32, 183, 253, 63, 32, 259, 195, 259, 245, 250, 32, 186, 32, 263, 195, 12, 244, 259, 109, 349, 176, 32, 264, 195, 50, 244, 222, 244, 108, 244, 259, 4, 263, 176, 109, 349, 176, 176, 74, 263, 32, 10, 244, 249, 74, 254, 245, 65, 244, 251, 109, 252, 176, 74, 255, 109, 264, 176, 32, 3, 32]}, {"code": "def masked_select_kernel(\n    inp_ptr,\n    select_mask_ptr,\n    prefix_sum_ptr,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    inp = tl.load(inp_ptr + offsets, mask=mask, other=0.0)\n    select_mask = tl.load(select_mask_ptr + offsets, mask=mask, other=0.0).to(tl.int1)\n    out_offset = tl.load(prefix_sum_ptr + offsets, mask=mask, other=0.0) - 1\n\n    tl.store(out_ptr + out_offset, inp, mask=(select_mask and mask))", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 62, 6, 176, 62, 32, -1, 254, 195, 172, 244, 255, 195, 349, 176, 32, 256, 195, 254, 245, 253, 74, 76, 244, 349, 109, 253, 176, 32, 257, 195, 256, 1, 252, 32, 258, 195, 55, 244, 248, 74, 256, 109, 257, 195, 257, 109, 259, 195, 349, 176, 32, 260, 195, 55, 244, 249, 74, 256, 109, 257, 195, 257, 109, 259, 195, 349, 176, 82, 261, 244, 121, 176, 32, 262, 195, 55, 244, 250, 74, 256, 109, 257, 195, 257, 109, 259, 195, 349, 176, 4, 350, 32, 10, 244, 251, 74, 262, 109, 258, 109, 257, 195, 260, 103, 257, 176, 32, 3, 32]}, {"code": "def matmul4_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    scales_ptr,\n    zeros_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    stride_scales_g,\n    stride_scales_n,\n    stride_zeros_g,\n    stride_zeros_n,\n    groupsize,\n    NO_GROUPS: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    a_mask = offs_am[:, None] < M\n\n    b_ptrs = b_ptr + (\n        (offs_k[:, None] // infearure_per_bits) * stride_bk\n        + offs_bn[None, :] * stride_bn\n    )\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n\n\n    zeros_ptrs = zeros_ptr + ((offs_bn // infearure_per_bits) * stride_zeros_n)\n\n    shifter = (offs_k % infearure_per_bits) * bits\n    zeros_shifter = (offs_bn % infearure_per_bits) * bits\n\n    if NO_GROUPS:\n\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n\n        zeros = (zeros >> zeros_shifter) & 0xF\n\n        zeros = zeros * scales\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)\n            ptr = zeros_ptrs + g_id * stride_zeros_g\n            zeros = tl.load(ptr)\n\n            zeros = (zeros >> zeros_shifter) & 0xF\n            zeros = (zeros) * scales\n\n        b = (b >> shifter[:, None]) & 0xF\n        b = b * scales[None, :] - zeros[None, :]\n\n        accumulator += tl.dot(a, b.to(a.dtype))\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk\n    c = accumulator.to(c_ptr.dtype.element_ty)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 109, 261, 109, 262, 109, 263, 109, 264, 109, 265, 109, 266, 109, 267, 63, 6, 109, 268, 63, 6, 109, 269, 63, 6, 109, 270, 63, 6, 109, 271, 63, 6, 176, 63, 32, -1, 272, 195, 349, 32, 273, 195, 350, 32, 274, 195, 172, 244, 275, 195, 351, 176, 32, 276, 195, 65, 244, 253, 109, 268, 176, 32, 277, 195, 65, 244, 254, 109, 269, 176, 32, 278, 195, 65, 244, 255, 109, 270, 176, 32, 279, 195, 271, 245, 277, 32, 280, 195, 274, 48, 279, 32, 281, 195, 280, 245, 271, 32, 282, 195, 39, 244, 276, 4, 281, 109, 271, 176, 32, 283, 195, 281, 74, 274, 227, 282, 32, 284, 195, 274, 227, 279, 48, 282, 32, 285, 195, 283, 245, 268, 74, 76, 244, 351, 109, 268, 176, 32, 286, 195, 284, 245, 269, 74, 76, 244, 351, 109, 269, 176, 32, 287, 195, 76, 244, 351, 109, 270, 176, 32, 288, 195, 248, 74, 244, 285, 231, 63, 109, 203, 27, 245, 256, 74, 287, 231, 203, 109, 63, 27, 245, 257, 176, 32, 289, 195, 285, 231, 63, 109, 203, 27, 1, 253, 32, 290, 195, 249, 74, 244, 287, 231, 63, 109, 203, 27, 48, 273, 245, 258, 74, 286, 231, 203, 109, 63, 27, 245, 259, 176, 32, 291, 195, 251, 74, 286, 245, 263, 32, 292, 195, 252, 74, 286, 48, 273, 245, 265, 32, 293, 195, 287, 227, 273, 245, 272, 32, 294, 195, 286, 227, 273, 245, 272, 32, 183, 267, 63, 32, 295, 195, 57, 244, 291, 176, 32, 296, 195, 57, 244, 292, 176, 32, 296, 195, 296, 128, 294, 174, 352, 353, 32, 296, 195, 296, 245, 295, 32, 186, 32, 297, 195, 177, 244, 244, 268, 109, 269, 176, 109, 92, 195, 146, 176, 32, 136, 298, 158, 5, 244, 351, 109, 278, 176, 63, 32, 299, 195, 57, 244, 288, 109, 300, 195, 289, 109, 301, 195, 351, 176, 32, 302, 195, 57, 244, 290, 176, 32, 183, 64, 267, 63, 32, 303, 195, 298, 48, 244, 266, 48, 270, 176, 32, 304, 195, 291, 74, 303, 245, 262, 32, 295, 195, 57, 244, 304, 176, 32, 304, 195, 292, 74, 303, 245, 264, 32, 296, 195, 57, 244, 304, 176, 32, 296, 195, 296, 128, 294, 174, 352, 353, 32, 296, 195, 296, 245, 295, 32, 186, 32, 302, 195, 302, 128, 293, 231, 63, 109, 203, 27, 174, 352, 353, 32, 302, 195, 302, 245, 295, 231, 203, 109, 63, 27, 4, 296, 231, 203, 109, 63, 27, 32, 297, 173, 15, 244, 299, 109, 302, 82, 305, 244, 299, 82, 92, 176, 176, 32, 288, 173, 270, 245, 257, 32, 290, 173, 270, 48, 273, 245, 258, 32, 78, 32, 306, 195, 297, 82, 305, 244, 250, 82, 92, 82, 115, 176, 32, 307, 195, 283, 245, 268, 74, 76, 244, 351, 109, 268, 176, 32, 308, 195, 284, 245, 269, 74, 76, 244, 351, 109, 269, 176, 32, 309, 195, 250, 74, 260, 245, 307, 231, 63, 109, 203, 27, 74, 261, 245, 308, 231, 203, 109, 63, 27, 32, 310, 195, 244, 307, 231, 63, 109, 203, 27, 1, 253, 176, 174, 244, 308, 231, 203, 109, 63, 27, 1, 254, 176, 32, 10, 244, 309, 109, 297, 109, 300, 195, 310, 176, 32, 3, 32]}, {"code": "def dequantize_kernel(\n    b_ptr,\n    b_scale_ptr,\n    b_zp_ptr,\n    fpb_ptr,\n    K,\n    N,\n    group_size,\n    stride_bk,\n    stride_bn,\n    stride_bsk,\n    stride_bsn,\n    stride_bzpk,\n    stride_bzpn,\n    stride_fpbk,\n    stride_fpbn,\n    BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = k_block_idx * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = n_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    fpb_offs = offs_k[:, None] * stride_fpbk + offs_n[None, :] * stride_fpbn\n    b_offs = (offs_k[:, None] // 8) * stride_bk + offs_n[None, :] * stride_bn\n    bzp_offs = (offs_k[:, None] // group_size) * stride_bzpk + (\n        offs_n[None, :] // 8\n    ) * stride_bzpn\n    bs_offs = (offs_k[:, None] // group_size) * stride_bsk + offs_n[\n        None, :\n    ] * stride_bsn\n    n_mask = offs_n[None, :] < N\n    k_mask = offs_k[:, None] < K\n    mask = n_mask & k_mask\n    int32_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    zp_b = tl.load(b_zp_ptr + bzp_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=mask, other=0.0)\n    b_shift = (offs_k[:, None] % 8) * 4\n    bzp_shift = (offs_n[None, :] % 8) * 4\n    fp_weight = (((int32_b >> b_shift) & 0xF) - ((zp_b >> bzp_shift) & 0xF)) * scale_b\n    tl.store(fpb_ptr + fpb_offs, fp_weight, mask=mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 109, 261, 109, 262, 109, 263, 62, 6, 109, 264, 62, 6, 176, 62, 32, -1, 265, 195, 172, 244, 266, 195, 349, 176, 32, 267, 195, 172, 244, 266, 195, 350, 176, 32, 268, 195, 265, 245, 263, 74, 76, 244, 349, 109, 263, 176, 32, 269, 195, 267, 245, 264, 74, 76, 244, 349, 109, 264, 176, 32, 270, 195, 268, 231, 62, 109, 203, 27, 245, 261, 74, 269, 231, 203, 109, 62, 27, 245, 262, 32, 271, 195, 268, 231, 62, 109, 203, 27, 48, 351, 245, 255, 74, 269, 231, 203, 109, 62, 27, 245, 256, 32, 272, 195, 268, 231, 62, 109, 203, 27, 48, 254, 245, 259, 74, 269, 231, 203, 109, 62, 27, 48, 351, 245, 260, 32, 273, 195, 268, 231, 62, 109, 203, 27, 48, 254, 245, 257, 74, 269, 231, 203, 109, 62, 27, 245, 258, 32, 274, 195, 269, 231, 203, 109, 62, 27, 1, 253, 32, 275, 195, 268, 231, 62, 109, 203, 27, 1, 252, 32, 276, 195, 274, 174, 275, 32, 277, 195, 55, 244, 248, 74, 271, 109, 276, 195, 276, 109, 278, 195, 349, 176, 32, 279, 195, 55, 244, 250, 74, 272, 109, 276, 195, 276, 109, 278, 195, 349, 176, 32, 280, 195, 55, 244, 249, 74, 273, 109, 276, 195, 276, 109, 278, 195, 349, 176, 32, 281, 195, 268, 231, 62, 109, 203, 27, 227, 351, 245, 352, 32, 282, 195, 269, 231, 203, 109, 62, 27, 227, 351, 245, 352, 32, 283, 195, 244, 244, 277, 128, 281, 174, 350, 353, 176, 4, 244, 279, 128, 282, 174, 350, 353, 176, 176, 245, 280, 32, 10, 244, 251, 74, 270, 109, 283, 109, 276, 195, 276, 176, 32, 3, 32]}, {"code": "def matmul4_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    scales_ptr,\n    zeros_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    stride_scales_g,\n    stride_scales_n,\n    stride_zeros_g,\n    stride_zeros_n,\n    groupsize,\n    NO_GROUPS: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (\n        (offs_k[:, None] // infearure_per_bits) * stride_bk\n        + offs_bn[None, :] * stride_bn\n    )\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n\n    zeros_ptrs = zeros_ptr + ((offs_bn // infearure_per_bits) * stride_zeros_n)\n    shifter = (offs_k % infearure_per_bits) * bits\n    zeros_shifter = (offs_bn % infearure_per_bits) * bits\n    if NO_GROUPS:\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = (zeros >> zeros_shifter) & 0xF\n        zeros = zeros * scales\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)\n            ptr = zeros_ptrs + g_id * stride_zeros_g\n            zeros = tl.load(ptr)\n            zeros = (zeros >> zeros_shifter) & 0xF\n            zeros = (zeros) * scales\n        b = (b >> shifter[:, None]) & 0xF\n        b = b * scales[None, :] - zeros[None, :]\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk\n    c = accumulator.to(tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 109, 261, 109, 262, 109, 263, 109, 264, 109, 265, 109, 266, 109, 267, 63, 6, 109, 268, 63, 6, 109, 269, 63, 6, 109, 270, 63, 6, 109, 271, 63, 6, 176, 63, 32, -1, 272, 195, 349, 32, 273, 195, 350, 32, 274, 195, 172, 244, 275, 195, 351, 176, 32, 276, 195, 65, 244, 253, 109, 268, 176, 32, 277, 195, 65, 244, 254, 109, 269, 176, 32, 278, 195, 65, 244, 255, 109, 270, 176, 32, 279, 195, 271, 245, 277, 32, 280, 195, 274, 48, 279, 32, 281, 195, 280, 245, 271, 32, 282, 195, 39, 244, 276, 4, 281, 109, 271, 176, 32, 283, 195, 281, 74, 274, 227, 282, 32, 284, 195, 274, 227, 279, 48, 282, 32, 285, 195, 283, 245, 268, 74, 76, 244, 351, 109, 268, 176, 32, 286, 195, 284, 245, 269, 74, 76, 244, 351, 109, 269, 176, 32, 287, 195, 76, 244, 351, 109, 270, 176, 32, 288, 195, 248, 74, 244, 285, 231, 63, 109, 203, 27, 245, 256, 74, 287, 231, 203, 109, 63, 27, 245, 257, 176, 32, 289, 195, 285, 231, 63, 109, 203, 27, 1, 253, 32, 290, 195, 249, 74, 244, 287, 231, 63, 109, 203, 27, 48, 273, 245, 258, 74, 286, 231, 203, 109, 63, 27, 245, 259, 176, 32, 291, 195, 251, 74, 286, 245, 263, 32, 292, 195, 252, 74, 286, 48, 273, 245, 265, 32, 293, 195, 287, 227, 273, 245, 272, 32, 294, 195, 286, 227, 273, 245, 272, 32, 183, 267, 63, 32, 295, 195, 57, 244, 291, 176, 32, 296, 195, 57, 244, 292, 176, 32, 296, 195, 296, 128, 294, 174, 352, 353, 32, 296, 195, 296, 245, 295, 32, 186, 32, 297, 195, 177, 244, 244, 268, 109, 269, 176, 109, 92, 195, 146, 176, 32, 136, 298, 158, 5, 244, 351, 109, 278, 176, 63, 32, 299, 195, 57, 244, 288, 109, 300, 195, 289, 109, 301, 195, 351, 176, 32, 302, 195, 57, 244, 290, 176, 32, 183, 64, 267, 63, 32, 303, 195, 298, 48, 244, 266, 48, 270, 176, 32, 304, 195, 291, 74, 303, 245, 262, 32, 295, 195, 57, 244, 304, 176, 32, 304, 195, 292, 74, 303, 245, 264, 32, 296, 195, 57, 244, 304, 176, 32, 296, 195, 296, 128, 294, 174, 352, 353, 32, 296, 195, 296, 245, 295, 32, 186, 32, 302, 195, 302, 128, 293, 231, 63, 109, 203, 27, 174, 352, 353, 32, 302, 195, 302, 245, 295, 231, 203, 109, 63, 27, 4, 296, 231, 203, 109, 63, 27, 32, 297, 173, 15, 244, 299, 109, 302, 176, 32, 288, 173, 270, 245, 257, 32, 290, 173, 270, 48, 273, 245, 258, 32, 78, 32, 305, 195, 297, 82, 306, 244, 21, 176, 32, 307, 195, 283, 245, 268, 74, 76, 244, 351, 109, 268, 176, 32, 308, 195, 284, 245, 269, 74, 76, 244, 351, 109, 269, 176, 32, 309, 195, 250, 74, 260, 245, 307, 231, 63, 109, 203, 27, 74, 261, 245, 308, 231, 203, 109, 63, 27, 32, 310, 195, 244, 307, 231, 63, 109, 203, 27, 1, 253, 176, 174, 244, 308, 231, 203, 109, 63, 27, 1, 254, 176, 32, 10, 244, 309, 109, 297, 109, 300, 195, 310, 176, 32, 3, 32]}, {"code": "def matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    bs_ptr,\n    bzp_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    stride_bsk,\n    stride_bsn,\n    stride_bzpk,\n    stride_bzpn,\n    group_size,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    SPLIT_K: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + (offs_k[:, None] // 8) * stride_bk + offs_bn[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        bs_ptrs = (\n            bs_ptr\n            + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size)\n            * stride_bsk\n            + offs_bn[None, :] * stride_bsn\n        )\n        bzp_ptrs = (\n            bzp_ptr\n            + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size)\n            * stride_bzpk\n            + (offs_bn[None, :] // 8) * stride_bzpn\n        )\n        b_shift_bits = (offs_k[:, None] % 8) * 4\n        bzp_shift_bits = (offs_bn[None, :] % 8) * 4\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        bs = tl.load(bs_ptrs)\n        bzp = tl.load(bzp_ptrs)\n\n        int_b = (b >> b_shift_bits) & 0xF\n        int_bzp = (bzp >> bzp_shift_bits) & 0xF\n        b = ((int_b - int_bzp) * bs).to(tl.float16)\n        accumulator += tl.dot(a.to(tl.float16), b.to(tl.float16))\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk // 8\n\n    c = accumulator.to(tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 109, 261, 109, 262, 109, 263, 109, 264, 109, 265, 109, 266, 109, 267, 62, 6, 109, 268, 62, 6, 109, 269, 62, 6, 109, 270, 62, 6, 109, 271, 62, 6, 176, 62, 32, -1, 272, 195, 172, 244, 273, 195, 349, 176, 32, 274, 195, 172, 244, 273, 195, 350, 176, 32, 275, 195, 64, 244, 253, 109, 267, 176, 32, 276, 195, 64, 244, 254, 109, 268, 176, 32, 277, 195, 64, 244, 255, 109, 269, 176, 32, 278, 195, 270, 245, 276, 32, 279, 195, 272, 48, 278, 32, 280, 195, 279, 245, 270, 32, 281, 195, 39, 244, 275, 4, 280, 109, 270, 176, 32, 282, 195, 280, 74, 272, 227, 281, 32, 283, 195, 272, 227, 278, 48, 281, 32, 284, 195, 244, 282, 245, 267, 74, 76, 244, 349, 109, 267, 176, 176, 227, 253, 32, 285, 195, 244, 283, 245, 268, 74, 76, 244, 349, 109, 268, 176, 176, 227, 254, 32, 286, 195, 274, 245, 269, 74, 76, 244, 349, 109, 269, 176, 32, 287, 195, 248, 74, 284, 231, 62, 109, 203, 27, 245, 256, 74, 286, 231, 203, 109, 62, 27, 245, 257, 32, 288, 195, 249, 74, 286, 231, 62, 109, 203, 27, 48, 351, 245, 258, 74, 285, 231, 203, 109, 62, 27, 245, 259, 32, 289, 195, 177, 244, 244, 267, 109, 268, 176, 109, 92, 195, 146, 176, 32, 136, 290, 158, 5, 244, 349, 109, 64, 244, 255, 109, 269, 245, 271, 176, 176, 62, 32, 291, 195, 251, 74, 244, 286, 231, 62, 109, 203, 27, 74, 290, 245, 269, 245, 271, 176, 48, 266, 245, 262, 74, 285, 231, 203, 109, 62, 27, 245, 263, 32, 292, 195, 252, 74, 244, 286, 231, 62, 109, 203, 27, 74, 290, 245, 269, 245, 271, 176, 48, 266, 245, 264, 74, 285, 231, 203, 109, 62, 27, 48, 351, 245, 265, 32, 293, 195, 286, 231, 62, 109, 203, 27, 227, 351, 245, 352, 32, 294, 195, 285, 231, 203, 109, 62, 27, 227, 351, 245, 352, 32, 295, 195, 55, 244, 287, 176, 32, 296, 195, 55, 244, 288, 176, 32, 297, 195, 55, 244, 291, 176, 32, 298, 195, 55, 244, 292, 176, 32, 299, 195, 296, 128, 293, 174, 350, 353, 32, 300, 195, 298, 128, 294, 174, 350, 353, 32, 296, 195, 244, 244, 299, 4, 300, 176, 245, 297, 176, 82, 301, 244, 21, 176, 32, 289, 173, 15, 244, 295, 82, 301, 244, 21, 176, 109, 296, 82, 301, 244, 21, 176, 176, 32, 287, 173, 269, 245, 271, 245, 257, 32, 288, 173, 269, 245, 271, 245, 258, 48, 351, 32, 78, 32, 302, 195, 289, 82, 301, 244, 21, 176, 32, 303, 195, 282, 245, 267, 74, 76, 244, 349, 109, 267, 176, 32, 304, 195, 283, 245, 268, 74, 76, 244, 349, 109, 268, 176, 32, 305, 195, 250, 74, 260, 245, 303, 231, 62, 109, 203, 27, 74, 261, 245, 304, 231, 203, 109, 62, 27, 32, 306, 195, 244, 303, 231, 62, 109, 203, 27, 1, 253, 176, 174, 244, 304, 231, 203, 109, 62, 27, 1, 254, 176, 32, 183, 271, 77, 350, 62, 32, 10, 244, 305, 109, 302, 109, 307, 195, 306, 176, 32, 186, 32, 30, 62, 32, 197, 244, 305, 109, 302, 109, 307, 195, 306, 176, 32, 56, 32, 3, 32]}, {"code": "def dequantize_kernel(\n    b_ptr,\n    b_scale_ptr,\n    b_zp_ptr,\n    fpb_ptr,\n    K,\n    N,\n    group_size,\n    stride_bk,\n    stride_bn,\n    stride_bsk,\n    stride_bsn,\n    stride_bzpk,\n    stride_bzpn,\n    stride_fpbk,\n    stride_fpbn,\n    BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n):\n\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = k_block_idx * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = n_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    fpb_offs = offs_k[:, None] * stride_fpbk + offs_n[None, :] * stride_fpbn\n    b_offs = (offs_k[:, None] // 8) * stride_bk + offs_n[None, :] * stride_bn\n    bzp_offs = (offs_k[:, None] // group_size) * stride_bzpk + (\n        offs_n[None, :] // 8\n    ) * stride_bzpn\n    bs_offs = (offs_k[:, None] // group_size) * stride_bsk + offs_n[\n        None, :\n    ] * stride_bsn\n    n_mask = offs_n[None, :] < N\n    k_mask = offs_k[:, None] < K\n    mask = n_mask & k_mask\n    int32_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    zp_b = tl.load(b_zp_ptr + bzp_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=mask, other=0.0)\n    b_shift = (offs_k[:, None] % 8) * 4\n    bzp_shift = (offs_n[None, :] % 8) * 4\n    fp_weight = (((int32_b >> b_shift) & 0xF) - ((zp_b >> bzp_shift) & 0xF)) * scale_b\n    tl.store(fpb_ptr + fpb_offs, fp_weight, mask=mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 109, 261, 109, 262, 109, 263, 63, 6, 109, 264, 63, 6, 176, 63, 32, -1, 265, 195, 172, 244, 266, 195, 349, 176, 32, 267, 195, 172, 244, 266, 195, 350, 176, 32, 268, 195, 265, 245, 263, 74, 76, 244, 349, 109, 263, 176, 32, 269, 195, 267, 245, 264, 74, 76, 244, 349, 109, 264, 176, 32, 270, 195, 268, 231, 63, 109, 203, 27, 245, 261, 74, 269, 231, 203, 109, 63, 27, 245, 262, 32, 271, 195, 268, 231, 63, 109, 203, 27, 48, 351, 245, 255, 74, 269, 231, 203, 109, 63, 27, 245, 256, 32, 272, 195, 268, 231, 63, 109, 203, 27, 48, 254, 245, 259, 74, 269, 231, 203, 109, 63, 27, 48, 351, 245, 260, 32, 273, 195, 268, 231, 63, 109, 203, 27, 48, 254, 245, 257, 74, 269, 231, 203, 109, 63, 27, 245, 258, 32, 274, 195, 269, 231, 203, 109, 63, 27, 1, 253, 32, 275, 195, 268, 231, 63, 109, 203, 27, 1, 252, 32, 276, 195, 274, 174, 275, 32, 277, 195, 57, 244, 248, 74, 271, 109, 276, 195, 276, 109, 278, 195, 349, 176, 32, 279, 195, 57, 244, 250, 74, 272, 109, 276, 195, 276, 109, 278, 195, 349, 176, 32, 280, 195, 57, 244, 249, 74, 273, 109, 276, 195, 276, 109, 278, 195, 349, 176, 32, 281, 195, 268, 231, 63, 109, 203, 27, 227, 351, 245, 352, 32, 282, 195, 269, 231, 203, 109, 63, 27, 227, 351, 245, 352, 32, 283, 195, 244, 244, 277, 128, 281, 174, 350, 353, 176, 4, 244, 279, 128, 282, 174, 350, 353, 176, 176, 245, 280, 32, 10, 244, 251, 74, 270, 109, 283, 109, 276, 195, 276, 176, 32, 3, 32]}, {"code": "def matmul4_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    scales_ptr,\n    zeros_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    stride_scales_g,\n    stride_scales_n,\n    stride_zeros_g,\n    stride_zeros_n,\n    groupsize,\n    NO_GROUPS: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    a_mask = offs_am[:, None] < M\n\n    b_ptrs = b_ptr + (\n        (offs_k[:, None] // infearure_per_bits) * stride_bk\n        + offs_bn[None, :] * stride_bn\n    )\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n\n\n    zeros_ptrs = zeros_ptr + ((offs_bn // infearure_per_bits) * stride_zeros_n)\n\n    shifter = (offs_k % infearure_per_bits) * bits\n    zeros_shifter = (offs_bn % infearure_per_bits) * bits\n\n    if NO_GROUPS:\n\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n\n        zeros = (zeros >> zeros_shifter) & 0xF\n\n        zeros = zeros * scales\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)\n            ptr = zeros_ptrs + g_id * stride_zeros_g\n            zeros = tl.load(ptr)\n\n            zeros = (zeros >> zeros_shifter) & 0xF\n            zeros = (zeros) * scales\n\n        b = (b >> shifter[:, None]) & 0xF\n        b = b * scales[None, :] - zeros[None, :]\n\n        accumulator += tl.dot(a, b.to(a.dtype))\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk\n    c = accumulator.to(c_ptr.dtype.element_ty)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 109, 261, 109, 262, 109, 263, 109, 264, 109, 265, 109, 266, 109, 267, 62, 6, 109, 268, 62, 6, 109, 269, 62, 6, 109, 270, 62, 6, 109, 271, 62, 6, 176, 62, 32, -1, 272, 195, 349, 32, 273, 195, 350, 32, 274, 195, 172, 244, 275, 195, 351, 176, 32, 276, 195, 64, 244, 253, 109, 268, 176, 32, 277, 195, 64, 244, 254, 109, 269, 176, 32, 278, 195, 64, 244, 255, 109, 270, 176, 32, 279, 195, 271, 245, 277, 32, 280, 195, 274, 48, 279, 32, 281, 195, 280, 245, 271, 32, 282, 195, 39, 244, 276, 4, 281, 109, 271, 176, 32, 283, 195, 281, 74, 274, 227, 282, 32, 284, 195, 274, 227, 279, 48, 282, 32, 285, 195, 283, 245, 268, 74, 76, 244, 351, 109, 268, 176, 32, 286, 195, 284, 245, 269, 74, 76, 244, 351, 109, 269, 176, 32, 287, 195, 76, 244, 351, 109, 270, 176, 32, 288, 195, 248, 74, 244, 285, 231, 62, 109, 203, 27, 245, 256, 74, 287, 231, 203, 109, 62, 27, 245, 257, 176, 32, 289, 195, 285, 231, 62, 109, 203, 27, 1, 253, 32, 290, 195, 249, 74, 244, 287, 231, 62, 109, 203, 27, 48, 273, 245, 258, 74, 286, 231, 203, 109, 62, 27, 245, 259, 176, 32, 291, 195, 251, 74, 286, 245, 263, 32, 292, 195, 252, 74, 286, 48, 273, 245, 265, 32, 293, 195, 287, 227, 273, 245, 272, 32, 294, 195, 286, 227, 273, 245, 272, 32, 183, 267, 62, 32, 295, 195, 55, 244, 291, 176, 32, 296, 195, 55, 244, 292, 176, 32, 296, 195, 296, 128, 294, 174, 352, 353, 32, 296, 195, 296, 245, 295, 32, 186, 32, 297, 195, 177, 244, 244, 268, 109, 269, 176, 109, 92, 195, 146, 176, 32, 136, 298, 158, 5, 244, 351, 109, 278, 176, 62, 32, 299, 195, 55, 244, 288, 109, 300, 195, 289, 109, 301, 195, 351, 176, 32, 302, 195, 55, 244, 290, 176, 32, 183, 63, 267, 62, 32, 303, 195, 298, 48, 244, 266, 48, 270, 176, 32, 304, 195, 291, 74, 303, 245, 262, 32, 295, 195, 55, 244, 304, 176, 32, 304, 195, 292, 74, 303, 245, 264, 32, 296, 195, 55, 244, 304, 176, 32, 296, 195, 296, 128, 294, 174, 352, 353, 32, 296, 195, 296, 245, 295, 32, 186, 32, 302, 195, 302, 128, 293, 231, 62, 109, 203, 27, 174, 352, 353, 32, 302, 195, 302, 245, 295, 231, 203, 109, 62, 27, 4, 296, 231, 203, 109, 62, 27, 32, 297, 173, 15, 244, 299, 109, 302, 82, 305, 244, 299, 82, 92, 176, 176, 32, 288, 173, 270, 245, 257, 32, 290, 173, 270, 48, 273, 245, 258, 32, 78, 32, 306, 195, 297, 82, 305, 244, 250, 82, 92, 82, 115, 176, 32, 307, 195, 283, 245, 268, 74, 76, 244, 351, 109, 268, 176, 32, 308, 195, 284, 245, 269, 74, 76, 244, 351, 109, 269, 176, 32, 309, 195, 250, 74, 260, 245, 307, 231, 62, 109, 203, 27, 74, 261, 245, 308, 231, 203, 109, 62, 27, 32, 310, 195, 244, 307, 231, 62, 109, 203, 27, 1, 253, 176, 174, 244, 308, 231, 203, 109, 62, 27, 1, 254, 176, 32, 10, 244, 309, 109, 297, 109, 300, 195, 310, 176, 32, 3, 32]}, {"code": "def matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if ACTIVATION == \"leaky_relu\":\n        accumulator = leaky_relu(accumulator)\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 63, 6, 109, 261, 63, 6, 109, 262, 63, 6, 109, 263, 63, 6, 109, 264, 63, 6, 176, 63, 32, -1, 265, 195, 172, 244, 266, 195, 349, 176, 32, 267, 195, 65, 244, 251, 109, 260, 176, 32, 268, 195, 65, 244, 252, 109, 261, 176, 32, 269, 195, 263, 245, 268, 32, 270, 195, 265, 48, 269, 32, 271, 195, 270, 245, 263, 32, 272, 195, 39, 244, 267, 4, 271, 109, 263, 176, 32, 273, 195, 271, 74, 265, 227, 269, 227, 272, 32, 274, 195, 265, 227, 269, 48, 272, 32, 275, 195, 244, 273, 245, 260, 74, 76, 244, 349, 109, 260, 176, 176, 227, 251, 32, 276, 195, 244, 274, 245, 261, 74, 76, 244, 349, 109, 261, 176, 176, 227, 252, 32, 277, 195, 76, 244, 349, 109, 262, 176, 32, 278, 195, 248, 74, 244, 275, 231, 63, 109, 203, 27, 245, 254, 74, 277, 231, 203, 109, 63, 27, 245, 255, 176, 32, 279, 195, 249, 74, 244, 277, 231, 63, 109, 203, 27, 245, 256, 74, 276, 231, 203, 109, 63, 27, 245, 257, 176, 32, 280, 195, 177, 244, 244, 260, 109, 261, 176, 109, 92, 195, 146, 176, 32, 136, 281, 158, 5, 244, 349, 109, 65, 244, 253, 109, 262, 176, 176, 63, 32, 282, 195, 57, 244, 278, 109, 283, 195, 277, 231, 203, 109, 63, 27, 1, 253, 4, 281, 245, 262, 109, 284, 195, 349, 176, 32, 285, 195, 57, 244, 279, 109, 283, 195, 277, 231, 63, 109, 203, 27, 1, 253, 4, 281, 245, 262, 109, 284, 195, 349, 176, 32, 280, 195, 15, 244, 282, 109, 285, 109, 280, 176, 32, 278, 173, 262, 245, 255, 32, 279, 173, 262, 245, 256, 32, 78, 32, 183, 264, 77, 350, 63, 32, 280, 195, 286, 244, 280, 176, 32, 186, 32, 287, 195, 280, 82, 288, 244, 21, 176, 32, 289, 195, 273, 245, 260, 74, 76, 244, 349, 109, 260, 176, 32, 290, 195, 274, 245, 261, 74, 76, 244, 349, 109, 261, 176, 32, 291, 195, 250, 74, 258, 245, 289, 231, 63, 109, 203, 27, 74, 259, 245, 290, 231, 203, 109, 63, 27, 32, 292, 195, 244, 289, 231, 63, 109, 203, 27, 1, 251, 176, 174, 244, 290, 231, 203, 109, 63, 27, 1, 252, 176, 32, 10, 244, 291, 109, 287, 109, 283, 195, 292, 176, 32, 3, 32]}, {"code": "def matmul_kernel_persistent(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    NUM_SMS: tl.constexpr,\n):\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n\n    offs_k_for_mask = tl.arange(0, BLOCK_SIZE_K)\n\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    pid_m = 0\n    pid_n = 0\n    offs_am = tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = tl.arange(0, BLOCK_SIZE_N)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_SIZE_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n            pid_m = first_pid_m + (tile_id % group_size_m)\n            pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n            start_m = pid_m * BLOCK_SIZE_M\n            start_n = pid_n * BLOCK_SIZE_N\n            offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n            offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n            offs_am = tl.where(offs_am < M, offs_am, 0)\n            offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n            offs_am = tl.max_contiguous(\n                tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M\n            )\n            offs_bn = tl.max_contiguous(\n                tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N\n            )\n        offs_k = ki * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n        a = tl.load(\n            a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_SIZE_K, other=0.0\n        )\n        b = tl.load(\n            b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_SIZE_K, other=0.0\n        )\n        accumulator = tl.dot(a, b, accumulator)\n\n        if ki == k_tiles - 1:\n            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n            c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n            if c_ptr.dtype.element_ty == tl.float8e4nv:\n                c = accumulator.to(tl.float8e4nv)\n            else:\n                c = accumulator.to(tl.float16)\n            tl.store(c_ptrs, c, mask=c_mask)\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 62, 6, 109, 261, 62, 6, 109, 262, 62, 6, 109, 263, 62, 6, 109, 264, 62, 6, 176, 62, 32, -1, 265, 195, 172, 244, 266, 195, 349, 176, 32, 267, 195, 64, 244, 251, 109, 260, 176, 32, 268, 195, 64, 244, 252, 109, 261, 176, 32, 269, 195, 64, 244, 253, 109, 262, 176, 32, 270, 195, 267, 245, 268, 32, 271, 195, 270, 48, 264, 32, 183, 265, 1, 270, 227, 264, 62, 32, 271, 173, 350, 32, 186, 32, 272, 195, 265, 4, 264, 32, 273, 195, 4, 350, 32, 274, 195, 76, 244, 349, 109, 262, 176, 32, 275, 195, 263, 245, 268, 32, 276, 195, 349, 32, 277, 195, 349, 32, 278, 195, 76, 244, 349, 109, 260, 176, 32, 279, 195, 76, 244, 349, 109, 261, 176, 32, 280, 195, 177, 244, 244, 260, 109, 261, 176, 109, 92, 195, 146, 176, 32, 136, 281, 158, 5, 244, 349, 109, 269, 245, 271, 176, 62, 32, 273, 195, 206, 244, 273, 77, 269, 4, 350, 109, 349, 109, 273, 74, 350, 176, 32, 183, 273, 77, 349, 62, 32, 272, 173, 264, 32, 282, 195, 272, 48, 275, 32, 283, 195, 282, 245, 263, 32, 284, 195, 39, 244, 267, 4, 283, 109, 263, 176, 32, 276, 195, 283, 74, 272, 227, 284, 32, 277, 195, 272, 227, 275, 48, 284, 32, 285, 195, 276, 245, 260, 32, 286, 195, 277, 245, 261, 32, 278, 195, 285, 74, 76, 244, 349, 109, 260, 176, 32, 279, 195, 286, 74, 76, 244, 349, 109, 261, 176, 32, 278, 195, 206, 244, 278, 1, 251, 109, 278, 109, 349, 176, 32, 279, 195, 206, 244, 279, 1, 252, 109, 279, 109, 349, 176, 32, 278, 195, 212, 244, 57, 244, 278, 109, 260, 176, 109, 260, 176, 32, 279, 195, 212, 244, 57, 244, 279, 109, 261, 176, 109, 261, 176, 32, 186, 32, 287, 195, 273, 245, 262, 74, 76, 244, 349, 109, 262, 176, 32, 288, 195, 248, 74, 244, 278, 231, 62, 109, 203, 27, 245, 254, 74, 287, 231, 203, 109, 62, 27, 245, 255, 176, 32, 289, 195, 249, 74, 244, 287, 231, 62, 109, 203, 27, 245, 256, 74, 279, 231, 203, 109, 62, 27, 245, 257, 176, 32, 290, 195, 55, 244, 288, 109, 291, 195, 274, 231, 203, 109, 62, 27, 1, 253, 4, 273, 245, 262, 109, 292, 195, 349, 176, 32, 293, 195, 55, 244, 289, 109, 291, 195, 274, 231, 62, 109, 203, 27, 1, 253, 4, 273, 245, 262, 109, 292, 195, 349, 176, 32, 280, 195, 15, 244, 290, 109, 293, 109, 280, 176, 32, 183, 273, 77, 269, 4, 350, 62, 32, 294, 195, 276, 245, 260, 74, 76, 244, 349, 109, 260, 176, 32, 295, 195, 277, 245, 261, 74, 76, 244, 349, 109, 261, 176, 32, 296, 195, 250, 74, 258, 245, 294, 231, 62, 109, 203, 27, 74, 259, 245, 295, 231, 203, 109, 62, 27, 32, 297, 195, 244, 294, 231, 62, 109, 203, 27, 1, 251, 176, 174, 244, 295, 231, 203, 109, 62, 27, 1, 252, 176, 32, 183, 250, 82, 92, 82, 115, 77, 223, 62, 32, 298, 195, 280, 82, 299, 244, 223, 176, 32, 186, 32, 30, 62, 32, 298, 195, 280, 82, 299, 244, 21, 176, 32, 56, 32, 10, 244, 296, 109, 298, 109, 291, 195, 297, 176, 32, 280, 195, 177, 244, 244, 260, 109, 261, 176, 109, 92, 195, 146, 176, 32, 186, 32, 78, 32, 3, 32]}, {"code": "def matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 63, 6, 109, 261, 63, 6, 109, 262, 63, 6, 109, 263, 63, 6, 176, 63, 32, -1, 264, 195, 172, 244, 265, 195, 349, 176, 32, 266, 195, 65, 244, 251, 109, 260, 176, 32, 267, 195, 65, 244, 252, 109, 261, 176, 32, 268, 195, 263, 245, 267, 32, 269, 195, 264, 48, 268, 32, 270, 195, 269, 245, 263, 32, 271, 195, 39, 244, 266, 4, 270, 109, 263, 176, 32, 272, 195, 270, 74, 264, 227, 271, 32, 273, 195, 264, 227, 268, 48, 271, 32, 274, 195, 272, 245, 260, 74, 76, 244, 349, 109, 260, 176, 32, 275, 195, 273, 245, 261, 74, 76, 244, 349, 109, 261, 176, 32, 276, 195, 76, 244, 349, 109, 262, 176, 32, 277, 195, 248, 74, 244, 274, 231, 63, 109, 203, 27, 245, 254, 74, 276, 231, 203, 109, 63, 27, 245, 255, 176, 32, 278, 195, 249, 74, 244, 276, 231, 63, 109, 203, 27, 245, 256, 74, 275, 231, 203, 109, 63, 27, 245, 257, 176, 32, 279, 195, 177, 244, 244, 260, 109, 261, 176, 109, 92, 195, 146, 176, 32, 136, 280, 158, 5, 244, 349, 109, 65, 244, 253, 109, 262, 176, 176, 63, 32, 281, 195, 57, 244, 277, 109, 282, 195, 276, 231, 203, 109, 63, 27, 1, 253, 4, 280, 245, 262, 109, 283, 195, 349, 176, 32, 284, 195, 57, 244, 278, 109, 282, 195, 276, 231, 63, 109, 203, 27, 1, 253, 4, 280, 245, 262, 109, 283, 195, 349, 176, 32, 279, 173, 15, 244, 281, 109, 284, 176, 32, 277, 173, 262, 245, 255, 32, 278, 173, 262, 245, 256, 32, 78, 32, 285, 195, 250, 74, 244, 274, 231, 63, 109, 203, 27, 245, 258, 74, 275, 231, 203, 109, 63, 27, 245, 259, 176, 32, 286, 195, 244, 274, 231, 63, 109, 203, 27, 1, 251, 176, 174, 244, 275, 231, 203, 109, 63, 27, 1, 252, 176, 32, 10, 244, 285, 109, 279, 109, 282, 195, 286, 176, 32, 3, 32]}, {"code": "def matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if ACTIVATION == \"leaky_relu\":\n        accumulator = leaky_relu(accumulator)\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 109, 258, 109, 259, 109, 260, 62, 6, 109, 261, 62, 6, 109, 262, 62, 6, 109, 263, 62, 6, 109, 264, 62, 6, 176, 62, 32, -1, 265, 195, 172, 244, 266, 195, 349, 176, 32, 267, 195, 64, 244, 251, 109, 260, 176, 32, 268, 195, 64, 244, 252, 109, 261, 176, 32, 269, 195, 263, 245, 268, 32, 270, 195, 265, 48, 269, 32, 271, 195, 270, 245, 263, 32, 272, 195, 39, 244, 267, 4, 271, 109, 263, 176, 32, 273, 195, 271, 74, 265, 227, 269, 227, 272, 32, 274, 195, 265, 227, 269, 48, 272, 32, 275, 195, 244, 273, 245, 260, 74, 76, 244, 349, 109, 260, 176, 176, 227, 251, 32, 276, 195, 244, 274, 245, 261, 74, 76, 244, 349, 109, 261, 176, 176, 227, 252, 32, 277, 195, 76, 244, 349, 109, 262, 176, 32, 278, 195, 248, 74, 244, 275, 231, 62, 109, 203, 27, 245, 254, 74, 277, 231, 203, 109, 62, 27, 245, 255, 176, 32, 279, 195, 249, 74, 244, 277, 231, 62, 109, 203, 27, 245, 256, 74, 276, 231, 203, 109, 62, 27, 245, 257, 176, 32, 280, 195, 177, 244, 244, 260, 109, 261, 176, 109, 92, 195, 146, 176, 32, 136, 281, 158, 5, 244, 349, 109, 64, 244, 253, 109, 262, 176, 176, 62, 32, 282, 195, 55, 244, 278, 109, 283, 195, 277, 231, 203, 109, 62, 27, 1, 253, 4, 281, 245, 262, 109, 284, 195, 349, 176, 32, 285, 195, 55, 244, 279, 109, 283, 195, 277, 231, 62, 109, 203, 27, 1, 253, 4, 281, 245, 262, 109, 284, 195, 349, 176, 32, 280, 195, 15, 244, 282, 109, 285, 109, 280, 176, 32, 278, 173, 262, 245, 255, 32, 279, 173, 262, 245, 256, 32, 78, 32, 183, 264, 77, 350, 62, 32, 280, 195, 286, 244, 280, 176, 32, 186, 32, 287, 195, 280, 82, 288, 244, 21, 176, 32, 289, 195, 273, 245, 260, 74, 76, 244, 349, 109, 260, 176, 32, 290, 195, 274, 245, 261, 74, 76, 244, 349, 109, 261, 176, 32, 291, 195, 250, 74, 258, 245, 289, 231, 62, 109, 203, 27, 74, 259, 245, 290, 231, 203, 109, 62, 27, 32, 292, 195, 244, 289, 231, 62, 109, 203, 27, 1, 251, 176, 174, 244, 290, 231, 203, 109, 62, 27, 1, 252, 176, 32, 10, 244, 291, 109, 287, 109, 283, 195, 292, 176, 32, 3, 32]}, {"code": "def mv_kernel(\n    A,\n    B,\n    C,\n    N,\n    M,\n    stride_an,\n    stride_am,\n    stride_bm,\n    stride_cn,\n    BLOCK_N: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    offset_n = pid * BLOCK_N + tl.arange(0, BLOCK_N)[:, None]\n    offset_m = tl.arange(0, BLOCK_M)[None, :]\n    n_mask = offset_n < N\n    A_ptrs = A + offset_n * stride_an + offset_m * stride_am\n    B_ptrs = B + offset_m * stride_bm\n    acc = tl.zeros((BLOCK_N, BLOCK_M), dtype=tl.float32)\n    for m in range(0, M, BLOCK_M):\n        m_mask = m + offset_m < M\n        a = tl.load(A_ptrs, mask=n_mask & m_mask, other=0.0).to(tl.float32)\n        b = tl.load(B_ptrs, mask=m_mask, other=0.0).to(tl.float32)\n        acc += a * b\n        A_ptrs += BLOCK_M * stride_am\n        B_ptrs += BLOCK_M * stride_bm\n\n    acc = tl.sum(acc, axis=1)\n    C_ptrs = C + offset_n * stride_cn\n    tl.store(C_ptrs, acc[:, None], mask=n_mask)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 109, 255, 109, 256, 109, 257, 63, 6, 109, 258, 63, 6, 176, 63, 32, -1, 259, 195, 172, 244, 349, 176, 32, 260, 195, 259, 245, 257, 74, 76, 244, 349, 109, 257, 176, 231, 63, 109, 203, 27, 32, 261, 195, 76, 244, 349, 109, 258, 176, 231, 203, 109, 63, 27, 32, 262, 195, 260, 1, 251, 32, 263, 195, 248, 74, 260, 245, 253, 74, 261, 245, 254, 32, 264, 195, 249, 74, 261, 245, 255, 32, 265, 195, 177, 244, 244, 257, 109, 258, 176, 109, 92, 195, 146, 176, 32, 136, 266, 158, 5, 244, 349, 109, 252, 109, 258, 176, 63, 32, 267, 195, 266, 74, 261, 1, 252, 32, 268, 195, 57, 244, 263, 109, 269, 195, 262, 174, 267, 109, 270, 195, 349, 176, 82, 271, 244, 146, 176, 32, 272, 195, 57, 244, 264, 109, 269, 195, 267, 109, 270, 195, 349, 176, 82, 271, 244, 146, 176, 32, 265, 173, 268, 245, 272, 32, 263, 173, 258, 245, 254, 32, 264, 173, 258, 245, 255, 32, 78, 32, 265, 195, 222, 244, 265, 109, 273, 195, 350, 176, 32, 274, 195, 250, 74, 260, 245, 256, 32, 10, 244, 274, 109, 265, 231, 63, 109, 203, 27, 109, 269, 195, 262, 176, 32, 3, 32]}, {"code": "def max_kernel(\n    inp,\n    out_value,\n    out_index,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offset = tl.arange(0, BLOCK_N)\n    offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n    offset_index = m_offset * K + pid_k\n\n    mask1 = m_offset < M\n    mask = m_offset[:, None] < M and n_offset[None, :] < N\n    inp_ptrs = inp + offset\n    inp_vals = tl.load(inp_ptrs, mask=mask, other=-float(\"inf\"))\n    result_value, result_index = tl.max(inp_vals, axis=1, return_indices=True)\n\n    out_value_ptrs = out_value + offset_index\n    out_index_ptrs = out_index + offset_index\n\n    tl.store(out_value_ptrs, result_value, mask=mask1)\n    tl.store(out_index_ptrs, result_index, mask=mask1)", "encoded": [29, 348, 244, 248, 109, 249, 109, 250, 109, 251, 109, 252, 109, 253, 109, 254, 62, 6, 109, 255, 62, 6, 176, 62, 32, -1, 256, 195, 172, 244, 349, 176, 32, 257, 195, 172, 244, 350, 176, 32, 258, 195, 256, 245, 254, 74, 76, 244, 349, 109, 254, 176, 32, 259, 195, 76, 244, 349, 109, 255, 176, 32, 260, 195, 258, 231, 62, 109, 203, 27, 245, 252, 245, 253, 74, 259, 231, 203, 109, 62, 27, 245, 253, 74, 257, 32, 261, 195, 258, 245, 253, 74, 257, 32, 262, 195, 258, 1, 251, 32, 263, 195, 258, 231, 62, 109, 203, 27, 1, 251, 103, 259, 231, 203, 109, 62, 27, 1, 252, 32, 264, 195, 248, 74, 260, 32, 265, 195, 55, 244, 264, 109, 263, 195, 263, 109, 266, 195, 4, 267, 244, 351, 176, 176, 32, 268, 109, 269, 195, 12, 244, 265, 109, 270, 195, 350, 109, 271, 195, 161, 176, 32, 272, 195, 249, 74, 261, 32, 273, 195, 250, 74, 261, 32, 10, 244, 272, 109, 268, 109, 263, 195, 262, 176, 32, 10, 244, 273, 109, 269, 109, 263, 195, 262, 176, 32, 3, 32]}, {"code": "def _quantize_global_transpose(\n    A,\n    absmax_inv_ptr,\n    B,\n    stride_am,\n    stride_an,\n    stride_bn,\n    stride_bm,\n    M,\n    N,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    GROUP_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (pid % group_size)\n    pid_n = (pid % width) // group_size\n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    A = A + (rm[:, None] * stride_am + rn[None, :] * stride_an)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    a = tl.load(A, mask=mask)\n    absmax_inv = tl.load(absmax_inv_ptr)\n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    B = B + (rm[:, None] * stride_bm + rn[None, :] * stride_bn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n\n    output = tl.extra.cuda.libdevice.llrint(127.0 * (a * absmax_inv))\n\n    tl.store(B, output, mask=mask)", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 110, 254, 110, 255, 110, 256, 110, 257, 110, 258, 64, 6, 110, 259, 64, 6, 110, 260, 64, 6, 177, 64, 33, -1, 261, 196, 173, 245, 350, 177, 33, 262, 196, 245, 256, 75, 258, 4, 351, 177, 49, 258, 33, 263, 196, 245, 257, 75, 259, 4, 351, 177, 49, 259, 33, 264, 196, 260, 246, 263, 33, 265, 196, 261, 49, 264, 33, 266, 196, 40, 245, 262, 4, 265, 246, 260, 110, 260, 177, 33, 267, 196, 265, 246, 260, 75, 261, 228, 266, 33, 268, 196, 261, 228, 264, 49, 266, 33, 269, 196, 267, 246, 258, 75, 77, 245, 350, 110, 258, 177, 33, 270, 196, 268, 246, 259, 75, 77, 245, 350, 110, 259, 177, 33, 249, 196, 249, 75, 245, 269, 232, 64, 110, 204, 28, 246, 252, 75, 270, 232, 204, 110, 64, 28, 246, 253, 177, 33, 271, 196, 245, 269, 1, 256, 177, 232, 64, 110, 204, 28, 175, 245, 270, 1, 257, 177, 232, 204, 110, 64, 28, 33, 272, 196, 58, 245, 249, 110, 271, 196, 271, 177, 33, 273, 196, 58, 245, 250, 177, 33, 269, 196, 267, 246, 258, 75, 77, 245, 350, 110, 258, 177, 33, 270, 196, 268, 246, 259, 75, 77, 245, 350, 110, 259, 177, 33, 251, 196, 251, 75, 245, 269, 232, 64, 110, 204, 28, 246, 255, 75, 270, 232, 204, 110, 64, 28, 246, 254, 177, 33, 271, 196, 245, 269, 1, 256, 177, 232, 64, 110, 204, 28, 175, 245, 270, 1, 257, 177, 232, 204, 110, 64, 28, 33, 274, 196, 16, 245, 352, 246, 245, 272, 246, 273, 177, 177, 33, 10, 245, 251, 110, 274, 110, 271, 196, 271, 177, 33, 3, 33]}, {"code": "def _quantize_global(\n    x_ptr,\n    absmax_inv_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    absmax_inv = tl.load(absmax_inv_ptr)\n    output = tl.extra.cuda.libdevice.llrint(127.0 * (x * absmax_inv))\n    tl.store(output_ptr + offsets, output, mask=mask)", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 63, 6, 177, 63, 33, -1, 254, 196, 173, 245, 255, 196, 350, 177, 33, 256, 196, 254, 246, 253, 33, 257, 196, 256, 75, 77, 245, 350, 110, 253, 177, 33, 258, 196, 257, 1, 252, 33, 259, 196, 56, 245, 249, 75, 257, 110, 258, 196, 258, 177, 33, 260, 196, 56, 245, 250, 177, 33, 261, 196, 16, 245, 351, 246, 245, 259, 246, 260, 177, 177, 33, 10, 245, 251, 75, 257, 110, 261, 110, 258, 196, 258, 177, 33, 3, 33]}, {"code": "def chunk_global_reversed_cumsum_vector_kernel(\n    s,\n    z,\n    s_s_h,\n    s_s_t,\n    s_s_d,\n    T: tl.constexpr,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\n\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_s = tl.make_block_ptr(\n            s + i_bh * s_s_h,\n            (T, S),\n            (s_s_t, s_s_d),\n            (i_t * BT, i_s * BS),\n            (BT, BS),\n            (1, 0),\n        )\n        p_z = tl.make_block_ptr(\n            z + i_bh * s_s_h,\n            (T, S),\n            (s_s_t, s_s_d),\n            (i_t * BT, i_s * BS),\n            (BT, BS),\n            (1, 0),\n        )\n\n        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))\n\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 110, 254, 64, 6, 110, 255, 64, 6, 110, 256, 64, 6, 110, 257, 64, 6, 177, 64, 33, -1, 258, 110, 259, 196, 245, 173, 245, 350, 177, 110, 173, 245, 351, 177, 177, 33, 260, 196, 77, 245, 350, 110, 256, 177, 33, 261, 196, 207, 245, 260, 232, 64, 110, 204, 28, 220, 260, 232, 204, 110, 64, 28, 110, 351, 110, 350, 177, 33, 262, 196, 178, 245, 232, 257, 28, 110, 93, 196, 147, 177, 33, 137, 263, 159, 5, 245, 66, 245, 254, 110, 256, 177, 4, 351, 110, 4, 351, 110, 4, 351, 177, 64, 33, 264, 196, 221, 245, 249, 75, 259, 246, 251, 110, 245, 254, 110, 255, 177, 110, 245, 252, 110, 253, 177, 110, 245, 263, 246, 256, 110, 258, 246, 257, 177, 110, 245, 256, 110, 257, 177, 110, 245, 351, 110, 350, 177, 177, 33, 265, 196, 221, 245, 250, 75, 259, 246, 251, 110, 245, 254, 110, 255, 177, 110, 245, 252, 110, 253, 177, 110, 245, 263, 246, 256, 110, 258, 246, 257, 177, 110, 245, 256, 110, 257, 177, 110, 245, 351, 110, 350, 177, 177, 33, 266, 196, 58, 245, 264, 110, 267, 196, 245, 350, 110, 351, 177, 177, 83, 268, 245, 147, 177, 33, 269, 196, 262, 232, 204, 110, 64, 28, 75, 15, 245, 261, 110, 266, 110, 270, 196, 60, 177, 33, 10, 245, 265, 110, 269, 83, 268, 245, 265, 83, 93, 83, 116, 177, 110, 267, 196, 245, 350, 110, 351, 177, 177, 33, 184, 263, 146, 350, 64, 33, 262, 174, 223, 245, 266, 110, 350, 177, 33, 187, 33, 79, 33, 3, 33]}, {"code": "def chunk_global_reversed_cumsum_scalar_kernel(\n    s,\n    o,\n    T: tl.constexpr,\n    BT: tl.constexpr,\n):\n    i_bh = tl.program_id(0)\n    b_z = tl.zeros([], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\n        b_zz = tl.sum(b_s, axis=0)\n        b_z += b_zz\n        b_o = b_s - tl.cumsum(b_s, axis=0) + b_z[None]\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 63, 6, 110, 252, 63, 6, 177, 63, 33, -1, 253, 196, 173, 245, 350, 177, 33, 254, 196, 178, 245, 232, 28, 110, 93, 196, 147, 177, 33, 137, 255, 159, 5, 245, 65, 245, 251, 110, 252, 177, 4, 351, 110, 4, 351, 110, 4, 351, 177, 63, 33, 256, 196, 221, 245, 249, 75, 253, 246, 251, 110, 245, 251, 110, 177, 110, 245, 351, 110, 177, 110, 245, 255, 246, 252, 110, 177, 110, 245, 252, 110, 177, 110, 245, 350, 110, 177, 177, 33, 257, 196, 221, 245, 250, 75, 253, 246, 251, 110, 245, 251, 110, 177, 110, 245, 351, 110, 177, 110, 245, 255, 246, 252, 110, 177, 110, 245, 252, 110, 177, 110, 245, 350, 110, 177, 177, 33, 258, 196, 56, 245, 256, 110, 259, 196, 245, 350, 110, 177, 177, 83, 260, 245, 147, 177, 33, 261, 196, 223, 245, 258, 110, 262, 196, 350, 177, 33, 254, 174, 261, 33, 263, 196, 258, 4, 87, 245, 258, 110, 262, 196, 350, 177, 75, 254, 232, 204, 28, 33, 10, 245, 257, 110, 263, 83, 260, 245, 257, 83, 93, 83, 116, 177, 110, 259, 196, 245, 350, 110, 177, 177, 33, 79, 33, 3, 33]}, {"code": "def rms_norm_kernel(\n    Y,\n    X,\n    W,\n    y_stride_r,\n    y_stride_c,\n    x_stride_r,\n    x_stride_c,\n    N,\n    eps,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    Y += pid * y_stride_r\n    X += pid * x_stride_r\n\n    mask = tl.arange(0, BLOCK_SIZE) < N\n    cols = tl.arange(0, BLOCK_SIZE)\n    x = tl.load(X + cols * x_stride_c, mask, other=0.0).to(tl.float32)\n\n    var = tl.sum(x * x, axis=0) / N\n    rrms = 1 / tl.sqrt(var + eps)\n\n    w = tl.load(W + tl.arange(0, BLOCK_SIZE), mask=mask, other=0.0)\n    y = (x * rrms).to(Y.dtype.element_ty) * w\n    tl.store(Y + cols * y_stride_c, y, mask=mask)", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 110, 254, 110, 255, 110, 256, 110, 257, 110, 258, 64, 6, 177, 64, 33, -1, 259, 196, 173, 245, 350, 177, 33, 249, 174, 259, 246, 252, 33, 250, 174, 259, 246, 254, 33, 260, 196, 77, 245, 350, 110, 258, 177, 1, 256, 33, 261, 196, 77, 245, 350, 110, 258, 177, 33, 262, 196, 58, 245, 250, 75, 261, 246, 255, 110, 260, 110, 263, 196, 350, 177, 83, 264, 245, 147, 177, 33, 265, 196, 223, 245, 262, 246, 262, 110, 266, 196, 350, 177, 43, 256, 33, 267, 196, 351, 43, 132, 245, 265, 75, 257, 177, 33, 268, 196, 58, 245, 251, 75, 77, 245, 350, 110, 258, 177, 110, 260, 196, 260, 110, 263, 196, 350, 177, 33, 269, 196, 245, 262, 246, 267, 177, 83, 264, 245, 249, 83, 93, 83, 116, 177, 246, 268, 33, 10, 245, 249, 75, 261, 246, 253, 110, 269, 110, 260, 196, 260, 177, 33, 3, 33]}, {"code": "def _rope_embedding(\n    Q,\n    Q_row_stride,\n    cos,\n    cos_row_stride,\n    sin,\n    sin_row_stride,\n    seqlen,\n    head_dim: tl.constexpr,\n    n_heads: tl.constexpr,\n    BACKWARD_PASS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    ROPE_GROUP_SIZE: tl.constexpr = 4,\n):\n\n    row_position = tl.program_id(0)\n    group_head_position = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    half_head_dim = head_dim // 2\n    mask = col_offsets < half_head_dim\n\n    sin1 = tl.load(\n        sin\n        + (row_position % seqlen) * sin_row_stride\n        + half_head_dim * 0\n        + col_offsets,\n        mask=mask,\n        other=0,\n    )\n    cos1 = tl.load(\n        cos\n        + (row_position % seqlen) * cos_row_stride\n        + half_head_dim * 0\n        + col_offsets,\n        mask=mask,\n        other=0,\n    )\n\n    if BACKWARD_PASS:\n\n        sin1 = -sin1\n\n    head_start = group_head_position * ROPE_GROUP_SIZE\n    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)\n\n    for k in range(head_start, head_end):\n        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\n        offs_q2 = (\n            row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim\n        )\n\n        Q1 = tl.load(Q + offs_q1, mask=mask, other=0).to(sin1.dtype)\n        Q2 = tl.load(Q + offs_q2, mask=mask, other=0).to(sin1.dtype)\n\n        tl.store(Q + offs_q1, Q1 * cos1 - Q2 * sin1, mask=mask)\n        tl.store(Q + offs_q2, Q2 * cos1 + Q1 * sin1, mask=mask)", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 110, 254, 110, 255, 110, 256, 63, 6, 110, 257, 63, 6, 110, 258, 63, 6, 110, 259, 63, 6, 110, 260, 63, 6, 196, 350, 177, 63, 33, -1, 261, 196, 173, 245, 351, 177, 33, 262, 196, 173, 245, 352, 177, 33, 263, 196, 77, 245, 351, 110, 259, 177, 33, 264, 196, 256, 49, 353, 33, 265, 196, 263, 1, 264, 33, 266, 196, 56, 245, 253, 75, 261, 228, 255, 246, 254, 75, 264, 246, 351, 75, 263, 110, 265, 196, 265, 110, 267, 196, 351, 177, 33, 268, 196, 56, 245, 251, 75, 261, 228, 255, 246, 252, 75, 264, 246, 351, 75, 263, 110, 265, 196, 265, 110, 267, 196, 351, 177, 33, 184, 258, 63, 33, 266, 196, 4, 266, 33, 187, 33, 269, 196, 262, 246, 260, 33, 270, 196, 40, 245, 269, 75, 260, 110, 257, 177, 33, 137, 271, 159, 5, 245, 269, 110, 270, 177, 63, 33, 272, 196, 261, 246, 250, 75, 271, 246, 256, 75, 263, 33, 273, 196, 261, 246, 250, 75, 271, 246, 256, 75, 263, 75, 264, 33, 274, 196, 56, 245, 249, 75, 272, 110, 265, 196, 265, 110, 267, 196, 351, 177, 83, 275, 245, 266, 83, 93, 177, 33, 276, 196, 56, 245, 249, 75, 273, 110, 265, 196, 265, 110, 267, 196, 351, 177, 83, 275, 245, 266, 83, 93, 177, 33, 10, 245, 249, 75, 272, 110, 274, 246, 268, 4, 276, 246, 266, 110, 265, 196, 265, 177, 33, 10, 245, 249, 75, 273, 110, 276, 246, 268, 75, 274, 246, 266, 110, 265, 196, 265, 177, 33, 79, 33, 3, 33]}, {"code": "def _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_maxs,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = tl.extra.cuda.libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_maxs + pid, max_val)", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 64, 6, 110, 254, 64, 6, 177, 64, 33, -1, 255, 196, 173, 245, 256, 196, 350, 177, 33, 257, 196, 255, 246, 253, 33, 258, 196, 77, 245, 350, 110, 254, 177, 33, 259, 196, 257, 75, 258, 33, 260, 196, 258, 1, 253, 33, 261, 196, 58, 245, 249, 75, 259, 110, 262, 196, 260, 177, 33, 263, 196, 102, 245, 261, 177, 33, 264, 196, 12, 245, 207, 245, 260, 110, 263, 110, 350, 177, 110, 256, 196, 350, 177, 33, 265, 196, 16, 245, 351, 246, 245, 261, 43, 264, 177, 177, 33, 10, 245, 250, 75, 259, 110, 265, 110, 262, 196, 260, 177, 33, 10, 245, 251, 75, 255, 110, 264, 177, 33, 3, 33]}, {"code": "def softmax_kernel_non_inner(\n    output_ptr,\n    input_ptr,\n    M,\n    N,\n    K,\n    TILE_N: tl.constexpr,\n    TILE_K: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_k = tl.program_id(1)\n    pid_m = tl.program_id(0)\n\n    k_offsets = pid_k * TILE_K + tl.arange(0, TILE_K)\n\n    if ONE_TILE_PER_CTA:\n        n_offsets = tl.arange(0, TILE_N)\n        offset = pid_m * N * K + n_offsets[:, None] * K + k_offsets\n        mask = (n_offsets[:, None] < N) & (k_offsets < K)\n        input_ptrs = input_ptr + offset\n        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\"))\n        m = tl.max(inp, 0)\n        e = tl.exp(inp - m[None, :])\n        z = tl.sum(e, 0)\n        out = e / z\n        output_ptrs = output_ptr + offset\n        tl.store(output_ptrs, out, mask=mask)\n    else:\n        m = tl.full([TILE_N, TILE_K], value=float(\"-inf\"), dtype=tl.float32)\n        z = tl.full([TILE_N, TILE_K], value=0.0, dtype=tl.float32)\n\n        for start_n in range(0, N, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)\n            offsets = pid_m * N * K + n_offsets[:, None] * K + k_offsets\n            mask = (n_offsets[:, None] < N) & (k_offsets < K)\n            inp = tl.load(input_ptr + offsets, mask=mask, other=-float(\"inf\"))\n            m_new = tl.maximum(m, inp)\n            alpha = tl.exp(m - m_new)\n            z = z * alpha + tl.exp(inp - m_new)\n            m = m_new\n\n        m_reduced = tl.max(m, 0)\n        z = tl.sum(z * tl.exp(m - m_reduced[None, :]), 0)\n        m = m_reduced\n\n        previous_multiple = prev_multiple_of(N, TILE_N)\n        for start_n in range(0, N, TILE_N):\n            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)\n            offsets = pid_m * N * K + n_offsets[:, None] * K + k_offsets\n            mask = (n_offsets[:, None] < N) & (k_offsets[None, :] < K)\n            inp = tl.load(input_ptr + offsets, mask=mask, other=-float(\"inf\"))\n            o = tl.exp(inp - m[None, :]) / z[None, :]\n            tl.store(output_ptr + offsets, o, mask=mask)", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 110, 254, 63, 6, 110, 255, 63, 6, 110, 256, 63, 6, 177, 63, 33, -1, 257, 196, 173, 245, 350, 177, 33, 258, 196, 173, 245, 351, 177, 33, 259, 196, 257, 246, 255, 75, 77, 245, 351, 110, 255, 177, 33, 184, 256, 63, 33, 260, 196, 77, 245, 351, 110, 254, 177, 33, 261, 196, 258, 246, 252, 246, 253, 75, 260, 232, 63, 110, 204, 28, 246, 253, 75, 259, 33, 262, 196, 245, 260, 232, 63, 110, 204, 28, 1, 252, 177, 175, 245, 259, 1, 253, 177, 33, 263, 196, 250, 75, 261, 33, 264, 196, 56, 245, 263, 110, 262, 196, 262, 110, 265, 196, 4, 266, 245, 352, 177, 177, 33, 267, 196, 12, 245, 264, 110, 351, 177, 33, 268, 196, 109, 245, 264, 4, 267, 232, 204, 110, 63, 28, 177, 33, 269, 196, 223, 245, 268, 110, 351, 177, 33, 14, 196, 268, 43, 269, 33, 270, 196, 249, 75, 261, 33, 10, 245, 270, 110, 14, 110, 262, 196, 262, 177, 33, 187, 33, 31, 63, 33, 267, 196, 239, 245, 232, 254, 110, 255, 28, 110, 271, 196, 266, 245, 353, 177, 110, 93, 196, 147, 177, 33, 269, 196, 239, 245, 232, 254, 110, 255, 28, 110, 271, 196, 351, 110, 93, 196, 147, 177, 33, 137, 272, 159, 5, 245, 351, 110, 252, 110, 254, 177, 63, 33, 260, 196, 272, 75, 77, 245, 351, 110, 254, 177, 33, 273, 196, 258, 246, 252, 246, 253, 75, 260, 232, 63, 110, 204, 28, 246, 253, 75, 259, 33, 262, 196, 245, 260, 232, 63, 110, 204, 28, 1, 252, 177, 175, 245, 259, 1, 253, 177, 33, 264, 196, 56, 245, 250, 75, 273, 110, 262, 196, 262, 110, 265, 196, 4, 266, 245, 352, 177, 177, 33, 274, 196, 194, 245, 267, 110, 264, 177, 33, 275, 196, 109, 245, 267, 4, 274, 177, 33, 269, 196, 269, 246, 275, 75, 109, 245, 264, 4, 274, 177, 33, 267, 196, 274, 33, 79, 33, 276, 196, 12, 245, 267, 110, 351, 177, 33, 269, 196, 223, 245, 269, 246, 109, 245, 267, 4, 276, 232, 204, 110, 63, 28, 177, 110, 351, 177, 33, 267, 196, 276, 33, 277, 196, 278, 245, 252, 110, 254, 177, 33, 137, 272, 159, 5, 245, 351, 110, 252, 110, 254, 177, 63, 33, 260, 196, 277, 4, 272, 75, 77, 245, 351, 110, 254, 177, 33, 273, 196, 258, 246, 252, 246, 253, 75, 260, 232, 63, 110, 204, 28, 246, 253, 75, 259, 33, 262, 196, 245, 260, 232, 63, 110, 204, 28, 1, 252, 177, 175, 245, 259, 232, 204, 110, 63, 28, 1, 253, 177, 33, 264, 196, 56, 245, 250, 75, 273, 110, 262, 196, 262, 110, 265, 196, 4, 266, 245, 352, 177, 177, 33, 279, 196, 109, 245, 264, 4, 267, 232, 204, 110, 63, 28, 177, 43, 269, 232, 204, 110, 63, 28, 33, 10, 245, 249, 75, 273, 110, 279, 110, 262, 196, 262, 177, 33, 79, 33, 57, 33, 3, 33]}, {"code": "def softmax_kernel_inner(\n    output_ptr,\n    input_ptr,\n    M,\n    N,\n    TILE_N: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    if ONE_TILE_PER_CTA:\n        n_offsets = tl.arange(0, TILE_N)\n        offset = pid_m * N + n_offsets\n        input_ptrs = input_ptr + offset\n        mask = n_offsets < N\n        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(\n            output_ptr.dtype.element_ty\n        )\n        m = tl.max(inp, 0)\n        e = tl.exp(inp - m)\n        z = tl.sum(e, 0)\n        out = e / z\n        output_ptrs = output_ptr + offset\n        tl.store(output_ptrs, out, mask=mask)\n    else:\n        m = tl.full([TILE_N], value=float(\"-inf\"), dtype=tl.float32)\n        z = tl.full([TILE_N], value=0.0, dtype=tl.float32)\n        input_ptr += pid_m * N\n        output_ptr += pid_m * N\n\n        previous_multiple = prev_multiple_of(N, TILE_N)\n        for start_n in range(0, previous_multiple, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)\n            inp = tl.load(input_ptr + n_offsets)\n            m_new = tl.maximum(m, inp)\n            z = z * tl.exp(m - m_new) + tl.exp(inp - m_new)\n            m = m_new\n\n        for start_n in range(previous_multiple, N, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)\n            mask = n_offsets < N\n            inp = tl.load(input_ptr + n_offsets, mask=mask, other=-float(\"inf\"))\n            m_new = tl.maximum(m, inp)\n            z = z * tl.exp(m - m_new) + tl.exp(inp - m_new)\n            m = m_new\n\n        m_reduced = tl.max(m, 0)\n        z = tl.sum(z * tl.exp(m - m_reduced), 0)\n        m = m_reduced\n\n        previous_multiple = prev_multiple_of(N, TILE_N)\n\n        for start_n in range(0, TILE_N, TILE_N):\n            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)\n            mask = n_offsets < N\n            inp = tl.load(\n                input_ptr + n_offsets,\n                mask=mask,\n                other=-float(\"inf\"),\n                eviction_policy=\"evict_first\",\n            )\n            o = tl.exp(inp - m) / z\n            tl.store(output_ptr + n_offsets, o, mask=mask)\n        for start_n in range(TILE_N, N, TILE_N):\n            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)\n            inp = tl.load(input_ptr + n_offsets, eviction_policy=\"evict_first\")\n            o = tl.exp(inp - m) / z\n            tl.store(output_ptr + n_offsets, o)", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 64, 6, 110, 254, 64, 6, 177, 64, 33, -1, 255, 196, 173, 245, 350, 177, 33, 184, 254, 64, 33, 256, 196, 77, 245, 350, 110, 253, 177, 33, 257, 196, 255, 246, 252, 75, 256, 33, 258, 196, 250, 75, 257, 33, 259, 196, 256, 1, 252, 33, 260, 196, 58, 245, 258, 110, 259, 196, 259, 110, 261, 196, 4, 262, 245, 351, 177, 177, 83, 263, 245, 249, 83, 93, 83, 116, 177, 33, 264, 196, 12, 245, 260, 110, 350, 177, 33, 265, 196, 109, 245, 260, 4, 264, 177, 33, 266, 196, 223, 245, 265, 110, 350, 177, 33, 14, 196, 265, 43, 266, 33, 267, 196, 249, 75, 257, 33, 10, 245, 267, 110, 14, 110, 259, 196, 259, 177, 33, 187, 33, 31, 64, 33, 264, 196, 239, 245, 232, 253, 28, 110, 268, 196, 262, 245, 352, 177, 110, 93, 196, 147, 177, 33, 266, 196, 239, 245, 232, 253, 28, 110, 268, 196, 350, 110, 93, 196, 147, 177, 33, 250, 174, 255, 246, 252, 33, 249, 174, 255, 246, 252, 33, 269, 196, 270, 245, 252, 110, 253, 177, 33, 137, 271, 159, 5, 245, 350, 110, 269, 110, 253, 177, 64, 33, 256, 196, 271, 75, 77, 245, 350, 110, 253, 177, 33, 260, 196, 58, 245, 250, 75, 256, 177, 33, 272, 196, 194, 245, 264, 110, 260, 177, 33, 266, 196, 266, 246, 109, 245, 264, 4, 272, 177, 75, 109, 245, 260, 4, 272, 177, 33, 264, 196, 272, 33, 79, 33, 137, 271, 159, 5, 245, 269, 110, 252, 110, 253, 177, 64, 33, 256, 196, 271, 75, 77, 245, 350, 110, 253, 177, 33, 259, 196, 256, 1, 252, 33, 260, 196, 58, 245, 250, 75, 256, 110, 259, 196, 259, 110, 261, 196, 4, 262, 245, 351, 177, 177, 33, 272, 196, 194, 245, 264, 110, 260, 177, 33, 266, 196, 266, 246, 109, 245, 264, 4, 272, 177, 75, 109, 245, 260, 4, 272, 177, 33, 264, 196, 272, 33, 79, 33, 273, 196, 12, 245, 264, 110, 350, 177, 33, 266, 196, 223, 245, 266, 246, 109, 245, 264, 4, 273, 177, 110, 350, 177, 33, 264, 196, 273, 33, 269, 196, 270, 245, 252, 110, 253, 177, 33, 137, 271, 159, 5, 245, 350, 110, 253, 110, 253, 177, 64, 33, 256, 196, 269, 4, 271, 75, 77, 245, 350, 110, 253, 177, 33, 259, 196, 256, 1, 252, 33, 260, 196, 58, 245, 250, 75, 256, 110, 259, 196, 259, 110, 261, 196, 4, 262, 245, 351, 177, 110, 274, 196, 353, 177, 33, 275, 196, 109, 245, 260, 4, 264, 177, 43, 266, 33, 10, 245, 249, 75, 256, 110, 275, 110, 259, 196, 259, 177, 33, 79, 33, 137, 271, 159, 5, 245, 253, 110, 252, 110, 253, 177, 64, 33, 256, 196, 269, 4, 271, 75, 77, 245, 350, 110, 253, 177, 33, 260, 196, 58, 245, 250, 75, 256, 110, 274, 196, 353, 177, 33, 275, 196, 109, 245, 260, 4, 264, 177, 43, 266, 33, 10, 245, 249, 75, 256, 110, 275, 177, 33, 79, 33, 57, 33, 3, 33]}, {"code": "def softmax_backward_kernel_non_inner(\n    out_ptr,\n    out_grad_ptr,\n    in_grad_ptr,\n    M,\n    N,\n    K,\n    TILE_N: tl.constexpr,\n    TILE_K: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    offsets_k = pid_k * TILE_K + tl.arange(0, TILE_K)\n\n    if ONE_TILE_PER_CTA:\n        offsets_n = tl.arange(0, TILE_N)\n        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k\n        mask = (offsets_n < N)[:, None] & (offsets_k < K)\n        out_tile = tl.load(out_ptr + offsets, mask=mask)\n        out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n        scale = tl.sum(out_tile * out_grad_tile, axis=0)\n        in_grad_tile = out_tile * (out_grad_tile - scale[None, :])\n        tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n    else:\n        offsets_n = tl.arange(0, TILE_N)\n        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k\n        scale = tl.zeros([TILE_N, TILE_K], dtype=tl.float32)\n        for _ in range(0, N, TILE_N):\n            mask = (offsets_n < N)[:, None] & (offsets_k < K)\n            out_tile = tl.load(out_ptr + offsets, mask=mask)\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            scale += out_tile * out_grad_tile\n            offsets_n += TILE_N\n            offsets += TILE_N * K\n        scale = tl.sum(scale, axis=0)\n\n        offsets_n = tl.arange(0, TILE_N)\n        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k\n        for _ in range(0, N, TILE_N):\n            mask = (offsets_n < N)[:, None] & (offsets_k < K)\n            out_tile = tl.load(out_ptr + offsets, mask=mask)\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            in_grad_tile = out_tile * (out_grad_tile - scale[None, :])\n            tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n            offsets_n += TILE_N\n            offsets += TILE_N * K", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 110, 254, 110, 255, 63, 6, 110, 256, 63, 6, 110, 257, 63, 6, 177, 63, 33, -1, 258, 196, 173, 245, 350, 177, 33, 259, 196, 173, 245, 351, 177, 33, 260, 196, 259, 246, 256, 75, 77, 245, 350, 110, 256, 177, 33, 184, 257, 63, 33, 261, 196, 77, 245, 350, 110, 255, 177, 33, 262, 196, 258, 246, 253, 246, 254, 75, 261, 232, 63, 110, 204, 28, 246, 254, 75, 260, 33, 263, 196, 245, 261, 1, 253, 177, 232, 63, 110, 204, 28, 175, 245, 260, 1, 254, 177, 33, 264, 196, 56, 245, 249, 75, 262, 110, 263, 196, 263, 177, 33, 265, 196, 56, 245, 250, 75, 262, 110, 263, 196, 263, 177, 33, 266, 196, 223, 245, 264, 246, 265, 110, 267, 196, 350, 177, 33, 268, 196, 264, 246, 245, 265, 4, 266, 232, 204, 110, 63, 28, 177, 33, 10, 245, 251, 75, 262, 110, 268, 110, 263, 196, 263, 177, 33, 187, 33, 31, 63, 33, 261, 196, 77, 245, 350, 110, 255, 177, 33, 262, 196, 258, 246, 253, 246, 254, 75, 261, 232, 63, 110, 204, 28, 246, 254, 75, 260, 33, 266, 196, 178, 245, 232, 255, 110, 256, 28, 110, 93, 196, 147, 177, 33, 137, 269, 159, 5, 245, 350, 110, 253, 110, 255, 177, 63, 33, 263, 196, 245, 261, 1, 253, 177, 232, 63, 110, 204, 28, 175, 245, 260, 1, 254, 177, 33, 264, 196, 56, 245, 249, 75, 262, 110, 263, 196, 263, 177, 33, 265, 196, 56, 245, 250, 75, 262, 110, 263, 196, 263, 177, 33, 266, 174, 264, 246, 265, 33, 261, 174, 255, 33, 262, 174, 255, 246, 254, 33, 79, 33, 266, 196, 223, 245, 266, 110, 267, 196, 350, 177, 33, 261, 196, 77, 245, 350, 110, 255, 177, 33, 262, 196, 258, 246, 253, 246, 254, 75, 261, 232, 63, 110, 204, 28, 246, 254, 75, 260, 33, 137, 269, 159, 5, 245, 350, 110, 253, 110, 255, 177, 63, 33, 263, 196, 245, 261, 1, 253, 177, 232, 63, 110, 204, 28, 175, 245, 260, 1, 254, 177, 33, 264, 196, 56, 245, 249, 75, 262, 110, 263, 196, 263, 177, 33, 265, 196, 56, 245, 250, 75, 262, 110, 263, 196, 263, 177, 33, 268, 196, 264, 246, 245, 265, 4, 266, 232, 204, 110, 63, 28, 177, 33, 10, 245, 251, 75, 262, 110, 268, 110, 263, 196, 263, 177, 33, 261, 174, 255, 33, 262, 174, 255, 246, 254, 33, 79, 33, 57, 33, 3, 33]}, {"code": "def softmax_backward_kernel_inner(\n    out_ptr,\n    out_grad_ptr,\n    in_grad_ptr,\n    M,\n    N,\n    TILE_M: tl.constexpr,\n    TILE_N: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    m_offsets = pid_m * TILE_M + tl.arange(0, TILE_M)\n    if ONE_TILE_PER_CTA:\n        n_offsets = tl.arange(0, TILE_N)\n        offsets = m_offsets[:, None] * N + n_offsets\n        mask = (m_offsets[:, None] < M) & (n_offsets < N)\n        out_tile = tl.load(out_ptr + offsets, mask=mask)\n        out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n        scale = tl.sum(out_tile * out_grad_tile, 1)\n        in_grad_tile = out_tile * (out_grad_tile - scale[:, None])\n        tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n    else:\n        scale = tl.zeros([TILE_M, TILE_N], dtype=tl.float32)\n\n        n_offsets = tl.arange(0, TILE_N)\n        offsets = m_offsets[:, None] * N + n_offsets\n        for _ in range(0, N, TILE_N):\n            mask = (m_offsets[:, None] < M) & (n_offsets < N)\n            out_tile = tl.load(\n                out_ptr + offsets, mask=mask, eviction_policy=\"evict_last\"\n            )\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            scale += out_tile * out_grad_tile\n            n_offsets += TILE_N\n            offsets += TILE_N\n        scale = tl.sum(scale, 1)\n\n        n_offsets = tl.arange(0, TILE_N)\n        offsets = m_offsets[:, None] * N + n_offsets\n        for _ in range(0, N, TILE_N):\n            mask = (m_offsets[:, None] < M) & (n_offsets < N)\n            out_tile = tl.load(\n                out_ptr + offsets, mask=mask, eviction_policy=\"evict_first\"\n            )\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            in_grad_tile = out_tile * (out_grad_tile - scale[:, None])\n            tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n            n_offsets += TILE_N\n            offsets += TILE_N", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 110, 254, 64, 6, 110, 255, 64, 6, 110, 256, 64, 6, 177, 64, 33, -1, 257, 196, 173, 245, 350, 177, 33, 258, 196, 257, 246, 254, 75, 77, 245, 350, 110, 254, 177, 33, 184, 256, 64, 33, 259, 196, 77, 245, 350, 110, 255, 177, 33, 260, 196, 258, 232, 64, 110, 204, 28, 246, 253, 75, 259, 33, 261, 196, 245, 258, 232, 64, 110, 204, 28, 1, 252, 177, 175, 245, 259, 1, 253, 177, 33, 262, 196, 58, 245, 249, 75, 260, 110, 261, 196, 261, 177, 33, 263, 196, 58, 245, 250, 75, 260, 110, 261, 196, 261, 177, 33, 264, 196, 223, 245, 262, 246, 263, 110, 351, 177, 33, 265, 196, 262, 246, 245, 263, 4, 264, 232, 64, 110, 204, 28, 177, 33, 10, 245, 251, 75, 260, 110, 265, 110, 261, 196, 261, 177, 33, 187, 33, 31, 64, 33, 264, 196, 178, 245, 232, 254, 110, 255, 28, 110, 93, 196, 147, 177, 33, 259, 196, 77, 245, 350, 110, 255, 177, 33, 260, 196, 258, 232, 64, 110, 204, 28, 246, 253, 75, 259, 33, 137, 266, 159, 5, 245, 350, 110, 253, 110, 255, 177, 64, 33, 261, 196, 245, 258, 232, 64, 110, 204, 28, 1, 252, 177, 175, 245, 259, 1, 253, 177, 33, 262, 196, 58, 245, 249, 75, 260, 110, 261, 196, 261, 110, 267, 196, 352, 177, 33, 263, 196, 58, 245, 250, 75, 260, 110, 261, 196, 261, 177, 33, 264, 174, 262, 246, 263, 33, 259, 174, 255, 33, 260, 174, 255, 33, 79, 33, 264, 196, 223, 245, 264, 110, 351, 177, 33, 259, 196, 77, 245, 350, 110, 255, 177, 33, 260, 196, 258, 232, 64, 110, 204, 28, 246, 253, 75, 259, 33, 137, 266, 159, 5, 245, 350, 110, 253, 110, 255, 177, 64, 33, 261, 196, 245, 258, 232, 64, 110, 204, 28, 1, 252, 177, 175, 245, 259, 1, 253, 177, 33, 262, 196, 58, 245, 249, 75, 260, 110, 261, 196, 261, 110, 267, 196, 353, 177, 33, 263, 196, 58, 245, 250, 75, 260, 110, 261, 196, 261, 177, 33, 265, 196, 262, 246, 245, 263, 4, 264, 232, 64, 110, 204, 28, 177, 33, 10, 245, 251, 75, 260, 110, 265, 110, 261, 196, 261, 177, 33, 259, 174, 255, 33, 260, 174, 255, 33, 79, 33, 57, 33, 3, 33]}, {"code": "def swizzle_tile(\n    tile_id,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n):\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n\n    width = GROUP_M * grid_n\n    group_id = tile_id // width\n    group_size = tl.minimum(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (tile_id % group_size)\n    pid_n = (tile_id % width) // group_size\n    return pid_m, pid_n", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 63, 6, 110, 254, 63, 6, 110, 255, 63, 6, 110, 256, 63, 6, 177, 63, 33, -1, 257, 196, 65, 245, 250, 110, 253, 177, 33, 258, 196, 65, 245, 251, 110, 254, 177, 33, 259, 196, 256, 246, 258, 33, 260, 196, 249, 49, 259, 33, 261, 196, 67, 245, 257, 4, 260, 246, 256, 110, 256, 177, 33, 262, 196, 260, 246, 256, 75, 249, 228, 261, 33, 263, 196, 249, 228, 259, 49, 261, 33, 233, 245, 262, 110, 263, 177, 33, 3, 33]}, {"code": "def linear_tile(\n    tile_id,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n):\n    pid_m = tile_id // tl.cdiv(N, BLOCK_N)\n    pid_n = tile_id % tl.cdiv(N, BLOCK_N)\n    return pid_m, pid_n", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 64, 6, 110, 254, 64, 6, 110, 255, 64, 6, 110, 256, 64, 6, 177, 64, 33, -1, 257, 196, 249, 49, 66, 245, 251, 110, 254, 177, 33, 258, 196, 249, 228, 66, 245, 251, 110, 254, 177, 33, 233, 245, 257, 110, 258, 177, 33, 3, 33]}, {"code": "def _swiglu_bwd_kernel(\n    X,\n    Y,\n    DOUT,\n    OUT,\n    DX,\n    DY,\n    stride_x_row,\n    stride_y_row,\n    stride_dout_row,\n    stride_out_row,\n    stride_dx_row,\n    stride_dy_row,\n    ncols,\n    BLOCK_N: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n):\n\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    DOUT += row * stride_dout_row\n    if RECOMPUTE_OUTPUT:\n        OUT += row * stride_out_row\n    DX += row * stride_dx_row\n    DY += row * stride_dy_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    dout = tl.load(DOUT + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    x_sigmoid = tl.sigmoid(x)\n    dx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout\n    dy = x * x_sigmoid * dout\n    tl.store(DX + cols, dx, mask=cols < ncols)\n    tl.store(DY + cols, dy, mask=cols < ncols)\n    if RECOMPUTE_OUTPUT:\n        out = x * x_sigmoid * y\n        tl.store(OUT + cols, out, mask=cols < ncols)", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 110, 254, 110, 255, 110, 256, 110, 257, 110, 258, 110, 259, 110, 260, 110, 261, 110, 262, 63, 6, 110, 263, 63, 6, 177, 63, 33, -1, 264, 196, 173, 245, 350, 177, 33, 265, 196, 173, 245, 351, 177, 246, 262, 33, 249, 174, 264, 246, 255, 33, 250, 174, 264, 246, 256, 33, 251, 174, 264, 246, 257, 33, 184, 263, 63, 33, 252, 174, 264, 246, 258, 33, 187, 33, 253, 174, 264, 246, 259, 33, 254, 174, 264, 246, 260, 33, 266, 196, 265, 75, 77, 245, 350, 110, 262, 177, 33, 267, 196, 56, 245, 249, 75, 266, 110, 268, 196, 266, 1, 261, 110, 269, 196, 350, 177, 83, 270, 245, 147, 177, 33, 271, 196, 56, 245, 250, 75, 266, 110, 268, 196, 266, 1, 261, 110, 269, 196, 350, 177, 83, 270, 245, 147, 177, 33, 272, 196, 56, 245, 251, 75, 266, 110, 268, 196, 266, 1, 261, 110, 269, 196, 350, 177, 83, 270, 245, 147, 177, 33, 273, 196, 208, 245, 267, 177, 33, 274, 196, 273, 246, 245, 351, 75, 267, 246, 245, 351, 4, 273, 177, 177, 246, 271, 246, 272, 33, 275, 196, 267, 246, 273, 246, 272, 33, 10, 245, 253, 75, 266, 110, 274, 110, 268, 196, 266, 1, 261, 177, 33, 10, 245, 254, 75, 266, 110, 275, 110, 268, 196, 266, 1, 261, 177, 33, 184, 263, 63, 33, 14, 196, 267, 246, 273, 246, 271, 33, 10, 245, 252, 75, 266, 110, 14, 110, 268, 196, 266, 1, 261, 177, 33, 187, 33, 3, 33]}, {"code": "def _swiglu_fwd_kernel(\n    X, Y, OUT, stride_x_row, stride_y_row, stride_out_row, ncols, BLOCK_N: tl.constexpr\n):\n\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    OUT += row * stride_out_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)\n    out = x * tl.sigmoid(x) * y\n    tl.store(OUT + cols, out, mask=cols < ncols)", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 110, 254, 110, 255, 110, 256, 64, 6, 177, 64, 33, -1, 257, 196, 173, 245, 350, 177, 33, 258, 196, 173, 245, 351, 177, 246, 256, 33, 249, 174, 257, 246, 252, 33, 250, 174, 257, 246, 253, 33, 251, 174, 257, 246, 254, 33, 259, 196, 258, 75, 77, 245, 350, 110, 256, 177, 33, 260, 196, 58, 245, 249, 75, 259, 110, 261, 196, 259, 1, 255, 110, 262, 196, 350, 177, 83, 263, 245, 147, 177, 33, 264, 196, 58, 245, 250, 75, 259, 110, 261, 196, 259, 1, 255, 110, 262, 196, 350, 177, 83, 263, 245, 147, 177, 33, 14, 196, 260, 246, 208, 245, 260, 177, 246, 264, 33, 10, 245, 251, 75, 259, 110, 14, 110, 261, 196, 259, 1, 255, 177, 33, 3, 33]}, {"code": "def update_fn_kernel(\n    p_ptr,\n    grad_ptr,\n    exp_avg_ptr,\n    lr,\n    wd,\n    beta1,\n    beta2,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    mask = offsets < n_elements\n\n    offset_p_ptr = p_ptr + offsets\n    offset_grad_ptr = grad_ptr + offsets\n    offset_exp_avg_ptr = exp_avg_ptr + offsets\n\n    p = tl.load(offset_p_ptr, mask=mask)\n    grad = tl.load(offset_grad_ptr, mask=mask)\n    exp_avg = tl.load(offset_exp_avg_ptr, mask=mask)\n\n    p = p * (1 - lr * wd)\n\n    diff = exp_avg - grad\n\n    update = diff * beta1 + grad\n\n    can_update = update != 0\n    update_sign = tl.where(update > 0, -lr, lr)\n\n    p = p + update_sign * can_update\n\n    exp_avg = diff * beta2 + grad\n\n    tl.store(offset_p_ptr, p, mask=mask)\n    tl.store(offset_exp_avg_ptr, exp_avg, mask=mask)", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 110, 254, 110, 255, 110, 256, 110, 257, 63, 6, 177, 63, 33, -1, 258, 196, 173, 245, 259, 196, 350, 177, 33, 260, 196, 258, 246, 257, 33, 261, 196, 260, 75, 77, 245, 350, 110, 257, 177, 33, 262, 196, 261, 1, 256, 33, 263, 196, 249, 75, 261, 33, 264, 196, 250, 75, 261, 33, 265, 196, 251, 75, 261, 33, 266, 196, 56, 245, 263, 110, 262, 196, 262, 177, 33, 105, 196, 56, 245, 264, 110, 262, 196, 262, 177, 33, 267, 196, 56, 245, 265, 110, 262, 196, 262, 177, 33, 266, 196, 266, 246, 245, 351, 4, 252, 246, 253, 177, 33, 268, 196, 267, 4, 105, 33, 269, 196, 268, 246, 254, 75, 105, 33, 270, 196, 269, 188, 350, 33, 271, 196, 207, 245, 269, 126, 350, 110, 4, 252, 110, 252, 177, 33, 266, 196, 266, 75, 271, 246, 270, 33, 267, 196, 268, 246, 255, 75, 105, 33, 10, 245, 263, 110, 266, 110, 262, 196, 262, 177, 33, 10, 245, 265, 110, 267, 110, 262, 196, 262, 177, 33, 3, 33]}, {"code": "def kernel_fma(\n    C,\n    ACT_INPUTS,\n    A,\n    B,\n    bias,\n    M,\n    N,\n    K,\n    CACHE_KEY_M,\n    CACHE_KEY_N,\n    CACHE_KEY_K,\n    output_m_stride,\n    output_n_stride,\n    act_inputs_m_stride,\n    act_inputs_n_stride,\n    a_m_stride,\n    a_k_stride,\n    b_n_stride,\n    b_k_stride,\n    BLOCK_M: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    SPLIT_K: tl.constexpr,\n    K_LOAD_MASK_NEEDED: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    SHOULD_SAVE_ACT_INPUTS: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n):\n\n    program_idx = tl.program_id(axis=0)\n\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n\n    width = GROUP_M * grid_n\n    group_idx = program_idx // width\n    group_size = min(grid_m - group_idx * GROUP_M, GROUP_M)\n    block_m_idx = group_idx * GROUP_M + (program_idx % group_size)\n    block_n_idx = (program_idx % width) // group_size\n\n    m_offs_untagged = block_m_idx * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offs_untagged = block_n_idx * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    m_offs = tl.max_contiguous(tl.multiple_of(m_offs_untagged % M, BLOCK_M), BLOCK_M)\n    n_offs = tl.max_contiguous(tl.multiple_of(n_offs_untagged % N, BLOCK_N), BLOCK_N)\n\n    k_range_offs = tl.arange(0, BLOCK_K)\n\n    A = A + (m_offs[:, None] * a_m_stride + k_range_offs[None, :] * a_k_stride)\n    B = B + (k_range_offs[:, None] * b_k_stride + n_offs[None, :] * b_n_stride)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    if HAS_BIAS:\n        bias = tl.load(bias + n_offs, mask=n_offs < N, other=0.0).to(tl.float32)\n        acc += bias[None, :]\n\n    for k in range(K, 0, -BLOCK_K):\n        if K_LOAD_MASK_NEEDED:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=k_range_offs[None, :] < k, other=0.0)\n            b = tl.load(B, mask=k_range_offs[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n\n        A += BLOCK_K * a_k_stride\n        B += BLOCK_K * b_k_stride\n\n    if SHOULD_SAVE_ACT_INPUTS:\n        act_in_ptrs = (\n            ACT_INPUTS\n            + m_offs[:, None] * act_inputs_m_stride\n            + n_offs[None, :] * act_inputs_n_stride\n        )\n        tl.store(act_in_ptrs, acc)\n\n    if ACTIVATION == \"tanh\":\n        acc = tanh(acc)\n    if ACTIVATION == \"gelu\":\n        acc = gelu(acc)\n    if ACTIVATION == \"fast_gelu\":\n        acc = fast_gelu(acc)\n    if ACTIVATION == \"relu\":\n        acc = relu(acc)\n\n    C = C + m_offs[:, None] * output_m_stride + n_offs[None, :] * output_n_stride\n    c_ptr_mask = (m_offs < M)[:, None] & (n_offs < N)[None, :]\n    tl.store(C, acc, mask=c_ptr_mask)", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 110, 254, 110, 255, 110, 256, 110, 257, 110, 258, 110, 259, 110, 260, 110, 261, 110, 262, 110, 263, 110, 264, 110, 265, 110, 266, 110, 267, 110, 268, 64, 6, 110, 269, 64, 6, 110, 270, 64, 6, 110, 271, 64, 6, 110, 272, 64, 6, 110, 273, 64, 6, 110, 274, 64, 6, 110, 275, 64, 6, 110, 276, 64, 6, 177, 64, 33, -1, 277, 196, 173, 245, 278, 196, 350, 177, 33, 279, 196, 245, 254, 75, 268, 4, 351, 177, 49, 268, 33, 280, 196, 245, 255, 75, 270, 4, 351, 177, 49, 270, 33, 281, 196, 269, 246, 280, 33, 282, 196, 277, 49, 281, 33, 283, 196, 40, 245, 279, 4, 282, 246, 269, 110, 269, 177, 33, 284, 196, 282, 246, 269, 75, 277, 228, 283, 33, 285, 196, 277, 228, 281, 49, 283, 33, 286, 196, 284, 246, 268, 75, 77, 245, 350, 110, 268, 177, 33, 287, 196, 285, 246, 270, 75, 77, 245, 350, 110, 270, 177, 33, 288, 196, 213, 245, 56, 245, 286, 228, 254, 110, 268, 177, 110, 268, 177, 33, 289, 196, 213, 245, 56, 245, 287, 228, 255, 110, 270, 177, 110, 270, 177, 33, 290, 196, 77, 245, 350, 110, 271, 177, 33, 251, 196, 251, 75, 245, 288, 232, 64, 110, 204, 28, 246, 264, 75, 290, 232, 204, 110, 64, 28, 246, 265, 177, 33, 252, 196, 252, 75, 245, 290, 232, 64, 110, 204, 28, 246, 267, 75, 289, 232, 204, 110, 64, 28, 246, 266, 177, 33, 291, 196, 178, 245, 245, 268, 110, 270, 177, 110, 93, 196, 147, 177, 33, 184, 274, 64, 33, 253, 196, 58, 245, 253, 75, 289, 110, 292, 196, 289, 1, 255, 110, 293, 196, 350, 177, 83, 294, 245, 147, 177, 33, 291, 174, 253, 232, 204, 110, 64, 28, 33, 187, 33, 137, 295, 159, 5, 245, 256, 110, 350, 110, 4, 271, 177, 64, 33, 184, 273, 64, 33, 296, 196, 58, 245, 251, 177, 33, 297, 196, 58, 245, 252, 177, 33, 187, 33, 31, 64, 33, 296, 196, 58, 245, 251, 110, 292, 196, 290, 232, 204, 110, 64, 28, 1, 295, 110, 293, 196, 350, 177, 33, 297, 196, 58, 245, 252, 110, 292, 196, 290, 232, 64, 110, 204, 28, 1, 295, 110, 293, 196, 350, 177, 33, 57, 33, 291, 174, 15, 245, 296, 110, 297, 177, 33, 251, 174, 271, 246, 265, 33, 252, 174, 271, 246, 267, 33, 79, 33, 184, 275, 64, 33, 298, 196, 250, 75, 288, 232, 64, 110, 204, 28, 246, 262, 75, 289, 232, 204, 110, 64, 28, 246, 263, 33, 10, 245, 298, 110, 291, 177, 33, 187, 33, 184, 276, 78, 352, 64, 33, 291, 196, 299, 245, 291, 177, 33, 187, 33, 184, 276, 78, 353, 64, 33, 291, 196, 300, 245, 291, 177, 33, 187, 33, 184, 276, 78, 354, 64, 33, 291, 196, 301, 245, 291, 177, 33, 187, 33, 184, 276, 78, 355, 64, 33, 291, 196, 302, 245, 291, 177, 33, 187, 33, 249, 196, 249, 75, 288, 232, 64, 110, 204, 28, 246, 260, 75, 289, 232, 204, 110, 64, 28, 246, 261, 33, 303, 196, 245, 288, 1, 254, 177, 232, 64, 110, 204, 28, 175, 245, 289, 1, 255, 177, 232, 204, 110, 64, 28, 33, 10, 245, 249, 110, 291, 110, 292, 196, 303, 177, 33, 3, 33]}, {"code": "def matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n\n    offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n    offs_am = tl.where(offs_am < M, offs_am, 0)\n    offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if c_ptr.dtype.element_ty == tl.float8e4nv:\n        c = accumulator.to(tl.float8e4nv)\n    else:\n        c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)", "encoded": [30, 349, 245, 249, 110, 250, 110, 251, 110, 252, 110, 253, 110, 254, 110, 255, 110, 256, 110, 257, 110, 258, 110, 259, 110, 260, 110, 261, 63, 6, 110, 262, 63, 6, 110, 263, 63, 6, 110, 264, 63, 6, 177, 63, 33, -1, 265, 196, 173, 245, 266, 196, 350, 177, 33, 267, 196, 65, 245, 252, 110, 261, 177, 33, 268, 196, 65, 245, 253, 110, 262, 177, 33, 269, 196, 264, 246, 268, 33, 270, 196, 265, 49, 269, 33, 271, 196, 270, 246, 264, 33, 272, 196, 40, 245, 267, 4, 271, 110, 264, 177, 33, 273, 196, 271, 75, 265, 228, 272, 33, 274, 196, 265, 228, 269, 49, 272, 33, 275, 196, 273, 246, 261, 33, 276, 196, 274, 246, 262, 33, 277, 196, 275, 75, 77, 245, 350, 110, 261, 177, 33, 278, 196, 276, 75, 77, 245, 350, 110, 262, 177, 33, 277, 196, 207, 245, 277, 1, 252, 110, 277, 110, 350, 177, 33, 278, 196, 207, 245, 278, 1, 253, 110, 278, 110, 350, 177, 33, 277, 196, 213, 245, 58, 245, 277, 110, 261, 177, 110, 261, 177, 33, 278, 196, 213, 245, 58, 245, 278, 110, 262, 177, 110, 262, 177, 33, 279, 196, 77, 245, 350, 110, 263, 177, 33, 280, 196, 249, 75, 245, 277, 232, 63, 110, 204, 28, 246, 255, 75, 279, 232, 204, 110, 63, 28, 246, 256, 177, 33, 281, 196, 250, 75, 245, 279, 232, 63, 110, 204, 28, 246, 257, 75, 278, 232, 204, 110, 63, 28, 246, 258, 177, 33, 282, 196, 178, 245, 245, 261, 110, 262, 177, 110, 93, 196, 147, 177, 33, 137, 283, 159, 5, 245, 350, 110, 65, 245, 254, 110, 263, 177, 177, 63, 33, 284, 196, 56, 245, 280, 110, 285, 196, 279, 232, 204, 110, 63, 28, 1, 254, 4, 283, 246, 263, 110, 286, 196, 350, 177, 33, 287, 196, 56, 245, 281, 110, 285, 196, 279, 232, 63, 110, 204, 28, 1, 254, 4, 283, 246, 263, 110, 286, 196, 350, 177, 33, 282, 196, 15, 245, 284, 110, 287, 110, 282, 177, 33, 280, 174, 263, 246, 256, 33, 281, 174, 263, 246, 257, 33, 79, 33, 184, 251, 83, 93, 83, 116, 78, 224, 63, 33, 288, 196, 282, 83, 289, 245, 224, 177, 33, 187, 33, 31, 63, 33, 288, 196, 282, 83, 289, 245, 22, 177, 33, 57, 33, 290, 196, 273, 246, 261, 75, 77, 245, 350, 110, 261, 177, 33, 291, 196, 274, 246, 262, 75, 77, 245, 350, 110, 262, 177, 33, 292, 196, 251, 75, 259, 246, 290, 232, 63, 110, 204, 28, 75, 260, 246, 291, 232, 204, 110, 63, 28, 33, 293, 196, 245, 290, 232, 63, 110, 204, 28, 1, 252, 177, 175, 245, 291, 232, 204, 110, 63, 28, 1, 253, 177, 33, 10, 245, 292, 110, 288, 110, 285, 196, 293, 177, 33, 3, 33]}, {"code": "def uniform_kernel(\n    out_ptr,\n    N,\n    philox_seed,\n    philox_offset,\n    from_,\n    to,\n    BLOCK: tl.constexpr,\n):\n    philox_seed = philox_seed.to(tl.int64)\n    philox_offset = philox_offset.to(tl.int64)\n    c0 = (philox_offset & 0xFFFFFFFF).to(tl.uint32)\n    c1 = ((philox_offset >> 32) & 0xFFFFFFFF).to(tl.uint32)\n    i4 = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n    c0 += i4\n    _O = c0 * 0\n    r0, r1, r2, r3 = tl.philox(philox_seed, c0, c1, _O, _O)\n    r0 = uint_to_uniform_float(r0) * (to - from_) + from_\n    r1 = uint_to_uniform_float(r1) * (to - from_) + from_\n    r2 = uint_to_uniform_float(r2) * (to - from_) + from_\n    r3 = uint_to_uniform_float(r3) * (to - from_) + from_\n    off_0 = tl.program_id(0) * BLOCK * 4 + tl.arange(0, BLOCK)\n    off_1 = off_0 + BLOCK\n    off_2 = off_1 + BLOCK\n    off_3 = off_2 + BLOCK\n    tl.store(out_ptr + off_0, r0, mask=off_0 < N, eviction_policy=\"evict_first\")\n    tl.store(out_ptr + off_1, r1, mask=off_1 < N, eviction_policy=\"evict_first\")\n    tl.store(out_ptr + off_2, r2, mask=off_2 < N, eviction_policy=\"evict_first\")\n    tl.store(out_ptr + off_3, r3, mask=off_3 < N, eviction_policy=\"evict_first\")", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 65, 6, 178, 65, 34, -1, 252, 197, 252, 84, 255, 246, 184, 178, 34, 253, 197, 253, 84, 255, 246, 184, 178, 34, 257, 197, 246, 253, 176, 351, 352, 353, 351, 353, 354, 355, 352, 353, 356, 178, 84, 255, 246, 168, 178, 34, 258, 197, 246, 253, 130, 357, 352, 176, 351, 352, 353, 351, 353, 354, 355, 352, 353, 356, 178, 84, 255, 246, 168, 178, 34, 259, 197, 174, 246, 358, 178, 247, 256, 76, 78, 246, 358, 111, 256, 178, 34, 257, 175, 259, 34, 260, 197, 257, 247, 358, 34, 261, 111, 262, 111, 263, 111, 264, 197, 17, 246, 252, 111, 257, 111, 258, 111, 260, 111, 260, 178, 34, 261, 197, 265, 246, 261, 178, 247, 246, 255, 4, 254, 178, 76, 254, 34, 262, 197, 265, 246, 262, 178, 247, 246, 255, 4, 254, 178, 76, 254, 34, 263, 197, 265, 246, 263, 178, 247, 246, 255, 4, 254, 178, 76, 254, 34, 264, 197, 265, 246, 264, 178, 247, 246, 255, 4, 254, 178, 76, 254, 34, 266, 197, 174, 246, 358, 178, 247, 256, 247, 351, 76, 78, 246, 358, 111, 256, 178, 34, 267, 197, 266, 76, 256, 34, 268, 197, 267, 76, 256, 34, 269, 197, 268, 76, 256, 34, 10, 246, 250, 76, 266, 111, 261, 111, 270, 197, 266, 1, 251, 111, 271, 197, 359, 178, 34, 10, 246, 250, 76, 267, 111, 262, 111, 270, 197, 267, 1, 251, 111, 271, 197, 359, 178, 34, 10, 246, 250, 76, 268, 111, 263, 111, 270, 197, 268, 1, 251, 111, 271, 197, 359, 178, 34, 10, 246, 250, 76, 269, 111, 264, 111, 270, 197, 269, 1, 251, 111, 271, 197, 359, 178, 34, 3, 34]}, {"code": "def fused_fwd_kernel_s_km(\n    K,\n    V,\n    S,\n    KM,\n    stride_qk_bh,\n    stride_qk_l,\n    stride_qk_d,\n    stride_vo_bh,\n    stride_vo_l,\n    stride_vo_d,\n    stride_s_bh,\n    stride_s_dk,\n    stride_s_dv,\n    stride_km_bh,\n    scale,\n    L: tl.constexpr,\n    DK: tl.constexpr,\n    DV: tl.constexpr,\n    BL: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n\n    start_v, start_k, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    K_block_ptr = tl.make_block_ptr(\n        base=K + off_bs_head * stride_qk_bh,\n        shape=(DK, L),\n        strides=(stride_qk_d, stride_qk_l),\n        offsets=(start_k * BK, 0),\n        block_shape=(BK, BL),\n        order=(0, 1),\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + off_bs_head * stride_vo_bh,\n        shape=(L, DV),\n        strides=(stride_vo_l, stride_vo_d),\n        offsets=(0, start_v * BV),\n        block_shape=(BL, BV),\n        order=(1, 0),\n    )\n\n    s = tl.zeros([BK, BV], dtype=tl.float32)\n    km = tl.zeros([BK], dtype=tl.float32)\n\n    for _ in range(0, L, BL):\n        k = tl.load(K_block_ptr, boundary_check=(0, 1))\n        v = tl.load(V_block_ptr, boundary_check=(0, 1))\n\n        v = (v * scale).to(v.dtype)\n        s += tl.dot(k, v, allow_tf32=False)\n        km += tl.sum(k, axis=1) / L\n\n        K_block_ptr = tl.advance(K_block_ptr, (0, BL))\n        V_block_ptr = tl.advance(V_block_ptr, (BL, 0))\n\n    S_block_ptr = tl.make_block_ptr(\n        base=S + off_bs_head * stride_s_bh,\n        shape=(DK, DV),\n        strides=(stride_s_dk, stride_s_dv),\n        offsets=(start_k * BK, start_v * BV),\n        block_shape=(BK, BV),\n        order=(1, 0),\n    )\n    tl.store(S_block_ptr, s.to(S.dtype.element_ty), boundary_check=(0, 1))\n\n    KM_block_ptr = KM + off_bs_head * stride_km_bh + start_k * BK + tl.arange(0, BK)\n    tl.store(\n        KM_block_ptr,\n        km.to(KM.dtype.element_ty),\n        mask=((start_k * BK + tl.arange(0, BK)) < DK),\n    )", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 111, 263, 111, 264, 111, 265, 64, 6, 111, 266, 64, 6, 111, 267, 64, 6, 111, 268, 64, 6, 111, 269, 64, 6, 111, 270, 64, 6, 178, 64, 34, -1, 271, 111, 272, 111, 273, 197, 246, 174, 246, 351, 178, 111, 174, 246, 352, 178, 111, 174, 246, 353, 178, 178, 34, 274, 197, 222, 246, 275, 197, 250, 76, 273, 247, 254, 111, 126, 197, 246, 266, 111, 265, 178, 111, 276, 197, 246, 256, 111, 255, 178, 111, 277, 197, 246, 272, 247, 269, 111, 351, 178, 111, 278, 197, 246, 269, 111, 268, 178, 111, 279, 197, 246, 351, 111, 352, 178, 178, 34, 280, 197, 222, 246, 275, 197, 251, 76, 273, 247, 257, 111, 126, 197, 246, 265, 111, 267, 178, 111, 276, 197, 246, 258, 111, 259, 178, 111, 277, 197, 246, 351, 111, 271, 247, 270, 178, 111, 278, 197, 246, 268, 111, 270, 178, 111, 279, 197, 246, 352, 111, 351, 178, 178, 34, 281, 197, 179, 246, 233, 269, 111, 270, 29, 111, 94, 197, 148, 178, 34, 282, 197, 179, 246, 233, 269, 29, 111, 94, 197, 148, 178, 34, 138, 283, 160, 5, 246, 351, 111, 265, 111, 268, 178, 64, 34, 284, 197, 57, 246, 274, 111, 285, 197, 246, 351, 111, 352, 178, 178, 34, 286, 197, 57, 246, 280, 111, 285, 197, 246, 351, 111, 352, 178, 178, 34, 286, 197, 246, 286, 247, 264, 178, 84, 287, 246, 286, 84, 94, 178, 34, 281, 175, 15, 246, 284, 111, 286, 111, 288, 197, 61, 178, 34, 282, 175, 224, 246, 284, 111, 289, 197, 352, 178, 44, 265, 34, 274, 197, 146, 246, 274, 111, 246, 351, 111, 268, 178, 178, 34, 280, 197, 146, 246, 280, 111, 246, 268, 111, 351, 178, 178, 34, 80, 34, 290, 197, 222, 246, 275, 197, 252, 76, 273, 247, 260, 111, 126, 197, 246, 266, 111, 267, 178, 111, 276, 197, 246, 261, 111, 262, 178, 111, 277, 197, 246, 272, 247, 269, 111, 271, 247, 270, 178, 111, 278, 197, 246, 269, 111, 270, 178, 111, 279, 197, 246, 352, 111, 351, 178, 178, 34, 10, 246, 290, 111, 281, 84, 287, 246, 252, 84, 94, 84, 117, 178, 111, 285, 197, 246, 351, 111, 352, 178, 178, 34, 291, 197, 253, 76, 273, 247, 263, 76, 272, 247, 269, 76, 78, 246, 351, 111, 269, 178, 34, 10, 246, 291, 111, 282, 84, 287, 246, 253, 84, 94, 84, 117, 178, 111, 292, 197, 272, 247, 269, 76, 78, 246, 351, 111, 269, 178, 1, 266, 178, 34, 3, 34]}, {"code": "def fused_fwd_kernel_o(\n    Q,\n    S,\n    O,\n    KM,\n    stride_qk_bh,\n    stride_qk_l,\n    stride_qk_d,\n    stride_vo_bh,\n    stride_vo_l,\n    stride_vo_d,\n    stride_s_bh,\n    stride_s_dk,\n    stride_s_dv,\n    stride_km_bh,\n    eps,\n    L: tl.constexpr,\n    DK: tl.constexpr,\n    DV: tl.constexpr,\n    BL: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n\n    start_v, start_l, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + off_bs_head * stride_qk_bh,\n        shape=(L, DK),\n        strides=(stride_qk_l, stride_qk_d),\n        offsets=(start_l * BL, 0),\n        block_shape=(BL, BK),\n        order=(1, 0),\n    )\n    S_block_ptr = tl.make_block_ptr(\n        base=S + off_bs_head * stride_s_bh,\n        shape=(DK, DV),\n        strides=(stride_s_dk, stride_s_dv),\n        offsets=(0, start_v * BV),\n        block_shape=(BK, BV),\n        order=(1, 0),\n    )\n    KM_block_ptr = KM + off_bs_head * stride_km_bh + tl.arange(0, BK)\n\n    o = tl.zeros([BL, BV], dtype=tl.float32)\n    z = tl.zeros([BL], dtype=tl.float32)\n\n    for offset_k in range(0, DK, BK):\n        q = tl.load(Q_block_ptr, boundary_check=(0, 1))\n        s = tl.load(S_block_ptr, boundary_check=(0, 1))\n        km = tl.load(KM_block_ptr, mask=((offset_k + tl.arange(0, BK)) < DK))\n\n        z += tl.sum(q * km[None, :], axis=1, keep_dims=False)\n        o += tl.dot(q, s, allow_tf32=False)\n\n        Q_block_ptr = tl.advance(Q_block_ptr, (0, BK))\n        S_block_ptr = tl.advance(S_block_ptr, (BK, 0))\n        KM_block_ptr = KM_block_ptr + tl.arange(0, BK)\n\n    o = o / (z[:, None] + eps)\n\n    O_block_ptr = tl.make_block_ptr(\n        base=O + off_bs_head * stride_vo_bh,\n        shape=(L, DV),\n        strides=(stride_vo_l, stride_vo_d),\n        offsets=(start_l * BL, start_v * BV),\n        block_shape=(BL, BV),\n        order=(1, 0),\n    )\n    tl.store(O_block_ptr, o.to(O.dtype.element_ty), boundary_check=(0, 1))", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 111, 263, 111, 264, 111, 265, 65, 6, 111, 266, 65, 6, 111, 267, 65, 6, 111, 268, 65, 6, 111, 269, 65, 6, 111, 270, 65, 6, 178, 65, 34, -1, 271, 111, 272, 111, 273, 197, 246, 174, 246, 351, 178, 111, 174, 246, 352, 178, 111, 174, 246, 353, 178, 178, 34, 274, 197, 222, 246, 275, 197, 250, 76, 273, 247, 254, 111, 126, 197, 246, 265, 111, 266, 178, 111, 276, 197, 246, 255, 111, 256, 178, 111, 277, 197, 246, 272, 247, 268, 111, 351, 178, 111, 278, 197, 246, 268, 111, 269, 178, 111, 279, 197, 246, 352, 111, 351, 178, 178, 34, 280, 197, 222, 246, 275, 197, 251, 76, 273, 247, 260, 111, 126, 197, 246, 266, 111, 267, 178, 111, 276, 197, 246, 261, 111, 262, 178, 111, 277, 197, 246, 351, 111, 271, 247, 270, 178, 111, 278, 197, 246, 269, 111, 270, 178, 111, 279, 197, 246, 352, 111, 351, 178, 178, 34, 281, 197, 253, 76, 273, 247, 263, 76, 78, 246, 351, 111, 269, 178, 34, 282, 197, 179, 246, 233, 268, 111, 270, 29, 111, 94, 197, 148, 178, 34, 283, 197, 179, 246, 233, 268, 29, 111, 94, 197, 148, 178, 34, 138, 284, 160, 5, 246, 351, 111, 266, 111, 269, 178, 65, 34, 285, 197, 59, 246, 274, 111, 286, 197, 246, 351, 111, 352, 178, 178, 34, 287, 197, 59, 246, 280, 111, 286, 197, 246, 351, 111, 352, 178, 178, 34, 288, 197, 59, 246, 281, 111, 289, 197, 284, 76, 78, 246, 351, 111, 269, 178, 1, 266, 178, 34, 283, 175, 224, 246, 285, 247, 288, 233, 205, 111, 65, 29, 111, 290, 197, 352, 111, 291, 197, 61, 178, 34, 282, 175, 15, 246, 285, 111, 287, 111, 292, 197, 61, 178, 34, 274, 197, 146, 246, 274, 111, 246, 351, 111, 269, 178, 178, 34, 280, 197, 146, 246, 280, 111, 246, 269, 111, 351, 178, 178, 34, 281, 197, 281, 76, 78, 246, 351, 111, 269, 178, 34, 80, 34, 282, 197, 282, 44, 246, 283, 233, 65, 111, 205, 29, 76, 264, 178, 34, 293, 197, 222, 246, 275, 197, 252, 76, 273, 247, 257, 111, 126, 197, 246, 265, 111, 267, 178, 111, 276, 197, 246, 258, 111, 259, 178, 111, 277, 197, 246, 272, 247, 268, 111, 271, 247, 270, 178, 111, 278, 197, 246, 268, 111, 270, 178, 111, 279, 197, 246, 352, 111, 351, 178, 178, 34, 10, 246, 293, 111, 282, 84, 294, 246, 252, 84, 94, 84, 117, 178, 111, 286, 197, 246, 351, 111, 352, 178, 178, 34, 3, 34]}, {"code": "def _fwd_kv_kernel(\n    K,\n    V,\n    S,\n    stride_qk_bh,\n    stride_qk_l,\n    stride_qk_d,\n    stride_vo_bh,\n    stride_vo_l,\n    stride_vo_d,\n    stride_s_bh,\n    stride_s_dk,\n    stride_s_dv,\n    scale,\n    L: tl.constexpr,\n    DK: tl.constexpr,\n    DV: tl.constexpr,\n    BL: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n    start_v, start_k, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    K_block_ptr = tl.make_block_ptr(\n        base=K + off_bs_head * stride_qk_bh,\n        shape=(DK, L),\n        strides=(stride_qk_d, stride_qk_l),\n        offsets=(start_k * BK, 0),\n        block_shape=(BK, BL),\n        order=(0, 1),\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + off_bs_head * stride_vo_bh,\n        shape=(L, DV),\n        strides=(stride_vo_l, stride_vo_d),\n        offsets=(0, start_v * BV),\n        block_shape=(BL, BV),\n        order=(1, 0),\n    )\n\n    s = tl.zeros([BK, BV], dtype=tl.float32)\n\n    for _ in range(0, L, BL):\n        k = tl.load(K_block_ptr, boundary_check=(0, 1))\n        v = tl.load(V_block_ptr, boundary_check=(0, 1))\n\n        v = (v * scale).to(v.dtype)\n        s += tl.dot(k, v, allow_tf32=False)\n\n        K_block_ptr = tl.advance(K_block_ptr, (0, BL))\n        V_block_ptr = tl.advance(V_block_ptr, (BL, 0))\n\n    S_block_ptr = tl.make_block_ptr(\n        base=S + off_bs_head * stride_s_bh,\n        shape=(DK, DV),\n        strides=(stride_s_dk, stride_s_dv),\n        offsets=(start_k * BK, start_v * BV),\n        block_shape=(BK, BV),\n        order=(1, 0),\n    )\n    tl.store(S_block_ptr, s.to(S.dtype.element_ty), boundary_check=(0, 1))", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 111, 263, 64, 6, 111, 264, 64, 6, 111, 265, 64, 6, 111, 266, 64, 6, 111, 267, 64, 6, 111, 268, 64, 6, 178, 64, 34, -1, 269, 111, 270, 111, 271, 197, 246, 174, 246, 351, 178, 111, 174, 246, 352, 178, 111, 174, 246, 353, 178, 178, 34, 272, 197, 222, 246, 273, 197, 250, 76, 271, 247, 253, 111, 126, 197, 246, 264, 111, 263, 178, 111, 274, 197, 246, 255, 111, 254, 178, 111, 275, 197, 246, 270, 247, 267, 111, 351, 178, 111, 276, 197, 246, 267, 111, 266, 178, 111, 277, 197, 246, 351, 111, 352, 178, 178, 34, 278, 197, 222, 246, 273, 197, 251, 76, 271, 247, 256, 111, 126, 197, 246, 263, 111, 265, 178, 111, 274, 197, 246, 257, 111, 258, 178, 111, 275, 197, 246, 351, 111, 269, 247, 268, 178, 111, 276, 197, 246, 266, 111, 268, 178, 111, 277, 197, 246, 352, 111, 351, 178, 178, 34, 279, 197, 179, 246, 233, 267, 111, 268, 29, 111, 94, 197, 148, 178, 34, 138, 280, 160, 5, 246, 351, 111, 263, 111, 266, 178, 64, 34, 281, 197, 57, 246, 272, 111, 282, 197, 246, 351, 111, 352, 178, 178, 34, 283, 197, 57, 246, 278, 111, 282, 197, 246, 351, 111, 352, 178, 178, 34, 283, 197, 246, 283, 247, 262, 178, 84, 284, 246, 283, 84, 94, 178, 34, 279, 175, 15, 246, 281, 111, 283, 111, 285, 197, 61, 178, 34, 272, 197, 146, 246, 272, 111, 246, 351, 111, 266, 178, 178, 34, 278, 197, 146, 246, 278, 111, 246, 266, 111, 351, 178, 178, 34, 80, 34, 286, 197, 222, 246, 273, 197, 252, 76, 271, 247, 259, 111, 126, 197, 246, 264, 111, 265, 178, 111, 274, 197, 246, 260, 111, 261, 178, 111, 275, 197, 246, 270, 247, 267, 111, 269, 247, 268, 178, 111, 276, 197, 246, 267, 111, 268, 178, 111, 277, 197, 246, 352, 111, 351, 178, 178, 34, 10, 246, 286, 111, 279, 84, 284, 246, 252, 84, 94, 84, 117, 178, 111, 282, 197, 246, 351, 111, 352, 178, 178, 34, 3, 34]}, {"code": "def _fwd_qs_kernel(\n    Q,\n    S,\n    O,\n    stride_qk_bh,\n    stride_qk_l,\n    stride_qk_d,\n    stride_vo_bh,\n    stride_vo_l,\n    stride_vo_d,\n    stride_s_bh,\n    stride_s_dk,\n    stride_s_dv,\n    L: tl.constexpr,\n    DK: tl.constexpr,\n    DV: tl.constexpr,\n    BL: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n    start_v, start_m, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + off_bs_head * stride_qk_bh,\n        shape=(L, DK),\n        strides=(stride_qk_l, stride_qk_d),\n        offsets=(start_m * BL, 0),\n        block_shape=(BL, BK),\n        order=(1, 0),\n    )\n    S_block_ptr = tl.make_block_ptr(\n        base=S + off_bs_head * stride_s_bh,\n        shape=(DK, DV),\n        strides=(stride_s_dk, stride_s_dv),\n        offsets=(0, start_v * BV),\n        block_shape=(BK, BV),\n        order=(1, 0),\n    )\n\n    o = tl.zeros([BL, BV], dtype=tl.float32)\n\n    for _ in range(0, DK, BK):\n        q = tl.load(Q_block_ptr, boundary_check=(0, 1))\n        s = tl.load(S_block_ptr, boundary_check=(0, 1))\n\n        o += tl.dot(q, s, allow_tf32=False)\n\n        Q_block_ptr = tl.advance(Q_block_ptr, (0, BK))\n        S_block_ptr = tl.advance(S_block_ptr, (BK, 0))\n\n    O_block_ptr = tl.make_block_ptr(\n        base=O + off_bs_head * stride_vo_bh,\n        shape=(L, DV),\n        strides=(stride_vo_l, stride_vo_d),\n        offsets=(start_m * BL, start_v * BV),\n        block_shape=(BL, BV),\n        order=(1, 0),\n    )\n    tl.store(O_block_ptr, o.to(O.dtype.element_ty), boundary_check=(0, 1))", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 65, 6, 111, 263, 65, 6, 111, 264, 65, 6, 111, 265, 65, 6, 111, 266, 65, 6, 111, 267, 65, 6, 178, 65, 34, -1, 268, 111, 269, 111, 270, 197, 246, 174, 246, 351, 178, 111, 174, 246, 352, 178, 111, 174, 246, 353, 178, 178, 34, 271, 197, 222, 246, 272, 197, 250, 76, 270, 247, 253, 111, 126, 197, 246, 262, 111, 263, 178, 111, 273, 197, 246, 254, 111, 255, 178, 111, 274, 197, 246, 269, 247, 265, 111, 351, 178, 111, 275, 197, 246, 265, 111, 266, 178, 111, 276, 197, 246, 352, 111, 351, 178, 178, 34, 277, 197, 222, 246, 272, 197, 251, 76, 270, 247, 259, 111, 126, 197, 246, 263, 111, 264, 178, 111, 273, 197, 246, 260, 111, 261, 178, 111, 274, 197, 246, 351, 111, 268, 247, 267, 178, 111, 275, 197, 246, 266, 111, 267, 178, 111, 276, 197, 246, 352, 111, 351, 178, 178, 34, 278, 197, 179, 246, 233, 265, 111, 267, 29, 111, 94, 197, 148, 178, 34, 138, 279, 160, 5, 246, 351, 111, 263, 111, 266, 178, 65, 34, 280, 197, 59, 246, 271, 111, 281, 197, 246, 351, 111, 352, 178, 178, 34, 282, 197, 59, 246, 277, 111, 281, 197, 246, 351, 111, 352, 178, 178, 34, 278, 175, 15, 246, 280, 111, 282, 111, 283, 197, 61, 178, 34, 271, 197, 146, 246, 271, 111, 246, 351, 111, 266, 178, 178, 34, 277, 197, 146, 246, 277, 111, 246, 266, 111, 351, 178, 178, 34, 80, 34, 284, 197, 222, 246, 272, 197, 252, 76, 270, 247, 256, 111, 126, 197, 246, 262, 111, 264, 178, 111, 273, 197, 246, 257, 111, 258, 178, 111, 274, 197, 246, 269, 247, 265, 111, 268, 247, 267, 178, 111, 275, 197, 246, 265, 111, 267, 178, 111, 276, 197, 246, 352, 111, 351, 178, 178, 34, 10, 246, 284, 111, 278, 84, 285, 246, 252, 84, 94, 84, 117, 178, 111, 281, 197, 246, 351, 111, 352, 178, 178, 34, 3, 34]}, {"code": "def _bwd_ds_kernel(\n    Q,\n    DO,\n    DS,\n    stride_qk_bh,\n    stride_qk_l,\n    stride_qk_d,\n    stride_vo_bh,\n    stride_vo_l,\n    stride_vo_d,\n    stride_s_bh,\n    stride_s_dk,\n    stride_s_dv,\n    L: tl.constexpr,\n    DK: tl.constexpr,\n    DV: tl.constexpr,\n    BL: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n    start_v, start_k, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + off_bs_head * stride_qk_bh,\n        shape=(DK, L),\n        strides=(stride_qk_d, stride_qk_l),\n        offsets=(start_k * BK, 0),\n        block_shape=(BK, BL),\n        order=(0, 1),\n    )\n    DO_block_ptr = tl.make_block_ptr(\n        base=DO + off_bs_head * stride_vo_bh,\n        shape=(L, DV),\n        strides=(stride_vo_l, stride_vo_d),\n        offsets=(0, start_v * BV),\n        block_shape=(BL, BV),\n        order=(1, 0),\n    )\n\n    ds = tl.zeros([BK, BV], dtype=tl.float32)\n\n    for i in range(0, L, BL):\n        q = tl.load(Q_block_ptr, boundary_check=(0, 1))\n        do = tl.load(DO_block_ptr, boundary_check=(0, 1))\n\n        ds += tl.dot(q, do, allow_tf32=False)\n\n        Q_block_ptr = tl.advance(Q_block_ptr, (0, BL))\n        DO_block_ptr = tl.advance(DO_block_ptr, (BL, 0))\n\n    DS_block_ptr = tl.make_block_ptr(\n        base=DS + off_bs_head * stride_s_bh,\n        shape=(DK, DV),\n        strides=(stride_s_dk, stride_s_dv),\n        offsets=(start_k * BK, start_v * BV),\n        block_shape=(BK, BV),\n        order=(1, 0),\n    )\n    tl.store(DS_block_ptr, ds.to(DS.dtype.element_ty), boundary_check=(0, 1))", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 64, 6, 111, 263, 64, 6, 111, 264, 64, 6, 111, 265, 64, 6, 111, 266, 64, 6, 111, 267, 64, 6, 178, 64, 34, -1, 268, 111, 269, 111, 270, 197, 246, 174, 246, 351, 178, 111, 174, 246, 352, 178, 111, 174, 246, 353, 178, 178, 34, 271, 197, 222, 246, 272, 197, 250, 76, 270, 247, 253, 111, 126, 197, 246, 263, 111, 262, 178, 111, 273, 197, 246, 255, 111, 254, 178, 111, 274, 197, 246, 269, 247, 266, 111, 351, 178, 111, 275, 197, 246, 266, 111, 265, 178, 111, 276, 197, 246, 351, 111, 352, 178, 178, 34, 277, 197, 222, 246, 272, 197, 251, 76, 270, 247, 256, 111, 126, 197, 246, 262, 111, 264, 178, 111, 273, 197, 246, 257, 111, 258, 178, 111, 274, 197, 246, 351, 111, 268, 247, 267, 178, 111, 275, 197, 246, 265, 111, 267, 178, 111, 276, 197, 246, 352, 111, 351, 178, 178, 34, 278, 197, 179, 246, 233, 266, 111, 267, 29, 111, 94, 197, 148, 178, 34, 138, 279, 160, 5, 246, 351, 111, 262, 111, 265, 178, 64, 34, 280, 197, 57, 246, 271, 111, 281, 197, 246, 351, 111, 352, 178, 178, 34, 282, 197, 57, 246, 277, 111, 281, 197, 246, 351, 111, 352, 178, 178, 34, 278, 175, 15, 246, 280, 111, 282, 111, 283, 197, 61, 178, 34, 271, 197, 146, 246, 271, 111, 246, 351, 111, 265, 178, 178, 34, 277, 197, 146, 246, 277, 111, 246, 265, 111, 351, 178, 178, 34, 80, 34, 284, 197, 222, 246, 272, 197, 252, 76, 270, 247, 259, 111, 126, 197, 246, 263, 111, 264, 178, 111, 273, 197, 246, 260, 111, 261, 178, 111, 274, 197, 246, 269, 247, 266, 111, 268, 247, 267, 178, 111, 275, 197, 246, 266, 111, 267, 178, 111, 276, 197, 246, 352, 111, 351, 178, 178, 34, 10, 246, 284, 111, 278, 84, 285, 246, 252, 84, 94, 84, 117, 178, 111, 281, 197, 246, 351, 111, 352, 178, 178, 34, 3, 34]}, {"code": "def _bwd_dqk_kernel(\n    V,\n    S,\n    dQ,\n    dK,\n    dS,\n    dO,\n    stride_qk_bh,\n    stride_qk_l,\n    stride_qk_d,\n    stride_vo_bh,\n    stride_vo_l,\n    stride_vo_d,\n    stride_s_bh,\n    stride_s_dk,\n    stride_s_dv,\n    scale,\n    L: tl.constexpr,\n    DK: tl.constexpr,\n    DV: tl.constexpr,\n    BL: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n    start_k, start_m, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    V_block_ptr = tl.make_block_ptr(\n        base=V + off_bs_head * stride_vo_bh,\n        shape=(L, DV),\n        strides=(stride_vo_l, stride_vo_d),\n        offsets=(start_m * BL, 0),\n        block_shape=(BL, BV),\n        order=(1, 0),\n    )\n    dO_block_ptr = tl.make_block_ptr(\n        base=dO + off_bs_head * stride_vo_bh,\n        shape=(L, DV),\n        strides=(stride_vo_l, stride_vo_d),\n        offsets=(start_m * BL, 0),\n        block_shape=(BL, BV),\n        order=(1, 0),\n    )\n    S_block_ptr = tl.make_block_ptr(\n        base=S + off_bs_head * stride_s_bh,\n        shape=(DV, DK),\n        strides=(stride_s_dv, stride_s_dk),\n        offsets=(0, start_k * BK),\n        block_shape=(BV, BK),\n        order=(0, 1),\n    )\n    dS_block_ptr = tl.make_block_ptr(\n        base=dS + off_bs_head * stride_s_bh,\n        shape=(DV, DK),\n        strides=(stride_s_dv, stride_s_dk),\n        offsets=(0, start_k * BK),\n        block_shape=(BV, BK),\n        order=(0, 1),\n    )\n\n    dq = tl.zeros([BL, BK], dtype=tl.float32)\n    dk = tl.zeros([BL, BK], dtype=tl.float32)\n\n    for _ in range(0, DV, BV):\n        v = tl.load(V_block_ptr, boundary_check=(0, 1))\n        do = tl.load(dO_block_ptr, boundary_check=(0, 1))\n\n        s = tl.load(S_block_ptr, boundary_check=(0, 1))\n        ds = tl.load(dS_block_ptr, boundary_check=(0, 1))\n\n        v = (v * scale).to(v.dtype)\n        dq += tl.dot(do, s.to(do.dtype), allow_tf32=False)\n        dk += tl.dot(v, ds.to(v.dtype), allow_tf32=False)\n\n        V_block_ptr = tl.advance(V_block_ptr, (0, BV))\n        dS_block_ptr = tl.advance(dS_block_ptr, (BV, 0))\n\n        dO_block_ptr = tl.advance(dO_block_ptr, (0, BV))\n        S_block_ptr = tl.advance(S_block_ptr, (BV, 0))\n\n    dQ_block_ptr = tl.make_block_ptr(\n        base=dQ + off_bs_head * stride_qk_bh,\n        shape=(L, DK),\n        strides=(stride_qk_l, stride_qk_d),\n        offsets=(start_m * BL, start_k * BK),\n        block_shape=(BL, BK),\n        order=(1, 0),\n    )\n    dK_block_ptr = tl.make_block_ptr(\n        base=dK + off_bs_head * stride_qk_bh,\n        shape=(L, DK),\n        strides=(stride_qk_l, stride_qk_d),\n        offsets=(start_m * BL, start_k * BK),\n        block_shape=(BL, BK),\n        order=(1, 0),\n    )\n    tl.store(dQ_block_ptr, dq.to(dQ.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(dK_block_ptr, dk.to(dK.dtype.element_ty), boundary_check=(0, 1))", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 111, 263, 111, 264, 111, 265, 111, 266, 65, 6, 111, 267, 65, 6, 111, 268, 65, 6, 111, 269, 65, 6, 111, 270, 65, 6, 111, 271, 65, 6, 178, 65, 34, -1, 272, 111, 273, 111, 274, 197, 246, 174, 246, 351, 178, 111, 174, 246, 352, 178, 111, 174, 246, 353, 178, 178, 34, 275, 197, 222, 246, 276, 197, 250, 76, 274, 247, 259, 111, 126, 197, 246, 266, 111, 268, 178, 111, 277, 197, 246, 260, 111, 261, 178, 111, 278, 197, 246, 273, 247, 269, 111, 351, 178, 111, 279, 197, 246, 269, 111, 271, 178, 111, 280, 197, 246, 352, 111, 351, 178, 178, 34, 281, 197, 222, 246, 276, 197, 255, 76, 274, 247, 259, 111, 126, 197, 246, 266, 111, 268, 178, 111, 277, 197, 246, 260, 111, 261, 178, 111, 278, 197, 246, 273, 247, 269, 111, 351, 178, 111, 279, 197, 246, 269, 111, 271, 178, 111, 280, 197, 246, 352, 111, 351, 178, 178, 34, 282, 197, 222, 246, 276, 197, 251, 76, 274, 247, 262, 111, 126, 197, 246, 268, 111, 267, 178, 111, 277, 197, 246, 264, 111, 263, 178, 111, 278, 197, 246, 351, 111, 272, 247, 270, 178, 111, 279, 197, 246, 271, 111, 270, 178, 111, 280, 197, 246, 351, 111, 352, 178, 178, 34, 283, 197, 222, 246, 276, 197, 254, 76, 274, 247, 262, 111, 126, 197, 246, 268, 111, 267, 178, 111, 277, 197, 246, 264, 111, 263, 178, 111, 278, 197, 246, 351, 111, 272, 247, 270, 178, 111, 279, 197, 246, 271, 111, 270, 178, 111, 280, 197, 246, 351, 111, 352, 178, 178, 34, 284, 197, 179, 246, 233, 269, 111, 270, 29, 111, 94, 197, 148, 178, 34, 285, 197, 179, 246, 233, 269, 111, 270, 29, 111, 94, 197, 148, 178, 34, 138, 286, 160, 5, 246, 351, 111, 268, 111, 271, 178, 65, 34, 287, 197, 59, 246, 275, 111, 288, 197, 246, 351, 111, 352, 178, 178, 34, 289, 197, 59, 246, 281, 111, 288, 197, 246, 351, 111, 352, 178, 178, 34, 290, 197, 59, 246, 282, 111, 288, 197, 246, 351, 111, 352, 178, 178, 34, 291, 197, 59, 246, 283, 111, 288, 197, 246, 351, 111, 352, 178, 178, 34, 287, 197, 246, 287, 247, 265, 178, 84, 292, 246, 287, 84, 94, 178, 34, 284, 175, 15, 246, 289, 111, 290, 84, 292, 246, 289, 84, 94, 178, 111, 293, 197, 61, 178, 34, 285, 175, 15, 246, 287, 111, 291, 84, 292, 246, 287, 84, 94, 178, 111, 293, 197, 61, 178, 34, 275, 197, 146, 246, 275, 111, 246, 351, 111, 271, 178, 178, 34, 283, 197, 146, 246, 283, 111, 246, 271, 111, 351, 178, 178, 34, 281, 197, 146, 246, 281, 111, 246, 351, 111, 271, 178, 178, 34, 282, 197, 146, 246, 282, 111, 246, 271, 111, 351, 178, 178, 34, 80, 34, 294, 197, 222, 246, 276, 197, 252, 76, 274, 247, 256, 111, 126, 197, 246, 266, 111, 267, 178, 111, 277, 197, 246, 257, 111, 258, 178, 111, 278, 197, 246, 273, 247, 269, 111, 272, 247, 270, 178, 111, 279, 197, 246, 269, 111, 270, 178, 111, 280, 197, 246, 352, 111, 351, 178, 178, 34, 295, 197, 222, 246, 276, 197, 253, 76, 274, 247, 256, 111, 126, 197, 246, 266, 111, 267, 178, 111, 277, 197, 246, 257, 111, 258, 178, 111, 278, 197, 246, 273, 247, 269, 111, 272, 247, 270, 178, 111, 279, 197, 246, 269, 111, 270, 178, 111, 280, 197, 246, 352, 111, 351, 178, 178, 34, 10, 246, 294, 111, 284, 84, 292, 246, 252, 84, 94, 84, 117, 178, 111, 288, 197, 246, 351, 111, 352, 178, 178, 34, 10, 246, 295, 111, 285, 84, 292, 246, 253, 84, 94, 84, 117, 178, 111, 288, 197, 246, 351, 111, 352, 178, 178, 34, 3, 34]}, {"code": "def _bwd_dv_kernel(\n    K,\n    dV,\n    dS,\n    stride_qk_bh,\n    stride_qk_l,\n    stride_qk_d,\n    stride_vo_bh,\n    stride_vo_l,\n    stride_vo_d,\n    stride_s_bh,\n    stride_s_dk,\n    stride_s_dv,\n    scale,\n    L: tl.constexpr,\n    DK: tl.constexpr,\n    DV: tl.constexpr,\n    BL: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n    start_v, start_m, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    K_block_ptr = tl.make_block_ptr(\n        base=K + off_bs_head * stride_qk_bh,\n        shape=(L, DK),\n        strides=(stride_qk_l, stride_qk_d),\n        offsets=(start_m * BL, 0),\n        block_shape=(BL, BK),\n        order=(1, 0),\n    )\n    dS_block_ptr = tl.make_block_ptr(\n        base=dS + off_bs_head * stride_s_bh,\n        shape=(DK, DV),\n        strides=(stride_s_dk, stride_s_dv),\n        offsets=(0, start_v * BV),\n        block_shape=(BK, BV),\n        order=(1, 0),\n    )\n\n    dv = tl.zeros([BL, BV], dtype=tl.float32)\n\n    for _ in range(0, DK, BK):\n        k = tl.load(K_block_ptr, boundary_check=(0, 1))\n        ds = tl.load(dS_block_ptr, boundary_check=(0, 1))\n\n        dv += tl.dot(k, ds.to(k.dtype), allow_tf32=False) * scale\n\n        K_block_ptr = tl.advance(K_block_ptr, (0, BK))\n        dS_block_ptr = tl.advance(dS_block_ptr, (BK, 0))\n\n    dV_block_ptr = tl.make_block_ptr(\n        base=dV + off_bs_head * stride_vo_bh,\n        shape=(L, DV),\n        strides=(stride_vo_l, stride_vo_d),\n        offsets=(start_m * BL, start_v * BV),\n        block_shape=(BL, BV),\n        order=(1, 0),\n    )\n    tl.store(dV_block_ptr, dv.to(dV.dtype.element_ty), boundary_check=(0, 1))", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 111, 263, 64, 6, 111, 264, 64, 6, 111, 265, 64, 6, 111, 266, 64, 6, 111, 267, 64, 6, 111, 268, 64, 6, 178, 64, 34, -1, 269, 111, 270, 111, 271, 197, 246, 174, 246, 351, 178, 111, 174, 246, 352, 178, 111, 174, 246, 353, 178, 178, 34, 272, 197, 222, 246, 273, 197, 250, 76, 271, 247, 253, 111, 126, 197, 246, 263, 111, 264, 178, 111, 274, 197, 246, 254, 111, 255, 178, 111, 275, 197, 246, 270, 247, 266, 111, 351, 178, 111, 276, 197, 246, 266, 111, 267, 178, 111, 277, 197, 246, 352, 111, 351, 178, 178, 34, 278, 197, 222, 246, 273, 197, 252, 76, 271, 247, 259, 111, 126, 197, 246, 264, 111, 265, 178, 111, 274, 197, 246, 260, 111, 261, 178, 111, 275, 197, 246, 351, 111, 269, 247, 268, 178, 111, 276, 197, 246, 267, 111, 268, 178, 111, 277, 197, 246, 352, 111, 351, 178, 178, 34, 279, 197, 179, 246, 233, 266, 111, 268, 29, 111, 94, 197, 148, 178, 34, 138, 280, 160, 5, 246, 351, 111, 264, 111, 267, 178, 64, 34, 281, 197, 57, 246, 272, 111, 282, 197, 246, 351, 111, 352, 178, 178, 34, 283, 197, 57, 246, 278, 111, 282, 197, 246, 351, 111, 352, 178, 178, 34, 279, 175, 15, 246, 281, 111, 283, 84, 284, 246, 281, 84, 94, 178, 111, 285, 197, 61, 178, 247, 262, 34, 272, 197, 146, 246, 272, 111, 246, 351, 111, 267, 178, 178, 34, 278, 197, 146, 246, 278, 111, 246, 267, 111, 351, 178, 178, 34, 80, 34, 286, 197, 222, 246, 273, 197, 251, 76, 271, 247, 256, 111, 126, 197, 246, 263, 111, 265, 178, 111, 274, 197, 246, 257, 111, 258, 178, 111, 275, 197, 246, 270, 247, 266, 111, 269, 247, 268, 178, 111, 276, 197, 246, 266, 111, 268, 178, 111, 277, 197, 246, 352, 111, 351, 178, 178, 34, 10, 246, 286, 111, 279, 84, 284, 246, 251, 84, 94, 84, 117, 178, 111, 282, 197, 246, 351, 111, 352, 178, 178, 34, 3, 34]}, {"code": "def _angular_lsh_kernel(\n    in_mat,\n    proj_dir,\n    perm,\n    enc_vec,\n    buckets,\n    stride_in_matb,\n    stride_in_math,\n    stride_in_matm,\n    stride_proj_dirb,\n    stride_proj_dirh,\n    stride_proj_dird,\n    stride_bucketsb,\n    stride_bucketsh,\n    nheads,\n    seqlen,\n    seqlen_rounded,\n    headdim,\n    NUM_PROJ_ROUNDED: tl.constexpr,\n    num_projs: tl.constexpr,\n    BLOCK_HEADDIM: tl.constexpr,\n    EVEN_M: tl.constexpr,\n    EVEN_HEADDIM: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, NUM_PROJ_ROUNDED)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n\n    in_mat_ptrs = (\n        in_mat\n        + off_b * stride_in_matb\n        + off_h * stride_in_math\n        + (offs_m[:, None] * stride_in_matm + offs_d[None, :])\n    )\n    proj_dir_ptrs = (\n        proj_dir\n        + off_b * stride_proj_dirb\n        + off_h * stride_proj_dirh\n        + (offs_d[:, None] * stride_proj_dird + offs_n[None, :])\n    )\n\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            mat = tl.load(in_mat_ptrs)\n        else:\n            mat = tl.load(in_mat_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    else:\n        if EVEN_HEADDIM:\n            mat = tl.load(in_mat_ptrs, mask=offs_m[:, None] < seqlen, other=0.0)\n        else:\n            mat = tl.load(\n                in_mat_ptrs,\n                mask=(offs_m[:, None] < seqlen) & (offs_d[None, :] < headdim),\n                other=0.0,\n            )\n\n    if EVEN_HEADDIM:\n        proj_dir_block = tl.load(\n            proj_dir_ptrs, mask=offs_n[None, :] < num_projs, other=0.0\n        )\n    else:\n        proj_dir_block = tl.load(\n            proj_dir_ptrs,\n            mask=(offs_n[None, :] < num_projs)\n            & (offs_d[:, None] * stride_proj_dird < headdim),\n            other=0.0,\n        )\n\n    mask = tl.dot(mat, proj_dir_block)\n    mask = tl.where(mask > 0.0, 1.0, 0.0)\n\n    encoding_vectors = tl.load(enc_vec + offs_n, mask=offs_n < num_projs, other=0.0)\n\n    bin_ids = tl.sum(mask * encoding_vectors[None, :], 1).to(tl.int32)\n\n    hash_buckets = tl.load(perm + bin_ids)\n\n    buckets_ptrs = buckets + off_b * stride_bucketsb + off_h * stride_bucketsh + offs_m\n    if EVEN_M:\n        tl.store(buckets_ptrs, hash_buckets)\n    else:\n        tl.store(buckets_ptrs, hash_buckets, mask=offs_m < seqlen)", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 111, 263, 111, 264, 111, 265, 111, 266, 111, 267, 65, 6, 111, 268, 65, 6, 111, 269, 65, 6, 111, 270, 65, 6, 111, 271, 65, 6, 111, 272, 65, 6, 178, 65, 34, -1, 273, 197, 174, 246, 351, 178, 34, 274, 197, 174, 246, 352, 178, 34, 275, 197, 274, 50, 263, 34, 276, 197, 274, 229, 263, 34, 277, 197, 273, 247, 272, 76, 78, 246, 351, 111, 272, 178, 34, 278, 197, 78, 246, 351, 111, 267, 178, 34, 279, 197, 78, 246, 351, 111, 269, 178, 34, 280, 197, 250, 76, 275, 247, 255, 76, 276, 247, 256, 76, 246, 277, 233, 65, 111, 205, 29, 247, 257, 76, 279, 233, 205, 111, 65, 29, 178, 34, 281, 197, 251, 76, 275, 247, 258, 76, 276, 247, 259, 76, 246, 279, 233, 65, 111, 205, 29, 247, 260, 76, 278, 233, 205, 111, 65, 29, 178, 34, 185, 270, 65, 34, 185, 271, 65, 34, 282, 197, 59, 246, 280, 178, 34, 188, 34, 32, 65, 34, 282, 197, 59, 246, 280, 111, 283, 197, 279, 233, 205, 111, 65, 29, 1, 266, 111, 284, 197, 351, 178, 34, 58, 34, 68, 34, 39, 271, 65, 34, 282, 197, 59, 246, 280, 111, 283, 197, 277, 233, 65, 111, 205, 29, 1, 264, 111, 284, 197, 351, 178, 34, 188, 34, 32, 65, 34, 282, 197, 59, 246, 280, 111, 283, 197, 246, 277, 233, 65, 111, 205, 29, 1, 264, 178, 176, 246, 279, 233, 205, 111, 65, 29, 1, 266, 178, 111, 284, 197, 351, 178, 34, 58, 34, 185, 271, 65, 34, 285, 197, 59, 246, 281, 111, 283, 197, 278, 233, 205, 111, 65, 29, 1, 268, 111, 284, 197, 351, 178, 34, 188, 34, 32, 65, 34, 285, 197, 59, 246, 281, 111, 283, 197, 246, 278, 233, 205, 111, 65, 29, 1, 268, 178, 176, 246, 279, 233, 65, 111, 205, 29, 247, 260, 1, 266, 178, 111, 284, 197, 351, 178, 34, 58, 34, 283, 197, 15, 246, 282, 111, 285, 178, 34, 283, 197, 208, 246, 283, 127, 351, 111, 352, 111, 351, 178, 34, 286, 197, 59, 246, 253, 76, 278, 111, 283, 197, 278, 1, 268, 111, 284, 197, 351, 178, 34, 287, 197, 224, 246, 283, 247, 286, 233, 205, 111, 65, 29, 111, 352, 178, 84, 288, 246, 64, 178, 34, 289, 197, 59, 246, 252, 76, 287, 178, 34, 290, 197, 254, 76, 275, 247, 261, 76, 276, 247, 262, 76, 277, 34, 185, 270, 65, 34, 10, 246, 290, 111, 289, 178, 34, 188, 34, 32, 65, 34, 10, 246, 290, 111, 289, 111, 283, 197, 277, 1, 264, 178, 34, 58, 34, 3, 34]}, {"code": "def _fwd_kernel(\n    Q,\n    K,\n    V,\n    Bias,\n    Out,\n    Lse,\n    softmax_scale,\n    stride_qb,\n    stride_qh,\n    stride_qm,\n    stride_kb,\n    stride_kh,\n    stride_kn,\n    stride_vb,\n    stride_vh,\n    stride_vn,\n    stride_bb,\n    stride_bh,\n    stride_bm,\n    stride_ob,\n    stride_oh,\n    stride_om,\n    nheads,\n    seqlen_q,\n    seqlen_k,\n    seqlen_q_rounded,\n    headdim,\n    CACHE_KEY_SEQLEN_Q,\n    CACHE_KEY_SEQLEN_K,\n    BIAS_TYPE: tl.constexpr,\n    IS_CAUSAL: tl.constexpr,\n    BLOCK_HEADDIM: tl.constexpr,\n    EVEN_M: tl.constexpr,\n    EVEN_N: tl.constexpr,\n    EVEN_HEADDIM: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n\n    q_ptrs = (\n        Q\n        + off_b * stride_qb\n        + off_h * stride_qh\n        + (offs_m[:, None] * stride_qm + offs_d[None, :])\n    )\n    k_ptrs = (\n        K\n        + off_b * stride_kb\n        + off_h * stride_kh\n        + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    )\n    v_ptrs = (\n        V\n        + off_b * stride_vb\n        + off_h * stride_vh\n        + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    )\n    if BIAS_TYPE == \"vector\":\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == \"matrix\":\n        b_ptrs = (\n            Bias\n            + off_b * stride_bb\n            + off_h * stride_bh\n            + (offs_m[:, None] * stride_bm + offs_n[None, :])\n        )\n\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    else:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n        else:\n            q = tl.load(\n                q_ptrs,\n                mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim),\n                other=0.0,\n            )\n\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) * BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(\n                    k_ptrs + start_n * stride_kn,\n                    mask=offs_d[None, :] < headdim,\n                    other=0.0,\n                )\n        else:\n            if EVEN_HEADDIM:\n                k = tl.load(\n                    k_ptrs + start_n * stride_kn,\n                    mask=(start_n + offs_n)[:, None] < seqlen_k,\n                    other=0.0,\n                )\n            else:\n                k = tl.load(\n                    k_ptrs + start_n * stride_kn,\n                    mask=((start_n + offs_n)[:, None] < seqlen_k)\n                    & (offs_d[None, :] < headdim),\n                    other=0.0,\n                )\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float(\"-inf\"))\n        if IS_CAUSAL:\n            qk += tl.where(\n                offs_m[:, None] >= (start_n + offs_n)[None, :], 0, float(\"-inf\")\n            )\n        if BIAS_TYPE != \"none\":\n            if BIAS_TYPE == \"vector\":\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n).to(tl.float32)\n                else:\n                    bias = tl.load(\n                        b_ptrs + start_n, mask=(start_n + offs_n) < seqlen_k, other=0.0\n                    ).to(tl.float32)\n                bias = bias[None, :]\n            elif BIAS_TYPE == \"matrix\":\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n).to(tl.float32)\n                else:\n                    bias = tl.load(\n                        b_ptrs + start_n,\n                        mask=(offs_m[:, None] < seqlen_q)\n                        & ((start_n + offs_n)[None, :] < seqlen_k),\n                        other=0.0,\n                    ).to(tl.float32)\n\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n\n        acc_o_scale = tl.exp(m_i - m_ij)\n\n        acc_o = acc_o * acc_o_scale[:, None]\n\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(\n                    v_ptrs + start_n * stride_vn,\n                    mask=offs_d[None, :] < headdim,\n                    other=0.0,\n                )\n        else:\n            if EVEN_HEADDIM:\n                v = tl.load(\n                    v_ptrs + start_n * stride_vn,\n                    mask=(start_n + offs_n)[:, None] < seqlen_k,\n                    other=0.0,\n                )\n            else:\n                v = tl.load(\n                    v_ptrs + start_n * stride_vn,\n                    mask=((start_n + offs_n)[:, None] < seqlen_k)\n                    & (offs_d[None, :] < headdim),\n                    other=0.0,\n                )\n        p = p.to(v.dtype)\n        acc_o += tl.dot(p, v)\n\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n\n    o_scale = tl.exp(m_i - lse_i)\n    acc_o = acc_o * o_scale[:, None]\n\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = (\n        Out\n        + off_b * stride_ob\n        + off_h * stride_oh\n        + (offs_m[:, None] * stride_om + offs_d[None, :])\n    )\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    else:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n        else:\n            tl.store(\n                out_ptrs,\n                acc_o,\n                mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim),\n            )", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 111, 263, 111, 264, 111, 265, 111, 266, 111, 267, 111, 268, 111, 269, 111, 270, 111, 271, 111, 272, 111, 273, 111, 274, 111, 275, 111, 276, 111, 277, 111, 278, 111, 279, 64, 6, 111, 280, 64, 6, 111, 281, 64, 6, 111, 282, 64, 6, 111, 283, 64, 6, 111, 284, 64, 6, 111, 285, 64, 6, 111, 286, 64, 6, 178, 64, 34, -1, 287, 197, 174, 246, 351, 178, 34, 288, 197, 174, 246, 352, 178, 34, 289, 197, 288, 50, 272, 34, 290, 197, 288, 229, 272, 34, 291, 197, 287, 247, 285, 76, 78, 246, 351, 111, 285, 178, 34, 292, 197, 78, 246, 351, 111, 286, 178, 34, 293, 197, 78, 246, 351, 111, 281, 178, 34, 294, 197, 250, 76, 289, 247, 257, 76, 290, 247, 258, 76, 246, 291, 233, 64, 111, 205, 29, 247, 259, 76, 293, 233, 205, 111, 64, 29, 178, 34, 295, 197, 251, 76, 289, 247, 260, 76, 290, 247, 261, 76, 246, 292, 233, 64, 111, 205, 29, 247, 262, 76, 293, 233, 205, 111, 64, 29, 178, 34, 296, 197, 252, 76, 289, 247, 263, 76, 290, 247, 264, 76, 246, 292, 233, 64, 111, 205, 29, 247, 265, 76, 293, 233, 205, 111, 64, 29, 178, 34, 185, 279, 79, 353, 64, 34, 297, 197, 253, 76, 289, 247, 266, 76, 290, 247, 267, 76, 292, 34, 67, 34, 39, 279, 79, 354, 64, 34, 297, 197, 253, 76, 289, 247, 266, 76, 290, 247, 267, 76, 246, 291, 233, 64, 111, 205, 29, 247, 268, 76, 292, 233, 205, 111, 64, 29, 178, 34, 188, 34, 298, 197, 179, 246, 233, 285, 29, 111, 94, 197, 148, 178, 4, 299, 246, 355, 178, 34, 300, 197, 179, 246, 233, 285, 29, 111, 94, 197, 148, 178, 4, 299, 246, 355, 178, 34, 301, 197, 179, 246, 233, 285, 111, 281, 29, 111, 94, 197, 148, 178, 34, 185, 282, 176, 283, 64, 34, 185, 284, 64, 34, 302, 197, 57, 246, 294, 178, 34, 188, 34, 32, 64, 34, 302, 197, 57, 246, 294, 111, 303, 197, 293, 233, 205, 111, 64, 29, 1, 276, 111, 304, 197, 351, 178, 34, 58, 34, 67, 34, 39, 284, 64, 34, 302, 197, 57, 246, 294, 111, 303, 197, 291, 233, 64, 111, 205, 29, 1, 273, 111, 304, 197, 351, 178, 34, 188, 34, 32, 64, 34, 302, 197, 57, 246, 294, 111, 303, 197, 246, 291, 233, 64, 111, 205, 29, 1, 273, 178, 176, 246, 293, 233, 205, 111, 64, 29, 1, 276, 178, 111, 304, 197, 351, 178, 34, 58, 34, 305, 197, 274, 185, 65, 280, 32, 68, 246, 246, 287, 76, 352, 178, 247, 285, 111, 274, 178, 34, 138, 306, 160, 5, 246, 351, 111, 305, 111, 286, 178, 64, 34, 306, 197, 59, 246, 306, 111, 286, 178, 34, 185, 283, 176, 282, 64, 34, 185, 284, 64, 34, 307, 197, 57, 246, 295, 76, 306, 247, 262, 178, 34, 188, 34, 32, 64, 34, 307, 197, 57, 246, 295, 76, 306, 247, 262, 111, 303, 197, 293, 233, 205, 111, 64, 29, 1, 276, 111, 304, 197, 351, 178, 34, 58, 34, 67, 34, 39, 284, 64, 34, 307, 197, 57, 246, 295, 76, 306, 247, 262, 111, 303, 197, 246, 306, 76, 292, 178, 233, 64, 111, 205, 29, 1, 274, 111, 304, 197, 351, 178, 34, 188, 34, 32, 64, 34, 307, 197, 57, 246, 295, 76, 306, 247, 262, 111, 303, 197, 246, 246, 306, 76, 292, 178, 233, 64, 111, 205, 29, 1, 274, 178, 176, 246, 293, 233, 205, 111, 64, 29, 1, 276, 178, 111, 304, 197, 351, 178, 34, 58, 34, 308, 197, 179, 246, 233, 285, 111, 286, 29, 111, 94, 197, 148, 178, 34, 308, 175, 15, 246, 302, 111, 74, 246, 307, 178, 178, 34, 185, 65, 283, 64, 34, 308, 175, 208, 246, 246, 306, 76, 292, 178, 233, 205, 111, 64, 29, 1, 274, 111, 351, 111, 299, 246, 356, 178, 178, 34, 188, 34, 185, 280, 64, 34, 308, 175, 208, 246, 291, 233, 64, 111, 205, 29, 147, 246, 306, 76, 292, 178, 233, 205, 111, 64, 29, 111, 351, 111, 299, 246, 356, 178, 178, 34, 188, 34, 185, 279, 189, 357, 64, 34, 185, 279, 79, 353, 64, 34, 185, 283, 64, 34, 309, 197, 57, 246, 297, 76, 306, 178, 84, 310, 246, 148, 178, 34, 188, 34, 32, 64, 34, 309, 197, 57, 246, 297, 76, 306, 111, 303, 197, 306, 76, 292, 1, 274, 111, 304, 197, 351, 178, 84, 310, 246, 148, 178, 34, 58, 34, 309, 197, 309, 233, 205, 111, 64, 29, 34, 67, 34, 39, 279, 79, 354, 64, 34, 185, 282, 176, 283, 64, 34, 309, 197, 57, 246, 297, 76, 306, 178, 84, 310, 246, 148, 178, 34, 188, 34, 32, 64, 34, 309, 197, 57, 246, 297, 76, 306, 111, 303, 197, 246, 291, 233, 64, 111, 205, 29, 1, 273, 178, 176, 246, 246, 306, 76, 292, 178, 233, 205, 111, 64, 29, 1, 274, 178, 111, 304, 197, 351, 178, 84, 310, 246, 148, 178, 34, 58, 34, 188, 34, 308, 197, 308, 247, 256, 76, 309, 34, 311, 197, 195, 246, 12, 246, 308, 111, 352, 178, 111, 298, 178, 34, 312, 197, 110, 246, 308, 4, 311, 233, 64, 111, 205, 29, 178, 34, 188, 34, 32, 64, 34, 311, 197, 195, 246, 12, 246, 308, 111, 352, 178, 247, 256, 111, 298, 178, 34, 312, 197, 110, 246, 308, 247, 256, 4, 311, 233, 64, 111, 205, 29, 178, 34, 58, 34, 313, 197, 224, 246, 312, 111, 352, 178, 34, 314, 197, 110, 246, 300, 4, 311, 178, 34, 301, 197, 301, 247, 314, 233, 64, 111, 205, 29, 34, 185, 283, 176, 282, 64, 34, 185, 284, 64, 34, 315, 197, 57, 246, 296, 76, 306, 247, 265, 178, 34, 188, 34, 32, 64, 34, 315, 197, 57, 246, 296, 76, 306, 247, 265, 111, 303, 197, 293, 233, 205, 111, 64, 29, 1, 276, 111, 304, 197, 351, 178, 34, 58, 34, 67, 34, 39, 284, 64, 34, 315, 197, 57, 246, 296, 76, 306, 247, 265, 111, 303, 197, 246, 306, 76, 292, 178, 233, 64, 111, 205, 29, 1, 274, 111, 304, 197, 351, 178, 34, 188, 34, 32, 64, 34, 315, 197, 57, 246, 296, 76, 306, 247, 265, 111, 303, 197, 246, 246, 306, 76, 292, 178, 233, 64, 111, 205, 29, 1, 274, 178, 176, 246, 293, 233, 205, 111, 64, 29, 1, 276, 178, 111, 304, 197, 351, 178, 34, 58, 34, 312, 197, 312, 84, 310, 246, 315, 84, 94, 178, 34, 301, 175, 15, 246, 312, 111, 315, 178, 34, 300, 197, 311, 34, 316, 197, 110, 246, 298, 4, 311, 178, 76, 313, 34, 298, 197, 311, 76, 52, 246, 316, 178, 34, 80, 34, 317, 197, 110, 246, 300, 4, 298, 178, 34, 301, 197, 301, 247, 317, 233, 64, 111, 205, 29, 34, 287, 197, 174, 246, 351, 178, 34, 291, 197, 287, 247, 285, 76, 78, 246, 351, 111, 285, 178, 34, 318, 197, 255, 76, 288, 247, 275, 76, 291, 34, 10, 246, 318, 111, 298, 178, 34, 293, 197, 78, 246, 351, 111, 281, 178, 34, 319, 197, 254, 76, 289, 247, 269, 76, 290, 247, 270, 76, 246, 291, 233, 64, 111, 205, 29, 247, 271, 76, 293, 233, 205, 111, 64, 29, 178, 34, 185, 282, 64, 34, 185, 284, 64, 34, 10, 246, 319, 111, 301, 178, 34, 188, 34, 32, 64, 34, 10, 246, 319, 111, 301, 111, 303, 197, 293, 233, 205, 111, 64, 29, 1, 276, 178, 34, 58, 34, 67, 34, 39, 284, 64, 34, 10, 246, 319, 111, 301, 111, 303, 197, 291, 233, 64, 111, 205, 29, 1, 273, 178, 34, 188, 34, 32, 64, 34, 10, 246, 319, 111, 301, 111, 303, 197, 246, 291, 233, 64, 111, 205, 29, 1, 273, 178, 176, 246, 293, 233, 205, 111, 64, 29, 1, 276, 178, 178, 34, 58, 34, 3, 34]}, {"code": "def _bwd_kernel(\n    Q,\n    K,\n    V,\n    Bias,\n    DO,\n    DQ,\n    DK,\n    DV,\n    LSE,\n    D,\n    softmax_scale,\n    stride_qb,\n    stride_qh,\n    stride_qm,\n    stride_kb,\n    stride_kh,\n    stride_kn,\n    stride_vb,\n    stride_vh,\n    stride_vn,\n    stride_bb,\n    stride_bh,\n    stride_bm,\n    stride_dob,\n    stride_doh,\n    stride_dom,\n    stride_dqb,\n    stride_dqh,\n    stride_dqm,\n    stride_dkb,\n    stride_dkh,\n    stride_dkn,\n    stride_dvb,\n    stride_dvh,\n    stride_dvn,\n    nheads,\n    seqlen_q,\n    seqlen_k,\n    seqlen_q_rounded,\n    headdim,\n    CACHE_KEY_SEQLEN_Q,\n    CACHE_KEY_SEQLEN_K,\n    BIAS_TYPE: tl.constexpr,\n    IS_CAUSAL: tl.constexpr,\n    BLOCK_HEADDIM: tl.constexpr,\n    SEQUENCE_PARALLEL: tl.constexpr,\n    EVEN_M: tl.constexpr,\n    EVEN_N: tl.constexpr,\n    EVEN_HEADDIM: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != \"none\":\n        Bias += off_b * stride_bb + off_h * stride_bh\n\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(\n                start_n,\n                Q,\n                K,\n                V,\n                Bias,\n                DO,\n                DQ,\n                DK,\n                DV,\n                LSE,\n                D,\n                softmax_scale,\n                stride_qm,\n                stride_kn,\n                stride_vn,\n                stride_bm,\n                stride_dom,\n                stride_dqm,\n                stride_dkn,\n                stride_dvn,\n                seqlen_q,\n                seqlen_k,\n                headdim,\n                ATOMIC_ADD=False,\n                BIAS_TYPE=BIAS_TYPE,\n                IS_CAUSAL=IS_CAUSAL,\n                BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M,\n                EVEN_N=EVEN_N,\n                EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M,\n                BLOCK_N=BLOCK_N,\n            )\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(\n            start_n,\n            Q,\n            K,\n            V,\n            Bias,\n            DO,\n            DQ,\n            DK,\n            DV,\n            LSE,\n            D,\n            softmax_scale,\n            stride_qm,\n            stride_kn,\n            stride_vn,\n            stride_bm,\n            stride_dom,\n            stride_dqm,\n            stride_dkn,\n            stride_dvn,\n            seqlen_q,\n            seqlen_k,\n            headdim,\n            ATOMIC_ADD=True,\n            BIAS_TYPE=BIAS_TYPE,\n            IS_CAUSAL=IS_CAUSAL,\n            BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M,\n            EVEN_N=EVEN_N,\n            EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M,\n            BLOCK_N=BLOCK_N,\n        )", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 111, 263, 111, 264, 111, 265, 111, 266, 111, 267, 111, 268, 111, 269, 111, 270, 111, 271, 111, 272, 111, 273, 111, 274, 111, 275, 111, 276, 111, 277, 111, 278, 111, 279, 111, 280, 111, 281, 111, 282, 111, 283, 111, 284, 111, 285, 111, 286, 111, 287, 111, 288, 111, 289, 111, 290, 111, 291, 111, 292, 65, 6, 111, 293, 65, 6, 111, 294, 65, 6, 111, 295, 65, 6, 111, 296, 65, 6, 111, 297, 65, 6, 111, 298, 65, 6, 111, 299, 65, 6, 111, 300, 65, 6, 178, 65, 34, -1, 301, 197, 174, 246, 351, 178, 34, 302, 197, 301, 50, 285, 34, 303, 197, 301, 229, 285, 34, 250, 175, 302, 247, 261, 76, 303, 247, 262, 34, 251, 175, 302, 247, 264, 76, 303, 247, 265, 34, 252, 175, 302, 247, 267, 76, 303, 247, 268, 34, 254, 175, 302, 247, 273, 76, 303, 247, 274, 34, 255, 175, 302, 247, 276, 76, 303, 247, 277, 34, 256, 175, 302, 247, 279, 76, 303, 247, 280, 34, 257, 175, 302, 247, 282, 76, 303, 247, 283, 34, 185, 292, 189, 352, 65, 34, 253, 175, 302, 247, 270, 76, 303, 247, 271, 34, 188, 34, 259, 175, 301, 247, 288, 34, 258, 175, 301, 247, 288, 34, 185, 66, 295, 65, 34, 304, 197, 67, 246, 287, 111, 300, 178, 34, 138, 305, 160, 5, 246, 353, 111, 304, 178, 65, 34, 306, 246, 305, 111, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 263, 111, 266, 111, 269, 111, 272, 111, 275, 111, 278, 111, 281, 111, 284, 111, 286, 111, 287, 111, 289, 111, 307, 197, 61, 111, 292, 197, 292, 111, 293, 197, 293, 111, 294, 197, 294, 111, 296, 197, 296, 111, 297, 197, 297, 111, 298, 197, 298, 111, 299, 197, 299, 111, 300, 197, 300, 178, 34, 80, 34, 188, 34, 32, 65, 34, 305, 197, 174, 246, 353, 178, 34, 306, 246, 305, 111, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 263, 111, 266, 111, 269, 111, 272, 111, 275, 111, 278, 111, 281, 111, 284, 111, 286, 111, 287, 111, 289, 111, 307, 197, 163, 111, 292, 197, 292, 111, 293, 197, 293, 111, 294, 197, 294, 111, 296, 197, 296, 111, 297, 197, 297, 111, 298, 197, 298, 111, 299, 197, 299, 111, 300, 197, 300, 178, 34, 58, 34, 3, 34]}, {"code": "def _fwd_hyper_kernel(\n    Q,\n    K,\n    V,\n    q_sort_idx,\n    k_sort_idx,\n    Out,\n    Lse,\n    softmax_scale,\n    stride_qb,\n    stride_qh,\n    stride_qm,\n    stride_kb,\n    stride_kh,\n    stride_kn,\n    stride_vb,\n    stride_vh,\n    stride_vn,\n    stride_q_sort_idxb,\n    stride_q_sort_idxh,\n    stride_q_sort_idxm,\n    stride_k_sort_idxb,\n    stride_k_sort_idxh,\n    stride_k_sort_idxn,\n    stride_ob,\n    stride_oh,\n    stride_om,\n    nheads,\n    block_size,\n    sample_size,\n    seqlen_k,\n    seqlen_q,\n    headdim,\n    v_headdim,\n    smooth_block,\n    CACHE_KEY_SEQLEN_Q,\n    CACHE_KEY_SEQLEN_K,\n    BLOCK_HEADDIM: tl.constexpr,\n    V_BLOCK_HEADDIM: tl.constexpr,\n    EVEN_HEADDIM: tl.constexpr,\n    EVEN_V_HEADDIM: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    offs_vd = tl.arange(0, V_BLOCK_HEADDIM)\n\n    q_idx_ptrs = (\n        q_sort_idx\n        + off_b * stride_q_sort_idxb\n        + off_h * stride_q_sort_idxh\n        + offs_m * stride_q_sort_idxm\n    )\n    q_idx = tl.load(q_idx_ptrs).to(tl.int32)\n\n    k_sort_idx += off_b * stride_k_sort_idxb + off_h * stride_k_sort_idxh\n\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    acc_o = tl.zeros([BLOCK_M, V_BLOCK_HEADDIM], dtype=tl.float32)\n    q_ptrs = (\n        Q\n        + off_b * stride_qb\n        + off_h * stride_qh\n        + (q_idx[:, None] * stride_qm + offs_d[None, :])\n    )\n    if EVEN_HEADDIM:\n        q = tl.load(q_ptrs)\n    else:\n        q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n\n    block_id = start_m // block_size\n    block_offs = (\n        seqlen_k + (start_m % block_size) * BLOCK_N - (block_size - 1) * BLOCK_N // 2\n    )\n    end_n = tl.minimum((block_id + 1) * BLOCK_N * block_size, seqlen_k)\n    for start_n in range(block_id * BLOCK_N * block_size, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if smooth_block:\n            k_idx_ptrs = (\n                (start_n + block_offs + offs_n) * stride_k_sort_idxn\n            ) % seqlen_k\n        else:\n            k_idx_ptrs = (start_n + offs_n) * stride_k_sort_idxn\n\n        k_idx = tl.load(k_sort_idx + k_idx_ptrs).to(tl.int32)\n        k_ptrs = (\n            K\n            + off_b * stride_kb\n            + off_h * stride_kh\n            + (k_idx[:, None] * stride_kn + offs_d[None, :])\n        )\n\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n        p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n\n        acc_o_scale = tl.exp(m_i - m_ij)\n\n        acc_o = acc_o * acc_o_scale[:, None]\n\n        v_ptrs = (\n            V\n            + off_b * stride_vb\n            + off_h * stride_vh\n            + (k_idx[:, None] * stride_vn + offs_vd[None, :])\n        )\n        if EVEN_V_HEADDIM:\n            v = tl.load(v_ptrs)\n        else:\n            v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\n        p = p.to(v.dtype)\n        acc_o += tl.dot(p, v)\n\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n\n    for col_block in range(0, sample_size):\n        curr_offs_n = col_block * BLOCK_N * stride_kn + offs_n\n        k_ptrs = (\n            K\n            + off_b * stride_kb\n            + off_h * stride_kh\n            + (curr_offs_n[:, None] * stride_kn + offs_d[None, :])\n        )\n\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n        p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n\n        acc_o_scale = tl.exp(m_i - m_ij)\n\n        acc_o = acc_o * acc_o_scale[:, None]\n\n        v_ptrs = (\n            V\n            + off_b * stride_vb\n            + off_h * stride_vh\n            + (curr_offs_n[:, None] * stride_vn + offs_vd[None, :])\n        )\n        if EVEN_V_HEADDIM:\n            v = tl.load(v_ptrs)\n        else:\n            v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\n        p = p.to(v.dtype)\n        acc_o += tl.dot(p, v)\n\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n\n    o_scale = tl.exp(m_i - lse_i)\n    acc_o = acc_o * o_scale[:, None]\n\n    lse_ptrs = Lse + off_hb * seqlen_q + q_idx\n    out_ptrs = (\n        Out\n        + off_b * stride_ob\n        + off_h * stride_oh\n        + (q_idx[:, None] * stride_om + offs_vd[None, :])\n    )\n\n    tl.store(lse_ptrs, lse_i)\n    if EVEN_V_HEADDIM:\n        tl.store(out_ptrs, acc_o)\n    else:\n        tl.store(out_ptrs, acc_o, mask=offs_vd[None, :] < v_headdim)", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 111, 263, 111, 264, 111, 265, 111, 266, 111, 267, 111, 268, 111, 269, 111, 270, 111, 271, 111, 272, 111, 273, 111, 274, 111, 275, 111, 276, 111, 277, 111, 278, 111, 279, 111, 280, 111, 281, 111, 282, 111, 283, 111, 284, 111, 285, 111, 286, 64, 6, 111, 287, 64, 6, 111, 288, 64, 6, 111, 289, 64, 6, 111, 290, 64, 6, 111, 291, 64, 6, 178, 64, 34, -1, 292, 197, 174, 246, 351, 178, 34, 293, 197, 174, 246, 352, 178, 34, 294, 197, 293, 50, 276, 34, 295, 197, 293, 229, 276, 34, 296, 197, 292, 247, 290, 76, 78, 246, 351, 111, 290, 178, 34, 297, 197, 78, 246, 351, 111, 291, 178, 34, 298, 197, 78, 246, 351, 111, 286, 178, 34, 299, 197, 78, 246, 351, 111, 287, 178, 34, 300, 197, 253, 76, 294, 247, 267, 76, 295, 247, 268, 76, 296, 247, 269, 34, 301, 197, 57, 246, 300, 178, 84, 302, 246, 249, 178, 34, 254, 175, 294, 247, 270, 76, 295, 247, 271, 34, 303, 197, 179, 246, 233, 290, 29, 111, 94, 197, 148, 178, 4, 304, 246, 353, 178, 34, 305, 197, 179, 246, 233, 290, 29, 111, 94, 197, 148, 178, 4, 304, 246, 353, 178, 34, 306, 197, 179, 246, 233, 290, 111, 287, 29, 111, 94, 197, 148, 178, 34, 307, 197, 250, 76, 294, 247, 258, 76, 295, 247, 259, 76, 246, 301, 233, 64, 111, 205, 29, 247, 260, 76, 298, 233, 205, 111, 64, 29, 178, 34, 185, 288, 64, 34, 308, 197, 57, 246, 307, 178, 34, 188, 34, 32, 64, 34, 308, 197, 57, 246, 307, 111, 309, 197, 298, 233, 205, 111, 64, 29, 1, 281, 111, 310, 197, 351, 178, 34, 58, 34, 311, 197, 292, 50, 277, 34, 312, 197, 279, 76, 292, 229, 277, 247, 291, 4, 246, 277, 4, 352, 178, 247, 291, 50, 354, 34, 313, 197, 68, 246, 246, 311, 76, 352, 178, 247, 291, 247, 277, 111, 279, 178, 34, 138, 314, 160, 5, 246, 311, 247, 291, 247, 277, 111, 313, 111, 291, 178, 64, 34, 314, 197, 59, 246, 314, 111, 291, 178, 34, 185, 283, 64, 34, 315, 197, 246, 314, 76, 312, 76, 297, 178, 247, 272, 229, 279, 34, 188, 34, 32, 64, 34, 315, 197, 246, 314, 76, 297, 178, 247, 272, 34, 58, 34, 316, 197, 57, 246, 254, 76, 315, 178, 84, 302, 246, 249, 178, 34, 317, 197, 251, 76, 294, 247, 261, 76, 295, 247, 262, 76, 246, 316, 233, 64, 111, 205, 29, 247, 263, 76, 298, 233, 205, 111, 64, 29, 178, 34, 185, 288, 64, 34, 318, 197, 57, 246, 317, 178, 34, 188, 34, 32, 64, 34, 318, 197, 57, 246, 317, 111, 309, 197, 298, 233, 205, 111, 64, 29, 1, 281, 111, 310, 197, 351, 178, 34, 58, 34, 319, 197, 179, 246, 233, 290, 111, 291, 29, 111, 94, 197, 148, 178, 34, 319, 175, 15, 246, 308, 111, 74, 246, 318, 178, 178, 34, 320, 197, 195, 246, 12, 246, 319, 111, 352, 178, 247, 257, 111, 303, 178, 34, 321, 197, 110, 246, 319, 247, 257, 4, 320, 233, 64, 111, 205, 29, 178, 34, 322, 197, 224, 246, 321, 111, 352, 178, 34, 323, 197, 110, 246, 305, 4, 320, 178, 34, 306, 197, 306, 247, 323, 233, 64, 111, 205, 29, 34, 324, 197, 252, 76, 294, 247, 264, 76, 295, 247, 265, 76, 246, 316, 233, 64, 111, 205, 29, 247, 266, 76, 299, 233, 205, 111, 64, 29, 178, 34, 185, 289, 64, 34, 325, 197, 57, 246, 324, 178, 34, 188, 34, 32, 64, 34, 325, 197, 57, 246, 324, 111, 309, 197, 299, 233, 205, 111, 64, 29, 1, 282, 111, 310, 197, 351, 178, 34, 58, 34, 321, 197, 321, 84, 302, 246, 325, 84, 94, 178, 34, 306, 175, 15, 246, 321, 111, 325, 178, 34, 305, 197, 320, 34, 326, 197, 110, 246, 303, 4, 320, 178, 76, 322, 34, 303, 197, 320, 76, 52, 246, 326, 178, 34, 80, 34, 138, 327, 160, 5, 246, 351, 111, 278, 178, 64, 34, 328, 197, 327, 247, 291, 247, 263, 76, 297, 34, 317, 197, 251, 76, 294, 247, 261, 76, 295, 247, 262, 76, 246, 328, 233, 64, 111, 205, 29, 247, 263, 76, 298, 233, 205, 111, 64, 29, 178, 34, 185, 288, 64, 34, 318, 197, 57, 246, 317, 178, 34, 188, 34, 32, 64, 34, 318, 197, 57, 246, 317, 111, 309, 197, 298, 233, 205, 111, 64, 29, 1, 281, 111, 310, 197, 351, 178, 34, 58, 34, 319, 197, 179, 246, 233, 290, 111, 291, 29, 111, 94, 197, 148, 178, 34, 319, 175, 15, 246, 308, 111, 74, 246, 318, 178, 178, 34, 320, 197, 195, 246, 12, 246, 319, 111, 352, 178, 247, 257, 111, 303, 178, 34, 321, 197, 110, 246, 319, 247, 257, 4, 320, 233, 64, 111, 205, 29, 178, 34, 322, 197, 224, 246, 321, 111, 352, 178, 34, 323, 197, 110, 246, 305, 4, 320, 178, 34, 306, 197, 306, 247, 323, 233, 64, 111, 205, 29, 34, 324, 197, 252, 76, 294, 247, 264, 76, 295, 247, 265, 76, 246, 328, 233, 64, 111, 205, 29, 247, 266, 76, 299, 233, 205, 111, 64, 29, 178, 34, 185, 289, 64, 34, 325, 197, 57, 246, 324, 178, 34, 188, 34, 32, 64, 34, 325, 197, 57, 246, 324, 111, 309, 197, 299, 233, 205, 111, 64, 29, 1, 282, 111, 310, 197, 351, 178, 34, 58, 34, 321, 197, 321, 84, 302, 246, 325, 84, 94, 178, 34, 306, 175, 15, 246, 321, 111, 325, 178, 34, 305, 197, 320, 34, 326, 197, 110, 246, 303, 4, 320, 178, 76, 322, 34, 303, 197, 320, 76, 52, 246, 326, 178, 34, 80, 34, 329, 197, 110, 246, 305, 4, 303, 178, 34, 306, 197, 306, 247, 329, 233, 64, 111, 205, 29, 34, 330, 197, 256, 76, 293, 247, 280, 76, 301, 34, 331, 197, 255, 76, 294, 247, 273, 76, 295, 247, 274, 76, 246, 301, 233, 64, 111, 205, 29, 247, 275, 76, 299, 233, 205, 111, 64, 29, 178, 34, 10, 246, 330, 111, 303, 178, 34, 185, 289, 64, 34, 10, 246, 331, 111, 306, 178, 34, 188, 34, 32, 64, 34, 10, 246, 331, 111, 306, 111, 309, 197, 299, 233, 205, 111, 64, 29, 1, 282, 178, 34, 58, 34, 3, 34]}, {"code": "def _bwd_permuted_block_diagonal_kernel(\n    Q,\n    K,\n    V,\n    q_sort_idx,\n    k_sort_idx,\n    DO,\n    DQ,\n    DK,\n    DV,\n    LSE,\n    D,\n    softmax_scale,\n    stride_qb,\n    stride_qh,\n    stride_qm,\n    stride_kb,\n    stride_kh,\n    stride_kn,\n    stride_vb,\n    stride_vh,\n    stride_vn,\n    stride_q_sort_idxb,\n    stride_q_sort_idxh,\n    stride_q_sort_idxm,\n    stride_k_sort_idxb,\n    stride_k_sort_idxh,\n    stride_k_sort_idxn,\n    stride_dob,\n    stride_doh,\n    stride_dom,\n    stride_dqb,\n    stride_dqh,\n    stride_dqm,\n    stride_dkb,\n    stride_dkh,\n    stride_dkn,\n    stride_dvb,\n    stride_dvh,\n    stride_dvn,\n    nheads,\n    seqlen_q,\n    block_size,\n    headdim,\n    v_headdim,\n    smooth_block,\n    CACHE_KEY_SEQLEN_Q,\n    CACHE_KEY_SEQLEN_K,\n    BLOCK_HEADDIM: tl.constexpr,\n    V_BLOCK_HEADDIM: tl.constexpr,\n    EVEN_HEADDIM: tl.constexpr,\n    EVEN_V_HEADDIM: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    Q_idx = q_sort_idx + off_b * stride_q_sort_idxb + off_h * stride_q_sort_idxh\n    K_idx = k_sort_idx + off_b * stride_k_sort_idxb + off_h * stride_k_sort_idxh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n\n    D += off_hb * seqlen_q\n    LSE += off_hb * seqlen_q\n\n    start_n = tl.program_id(0)\n    _bwd_blocked_kernel_one_col(\n        start_n=start_n,\n        Q=Q,\n        K=K,\n        V=V,\n        Q_idx=Q_idx,\n        K_idx=K_idx,\n        DO=DO,\n        DQ=DQ,\n        DK=DK,\n        DV=DV,\n        LSE=LSE,\n        D=D,\n        softmax_scale=softmax_scale,\n        stride_qm=stride_qm,\n        stride_kn=stride_kn,\n        stride_vn=stride_vn,\n        stride_dom=stride_dom,\n        stride_dqm=stride_dqm,\n        stride_dkn=stride_dkn,\n        stride_dvn=stride_dvn,\n        stride_q_idxm=stride_q_sort_idxm,\n        stride_k_idxn=stride_k_sort_idxn,\n        seqlen_q=seqlen_q,\n        block_size=block_size // BLOCK_N,\n        headdim=headdim,\n        v_headdim=v_headdim,\n        smooth_block=smooth_block,\n        BLOCK_HEADDIM=BLOCK_HEADDIM,\n        V_BLOCK_HEADDIM=V_BLOCK_HEADDIM,\n        EVEN_HEADDIM=EVEN_HEADDIM,\n        EVEN_V_HEADDIM=EVEN_V_HEADDIM,\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n    )", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 111, 263, 111, 264, 111, 265, 111, 266, 111, 267, 111, 268, 111, 269, 111, 270, 111, 271, 111, 272, 111, 273, 111, 274, 111, 275, 111, 276, 111, 277, 111, 278, 111, 279, 111, 280, 111, 281, 111, 282, 111, 283, 111, 284, 111, 285, 111, 286, 111, 287, 111, 288, 111, 289, 111, 290, 111, 291, 111, 292, 111, 293, 111, 294, 111, 295, 111, 296, 111, 297, 65, 6, 111, 298, 65, 6, 111, 299, 65, 6, 111, 300, 65, 6, 111, 301, 65, 6, 111, 302, 65, 6, 178, 65, 34, -1, 303, 197, 174, 246, 351, 178, 34, 304, 197, 303, 50, 289, 34, 305, 197, 303, 229, 289, 34, 250, 175, 304, 247, 262, 76, 305, 247, 263, 34, 251, 175, 304, 247, 265, 76, 305, 247, 266, 34, 252, 175, 304, 247, 268, 76, 305, 247, 269, 34, 306, 197, 253, 76, 304, 247, 271, 76, 305, 247, 272, 34, 307, 197, 254, 76, 304, 247, 274, 76, 305, 247, 275, 34, 255, 175, 304, 247, 277, 76, 305, 247, 278, 34, 256, 175, 304, 247, 280, 76, 305, 247, 281, 34, 257, 175, 304, 247, 283, 76, 305, 247, 284, 34, 258, 175, 304, 247, 286, 76, 305, 247, 287, 34, 260, 175, 303, 247, 290, 34, 259, 175, 303, 247, 290, 34, 308, 197, 174, 246, 352, 178, 34, 309, 246, 308, 197, 308, 111, 250, 197, 250, 111, 251, 197, 251, 111, 252, 197, 252, 111, 306, 197, 306, 111, 307, 197, 307, 111, 255, 197, 255, 111, 256, 197, 256, 111, 257, 197, 257, 111, 258, 197, 258, 111, 259, 197, 259, 111, 260, 197, 260, 111, 261, 197, 261, 111, 264, 197, 264, 111, 267, 197, 267, 111, 270, 197, 270, 111, 279, 197, 279, 111, 282, 197, 282, 111, 285, 197, 285, 111, 288, 197, 288, 111, 310, 197, 273, 111, 311, 197, 276, 111, 290, 197, 290, 111, 291, 197, 291, 50, 302, 111, 292, 197, 292, 111, 293, 197, 293, 111, 294, 197, 294, 111, 297, 197, 297, 111, 298, 197, 298, 111, 299, 197, 299, 111, 300, 197, 300, 111, 301, 197, 301, 111, 302, 197, 302, 178, 34, 3, 34]}, {"code": "def _bwd_sampled_col_kernel(\n    Q,\n    K,\n    V,\n    DO,\n    DQ,\n    DK,\n    DV,\n    LSE,\n    D,\n    softmax_scale,\n    stride_qb,\n    stride_qh,\n    stride_qm,\n    stride_kb,\n    stride_kh,\n    stride_kn,\n    stride_vb,\n    stride_vh,\n    stride_vn,\n    stride_dob,\n    stride_doh,\n    stride_dom,\n    stride_dqb,\n    stride_dqh,\n    stride_dqm,\n    stride_dkb,\n    stride_dkh,\n    stride_dkn,\n    stride_dvb,\n    stride_dvh,\n    stride_dvn,\n    nheads,\n    seqlen_q,\n    headdim,\n    v_headdim,\n    CACHE_KEY_SEQLEN_Q,\n    CACHE_KEY_SEQLEN_K,\n    BLOCK_HEADDIM: tl.constexpr,\n    V_BLOCK_HEADDIM: tl.constexpr,\n    EVEN_HEADDIM: tl.constexpr,\n    EVEN_V_HEADDIM: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n\n    Q += off_b * stride_qb + off_h * stride_qh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n\n    D += off_hb * seqlen_q\n    LSE += off_hb * seqlen_q\n\n    start_n = tl.program_id(0)\n\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    offs_vd = tl.arange(0, V_BLOCK_HEADDIM)\n\n    k_ptrs = (\n        K\n        + off_b * stride_kb\n        + off_h * stride_kh\n        + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    )\n    v_ptrs = (\n        V\n        + off_b * stride_vb\n        + off_h * stride_vh\n        + (offs_n[:, None] * stride_vn + offs_vd[None, :])\n    )\n\n    dv = tl.zeros([BLOCK_N, V_BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n\n    if EVEN_HEADDIM:\n        k = tl.load(k_ptrs)\n    else:\n        k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    if EVEN_V_HEADDIM:\n        v = tl.load(v_ptrs)\n    else:\n        v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\n\n    for start_m in range(0, seqlen_q, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        q_ptrs = Q + (offs_m_curr[:, None] * stride_qm + offs_d[None, :])\n\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n\n        qk = tl.dot(q, tl.trans(k))\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        p = tl.exp(qk * softmax_scale - lse_i[:, None])\n\n        do_ptrs = DO + (offs_m_curr[:, None] * stride_dom + offs_vd[None, :])\n        if EVEN_V_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\n        dv += tl.dot(tl.trans(p.to(do.dtype)), do)\n\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, tl.trans(v))\n\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n\n        Di = tl.load(D + offs_m_curr)\n\n        ds = (p * (dp - Di[:, None]) * softmax_scale).to(q.dtype)\n\n        dk += tl.dot(tl.trans(ds), q)\n\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n\n        dq_ptrs = DQ + (offs_m_curr[:, None] * stride_dqm + offs_d[None, :])\n        dq = tl.dot(ds, k)\n        if EVEN_HEADDIM:\n            tl.atomic_add(dq_ptrs, dq)\n        else:\n            tl.atomic_add(dq_ptrs, dq, mask=offs_d[None, :] < headdim)\n\n    dv_ptrs = (\n        DV\n        + off_b * stride_dvb\n        + off_h * stride_dvh\n        + (offs_n[:, None] * stride_dvn + offs_vd[None, :])\n    )\n    dk_ptrs = (\n        DK\n        + off_b * stride_dkb\n        + off_h * stride_dkh\n        + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    )\n    dk += tl.load(dk_ptrs)\n    dv += tl.load(dv_ptrs)\n    _bwd_store_dx(\n        dk_ptrs,\n        dk,\n        offs_d,\n        headdim,\n        even_headdim=EVEN_HEADDIM,\n    )\n    _bwd_store_dx(\n        dv_ptrs,\n        dv,\n        offs_vd,\n        v_headdim,\n        even_headdim=EVEN_V_HEADDIM,\n    )\n\n    return", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 111, 263, 111, 264, 111, 265, 111, 266, 111, 267, 111, 268, 111, 269, 111, 270, 111, 271, 111, 272, 111, 273, 111, 274, 111, 275, 111, 276, 111, 277, 111, 278, 111, 279, 111, 280, 111, 281, 111, 282, 111, 283, 111, 284, 111, 285, 111, 286, 111, 287, 64, 6, 111, 288, 64, 6, 111, 289, 64, 6, 111, 290, 64, 6, 111, 291, 64, 6, 111, 292, 64, 6, 178, 64, 34, -1, 293, 197, 174, 246, 351, 178, 34, 294, 197, 293, 50, 281, 34, 295, 197, 293, 229, 281, 34, 250, 175, 294, 247, 260, 76, 295, 247, 261, 34, 253, 175, 294, 247, 269, 76, 295, 247, 270, 34, 254, 175, 294, 247, 272, 76, 295, 247, 273, 34, 258, 175, 293, 247, 282, 34, 257, 175, 293, 247, 282, 34, 296, 197, 174, 246, 352, 178, 34, 297, 197, 296, 247, 292, 76, 78, 246, 352, 111, 292, 178, 34, 298, 197, 78, 246, 352, 111, 291, 178, 34, 299, 197, 78, 246, 352, 111, 287, 178, 34, 300, 197, 78, 246, 352, 111, 288, 178, 34, 301, 197, 251, 76, 294, 247, 263, 76, 295, 247, 264, 76, 246, 297, 233, 64, 111, 205, 29, 247, 265, 76, 299, 233, 205, 111, 64, 29, 178, 34, 302, 197, 252, 76, 294, 247, 266, 76, 295, 247, 267, 76, 246, 297, 233, 64, 111, 205, 29, 247, 268, 76, 300, 233, 205, 111, 64, 29, 178, 34, 303, 197, 179, 246, 233, 292, 111, 288, 29, 111, 94, 197, 148, 178, 34, 304, 197, 179, 246, 233, 292, 111, 287, 29, 111, 94, 197, 148, 178, 34, 185, 289, 64, 34, 305, 197, 57, 246, 301, 178, 34, 188, 34, 32, 64, 34, 305, 197, 57, 246, 301, 111, 306, 197, 299, 233, 205, 111, 64, 29, 1, 283, 111, 307, 197, 352, 178, 34, 58, 34, 185, 290, 64, 34, 308, 197, 57, 246, 302, 178, 34, 188, 34, 32, 64, 34, 308, 197, 57, 246, 302, 111, 306, 197, 300, 233, 205, 111, 64, 29, 1, 284, 111, 307, 197, 352, 178, 34, 58, 34, 138, 309, 160, 5, 246, 352, 111, 282, 111, 291, 178, 64, 34, 309, 197, 59, 246, 309, 111, 291, 178, 34, 310, 197, 309, 76, 298, 34, 311, 197, 250, 76, 246, 310, 233, 64, 111, 205, 29, 247, 262, 76, 299, 233, 205, 111, 64, 29, 178, 34, 185, 289, 64, 34, 312, 197, 57, 246, 311, 178, 34, 188, 34, 32, 64, 34, 312, 197, 57, 246, 311, 111, 306, 197, 299, 233, 205, 111, 64, 29, 1, 283, 111, 307, 197, 352, 178, 34, 58, 34, 313, 197, 15, 246, 312, 111, 74, 246, 305, 178, 178, 34, 185, 65, 289, 64, 34, 53, 246, 178, 34, 188, 34, 314, 197, 57, 246, 257, 76, 310, 178, 34, 315, 197, 110, 246, 313, 247, 259, 4, 314, 233, 64, 111, 205, 29, 178, 34, 316, 197, 253, 76, 246, 310, 233, 64, 111, 205, 29, 247, 271, 76, 300, 233, 205, 111, 64, 29, 178, 34, 185, 290, 64, 34, 317, 197, 57, 246, 316, 178, 34, 188, 34, 32, 64, 34, 317, 197, 57, 246, 316, 111, 306, 197, 300, 233, 205, 111, 64, 29, 1, 284, 111, 307, 197, 352, 178, 34, 58, 34, 303, 175, 15, 246, 74, 246, 315, 84, 318, 246, 317, 84, 94, 178, 178, 111, 317, 178, 34, 185, 65, 289, 64, 34, 53, 246, 178, 34, 188, 34, 319, 197, 15, 246, 317, 111, 74, 246, 308, 178, 178, 34, 185, 65, 289, 64, 34, 53, 246, 178, 34, 188, 34, 320, 197, 57, 246, 258, 76, 310, 178, 34, 321, 197, 246, 315, 247, 246, 319, 4, 320, 233, 64, 111, 205, 29, 178, 247, 259, 178, 84, 318, 246, 312, 84, 94, 178, 34, 304, 175, 15, 246, 74, 246, 321, 178, 111, 312, 178, 34, 185, 65, 289, 64, 34, 53, 246, 178, 34, 188, 34, 322, 197, 254, 76, 246, 310, 233, 64, 111, 205, 29, 247, 274, 76, 299, 233, 205, 111, 64, 29, 178, 34, 323, 197, 15, 246, 321, 111, 305, 178, 34, 185, 289, 64, 34, 199, 246, 322, 111, 323, 178, 34, 188, 34, 32, 64, 34, 199, 246, 322, 111, 323, 111, 306, 197, 299, 233, 205, 111, 64, 29, 1, 283, 178, 34, 58, 34, 80, 34, 324, 197, 256, 76, 294, 247, 278, 76, 295, 247, 279, 76, 246, 297, 233, 64, 111, 205, 29, 247, 280, 76, 300, 233, 205, 111, 64, 29, 178, 34, 325, 197, 255, 76, 294, 247, 275, 76, 295, 247, 276, 76, 246, 297, 233, 64, 111, 205, 29, 247, 277, 76, 299, 233, 205, 111, 64, 29, 178, 34, 304, 175, 57, 246, 325, 178, 34, 303, 175, 57, 246, 324, 178, 34, 326, 246, 325, 111, 304, 111, 299, 111, 283, 111, 327, 197, 289, 178, 34, 326, 246, 324, 111, 303, 111, 300, 111, 284, 111, 327, 197, 290, 178, 34, 234, 34, 3, 34]}, {"code": "def _rms_layernorm_backward(\n    dY,\n    dY_row_stride,\n    X,\n    X_row_stride,\n    W,\n    W_row_stride,\n    r,\n    r_row_stride,\n    dX,\n    dX_row_stride,\n    dW,\n    n_cols,\n    eps,\n    BLOCK_SIZE: tl.constexpr,\n    NUM_WARPS: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_pids = tl.num_programs(0)\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    dY_ptr = dY + pid * dY_row_stride + col_offsets\n    X_ptr = X + pid * X_row_stride + col_offsets\n    dX_ptr = dX + pid * dX_row_stride + col_offsets\n\n    dY_row = tl.load(dY_ptr, mask=mask, other=0).to(tl.float32)\n    X_row = tl.load(X_ptr, mask=mask, other=0).to(tl.float32)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0).to(tl.float32)\n    rms = tl.load(r + pid).to(tl.float32)\n\n    X_norm = X_row * rms\n    dY_W = dY_row * W_row\n    sum_dY_X = tl.sum(dY_W * X_norm, axis=0)\n    dX = rms * (dY_W - X_norm * (sum_dY_X / n_cols))\n    dW_row = dY_row * X_norm\n    tl.atomic_add(dW + col_offsets, dW_row, mask=mask)\n    tl.store(dX_ptr, dX, mask=mask)", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 111, 260, 111, 261, 111, 262, 111, 263, 65, 6, 111, 264, 65, 6, 178, 65, 34, -1, 265, 197, 174, 246, 351, 178, 34, 266, 197, 141, 246, 351, 178, 34, 267, 197, 78, 246, 351, 111, 263, 178, 34, 268, 197, 267, 1, 261, 34, 269, 197, 250, 76, 265, 247, 251, 76, 267, 34, 270, 197, 252, 76, 265, 247, 253, 76, 267, 34, 271, 197, 258, 76, 265, 247, 259, 76, 267, 34, 272, 197, 59, 246, 269, 111, 268, 197, 268, 111, 273, 197, 351, 178, 84, 274, 246, 148, 178, 34, 275, 197, 59, 246, 270, 111, 268, 197, 268, 111, 273, 197, 351, 178, 84, 274, 246, 148, 178, 34, 276, 197, 59, 246, 254, 76, 267, 111, 268, 197, 268, 111, 273, 197, 351, 178, 84, 274, 246, 148, 178, 34, 277, 197, 59, 246, 256, 76, 265, 178, 84, 274, 246, 148, 178, 34, 278, 197, 275, 247, 277, 34, 279, 197, 272, 247, 276, 34, 280, 197, 224, 246, 279, 247, 278, 111, 281, 197, 351, 178, 34, 258, 197, 277, 247, 246, 279, 4, 278, 247, 246, 280, 44, 261, 178, 178, 34, 282, 197, 272, 247, 278, 34, 199, 246, 260, 76, 267, 111, 282, 111, 268, 197, 268, 178, 34, 10, 246, 271, 111, 258, 111, 268, 197, 268, 178, 34, 3, 34]}, {"code": "def _rope_embedding(\n    Q,\n    Q_row_stride,\n    cos,\n    cos_row_stride,\n    sin,\n    sin_row_stride,\n    seqlen,\n    head_dim: tl.constexpr,\n    n_heads: tl.constexpr,\n    BACKWARD_PASS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    ROPE_GROUP_SIZE = 4\n    row_position = tl.program_id(0)\n    group_head_position = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    half_head_dim = head_dim // 2\n    mask = col_offsets < half_head_dim\n\n    sin1 = tl.load(\n        sin\n        + (row_position % seqlen) * sin_row_stride\n        + half_head_dim * 0\n        + col_offsets,\n        mask=mask,\n        other=0,\n    )\n    cos1 = tl.load(\n        cos\n        + (row_position % seqlen) * cos_row_stride\n        + half_head_dim * 0\n        + col_offsets,\n        mask=mask,\n        other=0,\n    )\n\n    if BACKWARD_PASS:\n\n        sin1 = -sin1\n    pass\n\n    head_start = group_head_position * ROPE_GROUP_SIZE\n    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)\n\n    for k in range(head_start, head_end):\n        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\n        offs_q2 = (\n            row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim\n        )\n\n        Q1 = tl.load(Q + offs_q1, mask=mask, other=0).to(sin1.dtype)\n        Q2 = tl.load(Q + offs_q2, mask=mask, other=0).to(sin1.dtype)\n\n        tl.store(Q + offs_q1, Q1 * cos1 - Q2 * sin1, mask=mask)\n        tl.store(Q + offs_q2, Q2 * cos1 + Q1 * sin1, mask=mask)\n    pass", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 64, 6, 111, 258, 64, 6, 111, 259, 64, 6, 111, 260, 64, 6, 178, 64, 34, -1, 261, 197, 351, 34, 262, 197, 174, 246, 352, 178, 34, 263, 197, 174, 246, 353, 178, 34, 264, 197, 78, 246, 352, 111, 260, 178, 34, 265, 197, 257, 50, 354, 34, 266, 197, 264, 1, 265, 34, 267, 197, 57, 246, 254, 76, 262, 229, 256, 247, 255, 76, 265, 247, 352, 76, 264, 111, 266, 197, 266, 111, 268, 197, 352, 178, 34, 269, 197, 57, 246, 252, 76, 262, 229, 256, 247, 253, 76, 265, 247, 352, 76, 264, 111, 266, 197, 266, 111, 268, 197, 352, 178, 34, 185, 259, 64, 34, 267, 197, 4, 267, 34, 188, 34, 242, 34, 270, 197, 263, 247, 261, 34, 271, 197, 41, 246, 270, 76, 261, 111, 258, 178, 34, 138, 272, 160, 5, 246, 270, 111, 271, 178, 64, 34, 273, 197, 262, 247, 251, 76, 272, 247, 257, 76, 264, 34, 274, 197, 262, 247, 251, 76, 272, 247, 257, 76, 264, 76, 265, 34, 275, 197, 57, 246, 250, 76, 273, 111, 266, 197, 266, 111, 268, 197, 352, 178, 84, 276, 246, 267, 84, 94, 178, 34, 277, 197, 57, 246, 250, 76, 274, 111, 266, 197, 266, 111, 268, 197, 352, 178, 84, 276, 246, 267, 84, 94, 178, 34, 10, 246, 250, 76, 273, 111, 275, 247, 269, 4, 277, 247, 267, 111, 266, 197, 266, 178, 34, 10, 246, 250, 76, 274, 111, 277, 247, 269, 76, 275, 247, 267, 111, 266, 197, 266, 178, 34, 80, 34, 242, 34, 3, 34]}, {"code": "def apply_clip_kernel(\n    samples_ptr, min, max, output_ptr, n_audios, audio_len, BLOCK_SIZE: tl.constexpr\n):\n    audio_idx = tl.program_id(0)\n\n    if audio_idx >= n_audios:\n        return\n\n    for i in range(0, audio_len, BLOCK_SIZE):\n        sample_idx = i + tl.arange(0, BLOCK_SIZE)\n        mask = sample_idx < audio_len\n\n        samples = tl.load(samples_ptr + audio_idx * audio_len + sample_idx, mask=mask)\n        result = tl.where(samples > max, max, samples)\n        result = tl.where(result < min, min, result)\n        tl.store(output_ptr + audio_idx * audio_len + sample_idx, result, mask=mask)", "encoded": [31, 350, 246, 250, 111, 41, 111, 51, 111, 251, 111, 252, 111, 253, 111, 254, 65, 6, 178, 65, 34, -1, 255, 197, 174, 246, 351, 178, 34, 185, 255, 147, 252, 65, 34, 234, 34, 188, 34, 138, 256, 160, 5, 246, 351, 111, 253, 111, 254, 178, 65, 34, 257, 197, 256, 76, 78, 246, 351, 111, 254, 178, 34, 258, 197, 257, 1, 253, 34, 259, 197, 59, 246, 250, 76, 255, 247, 253, 76, 257, 111, 258, 197, 258, 178, 34, 260, 197, 208, 246, 259, 127, 51, 111, 51, 111, 259, 178, 34, 260, 197, 208, 246, 260, 1, 41, 111, 41, 111, 260, 178, 34, 10, 246, 251, 76, 255, 247, 253, 76, 257, 111, 260, 111, 258, 197, 258, 178, 34, 80, 34, 3, 34]}, {"code": "def unfold_kernel(\n    input_ptr,\n    output_ptr,\n    length,\n    kernel_size,\n    stride,\n    n_frames,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    batch_idx = tl.program_id(0)\n\n    frame_idx = tl.program_id(1) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n\n    mask = frame_idx < n_frames\n\n    input_pos = frame_idx * stride\n\n    for i in range(kernel_size):\n        in_bounds = mask & ((input_pos + i) < length)\n\n        val = tl.where(\n            in_bounds,\n            tl.load(input_ptr + batch_idx * length + input_pos + i, mask=in_bounds),\n            0,\n        )\n\n        out_idx = batch_idx * n_frames * kernel_size + frame_idx * kernel_size + i\n        tl.store(output_ptr + out_idx, val, mask=in_bounds)", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 216, 111, 254, 111, 255, 64, 6, 178, 64, 34, -1, 256, 197, 174, 246, 351, 178, 34, 257, 197, 174, 246, 352, 178, 247, 255, 76, 78, 246, 351, 111, 255, 178, 34, 258, 197, 257, 1, 254, 34, 259, 197, 257, 247, 216, 34, 138, 260, 160, 5, 246, 253, 178, 64, 34, 261, 197, 258, 176, 246, 259, 76, 260, 1, 252, 178, 34, 262, 197, 208, 246, 261, 111, 57, 246, 250, 76, 256, 247, 252, 76, 259, 76, 260, 111, 258, 197, 261, 178, 111, 351, 178, 34, 263, 197, 256, 247, 254, 247, 253, 76, 257, 247, 253, 76, 260, 34, 10, 246, 251, 76, 263, 111, 262, 111, 258, 197, 261, 178, 34, 80, 34, 3, 34]}, {"code": "def complex_mul_conjugate_kernel(\n    a_real_ptr,\n    b_real_ptr,\n    a_imag_ptr,\n    b_imag_ptr,\n    output1_ptr,\n    output2_ptr,\n    num_batches,\n    num_frames,\n    fft_size,\n    BLOCK_SIZE: tl.constexpr,\n):\n\n    batch_idx = tl.program_id(0)\n\n    if batch_idx >= num_batches:\n        return\n\n    fft_idx = tl.arange(0, BLOCK_SIZE)\n    fft_mask = fft_idx < fft_size\n\n    batch_by_fft = batch_idx * fft_size\n\n    b_real_val = tl.load(b_real_ptr + batch_by_fft + fft_idx, mask=fft_mask)\n    b_imag_val = tl.load(b_imag_ptr + batch_by_fft + fft_idx, mask=fft_mask)\n\n    for frame_idx in range(num_frames):\n        global_idx = num_frames * batch_by_fft + frame_idx * fft_size + fft_idx\n\n        a_real_val = tl.load(a_real_ptr + global_idx, mask=fft_mask)\n        a_imag_val = tl.load(a_imag_ptr + global_idx, mask=fft_mask)\n\n        result1 = a_real_val * b_real_val + a_imag_val * b_imag_val\n        result2 = a_imag_val * b_real_val - a_real_val * b_imag_val\n\n        tl.store(output1_ptr + global_idx, result1, mask=fft_mask)\n        tl.store(output2_ptr + global_idx, result2, mask=fft_mask)", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 111, 256, 111, 257, 111, 258, 111, 259, 65, 6, 178, 65, 34, -1, 260, 197, 174, 246, 351, 178, 34, 185, 260, 147, 256, 65, 34, 234, 34, 188, 34, 261, 197, 78, 246, 351, 111, 259, 178, 34, 262, 197, 261, 1, 258, 34, 263, 197, 260, 247, 258, 34, 264, 197, 59, 246, 251, 76, 263, 76, 261, 111, 265, 197, 262, 178, 34, 266, 197, 59, 246, 253, 76, 263, 76, 261, 111, 265, 197, 262, 178, 34, 138, 267, 160, 5, 246, 257, 178, 65, 34, 268, 197, 257, 247, 263, 76, 267, 247, 258, 76, 261, 34, 269, 197, 59, 246, 250, 76, 268, 111, 265, 197, 262, 178, 34, 270, 197, 59, 246, 252, 76, 268, 111, 265, 197, 262, 178, 34, 271, 197, 269, 247, 264, 76, 270, 247, 266, 34, 272, 197, 270, 247, 264, 4, 269, 247, 266, 34, 10, 246, 254, 76, 268, 111, 271, 111, 265, 197, 262, 178, 34, 10, 246, 255, 76, 268, 111, 272, 111, 265, 197, 262, 178, 34, 80, 34, 3, 34]}, {"code": "def apply_gain_kernel(\n    samples_ptr,\n    amplitude_ratios_ptr,\n    output_ptr,\n    n_audios,\n    audio_len,\n    BLOCK_SIZE: tl.constexpr,\n):\n    audio_idx = tl.program_id(0)\n\n    if audio_idx >= n_audios:\n        return\n\n    gain = tl.load(amplitude_ratios_ptr + audio_idx)\n\n    for i in range(0, audio_len, BLOCK_SIZE):\n        sample_idx = i + tl.arange(0, BLOCK_SIZE)\n        mask = sample_idx < audio_len\n        samples = tl.load(samples_ptr + audio_idx * audio_len + sample_idx, mask=mask)\n        result = samples * gain\n        tl.store(output_ptr + audio_idx * audio_len + sample_idx, result, mask=mask)", "encoded": [31, 350, 246, 250, 111, 251, 111, 252, 111, 253, 111, 254, 111, 255, 64, 6, 178, 64, 34, -1, 256, 197, 174, 246, 351, 178, 34, 185, 256, 147, 253, 64, 34, 234, 34, 188, 34, 257, 197, 57, 246, 251, 76, 256, 178, 34, 138, 258, 160, 5, 246, 351, 111, 254, 111, 255, 178, 64, 34, 259, 197, 258, 76, 78, 246, 351, 111, 255, 178, 34, 260, 197, 259, 1, 254, 34, 261, 197, 57, 246, 250, 76, 256, 247, 254, 76, 259, 111, 260, 197, 260, 178, 34, 262, 197, 261, 247, 257, 34, 10, 246, 252, 76, 256, 247, 254, 76, 259, 111, 262, 111, 260, 197, 260, 178, 34, 80, 34, 3, 34]}, {"code": "def sum_with_snr_kernel(\n    clean_audio,\n    clean_audio_real_lens,\n    clean_audio_max_len,\n    desired_rms,\n    noisy_audio_ptr,\n    noisy_audio_real_lens,\n    noisy_audio_max_len,\n    output_ptr,\n    BLOCK_SIZE_SUM: tl.constexpr,\n    BLOCK_SIZE_RMS: tl.constexpr,\n):\n    batch_idx = tl.program_id(0)\n\n    clean_audio_real_lens_val = tl.load(clean_audio_real_lens + batch_idx)\n    clean_audio_rms = rms_kernel(\n        clean_audio,\n        clean_audio_real_lens,\n        clean_audio_max_len,\n        batch_idx,\n        BLOCK_SIZE_RMS,\n    )\n\n    noisy_audio_real_lens_val = tl.load(noisy_audio_real_lens + batch_idx)\n\n    noisy_audio_rms = rms_kernel(\n        noisy_audio_ptr,\n        noisy_audio_real_lens,\n        noisy_audio_max_len,\n        batch_idx,\n        BLOCK_SIZE_RMS,\n    )\n\n    desired_rms_val = tl.load(desired_rms + batch_idx)\n    relative_rms = clean_audio_rms / tl.math.pow(10.0, desired_rms_val / 20.0)\n\n    for offset in range(0, clean_audio_max_len, BLOCK_SIZE_SUM):\n        clean_audio_block_ptr = offset + tl.arange(0, BLOCK_SIZE_SUM)\n        clean_audio_mask = clean_audio_block_ptr < clean_audio_real_lens_val\n        clean_audio_vals = tl.load(\n            clean_audio + batch_idx * clean_audio_max_len + clean_audio_block_ptr,\n            mask=clean_audio_mask,\n        )\n\n        offset_over_max = offset % noisy_audio_real_lens_val\n\n        offset_adjusted = offset_over_max - tl.math.min(\n            offset_over_max,\n            tl.math.max(\n                0, (offset_over_max + BLOCK_SIZE_SUM) - noisy_audio_real_lens_val\n            ),\n        )\n\n        noisy_audio_block_ptr = offset_adjusted + tl.arange(0, BLOCK_SIZE_SUM)\n\n        noisy_audio_val = tl.load(\n            noisy_audio_ptr + batch_idx * noisy_audio_max_len + noisy_audio_block_ptr,\n            mask=noisy_audio_block_ptr < noisy_audio_real_lens_val,\n        )\n\n        tl.store(\n            output_ptr + batch_idx * clean_audio_max_len + clean_audio_block_ptr,\n            clean_audio_vals + noisy_audio_val * (relative_rms / noisy_audio_rms),\n            mask=clean_audio_mask,\n        )", "encoded": [32, 352, 248, 252, 112, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 112, 260, 66, 6, 112, 261, 66, 6, 179, 66, 35, -1, 262, 198, 175, 248, 353, 179, 35, 263, 198, 60, 248, 253, 77, 262, 179, 35, 264, 198, 265, 248, 252, 112, 253, 112, 254, 112, 262, 112, 261, 179, 35, 266, 198, 60, 248, 257, 77, 262, 179, 35, 267, 198, 265, 248, 256, 112, 257, 112, 258, 112, 262, 112, 261, 179, 35, 268, 198, 60, 248, 255, 77, 262, 179, 35, 269, 198, 264, 45, 29, 248, 354, 112, 268, 45, 355, 179, 35, 139, 270, 161, 5, 248, 353, 112, 254, 112, 260, 179, 66, 35, 271, 198, 270, 77, 79, 248, 353, 112, 260, 179, 35, 272, 198, 271, 1, 263, 35, 273, 198, 60, 248, 252, 77, 262, 249, 254, 77, 271, 112, 274, 198, 272, 179, 35, 275, 198, 270, 230, 266, 35, 276, 198, 275, 4, 246, 248, 275, 112, 46, 248, 353, 112, 275, 77, 260, 4, 266, 179, 179, 35, 277, 198, 276, 77, 79, 248, 353, 112, 260, 179, 35, 278, 198, 60, 248, 256, 77, 262, 249, 258, 77, 277, 112, 274, 198, 277, 1, 266, 179, 35, 10, 248, 259, 77, 262, 249, 254, 77, 271, 112, 273, 77, 278, 249, 248, 269, 45, 267, 179, 112, 274, 198, 272, 179, 35, 81, 35, 3, 35]}, {"code": "def min_kernel(\n    inp,\n    out_value,\n    out_index,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\n    min_values = tl.full([BLOCK_M], dtype=tl.float32, value=float(\"inf\"))\n    argmin_values = tl.full([BLOCK_M], dtype=tl.int64, value=0)\n    for start_n in range(0, N, BLOCK_N):\n\n        n_offset = start_n + tl.arange(0, BLOCK_N)\n\n        offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n\n        mask = m_offset[:, None] < M and n_offset[None, :] < N\n        inp_ptrs = inp + offset\n\n        inp_vals = tl.load(inp_ptrs, mask=mask, other=float(\"inf\"))\n        local_min, local_argmin = tl.min(inp_vals, 1, return_indices=True)\n\n        update = local_min < min_values\n        min_values = tl.where(update, local_min, min_values)\n        argmin_values = tl.where(update, start_n + local_argmin, argmin_values)\n\n    offset_index = m_offset * K + pid_k\n    out_value_ptrs = out_value + offset_index\n    out_index_ptrs = out_index + offset_index\n    mask1 = m_offset < M\n\n    tl.store(out_value_ptrs, min_values, mask=mask1)\n    tl.store(out_index_ptrs, argmin_values, mask=mask1)", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 65, 6, 112, 260, 65, 6, 180, 65, 35, -1, 261, 199, 176, 249, 354, 180, 35, 262, 199, 176, 249, 355, 180, 35, 263, 199, 261, 250, 259, 77, 79, 249, 354, 112, 259, 180, 35, 264, 199, 242, 249, 235, 259, 30, 112, 95, 199, 150, 112, 265, 199, 266, 249, 356, 180, 180, 35, 267, 199, 242, 249, 235, 259, 30, 112, 95, 199, 186, 112, 265, 199, 354, 180, 35, 140, 268, 162, 5, 249, 354, 112, 257, 112, 260, 180, 65, 35, 269, 199, 268, 77, 79, 249, 354, 112, 260, 180, 35, 270, 199, 263, 235, 65, 112, 207, 30, 250, 257, 250, 258, 77, 269, 235, 207, 112, 65, 30, 250, 258, 77, 262, 35, 271, 199, 263, 235, 65, 112, 207, 30, 1, 256, 106, 269, 235, 207, 112, 65, 30, 1, 257, 35, 272, 199, 253, 77, 270, 35, 273, 199, 58, 249, 272, 112, 271, 199, 271, 112, 274, 199, 266, 249, 356, 180, 180, 35, 275, 112, 276, 199, 134, 249, 273, 112, 355, 112, 277, 199, 165, 180, 35, 278, 199, 275, 1, 264, 35, 264, 199, 210, 249, 278, 112, 275, 112, 264, 180, 35, 267, 199, 210, 249, 278, 112, 268, 77, 276, 112, 267, 180, 35, 81, 35, 279, 199, 263, 250, 258, 77, 262, 35, 280, 199, 254, 77, 279, 35, 281, 199, 255, 77, 279, 35, 282, 199, 263, 1, 256, 35, 10, 249, 280, 112, 264, 112, 271, 199, 282, 180, 35, 10, 249, 281, 112, 267, 112, 271, 199, 282, 180, 35, 3, 35]}, {"code": "def fp8_gemm_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    a_s_ptr,\n    b_s_ptr,\n    M,\n    N: tl.constexpr,\n    K: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n    k = tl.cdiv(K, BLOCK_SIZE_K)\n    offs_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + offs_m[:, None] * K + offs_k[None, :]\n    b_ptrs = b_ptr + offs_n[None, :] * K + offs_k[:, None]\n    a_s_ptrs = a_s_ptr + offs_m * k\n    b_s_ptrs = b_s_ptr + (offs_n // BLOCK_SIZE_K) * k\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(k):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - i * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - i * BLOCK_SIZE_K, other=0.0)\n        a_s = tl.load(a_s_ptrs)\n        b_s = tl.load(b_s_ptrs)\n        accumulator += tl.dot(a, b) * a_s[:, None] * b_s[None, :]\n        a_ptrs += BLOCK_SIZE_K\n        b_ptrs += BLOCK_SIZE_K\n        a_s_ptrs += 1\n        b_s_ptrs += 1\n    c = accumulator.to(c_ptr.dtype.element_ty)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_m[:, None] * N + offs_n[None, :]\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, c, mask=mask)", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 66, 6, 112, 260, 66, 6, 112, 261, 66, 6, 112, 262, 66, 6, 112, 263, 66, 6, 180, 66, 35, -1, 264, 199, 176, 249, 265, 199, 354, 180, 35, 266, 199, 176, 249, 265, 199, 355, 180, 35, 267, 199, 68, 249, 260, 112, 263, 180, 35, 268, 199, 249, 264, 250, 261, 77, 79, 249, 354, 112, 261, 180, 180, 231, 258, 35, 269, 199, 249, 266, 250, 262, 77, 79, 249, 354, 112, 262, 180, 180, 231, 259, 35, 270, 199, 79, 249, 354, 112, 263, 180, 35, 271, 199, 253, 77, 268, 235, 66, 112, 207, 30, 250, 260, 77, 270, 235, 207, 112, 66, 30, 35, 272, 199, 254, 77, 269, 235, 207, 112, 66, 30, 250, 260, 77, 270, 235, 66, 112, 207, 30, 35, 273, 199, 256, 77, 268, 250, 267, 35, 274, 199, 257, 77, 269, 51, 263, 250, 267, 35, 275, 199, 181, 249, 249, 261, 112, 262, 180, 112, 95, 199, 150, 180, 35, 140, 276, 162, 5, 249, 267, 180, 66, 35, 277, 199, 60, 249, 271, 112, 278, 199, 270, 235, 207, 112, 66, 30, 1, 260, 4, 276, 250, 263, 112, 279, 199, 354, 180, 35, 280, 199, 60, 249, 272, 112, 278, 199, 270, 235, 66, 112, 207, 30, 1, 260, 4, 276, 250, 263, 112, 279, 199, 354, 180, 35, 281, 199, 60, 249, 273, 180, 35, 282, 199, 60, 249, 274, 180, 35, 275, 177, 15, 249, 277, 112, 280, 180, 250, 281, 235, 66, 112, 207, 30, 250, 282, 235, 207, 112, 66, 30, 35, 271, 177, 263, 35, 272, 177, 263, 35, 273, 177, 355, 35, 274, 177, 355, 35, 81, 35, 283, 199, 275, 85, 284, 249, 255, 85, 95, 85, 118, 180, 35, 268, 199, 264, 250, 261, 77, 79, 249, 354, 112, 261, 180, 35, 269, 199, 266, 250, 262, 77, 79, 249, 354, 112, 262, 180, 35, 285, 199, 255, 77, 268, 235, 66, 112, 207, 30, 250, 259, 77, 269, 235, 207, 112, 66, 30, 35, 278, 199, 249, 268, 235, 66, 112, 207, 30, 1, 258, 180, 178, 249, 269, 235, 207, 112, 66, 30, 1, 259, 180, 35, 10, 249, 285, 112, 283, 112, 278, 199, 278, 180, 35, 3, 35]}, {"code": "def matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    need_silu,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if need_silu:\n        sigmoid_x = 1.0 / (1.0 + tl.exp(-accumulator))\n        c = accumulator.to(tl.float16) * sigmoid_x.to(tl.float16)\n    else:\n        c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 112, 260, 112, 261, 112, 262, 112, 263, 112, 264, 112, 265, 112, 266, 65, 6, 112, 267, 65, 6, 112, 268, 65, 6, 112, 269, 65, 6, 180, 65, 35, -1, 270, 199, 176, 249, 271, 199, 354, 180, 35, 272, 199, 67, 249, 256, 112, 266, 180, 35, 273, 199, 67, 249, 257, 112, 267, 180, 35, 274, 199, 269, 250, 273, 35, 275, 199, 270, 51, 274, 35, 276, 199, 275, 250, 269, 35, 277, 199, 42, 249, 272, 4, 276, 112, 269, 180, 35, 278, 199, 276, 77, 270, 231, 274, 231, 277, 35, 279, 199, 270, 231, 274, 51, 277, 35, 280, 199, 249, 278, 250, 266, 77, 79, 249, 354, 112, 266, 180, 180, 231, 256, 35, 281, 199, 249, 279, 250, 267, 77, 79, 249, 354, 112, 267, 180, 180, 231, 257, 35, 282, 199, 79, 249, 354, 112, 268, 180, 35, 283, 199, 253, 77, 249, 280, 235, 65, 112, 207, 30, 250, 259, 77, 282, 235, 207, 112, 65, 30, 250, 260, 180, 35, 284, 199, 254, 77, 249, 282, 235, 65, 112, 207, 30, 250, 261, 77, 281, 235, 207, 112, 65, 30, 250, 262, 180, 35, 285, 199, 181, 249, 249, 266, 112, 267, 180, 112, 95, 199, 150, 180, 35, 140, 286, 162, 5, 249, 354, 112, 67, 249, 258, 112, 268, 180, 180, 65, 35, 287, 199, 58, 249, 283, 112, 288, 199, 282, 235, 207, 112, 65, 30, 1, 258, 4, 286, 250, 268, 112, 289, 199, 354, 180, 35, 290, 199, 58, 249, 284, 112, 288, 199, 282, 235, 65, 112, 207, 30, 1, 258, 4, 286, 250, 268, 112, 289, 199, 354, 180, 35, 285, 199, 15, 249, 287, 112, 290, 112, 285, 180, 35, 283, 177, 268, 250, 260, 35, 284, 177, 268, 250, 261, 35, 81, 35, 187, 265, 65, 35, 291, 199, 355, 45, 249, 355, 77, 111, 249, 4, 285, 180, 180, 35, 292, 199, 285, 85, 293, 249, 23, 180, 250, 291, 85, 293, 249, 23, 180, 35, 190, 35, 33, 65, 35, 292, 199, 285, 85, 293, 249, 23, 180, 35, 59, 35, 294, 199, 278, 250, 266, 77, 79, 249, 354, 112, 266, 180, 35, 295, 199, 279, 250, 267, 77, 79, 249, 354, 112, 267, 180, 35, 296, 199, 255, 77, 263, 250, 294, 235, 65, 112, 207, 30, 77, 264, 250, 295, 235, 207, 112, 65, 30, 35, 297, 199, 249, 294, 235, 65, 112, 207, 30, 1, 256, 180, 178, 249, 295, 235, 207, 112, 65, 30, 1, 257, 180, 35, 10, 249, 296, 112, 292, 112, 288, 199, 297, 180, 35, 3, 35]}, {"code": "def flash_attention2_nopad_kernel(\n    Q,\n    K,\n    V,\n    O,\n    B_Start_Loc,\n    B_Seqlen,\n    sm_scale,\n    heads,\n    num_kv_groups,\n    stride_q_bs,\n    stride_q_heads,\n    stride_q_dim,\n    stride_k_bs,\n    stride_k_heads,\n    stride_k_dim,\n    stride_v_bs,\n    stride_v_heads,\n    stride_v_dim,\n    stride_o_bs,\n    stride_o_heads,\n    stride_o_dim,\n    BLOCK_DHEAD_SIZE: tl.constexpr,\n    BLOCK_M_SIZE: tl.constexpr,\n    BLOCK_N_SIZE: tl.constexpr,\n):\n\n    block_m_idx = tl.program_id(0)\n    cur_bh = tl.program_id(1)\n    cur_batch_idx = cur_bh // heads\n    cur_head_idx = cur_bh % heads\n    cur_kv_head_idx = cur_head_idx // num_kv_groups\n\n    cur_seq_len = tl.load(B_Seqlen + cur_batch_idx)\n\n    cur_seq_start_loc = tl.load(B_Start_Loc + cur_batch_idx)\n\n    block_start_loc = block_m_idx * BLOCK_M_SIZE\n\n    offs_n = tl.arange(0, BLOCK_N_SIZE)\n    offs_d = tl.arange(0, BLOCK_DHEAD_SIZE)\n    offs_m = block_start_loc + tl.arange(0, BLOCK_M_SIZE)\n\n    q_offs = (\n        (cur_seq_start_loc + offs_m[:, None]) * stride_q_bs\n        + cur_head_idx * stride_q_heads\n        + offs_d[None, :] * stride_q_dim\n    )\n    q = tl.load(Q + q_offs, mask=offs_m[:, None] < cur_seq_len, other=0.0)\n\n    k_offs = (\n        offs_n[None, :] * stride_k_bs\n        + cur_kv_head_idx * stride_k_heads\n        + offs_d[:, None] * stride_k_dim\n    )\n    v_offs = (\n        offs_n[:, None] * stride_v_bs\n        + cur_kv_head_idx * stride_v_heads\n        + offs_d[None, :] * stride_v_dim\n    )\n\n    k_ptrs = K + k_offs\n    v_ptrs = V + v_offs\n\n    m_i = tl.zeros((BLOCK_M_SIZE,), dtype=tl.float32) - float(\"inf\")\n    d_i = tl.zeros((BLOCK_M_SIZE,), dtype=tl.float32)\n    acc = tl.zeros((BLOCK_M_SIZE, BLOCK_DHEAD_SIZE), dtype=tl.float32)\n\n    block_mask = tl.where(block_start_loc < cur_seq_len, 1, 0)\n    block_end_loc = tl.minimum(block_start_loc + BLOCK_M_SIZE, cur_seq_len)\n\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N_SIZE):\n        start_n = tl.multiple_of(start_n, BLOCK_N_SIZE)\n\n        k = tl.load(\n            k_ptrs + (cur_seq_start_loc + start_n) * stride_k_bs,\n            mask=(start_n + offs_n[None, :]) < block_end_loc,\n            other=0.0,\n        )\n\n        qk = tl.dot(q, k)\n\n        casual_mask = offs_m[:, None] >= (start_n + offs_n[None, :])\n        qk = tl.where(casual_mask, qk * sm_scale, -1.0e8)\n\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk -= m_ij[:, None]\n        p = tl.math.exp2(qk)\n        d_ij = tl.sum(p, 1)\n\n        alpha = tl.math.exp2(m_i - m_ij)\n        d_i = d_i * alpha + d_ij\n\n        acc = acc * alpha[:, None]\n\n        v = tl.load(\n            v_ptrs + (cur_seq_start_loc + start_n) * stride_v_bs,\n            mask=(start_n + offs_n[:, None]) < block_end_loc,\n            other=0.0,\n        )\n        p = p.to(v.dtype)\n        acc = tl.dot(p, v, acc)\n\n        m_i = m_ij\n\n    acc = acc / d_i[:, None]\n    off_o = (\n        (cur_seq_start_loc + offs_m[:, None]) * stride_o_bs\n        + cur_head_idx * stride_o_heads\n        + offs_d[None, :] * stride_o_dim\n    )\n    out_ptrs = O + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_seq_len)", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 112, 260, 112, 261, 112, 262, 112, 263, 112, 264, 112, 265, 112, 266, 112, 267, 112, 268, 112, 269, 112, 270, 112, 271, 112, 272, 112, 273, 112, 274, 66, 6, 112, 275, 66, 6, 112, 276, 66, 6, 180, 66, 35, -1, 277, 199, 176, 249, 354, 180, 35, 278, 199, 176, 249, 355, 180, 35, 279, 199, 278, 51, 260, 35, 280, 199, 278, 231, 260, 35, 281, 199, 280, 51, 261, 35, 282, 199, 60, 249, 258, 77, 279, 180, 35, 283, 199, 60, 249, 257, 77, 279, 180, 35, 284, 199, 277, 250, 275, 35, 285, 199, 79, 249, 354, 112, 276, 180, 35, 286, 199, 79, 249, 354, 112, 274, 180, 35, 287, 199, 284, 77, 79, 249, 354, 112, 275, 180, 35, 288, 199, 249, 283, 77, 287, 235, 66, 112, 207, 30, 180, 250, 262, 77, 280, 250, 263, 77, 286, 235, 207, 112, 66, 30, 250, 264, 35, 289, 199, 60, 249, 253, 77, 288, 112, 290, 199, 287, 235, 66, 112, 207, 30, 1, 282, 112, 291, 199, 354, 180, 35, 292, 199, 285, 235, 207, 112, 66, 30, 250, 265, 77, 281, 250, 266, 77, 286, 235, 66, 112, 207, 30, 250, 267, 35, 293, 199, 285, 235, 66, 112, 207, 30, 250, 268, 77, 281, 250, 269, 77, 286, 235, 207, 112, 66, 30, 250, 270, 35, 294, 199, 254, 77, 292, 35, 295, 199, 255, 77, 293, 35, 296, 199, 181, 249, 249, 275, 112, 180, 112, 95, 199, 150, 180, 4, 297, 249, 356, 180, 35, 298, 199, 181, 249, 249, 275, 112, 180, 112, 95, 199, 150, 180, 35, 299, 199, 181, 249, 249, 275, 112, 274, 180, 112, 95, 199, 150, 180, 35, 300, 199, 210, 249, 284, 1, 282, 112, 355, 112, 354, 180, 35, 301, 199, 70, 249, 284, 77, 275, 112, 282, 180, 35, 140, 302, 162, 5, 249, 354, 112, 300, 250, 301, 112, 276, 180, 66, 35, 302, 199, 58, 249, 302, 112, 276, 180, 35, 303, 199, 60, 249, 294, 77, 249, 283, 77, 302, 180, 250, 265, 112, 290, 199, 302, 77, 285, 235, 207, 112, 66, 30, 1, 301, 112, 291, 199, 354, 180, 35, 304, 199, 15, 249, 289, 112, 303, 180, 35, 305, 199, 287, 235, 66, 112, 207, 30, 149, 302, 77, 285, 235, 207, 112, 66, 30, 35, 304, 199, 210, 249, 305, 112, 304, 250, 259, 112, 4, 357, 180, 35, 306, 199, 197, 249, 296, 112, 12, 249, 304, 112, 355, 180, 180, 35, 304, 2, 306, 235, 66, 112, 207, 30, 35, 307, 199, 160, 249, 304, 180, 35, 308, 199, 226, 249, 307, 112, 355, 180, 35, 309, 199, 160, 249, 296, 4, 306, 180, 35, 298, 199, 298, 250, 309, 77, 308, 35, 299, 199, 299, 250, 309, 235, 66, 112, 207, 30, 35, 310, 199, 60, 249, 295, 77, 249, 283, 77, 302, 180, 250, 268, 112, 290, 199, 302, 77, 285, 235, 66, 112, 207, 30, 1, 301, 112, 291, 199, 354, 180, 35, 307, 199, 307, 85, 311, 249, 310, 85, 95, 180, 35, 299, 199, 15, 249, 307, 112, 310, 112, 299, 180, 35, 296, 199, 306, 35, 81, 35, 299, 199, 299, 45, 298, 235, 66, 112, 207, 30, 35, 312, 199, 249, 283, 77, 287, 235, 66, 112, 207, 30, 180, 250, 271, 77, 280, 250, 272, 77, 286, 235, 207, 112, 66, 30, 250, 273, 35, 313, 199, 256, 77, 312, 35, 10, 249, 313, 112, 299, 112, 290, 199, 287, 235, 66, 112, 207, 30, 1, 282, 180, 35, 3, 35]}, {"code": "def _fwd_kernel(\n    Q,\n    K,\n    V,\n    Bias,\n    Out,\n    Lse,\n    TMP,\n    softmax_scale,\n    stride_qb,\n    stride_qh,\n    stride_qm,\n    stride_kb,\n    stride_kh,\n    stride_kn,\n    stride_vb,\n    stride_vh,\n    stride_vn,\n    stride_bb,\n    stride_bh,\n    stride_bm,\n    stride_ob,\n    stride_oh,\n    stride_om,\n    nheads,\n    seqlen_q,\n    seqlen_k,\n    seqlen_q_rounded,\n    headdim,\n    CACHE_KEY_SEQLEN_Q,\n    CACHE_KEY_SEQLEN_K,\n    BIAS_TYPE: tl.constexpr,\n    IS_CAUSAL: tl.constexpr,\n    BLOCK_HEADDIM: tl.constexpr,\n    EVEN_M: tl.constexpr,\n    EVEN_N: tl.constexpr,\n    EVEN_HEADDIM: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n\n    q_ptrs = (\n        Q\n        + off_b * stride_qb\n        + off_h * stride_qh\n        + (offs_m[:, None] * stride_qm + offs_d[None, :])\n    )\n    k_ptrs = (\n        K\n        + off_b * stride_kb\n        + off_h * stride_kh\n        + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    )\n    v_ptrs = (\n        V\n        + off_b * stride_vb\n        + off_h * stride_vh\n        + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    )\n    if BIAS_TYPE == \"vector\":\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == \"matrix\":\n        b_ptrs = (\n            Bias\n            + off_b * stride_bb\n            + off_h * stride_bh\n            + (offs_m[:, None] * stride_bm + offs_n[None, :])\n        )\n\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    else:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n        else:\n            q = tl.load(\n                q_ptrs,\n                mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim),\n                other=0.0,\n            )\n\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) * BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(\n                    k_ptrs + start_n * stride_kn,\n                    mask=offs_d[None, :] < headdim,\n                    other=0.0,\n                )\n        else:\n            if EVEN_HEADDIM:\n                k = tl.load(\n                    k_ptrs + start_n * stride_kn,\n                    mask=(start_n + offs_n)[:, None] < seqlen_k,\n                    other=0.0,\n                )\n            else:\n                k = tl.load(\n                    k_ptrs + start_n * stride_kn,\n                    mask=((start_n + offs_n)[:, None] < seqlen_k)\n                    & (offs_d[None, :] < headdim),\n                    other=0.0,\n                )\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float(\"-inf\"))\n        if IS_CAUSAL:\n            qk += tl.where(\n                offs_m[:, None] >= (start_n + offs_n)[None, :], 0, float(\"-inf\")\n            )\n        if BIAS_TYPE != \"none\":\n            if BIAS_TYPE == \"vector\":\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n).to(tl.float32)\n                else:\n                    bias = tl.load(\n                        b_ptrs + start_n, mask=(start_n + offs_n) < seqlen_k, other=0.0\n                    ).to(tl.float32)\n                bias = bias[None, :]\n            elif BIAS_TYPE == \"matrix\":\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n).to(tl.float32)\n                else:\n                    bias = tl.load(\n                        b_ptrs + start_n,\n                        mask=(offs_m[:, None] < seqlen_q)\n                        & ((start_n + offs_n)[None, :] < seqlen_k),\n                        other=0.0,\n                    ).to(tl.float32)\n\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n\n        acc_o_scale = tl.exp(m_i - m_ij)\n\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(\n                    v_ptrs + start_n * stride_vn,\n                    mask=offs_d[None, :] < headdim,\n                    other=0.0,\n                )\n        else:\n            if EVEN_HEADDIM:\n                v = tl.load(\n                    v_ptrs + start_n * stride_vn,\n                    mask=(start_n + offs_n)[:, None] < seqlen_k,\n                    other=0.0,\n                )\n            else:\n                v = tl.load(\n                    v_ptrs + start_n * stride_vn,\n                    mask=((start_n + offs_n)[:, None] < seqlen_k)\n                    & (offs_d[None, :] < headdim),\n                    other=0.0,\n                )\n        p = p.to(v.dtype)\n        acc_o += tl.dot(p, v)\n\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n\n    o_scale = tl.exp(m_i - lse_i)\n\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = (\n        Out\n        + off_b * stride_ob\n        + off_h * stride_oh\n        + (offs_m[:, None] * stride_om + offs_d[None, :])\n    )\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    else:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n        else:\n            tl.store(\n                out_ptrs,\n                acc_o,\n                mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim),\n            )", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 112, 260, 112, 261, 112, 262, 112, 263, 112, 264, 112, 265, 112, 266, 112, 267, 112, 268, 112, 269, 112, 270, 112, 271, 112, 272, 112, 273, 112, 274, 112, 275, 112, 276, 112, 277, 112, 278, 112, 279, 112, 280, 112, 281, 112, 282, 112, 283, 65, 6, 112, 284, 65, 6, 112, 285, 65, 6, 112, 286, 65, 6, 112, 287, 65, 6, 112, 288, 65, 6, 112, 289, 65, 6, 112, 290, 65, 6, 180, 65, 35, -1, 291, 199, 176, 249, 354, 180, 35, 292, 199, 176, 249, 355, 180, 35, 293, 199, 292, 51, 276, 35, 294, 199, 292, 231, 276, 35, 295, 199, 291, 250, 289, 77, 79, 249, 354, 112, 289, 180, 35, 296, 199, 79, 249, 354, 112, 290, 180, 35, 297, 199, 79, 249, 354, 112, 285, 180, 35, 298, 199, 253, 77, 293, 250, 261, 77, 294, 250, 262, 77, 249, 295, 235, 65, 112, 207, 30, 250, 263, 77, 297, 235, 207, 112, 65, 30, 180, 35, 299, 199, 254, 77, 293, 250, 264, 77, 294, 250, 265, 77, 249, 296, 235, 65, 112, 207, 30, 250, 266, 77, 297, 235, 207, 112, 65, 30, 180, 35, 300, 199, 255, 77, 293, 250, 267, 77, 294, 250, 268, 77, 249, 296, 235, 65, 112, 207, 30, 250, 269, 77, 297, 235, 207, 112, 65, 30, 180, 35, 187, 283, 80, 356, 65, 35, 301, 199, 256, 77, 293, 250, 270, 77, 294, 250, 271, 77, 296, 35, 68, 35, 40, 283, 80, 357, 65, 35, 301, 199, 256, 77, 293, 250, 270, 77, 294, 250, 271, 77, 249, 295, 235, 65, 112, 207, 30, 250, 272, 77, 296, 235, 207, 112, 65, 30, 180, 35, 190, 35, 302, 199, 259, 77, 292, 250, 279, 77, 295, 35, 303, 199, 181, 249, 235, 289, 30, 112, 95, 199, 150, 180, 4, 304, 249, 358, 180, 35, 305, 199, 181, 249, 235, 289, 30, 112, 95, 199, 150, 180, 4, 304, 249, 358, 180, 35, 306, 199, 181, 249, 235, 289, 112, 285, 30, 112, 95, 199, 150, 180, 35, 187, 286, 178, 287, 65, 35, 187, 288, 65, 35, 307, 199, 58, 249, 298, 180, 35, 190, 35, 33, 65, 35, 307, 199, 58, 249, 298, 112, 308, 199, 297, 235, 207, 112, 65, 30, 1, 280, 112, 309, 199, 354, 180, 35, 59, 35, 68, 35, 40, 288, 65, 35, 307, 199, 58, 249, 298, 112, 308, 199, 295, 235, 65, 112, 207, 30, 1, 277, 112, 309, 199, 354, 180, 35, 190, 35, 33, 65, 35, 307, 199, 58, 249, 298, 112, 308, 199, 249, 295, 235, 65, 112, 207, 30, 1, 277, 180, 178, 249, 297, 235, 207, 112, 65, 30, 1, 280, 180, 112, 309, 199, 354, 180, 35, 59, 35, 310, 199, 278, 187, 66, 284, 33, 69, 249, 249, 291, 77, 355, 180, 250, 289, 112, 278, 180, 35, 140, 311, 162, 5, 249, 354, 112, 310, 112, 290, 180, 65, 35, 311, 199, 60, 249, 311, 112, 290, 180, 35, 187, 287, 178, 286, 65, 35, 187, 288, 65, 35, 312, 199, 58, 249, 299, 77, 311, 250, 266, 180, 35, 190, 35, 33, 65, 35, 312, 199, 58, 249, 299, 77, 311, 250, 266, 112, 308, 199, 297, 235, 207, 112, 65, 30, 1, 280, 112, 309, 199, 354, 180, 35, 59, 35, 68, 35, 40, 288, 65, 35, 312, 199, 58, 249, 299, 77, 311, 250, 266, 112, 308, 199, 249, 311, 77, 296, 180, 235, 65, 112, 207, 30, 1, 278, 112, 309, 199, 354, 180, 35, 190, 35, 33, 65, 35, 312, 199, 58, 249, 299, 77, 311, 250, 266, 112, 308, 199, 249, 249, 311, 77, 296, 180, 235, 65, 112, 207, 30, 1, 278, 180, 178, 249, 297, 235, 207, 112, 65, 30, 1, 280, 180, 112, 309, 199, 354, 180, 35, 59, 35, 313, 199, 181, 249, 235, 289, 112, 290, 30, 112, 95, 199, 150, 180, 35, 313, 177, 15, 249, 307, 112, 312, 112, 314, 199, 165, 180, 35, 187, 66, 287, 65, 35, 313, 177, 210, 249, 249, 311, 77, 296, 180, 235, 207, 112, 65, 30, 1, 278, 112, 354, 112, 304, 249, 359, 180, 180, 35, 190, 35, 187, 284, 65, 35, 313, 177, 210, 249, 295, 235, 65, 112, 207, 30, 149, 249, 311, 77, 296, 180, 235, 207, 112, 65, 30, 112, 354, 112, 304, 249, 359, 180, 180, 35, 190, 35, 187, 283, 191, 360, 65, 35, 187, 283, 80, 356, 65, 35, 187, 287, 65, 35, 315, 199, 58, 249, 301, 77, 311, 180, 85, 316, 249, 150, 180, 35, 190, 35, 33, 65, 35, 315, 199, 58, 249, 301, 77, 311, 112, 308, 199, 311, 77, 296, 1, 278, 112, 309, 199, 354, 180, 85, 316, 249, 150, 180, 35, 59, 35, 315, 199, 315, 235, 207, 112, 65, 30, 35, 68, 35, 40, 283, 80, 357, 65, 35, 187, 286, 178, 287, 65, 35, 315, 199, 58, 249, 301, 77, 311, 180, 85, 316, 249, 150, 180, 35, 190, 35, 33, 65, 35, 315, 199, 58, 249, 301, 77, 311, 112, 308, 199, 249, 295, 235, 65, 112, 207, 30, 1, 277, 180, 178, 249, 249, 311, 77, 296, 180, 235, 207, 112, 65, 30, 1, 278, 180, 112, 309, 199, 354, 180, 85, 316, 249, 150, 180, 35, 59, 35, 190, 35, 313, 199, 313, 250, 260, 77, 315, 35, 317, 199, 197, 249, 12, 249, 313, 112, 355, 180, 112, 303, 180, 35, 318, 199, 111, 249, 313, 4, 317, 235, 65, 112, 207, 30, 180, 35, 190, 35, 33, 65, 35, 317, 199, 197, 249, 12, 249, 313, 112, 355, 180, 250, 260, 112, 303, 180, 35, 318, 199, 111, 249, 313, 250, 260, 4, 317, 235, 65, 112, 207, 30, 180, 35, 59, 35, 319, 199, 226, 249, 318, 112, 355, 180, 35, 320, 199, 111, 249, 305, 4, 317, 180, 35, 10, 249, 302, 112, 320, 180, 35, 320, 199, 58, 249, 302, 180, 35, 306, 199, 306, 250, 320, 235, 65, 112, 207, 30, 35, 187, 287, 178, 286, 65, 35, 187, 288, 65, 35, 321, 199, 58, 249, 300, 77, 311, 250, 269, 180, 35, 190, 35, 33, 65, 35, 321, 199, 58, 249, 300, 77, 311, 250, 269, 112, 308, 199, 297, 235, 207, 112, 65, 30, 1, 280, 112, 309, 199, 354, 180, 35, 59, 35, 68, 35, 40, 288, 65, 35, 321, 199, 58, 249, 300, 77, 311, 250, 269, 112, 308, 199, 249, 311, 77, 296, 180, 235, 65, 112, 207, 30, 1, 278, 112, 309, 199, 354, 180, 35, 190, 35, 33, 65, 35, 321, 199, 58, 249, 300, 77, 311, 250, 269, 112, 308, 199, 249, 249, 311, 77, 296, 180, 235, 65, 112, 207, 30, 1, 278, 180, 178, 249, 297, 235, 207, 112, 65, 30, 1, 280, 180, 112, 309, 199, 354, 180, 35, 59, 35, 318, 199, 318, 85, 316, 249, 321, 85, 95, 180, 35, 306, 177, 15, 249, 318, 112, 321, 180, 35, 305, 199, 317, 35, 322, 199, 111, 249, 303, 4, 317, 180, 77, 319, 35, 303, 199, 317, 77, 53, 249, 322, 180, 35, 81, 35, 323, 199, 111, 249, 305, 4, 303, 180, 35, 10, 249, 302, 112, 323, 180, 35, 323, 199, 58, 249, 302, 180, 35, 306, 199, 306, 250, 323, 235, 65, 112, 207, 30, 35, 291, 199, 176, 249, 354, 180, 35, 295, 199, 291, 250, 289, 77, 79, 249, 354, 112, 289, 180, 35, 324, 199, 258, 77, 292, 250, 279, 77, 295, 35, 10, 249, 324, 112, 303, 180, 35, 297, 199, 79, 249, 354, 112, 285, 180, 35, 325, 199, 257, 77, 293, 250, 273, 77, 294, 250, 274, 77, 249, 295, 235, 65, 112, 207, 30, 250, 275, 77, 297, 235, 207, 112, 65, 30, 180, 35, 187, 286, 65, 35, 187, 288, 65, 35, 10, 249, 325, 112, 306, 180, 35, 190, 35, 33, 65, 35, 10, 249, 325, 112, 306, 112, 308, 199, 297, 235, 207, 112, 65, 30, 1, 280, 180, 35, 59, 35, 68, 35, 40, 288, 65, 35, 10, 249, 325, 112, 306, 112, 308, 199, 295, 235, 65, 112, 207, 30, 1, 277, 180, 35, 190, 35, 33, 65, 35, 10, 249, 325, 112, 306, 112, 308, 199, 249, 295, 235, 65, 112, 207, 30, 1, 277, 180, 178, 249, 297, 235, 207, 112, 65, 30, 1, 280, 180, 180, 35, 59, 35, 3, 35]}, {"code": "def _bwd_kernel(\n    Q,\n    K,\n    V,\n    Bias,\n    DO,\n    DQ,\n    DK,\n    DV,\n    LSE,\n    D,\n    softmax_scale,\n    stride_qb,\n    stride_qh,\n    stride_qm,\n    stride_kb,\n    stride_kh,\n    stride_kn,\n    stride_vb,\n    stride_vh,\n    stride_vn,\n    stride_bb,\n    stride_bh,\n    stride_bm,\n    stride_dob,\n    stride_doh,\n    stride_dom,\n    stride_dqb,\n    stride_dqh,\n    stride_dqm,\n    stride_dkb,\n    stride_dkh,\n    stride_dkn,\n    stride_dvb,\n    stride_dvh,\n    stride_dvn,\n    nheads,\n    seqlen_q,\n    seqlen_k,\n    seqlen_q_rounded,\n    headdim,\n    CACHE_KEY_SEQLEN_Q,\n    CACHE_KEY_SEQLEN_K,\n    BIAS_TYPE: tl.constexpr,\n    IS_CAUSAL: tl.constexpr,\n    BLOCK_HEADDIM: tl.constexpr,\n    SEQUENCE_PARALLEL: tl.constexpr,\n    EVEN_M: tl.constexpr,\n    EVEN_N: tl.constexpr,\n    EVEN_HEADDIM: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != \"none\":\n        Bias += off_b * stride_bb + off_h * stride_bh\n\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(\n                start_n,\n                Q,\n                K,\n                V,\n                Bias,\n                DO,\n                DQ,\n                DK,\n                DV,\n                LSE,\n                D,\n                softmax_scale,\n                stride_qm,\n                stride_kn,\n                stride_vn,\n                stride_bm,\n                stride_dom,\n                stride_dqm,\n                stride_dkn,\n                stride_dvn,\n                seqlen_q,\n                seqlen_k,\n                headdim,\n                ATOMIC_ADD=False,\n                BIAS_TYPE=BIAS_TYPE,\n                IS_CAUSAL=IS_CAUSAL,\n                BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M,\n                EVEN_N=EVEN_N,\n                EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M,\n                BLOCK_N=BLOCK_N,\n            )\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(\n            start_n,\n            Q,\n            K,\n            V,\n            Bias,\n            DO,\n            DQ,\n            DK,\n            DV,\n            LSE,\n            D,\n            softmax_scale,\n            stride_qm,\n            stride_kn,\n            stride_vn,\n            stride_bm,\n            stride_dom,\n            stride_dqm,\n            stride_dkn,\n            stride_dvn,\n            seqlen_q,\n            seqlen_k,\n            headdim,\n            ATOMIC_ADD=True,\n            BIAS_TYPE=BIAS_TYPE,\n            IS_CAUSAL=IS_CAUSAL,\n            BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M,\n            EVEN_N=EVEN_N,\n            EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M,\n            BLOCK_N=BLOCK_N,\n        )", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 112, 260, 112, 261, 112, 262, 112, 263, 112, 264, 112, 265, 112, 266, 112, 267, 112, 268, 112, 269, 112, 270, 112, 271, 112, 272, 112, 273, 112, 274, 112, 275, 112, 276, 112, 277, 112, 278, 112, 279, 112, 280, 112, 281, 112, 282, 112, 283, 112, 284, 112, 285, 112, 286, 112, 287, 112, 288, 112, 289, 112, 290, 112, 291, 112, 292, 112, 293, 112, 294, 112, 295, 66, 6, 112, 296, 66, 6, 112, 297, 66, 6, 112, 298, 66, 6, 112, 299, 66, 6, 112, 300, 66, 6, 112, 301, 66, 6, 112, 302, 66, 6, 112, 303, 66, 6, 180, 66, 35, -1, 304, 199, 176, 249, 354, 180, 35, 305, 199, 304, 51, 288, 35, 306, 199, 304, 231, 288, 35, 253, 177, 305, 250, 264, 77, 306, 250, 265, 35, 254, 177, 305, 250, 267, 77, 306, 250, 268, 35, 255, 177, 305, 250, 270, 77, 306, 250, 271, 35, 257, 177, 305, 250, 276, 77, 306, 250, 277, 35, 258, 177, 305, 250, 279, 77, 306, 250, 280, 35, 259, 177, 305, 250, 282, 77, 306, 250, 283, 35, 260, 177, 305, 250, 285, 77, 306, 250, 286, 35, 187, 295, 191, 355, 66, 35, 256, 177, 305, 250, 273, 77, 306, 250, 274, 35, 190, 35, 262, 177, 304, 250, 291, 35, 261, 177, 304, 250, 291, 35, 187, 67, 298, 66, 35, 307, 199, 68, 249, 290, 112, 303, 180, 35, 140, 308, 162, 5, 249, 356, 112, 307, 180, 66, 35, 309, 249, 308, 112, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 112, 260, 112, 261, 112, 262, 112, 263, 112, 266, 112, 269, 112, 272, 112, 275, 112, 278, 112, 281, 112, 284, 112, 287, 112, 289, 112, 290, 112, 292, 112, 310, 199, 62, 112, 295, 199, 295, 112, 296, 199, 296, 112, 297, 199, 297, 112, 299, 199, 299, 112, 300, 199, 300, 112, 301, 199, 301, 112, 302, 199, 302, 112, 303, 199, 303, 180, 35, 81, 35, 190, 35, 33, 66, 35, 308, 199, 176, 249, 356, 180, 35, 309, 249, 308, 112, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 112, 260, 112, 261, 112, 262, 112, 263, 112, 266, 112, 269, 112, 272, 112, 275, 112, 278, 112, 281, 112, 284, 112, 287, 112, 289, 112, 290, 112, 292, 112, 310, 199, 165, 112, 295, 199, 295, 112, 296, 199, 296, 112, 297, 199, 297, 112, 299, 199, 299, 112, 300, 199, 300, 112, 301, 199, 301, 112, 302, 199, 302, 112, 303, 199, 303, 180, 35, 59, 35, 3, 35]}, {"code": "def swizzle_tile(\n    tile_id,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n):\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n\n    width = GROUP_M * grid_n\n    group_id = tile_id // width\n    group_size = tl.minimum(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (tile_id % group_size)\n    pid_n = (tile_id % width) // group_size\n    return pid_m, pid_n", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 65, 6, 112, 258, 65, 6, 112, 259, 65, 6, 112, 260, 65, 6, 180, 65, 35, -1, 261, 199, 67, 249, 254, 112, 257, 180, 35, 262, 199, 67, 249, 255, 112, 258, 180, 35, 263, 199, 260, 250, 262, 35, 264, 199, 253, 51, 263, 35, 265, 199, 69, 249, 261, 4, 264, 250, 260, 112, 260, 180, 35, 266, 199, 264, 250, 260, 77, 253, 231, 265, 35, 267, 199, 253, 231, 263, 51, 265, 35, 236, 249, 266, 112, 267, 180, 35, 3, 35]}, {"code": "def linear_tile(\n    tile_id,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n):\n    pid_m = tile_id // tl.cdiv(N, BLOCK_N)\n    pid_n = tile_id % tl.cdiv(N, BLOCK_N)\n    return pid_m, pid_n", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 66, 6, 112, 258, 66, 6, 112, 259, 66, 6, 112, 260, 66, 6, 180, 66, 35, -1, 261, 199, 253, 51, 68, 249, 255, 112, 258, 180, 35, 262, 199, 253, 231, 68, 249, 255, 112, 258, 180, 35, 236, 249, 261, 112, 262, 180, 35, 3, 35]}, {"code": "def matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n\n    pid = tl.program_id(axis=0)\n\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n\n        accumulator += tl.dot(a, b)\n\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 112, 260, 112, 261, 112, 262, 112, 263, 112, 264, 112, 265, 65, 6, 112, 266, 65, 6, 112, 267, 65, 6, 112, 268, 65, 6, 180, 65, 35, -1, 269, 199, 176, 249, 270, 199, 354, 180, 35, 271, 199, 67, 249, 256, 112, 265, 180, 35, 272, 199, 67, 249, 257, 112, 266, 180, 35, 273, 199, 268, 250, 272, 35, 274, 199, 269, 51, 273, 35, 275, 199, 274, 250, 268, 35, 276, 199, 42, 249, 271, 4, 275, 112, 268, 180, 35, 277, 199, 275, 77, 269, 231, 276, 35, 278, 199, 269, 231, 273, 51, 276, 35, 279, 199, 277, 250, 265, 77, 79, 249, 354, 112, 265, 180, 35, 280, 199, 278, 250, 266, 77, 79, 249, 354, 112, 266, 180, 35, 281, 199, 79, 249, 354, 112, 267, 180, 35, 282, 199, 253, 77, 249, 279, 235, 65, 112, 207, 30, 250, 259, 77, 281, 235, 207, 112, 65, 30, 250, 260, 180, 35, 283, 199, 254, 77, 249, 281, 235, 65, 112, 207, 30, 250, 261, 77, 280, 235, 207, 112, 65, 30, 250, 262, 180, 35, 284, 199, 181, 249, 249, 265, 112, 266, 180, 112, 95, 199, 150, 180, 35, 140, 285, 162, 5, 249, 354, 112, 258, 112, 267, 180, 65, 35, 286, 199, 58, 249, 282, 180, 35, 287, 199, 58, 249, 283, 180, 35, 284, 177, 15, 249, 286, 112, 287, 180, 35, 282, 177, 267, 250, 260, 35, 283, 177, 267, 250, 261, 35, 81, 35, 288, 199, 284, 85, 289, 249, 23, 180, 35, 290, 199, 277, 250, 265, 77, 79, 249, 354, 112, 265, 180, 35, 291, 199, 278, 250, 266, 77, 79, 249, 354, 112, 266, 180, 35, 292, 199, 255, 77, 263, 250, 290, 235, 65, 112, 207, 30, 77, 264, 250, 291, 235, 207, 112, 65, 30, 35, 293, 199, 249, 290, 235, 65, 112, 207, 30, 1, 256, 180, 178, 249, 291, 235, 207, 112, 65, 30, 1, 257, 180, 35, 10, 249, 292, 112, 288, 112, 294, 199, 293, 180, 35, 3, 35]}, {"code": "def tile_swizzling(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M: tl.constexpr):\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n\n    width = GROUP_M * grid_n\n    group_id = tile_id // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (tile_id % group_size)\n    pid_n = (tile_id % width) // group_size\n    return pid_m, pid_n", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 112, 260, 66, 6, 180, 66, 35, -1, 261, 199, 68, 249, 254, 112, 257, 180, 35, 262, 199, 68, 249, 255, 112, 258, 180, 35, 263, 199, 260, 250, 262, 35, 264, 199, 253, 51, 263, 35, 265, 199, 42, 249, 261, 4, 264, 250, 260, 112, 260, 180, 35, 266, 199, 264, 250, 260, 77, 253, 231, 265, 35, 267, 199, 253, 231, 263, 51, 265, 35, 236, 249, 266, 112, 267, 180, 35, 3, 35]}, {"code": "def tile_classic(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M: tl.constexpr):\n    pid_m = tile_id // tl.cdiv(N, BLOCK_N)\n    pid_n = tile_id % tl.cdiv(N, BLOCK_N)\n    return pid_m, pid_n", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 112, 260, 65, 6, 180, 65, 35, -1, 261, 199, 253, 51, 67, 249, 255, 112, 258, 180, 35, 262, 199, 253, 231, 67, 249, 255, 112, 258, 180, 35, 236, 249, 261, 112, 262, 180, 35, 3, 35]}, {"code": "def _rms_norm_fwd_kernel(\n    X,\n    stride_x,\n    Y,\n    stride_y,\n    W,\n    Rstd,\n    eps,\n    M,\n    N,\n    block_N: tl.constexpr,\n):\n\n    row = tl.program_id(0)\n    cols = tl.arange(0, block_N)\n\n    mask = cols < N\n    x = tl.load(X + row * stride_x + cols, mask=mask, other=0.0).to(tl.float32)\n    w = tl.load(W + cols, mask=mask, other=0.0).to(tl.float32)\n\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n\n    tl.store(Rstd + row, rstd)\n\n    x_hat = x * rstd\n    y = x_hat * w\n\n    tl.store(Y + row * stride_y + cols, y, mask=mask)", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 112, 260, 112, 261, 112, 262, 66, 6, 180, 66, 35, -1, 263, 199, 176, 249, 354, 180, 35, 264, 199, 79, 249, 354, 112, 262, 180, 35, 265, 199, 264, 1, 261, 35, 266, 199, 60, 249, 253, 77, 263, 250, 254, 77, 264, 112, 265, 199, 265, 112, 267, 199, 354, 180, 85, 268, 249, 150, 180, 35, 269, 199, 60, 249, 257, 77, 264, 112, 265, 199, 265, 112, 267, 199, 354, 180, 85, 268, 249, 150, 180, 35, 270, 199, 210, 249, 264, 1, 261, 112, 266, 112, 354, 180, 35, 271, 199, 226, 249, 270, 250, 270, 112, 272, 199, 354, 180, 45, 261, 35, 273, 199, 355, 45, 134, 249, 271, 77, 259, 180, 35, 10, 249, 258, 77, 263, 112, 273, 180, 35, 274, 199, 266, 250, 273, 35, 275, 199, 274, 250, 269, 35, 10, 249, 255, 77, 263, 250, 256, 77, 264, 112, 275, 112, 265, 199, 265, 180, 35, 3, 35]}, {"code": "def _rms_norm_bwd_kernel_sm(\n    X,\n    stride_x,\n    W,\n    DY,\n    stride_dy,\n    DX,\n    stride_dx,\n    Rstd,\n    DW,\n    eps,\n    M,\n    N,\n    rows_per_program,\n    block_N: tl.constexpr,\n):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, block_N)\n    mask = cols < N\n\n    w = tl.load(W + cols, mask=mask, other=0.0).to(tl.float32)\n\n    dw = tl.zeros((block_N,), dtype=tl.float32)\n\n    row_end = min(row_start + rows_per_program, M)\n    for row in range(row_start, row_end):\n\n        x = tl.load(X + row * stride_x + cols, mask=mask, other=0.0).to(tl.float32)\n        dy = tl.load(DY + row * stride_dy + cols, mask=mask, other=0.0).to(tl.float32)\n        rstd = tl.load(Rstd + row)\n\n        x_hat = x * rstd\n        wdy = w * dy\n        dw += dy * x_hat\n        c1 = tl.sum(x_hat * wdy, axis=0) / N\n        dx = (wdy - x_hat * c1) * rstd\n\n        tl.store(DX + row * stride_dx + cols, dx, mask=mask)\n\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 112, 260, 112, 261, 112, 262, 112, 263, 112, 264, 112, 265, 112, 266, 65, 6, 180, 65, 35, -1, 267, 199, 176, 249, 354, 180, 35, 268, 199, 267, 250, 265, 35, 269, 199, 79, 249, 354, 112, 266, 180, 35, 270, 199, 269, 1, 264, 35, 271, 199, 58, 249, 255, 77, 269, 112, 270, 199, 270, 112, 272, 199, 354, 180, 85, 273, 249, 150, 180, 35, 274, 199, 181, 249, 249, 266, 112, 180, 112, 95, 199, 150, 180, 35, 275, 199, 42, 249, 268, 77, 265, 112, 263, 180, 35, 140, 276, 162, 5, 249, 268, 112, 275, 180, 65, 35, 277, 199, 58, 249, 253, 77, 276, 250, 254, 77, 269, 112, 270, 199, 270, 112, 272, 199, 354, 180, 85, 273, 249, 150, 180, 35, 278, 199, 58, 249, 256, 77, 276, 250, 257, 77, 269, 112, 270, 199, 270, 112, 272, 199, 354, 180, 85, 273, 249, 150, 180, 35, 279, 199, 58, 249, 260, 77, 276, 180, 35, 280, 199, 277, 250, 279, 35, 281, 199, 271, 250, 278, 35, 274, 177, 278, 250, 280, 35, 282, 199, 226, 249, 280, 250, 281, 112, 283, 199, 354, 180, 45, 264, 35, 284, 199, 249, 281, 4, 280, 250, 282, 180, 250, 279, 35, 10, 249, 258, 77, 276, 250, 259, 77, 269, 112, 284, 112, 270, 199, 270, 180, 35, 81, 35, 10, 249, 261, 77, 267, 250, 264, 77, 269, 112, 274, 112, 270, 199, 270, 180, 35, 3, 35]}, {"code": "def _attn_bwd(\n    Q,\n    K,\n    V,\n    B,\n    Do,\n    M,\n    D,\n    softmax_scale,\n    dropout_prob,\n    dropout_seed,\n    stride_qz,\n    stride_qm,\n    stride_qh,\n    stride_kz,\n    stride_kn,\n    stride_kh,\n    stride_vz,\n    stride_vn,\n    stride_vh,\n    stride_bz,\n    stride_bm,\n    stride_bh,\n    stride_doz,\n    stride_dom,\n    stride_doh,\n    stride_dqz,\n    stride_dqm,\n    stride_dqh,\n    stride_dkz,\n    stride_dkn,\n    stride_dkh,\n    stride_dvz,\n    stride_dvn,\n    stride_dvh,\n    nheads_q,\n    num_repeats,\n    QSeq,\n    cum_seqlens_q,\n    KSeq,\n    cum_seqlens_k,\n    seqlen_q_rounded,\n    headdim,\n    CQSeq,\n    CKSeq,\n    DRuntime,\n    Dq,\n    Dk,\n    Dv,\n    VARLEN: tl.constexpr,\n    IS_CAUSAL: tl.constexpr,\n    BIAS_ON: tl.constexpr,\n    USE_DROPOUT: tl.constexpr,\n    BLOCK_HEADDIM: tl.constexpr,\n    EVEN_M1: tl.constexpr,\n    EVEN_N1: tl.constexpr,\n    EVEN_M2: tl.constexpr,\n    EVEN_N2: tl.constexpr,\n    NUM_BLOCKS_KV: tl.constexpr,\n    HEADS_PADDED: tl.constexpr,\n    BLOCK_M1: tl.constexpr,\n    BLOCK_N1: tl.constexpr,\n    BLOCK_M2: tl.constexpr,\n    BLOCK_N2: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    off_zh = tl.program_id(1)\n    off_z = off_zh // nheads_q\n    off_head_q = off_zh % nheads_q\n    off_head_kv = off_head_q // num_repeats\n\n    if VARLEN:\n        cu_seq_start_q = tl.load(cum_seqlens_q + off_z)\n        cu_seq_start_k = tl.load(cum_seqlens_k + off_z)\n        actual_seqlen_q = tl.load(cum_seqlens_q + off_z + 1) - cu_seq_start_q\n        actual_seqlen_k = tl.load(cum_seqlens_k + off_z + 1) - cu_seq_start_k\n        off_z = 0\n    else:\n        cu_seq_start_q = 0\n        cu_seq_start_k = 0\n        actual_seqlen_q = QSeq\n        actual_seqlen_k = KSeq\n\n    Q += off_z * stride_qz + off_head_q * stride_qh + cu_seq_start_q * stride_qm\n    K += off_z * stride_kz + off_head_kv * stride_kh + cu_seq_start_k * stride_kn\n    V += off_z * stride_vz + off_head_kv * stride_vh + cu_seq_start_k * stride_vn\n\n    Do += off_z * stride_doz + off_head_q * stride_doh + cu_seq_start_q * stride_dom\n    Dq += off_z * stride_dqz + off_head_q * stride_dqh + cu_seq_start_q * stride_dqm\n    Dk += off_z * stride_dkz + off_head_q * stride_dkh + cu_seq_start_k * stride_dkn\n    Dv += off_z * stride_dvz + off_head_q * stride_dvh + cu_seq_start_k * stride_dvn\n\n    if BIAS_ON:\n        B += off_z * stride_bz + off_head_q * stride_bh + cu_seq_start_q * stride_bm\n    if USE_DROPOUT:\n        Dropout = actual_seqlen_k * (\n            cu_seq_start_q + actual_seqlen_q * (off_head_q + nheads_q * off_z)\n        )\n    else:\n        Dropout = None\n\n    D += off_zh * seqlen_q_rounded\n    M += off_zh * seqlen_q_rounded\n\n    if pid < NUM_BLOCKS_KV:\n        i_start_n = pid\n        pad_cols = (not EVEN_N1) or (\n            VARLEN and ((i_start_n + 1) * BLOCK_N1 > actual_seqlen_k)\n        )\n        _attn_bwd_block_dkdv(\n            i_start_n * BLOCK_N1,\n            Q,\n            K,\n            V,\n            B,\n            Dropout,\n            Do,\n            Dk,\n            Dv,\n            M,\n            D,\n            softmax_scale,\n            stride_qm,\n            stride_kn,\n            stride_vn,\n            stride_bm,\n            stride_dom,\n            stride_dkn,\n            stride_dvn,\n            actual_seqlen_q,\n            actual_seqlen_k,\n            headdim,\n            IS_CAUSAL=IS_CAUSAL,\n            BIAS_ON=BIAS_ON,\n            USE_DROPOUT=USE_DROPOUT,\n            PAD_COLS=pad_cols,\n            HEADS_PADDED=HEADS_PADDED,\n            BLOCK_M=BLOCK_M1,\n            BLOCK_N=BLOCK_N1,\n            BLOCK_HEADDIM=BLOCK_HEADDIM,\n        )\n\n    else:\n        i_start_m = pid - NUM_BLOCKS_KV\n        pad_rows = (not EVEN_M2) or (\n            VARLEN and ((i_start_m + 1) * BLOCK_M2 > actual_seqlen_q)\n        )\n        _attn_bwd_block_dq(\n            i_start_m * BLOCK_M2,\n            Q,\n            K,\n            V,\n            B,\n            Dropout,\n            Do,\n            Dq,\n            M,\n            D,\n            softmax_scale,\n            dropout_prob,\n            dropout_seed,\n            stride_qm,\n            stride_kn,\n            stride_vn,\n            stride_bm,\n            stride_dom,\n            stride_dqm,\n            actual_seqlen_q,\n            actual_seqlen_k,\n            headdim,\n            VARLEN=VARLEN,\n            IS_CAUSAL=IS_CAUSAL,\n            BIAS_ON=BIAS_ON,\n            USE_DROPOUT=USE_DROPOUT,\n            PAD_ROWS=pad_rows,\n            HEADS_PADDED=HEADS_PADDED,\n            BLOCK_M=BLOCK_M2,\n            BLOCK_N=BLOCK_N2,\n            BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_N=EVEN_N2,\n        )", "encoded": [32, 353, 249, 253, 112, 254, 112, 255, 112, 256, 112, 257, 112, 258, 112, 259, 112, 260, 112, 261, 112, 262, 112, 263, 112, 264, 112, 265, 112, 266, 112, 267, 112, 268, 112, 269, 112, 270, 112, 271, 112, 272, 112, 273, 112, 274, 112, 275, 112, 276, 112, 277, 112, 278, 112, 279, 112, 280, 112, 281, 112, 282, 112, 283, 112, 284, 112, 285, 112, 286, 112, 287, 112, 288, 112, 289, 112, 290, 112, 291, 112, 292, 112, 293, 112, 294, 112, 295, 112, 296, 112, 297, 112, 298, 112, 299, 112, 300, 112, 301, 66, 6, 112, 302, 66, 6, 112, 303, 66, 6, 112, 304, 66, 6, 112, 305, 66, 6, 112, 306, 66, 6, 112, 307, 66, 6, 112, 308, 66, 6, 112, 309, 66, 6, 112, 310, 66, 6, 112, 311, 66, 6, 112, 312, 66, 6, 112, 313, 66, 6, 112, 314, 66, 6, 112, 315, 66, 6, 180, 66, 35, -1, 316, 199, 176, 249, 354, 180, 35, 317, 199, 176, 249, 355, 180, 35, 318, 199, 317, 51, 287, 35, 319, 199, 317, 231, 287, 35, 320, 199, 319, 51, 288, 35, 187, 301, 66, 35, 321, 199, 60, 249, 290, 77, 318, 180, 35, 322, 199, 60, 249, 292, 77, 318, 180, 35, 323, 199, 60, 249, 290, 77, 318, 77, 355, 180, 4, 321, 35, 324, 199, 60, 249, 292, 77, 318, 77, 355, 180, 4, 322, 35, 318, 199, 354, 35, 190, 35, 33, 66, 35, 321, 199, 354, 35, 322, 199, 354, 35, 323, 199, 289, 35, 324, 199, 291, 35, 59, 35, 253, 177, 318, 250, 263, 77, 319, 250, 265, 77, 321, 250, 264, 35, 254, 177, 318, 250, 266, 77, 320, 250, 268, 77, 322, 250, 267, 35, 255, 177, 318, 250, 269, 77, 320, 250, 271, 77, 322, 250, 270, 35, 257, 177, 318, 250, 275, 77, 319, 250, 277, 77, 321, 250, 276, 35, 298, 177, 318, 250, 278, 77, 319, 250, 280, 77, 321, 250, 279, 35, 299, 177, 318, 250, 281, 77, 319, 250, 283, 77, 322, 250, 282, 35, 300, 177, 318, 250, 284, 77, 319, 250, 286, 77, 322, 250, 285, 35, 187, 303, 66, 35, 256, 177, 318, 250, 272, 77, 319, 250, 274, 77, 321, 250, 273, 35, 190, 35, 187, 304, 66, 35, 325, 199, 324, 250, 249, 321, 77, 323, 250, 249, 319, 77, 287, 250, 318, 180, 180, 35, 190, 35, 33, 66, 35, 325, 199, 207, 35, 59, 35, 259, 177, 317, 250, 293, 35, 258, 177, 317, 250, 293, 35, 187, 316, 1, 310, 66, 35, 326, 199, 316, 35, 327, 199, 67, 307, 146, 249, 301, 106, 249, 326, 77, 355, 180, 250, 313, 128, 324, 180, 35, 328, 249, 326, 250, 313, 112, 253, 112, 254, 112, 255, 112, 256, 112, 325, 112, 257, 112, 299, 112, 300, 112, 258, 112, 259, 112, 260, 112, 264, 112, 267, 112, 270, 112, 273, 112, 276, 112, 282, 112, 285, 112, 323, 112, 324, 112, 294, 112, 302, 199, 302, 112, 303, 199, 303, 112, 304, 199, 304, 112, 329, 199, 327, 112, 311, 199, 311, 112, 330, 199, 312, 112, 331, 199, 313, 112, 305, 199, 305, 180, 35, 190, 35, 33, 66, 35, 332, 199, 316, 4, 310, 35, 333, 199, 67, 308, 146, 249, 301, 106, 249, 332, 77, 355, 180, 250, 314, 128, 323, 180, 35, 334, 249, 332, 250, 314, 112, 253, 112, 254, 112, 255, 112, 256, 112, 325, 112, 257, 112, 298, 112, 258, 112, 259, 112, 260, 112, 261, 112, 262, 112, 264, 112, 267, 112, 270, 112, 273, 112, 276, 112, 279, 112, 323, 112, 324, 112, 294, 112, 301, 199, 301, 112, 302, 199, 302, 112, 303, 199, 303, 112, 304, 199, 304, 112, 335, 199, 333, 112, 311, 199, 311, 112, 330, 199, 314, 112, 331, 199, 315, 112, 305, 199, 305, 112, 336, 199, 309, 180, 35, 59, 35, 3, 35]}, {"code": "def _attn_fwd(\n    q,\n    k,\n    v,\n    B,\n    softmax_scale,\n    dropout_prob,\n    dropout_seed,\n    stride_qz,\n    stride_qm,\n    stride_qh,\n    stride_kz,\n    stride_kn,\n    stride_kh,\n    stride_vz,\n    stride_vn,\n    stride_vh,\n    stride_oz,\n    stride_om,\n    stride_oh,\n    stride_bz,\n    stride_bm,\n    stride_bh,\n    nheads_q,\n    num_repeats,\n    QSeq,\n    cum_seqlens_q,\n    KSeq,\n    max_seqlen_q_rounded,\n    headdim,\n    CQSeq,\n    CKSeq,\n    DRuntime,\n    Po,\n    M,\n    VARLEN: tl.constexpr,\n    USE_DROPOUT: tl.constexpr,\n    IS_CAUSAL: tl.constexpr,\n    BIAS_ON: tl.constexpr,\n    BLOCK_HEADDIM: tl.constexpr,\n    PADDED_HEADS: tl.constexpr,\n    EVEN_M: tl.constexpr,\n    EVEN_N: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    LN2: tl.constexpr = 1.44269504089\n    i_start_m = tl.program_id(0)\n    off_zh = tl.program_id(1)\n    off_head_q = off_zh % nheads_q\n    off_head_kv = off_head_q // num_repeats\n    off_z = off_zh // nheads_q\n\n    if VARLEN:\n        cu_seq_start_q = tl.load(cum_seqlens_q + off_z)\n        actual_seqlen_q = tl.load(cum_seqlens_q + off_z + 1) - cu_seq_start_q\n        if i_start_m * BLOCK_M >= actual_seqlen_q:\n            return\n        actual_seqlen_k = actual_seqlen_q\n        cu_seq_start_k = cu_seq_start_q\n        off_z = 0\n    else:\n        actual_seqlen_q = QSeq\n        actual_seqlen_k = KSeq\n        cu_seq_start_q = 0\n        cu_seq_start_k = 0\n\n    softmax_scale = softmax_scale * LN2\n\n    offs_m = i_start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n\n    fully_masked_lines = actual_seqlen_q - actual_seqlen_k if IS_CAUSAL else 0\n    if fully_masked_lines >= (i_start_m + 1) * BLOCK_M:\n        return\n\n    q_ptrs = (\n        q\n        + off_z * stride_qz\n        + off_head_q * stride_qh\n        + cu_seq_start_q * stride_qm\n        + (offs_m[:, None] * stride_qm + offs_d[None, :])\n    )\n\n    k_ptrs = (\n        k\n        + off_z * stride_kz\n        + off_head_kv * stride_kh\n        + cu_seq_start_k * stride_kn\n        + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    )\n\n    v_ptrs = (\n        v\n        + off_z * stride_vz\n        + off_head_kv * stride_vh\n        + cu_seq_start_k * stride_vn\n        + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    )\n\n    if BIAS_ON:\n        bias_ptrs = (\n            B\n            + off_z * stride_bz\n            + off_head_kv * stride_bh\n            + cu_seq_start_q * stride_bm\n            + (offs_m[:, None] * stride_bm + offs_n[None, :])\n        )\n    else:\n        bias_ptrs = None\n    if USE_DROPOUT:\n        dropout_off = actual_seqlen_k * (\n            cu_seq_start_q + actual_seqlen_q * (off_head_q + nheads_q * off_z)\n        )\n        dropout_offs = dropout_off + offs_m[:, None] * actual_seqlen_k + offs_n[None, :]\n    else:\n        dropout_offs = None\n\n    me_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n\n    pad_rows = (not EVEN_M) or (VARLEN and (i_start_m * BLOCK_M > actual_seqlen_q))\n    q = padded_load(\n        q_ptrs,\n        offs_m,\n        offs_d,\n        PA0=pad_rows,\n        PA1=PADDED_HEADS,\n        LA0=actual_seqlen_q,\n        LA1=headdim,\n    )\n    if IS_CAUSAL:\n        end_n = min(\n            actual_seqlen_k - actual_seqlen_q + (i_start_m + 1) * BLOCK_M,\n            actual_seqlen_k,\n        )\n        if end_n < 0:\n            return\n    else:\n        end_n = actual_seqlen_k\n\n    uneven_n = actual_seqlen_k % BLOCK_N != 0\n    attention_padding = VARLEN & uneven_n\n    if IS_CAUSAL:\n        first_masked_col = i_start_m * BLOCK_M + 1 + actual_seqlen_k - actual_seqlen_q\n    elif attention_padding:\n        first_masked_col = actual_seqlen_k\n    else:\n        first_masked_col = end_n\n    nb_full_blocks = first_masked_col // BLOCK_N\n\n    next_start_n = 0\n    if nb_full_blocks > 0:\n        for _ in range(0, nb_full_blocks):\n            m_i, me_i, acc_o = _attn_fwd_inner(\n                q,\n                m_i,\n                me_i,\n                k_ptrs,\n                v_ptrs,\n                bias_ptrs,\n                acc_o,\n                offs_m,\n                offs_n,\n                offs_d,\n                softmax_scale,\n                dropout_prob,\n                dropout_seed,\n                dropout_offs,\n                stride_kn,\n                stride_vn,\n                next_start_n,\n                actual_seqlen_q,\n                actual_seqlen_k,\n                headdim,\n                USE_DROPOUT=USE_DROPOUT,\n                IS_CAUSAL=IS_CAUSAL,\n                BIAS_ON=BIAS_ON,\n                MASKED=False,\n                PADDED_COLS=False,\n                PADDED_HEADS=PADDED_HEADS,\n                BLOCK_M=BLOCK_M,\n                BLOCK_N=BLOCK_N,\n            )\n            next_start_n += BLOCK_N\n    if next_start_n < end_n:\n        for index_start_n in range(next_start_n, end_n, BLOCK_N):\n            pad_cols = (not EVEN_N) or VARLEN\n            m_i, me_i, acc_o = _attn_fwd_inner(\n                q,\n                m_i,\n                me_i,\n                k_ptrs,\n                v_ptrs,\n                bias_ptrs,\n                acc_o,\n                offs_m,\n                offs_n,\n                offs_d,\n                softmax_scale,\n                dropout_prob,\n                dropout_seed,\n                dropout_offs,\n                stride_kn,\n                stride_vn,\n                index_start_n,\n                actual_seqlen_q,\n                actual_seqlen_k,\n                headdim,\n                USE_DROPOUT=USE_DROPOUT,\n                IS_CAUSAL=IS_CAUSAL,\n                BIAS_ON=BIAS_ON,\n                MASKED=True,\n                PADDED_COLS=pad_cols,\n                PADDED_HEADS=PADDED_HEADS,\n                BLOCK_M=BLOCK_M,\n                BLOCK_N=BLOCK_N,\n            )\n\n    if USE_DROPOUT:\n        o_scale = tl.exp2((m_i - me_i) - tl.log2(1 - dropout_prob))\n    else:\n        o_scale = tl.exp2(m_i - me_i)\n    acc_o = acc_o * o_scale[:, None]\n    if fully_masked_lines > i_start_m * BLOCK_M:\n        acc_o = tl.where(offs_m[:, None] < fully_masked_lines, 0, acc_o)\n    i_start_m = tl.program_id(0)\n    offs_m = i_start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = M + off_zh * max_seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, me_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = (\n        Po\n        + off_z * stride_oz\n        + off_head_q * stride_oh\n        + cu_seq_start_q * stride_om\n        + (offs_m[:, None] * stride_om + offs_d[None, :])\n    )\n\n    tl.store(\n        out_ptrs,\n        acc_o,\n        mask=(offs_m[:, None] < actual_seqlen_q) & (offs_d[None, :] < headdim),\n    )", "encoded": [32, 354, 250, 254, 113, 255, 113, 256, 113, 257, 113, 258, 113, 259, 113, 260, 113, 261, 113, 262, 113, 263, 113, 264, 113, 265, 113, 266, 113, 267, 113, 268, 113, 269, 113, 270, 113, 271, 113, 272, 113, 273, 113, 274, 113, 275, 113, 276, 113, 277, 113, 278, 113, 279, 113, 280, 113, 281, 113, 282, 113, 283, 113, 284, 113, 285, 113, 286, 113, 287, 113, 288, 65, 6, 113, 289, 65, 6, 113, 290, 65, 6, 113, 291, 65, 6, 113, 292, 65, 6, 113, 293, 65, 6, 113, 294, 65, 6, 113, 295, 65, 6, 113, 296, 65, 6, 113, 297, 65, 6, 181, 65, 35, -1, 298, 65, 6, 200, 355, 35, 299, 200, 177, 250, 356, 181, 35, 300, 200, 177, 250, 357, 181, 35, 301, 200, 300, 232, 276, 35, 302, 200, 301, 51, 277, 35, 303, 200, 300, 51, 276, 35, 188, 288, 65, 35, 304, 200, 58, 250, 279, 77, 303, 181, 35, 305, 200, 58, 250, 279, 77, 303, 77, 357, 181, 4, 304, 35, 188, 299, 251, 296, 150, 305, 65, 35, 237, 35, 191, 35, 306, 200, 305, 35, 307, 200, 304, 35, 303, 200, 356, 35, 191, 35, 33, 65, 35, 305, 200, 278, 35, 306, 200, 280, 35, 304, 200, 356, 35, 307, 200, 356, 35, 59, 35, 258, 200, 258, 251, 298, 35, 308, 200, 299, 251, 296, 77, 79, 250, 356, 113, 296, 181, 35, 309, 200, 79, 250, 356, 113, 297, 181, 35, 310, 200, 79, 250, 356, 113, 292, 181, 35, 311, 200, 305, 4, 306, 188, 290, 33, 356, 35, 188, 311, 150, 250, 299, 77, 357, 181, 251, 296, 65, 35, 237, 35, 191, 35, 312, 200, 254, 77, 303, 251, 261, 77, 301, 251, 263, 77, 304, 251, 262, 77, 250, 308, 236, 65, 113, 208, 30, 251, 262, 77, 310, 236, 208, 113, 65, 30, 181, 35, 313, 200, 255, 77, 303, 251, 264, 77, 302, 251, 266, 77, 307, 251, 265, 77, 250, 309, 236, 65, 113, 208, 30, 251, 265, 77, 310, 236, 208, 113, 65, 30, 181, 35, 314, 200, 256, 77, 303, 251, 267, 77, 302, 251, 269, 77, 307, 251, 268, 77, 250, 309, 236, 65, 113, 208, 30, 251, 268, 77, 310, 236, 208, 113, 65, 30, 181, 35, 188, 291, 65, 35, 315, 200, 257, 77, 303, 251, 273, 77, 302, 251, 275, 77, 304, 251, 274, 77, 250, 308, 236, 65, 113, 208, 30, 251, 274, 77, 309, 236, 208, 113, 65, 30, 181, 35, 191, 35, 33, 65, 35, 315, 200, 208, 35, 59, 35, 188, 289, 65, 35, 316, 200, 306, 251, 250, 304, 77, 305, 251, 250, 301, 77, 276, 251, 303, 181, 181, 35, 317, 200, 316, 77, 308, 236, 65, 113, 208, 30, 251, 306, 77, 309, 236, 208, 113, 65, 30, 35, 191, 35, 33, 65, 35, 317, 200, 208, 35, 59, 35, 318, 200, 182, 250, 236, 296, 30, 113, 95, 200, 151, 181, 4, 319, 250, 358, 181, 35, 320, 200, 182, 250, 236, 296, 30, 113, 95, 200, 151, 181, 4, 319, 250, 358, 181, 35, 321, 200, 182, 250, 236, 296, 113, 292, 30, 113, 95, 200, 151, 181, 35, 322, 200, 66, 294, 147, 250, 288, 106, 299, 251, 296, 129, 305, 181, 35, 254, 200, 323, 250, 312, 113, 308, 113, 310, 113, 324, 200, 322, 113, 325, 200, 293, 113, 326, 200, 305, 113, 327, 200, 282, 181, 35, 188, 290, 65, 35, 328, 200, 42, 250, 306, 4, 305, 77, 250, 299, 77, 357, 181, 251, 296, 113, 306, 181, 35, 188, 328, 1, 356, 65, 35, 237, 35, 191, 35, 191, 35, 33, 65, 35, 328, 200, 306, 35, 59, 35, 329, 200, 306, 232, 297, 192, 356, 35, 330, 200, 288, 179, 329, 35, 188, 290, 65, 35, 331, 200, 299, 251, 296, 77, 357, 77, 306, 4, 305, 35, 68, 35, 40, 330, 65, 35, 331, 200, 306, 35, 191, 35, 33, 65, 35, 331, 200, 328, 35, 59, 35, 332, 200, 331, 51, 297, 35, 333, 200, 356, 35, 188, 332, 129, 356, 65, 35, 141, 334, 163, 5, 250, 356, 113, 332, 181, 65, 35, 320, 113, 318, 113, 321, 200, 335, 250, 254, 113, 320, 113, 318, 113, 313, 113, 314, 113, 315, 113, 321, 113, 308, 113, 309, 113, 310, 113, 258, 113, 259, 113, 260, 113, 317, 113, 265, 113, 268, 113, 333, 113, 305, 113, 306, 113, 282, 113, 289, 200, 289, 113, 290, 200, 290, 113, 291, 200, 291, 113, 336, 200, 62, 113, 337, 200, 62, 113, 293, 200, 293, 113, 296, 200, 296, 113, 297, 200, 297, 181, 35, 333, 178, 297, 35, 81, 35, 191, 35, 188, 333, 1, 328, 65, 35, 141, 338, 163, 5, 250, 333, 113, 328, 113, 297, 181, 65, 35, 339, 200, 66, 295, 147, 288, 35, 320, 113, 318, 113, 321, 200, 335, 250, 254, 113, 320, 113, 318, 113, 313, 113, 314, 113, 315, 113, 321, 113, 308, 113, 309, 113, 310, 113, 258, 113, 259, 113, 260, 113, 317, 113, 265, 113, 268, 113, 338, 113, 305, 113, 306, 113, 282, 113, 289, 200, 289, 113, 290, 200, 290, 113, 291, 200, 291, 113, 336, 200, 166, 113, 337, 200, 339, 113, 293, 200, 293, 113, 296, 200, 296, 113, 297, 200, 297, 181, 35, 81, 35, 191, 35, 188, 289, 65, 35, 340, 200, 218, 250, 320, 4, 318, 4, 108, 250, 357, 4, 259, 181, 181, 35, 191, 35, 33, 65, 35, 340, 200, 218, 250, 320, 4, 318, 181, 35, 59, 35, 321, 200, 321, 251, 340, 236, 65, 113, 208, 30, 35, 188, 311, 129, 299, 251, 296, 65, 35, 321, 200, 211, 250, 308, 236, 65, 113, 208, 30, 1, 311, 113, 356, 113, 321, 181, 35, 191, 35, 299, 200, 177, 250, 356, 181, 35, 308, 200, 299, 251, 296, 77, 79, 250, 356, 113, 296, 181, 35, 341, 200, 287, 77, 300, 251, 281, 77, 308, 35, 10, 250, 341, 113, 318, 181, 35, 310, 200, 79, 250, 356, 113, 292, 181, 35, 342, 200, 286, 77, 303, 251, 270, 77, 301, 251, 272, 77, 304, 251, 271, 77, 250, 308, 236, 65, 113, 208, 30, 251, 271, 77, 310, 236, 208, 113, 65, 30, 181, 35, 10, 250, 342, 113, 321, 113, 343, 200, 250, 308, 236, 65, 113, 208, 30, 1, 305, 181, 179, 250, 310, 236, 208, 113, 65, 30, 1, 282, 181, 181, 35, 3, 35]}, {"code": "def _attn_fwd(\n    Q,\n    K,\n    V,\n    sm_scale,\n    M,\n    Out,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_on,\n    Z,\n    H,\n    N_CTX,\n    HEAD_DIM: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    tl.static_assert(BLOCK_N <= HEAD_DIM)\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + qvk_offset,\n        shape=(N_CTX, HEAD_DIM),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, HEAD_DIM),\n        order=(1, 0),\n    )\n    v_order: tl.constexpr = (1, 0)\n    V_block_ptr = tl.make_block_ptr(\n        base=V + qvk_offset,\n        shape=(N_CTX, HEAD_DIM),\n        strides=(stride_vk, stride_vn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, HEAD_DIM),\n        order=v_order,\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + qvk_offset,\n        shape=(HEAD_DIM, N_CTX),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(HEAD_DIM, BLOCK_N),\n        order=(0, 1),\n    )\n    O_block_ptr = tl.make_block_ptr(\n        base=Out + qvk_offset,\n        shape=(N_CTX, HEAD_DIM),\n        strides=(stride_om, stride_on),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, HEAD_DIM),\n        order=(1, 0),\n    )\n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n\n    qk_scale = sm_scale\n    qk_scale *= 1.44269504\n\n    q = tl.load(Q_block_ptr)\n    acc, l_i, m_i = _attn_fwd_inner(\n        acc,\n        l_i,\n        m_i,\n        q,\n        K_block_ptr,\n        V_block_ptr,\n        qk_scale,\n        BLOCK_N,\n        N_CTX,\n    )\n\n    m_i += tl.math.log2(l_i)\n    acc = acc / l_i[:, None]\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty))", "encoded": [32, 354, 250, 254, 113, 255, 113, 256, 113, 257, 113, 258, 113, 259, 113, 260, 113, 261, 113, 262, 113, 263, 113, 264, 113, 265, 113, 266, 113, 267, 113, 268, 113, 269, 113, 270, 113, 271, 113, 272, 113, 273, 113, 274, 113, 275, 113, 276, 113, 277, 113, 278, 113, 279, 66, 6, 113, 280, 66, 6, 113, 281, 66, 6, 181, 66, 35, -1, 86, 250, 281, 224, 279, 181, 35, 282, 200, 177, 250, 355, 181, 35, 283, 200, 177, 250, 356, 181, 35, 284, 200, 283, 51, 277, 35, 285, 200, 283, 232, 277, 35, 286, 200, 284, 85, 287, 250, 187, 181, 251, 260, 77, 285, 85, 287, 250, 187, 181, 251, 261, 35, 288, 200, 225, 250, 289, 200, 254, 77, 286, 113, 128, 200, 250, 278, 113, 279, 181, 113, 290, 200, 250, 262, 113, 263, 181, 113, 291, 200, 250, 282, 251, 280, 113, 355, 181, 113, 292, 200, 250, 280, 113, 279, 181, 113, 293, 200, 250, 356, 113, 355, 181, 181, 35, 294, 66, 6, 200, 250, 356, 113, 355, 181, 35, 295, 200, 225, 250, 289, 200, 256, 77, 286, 113, 128, 200, 250, 278, 113, 279, 181, 113, 290, 200, 250, 270, 113, 271, 181, 113, 291, 200, 250, 355, 113, 355, 181, 113, 292, 200, 250, 281, 113, 279, 181, 113, 293, 200, 294, 181, 35, 296, 200, 225, 250, 289, 200, 255, 77, 286, 113, 128, 200, 250, 279, 113, 278, 181, 113, 290, 200, 250, 267, 113, 266, 181, 113, 291, 200, 250, 355, 113, 355, 181, 113, 292, 200, 250, 279, 113, 281, 181, 113, 293, 200, 250, 355, 113, 356, 181, 181, 35, 297, 200, 225, 250, 289, 200, 259, 77, 286, 113, 128, 200, 250, 278, 113, 279, 181, 113, 290, 200, 250, 274, 113, 275, 181, 113, 291, 200, 250, 282, 251, 280, 113, 355, 181, 113, 292, 200, 250, 280, 113, 279, 181, 113, 293, 200, 250, 356, 113, 355, 181, 181, 35, 298, 200, 282, 251, 280, 77, 79, 250, 355, 113, 280, 181, 35, 299, 200, 182, 250, 236, 280, 30, 113, 95, 200, 151, 181, 4, 300, 250, 357, 181, 35, 301, 200, 182, 250, 236, 280, 30, 113, 95, 200, 151, 181, 77, 356, 35, 302, 200, 182, 250, 236, 280, 113, 279, 30, 113, 95, 200, 151, 181, 35, 303, 200, 257, 35, 303, 26, 358, 35, 304, 200, 60, 250, 288, 181, 35, 302, 113, 301, 113, 299, 200, 305, 250, 302, 113, 301, 113, 299, 113, 304, 113, 296, 113, 295, 113, 303, 113, 281, 113, 278, 181, 35, 299, 178, 116, 250, 301, 181, 35, 302, 200, 302, 45, 301, 236, 66, 113, 208, 30, 35, 306, 200, 258, 77, 283, 251, 278, 77, 298, 35, 10, 250, 306, 113, 299, 181, 35, 10, 250, 297, 113, 302, 85, 287, 250, 259, 85, 193, 85, 119, 181, 181, 35, 3, 35]}, {"code": "def _linear_fwd(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    bias_ptr,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    HAS_BIAS: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n\n    offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n    offs_am = tl.where(offs_am < M, offs_am, 0)\n    offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if HAS_BIAS:\n        bias_ptrs = bias_ptr + offs_bn[None, :]\n        bias = tl.load(bias_ptrs).to(tl.float32)\n        bias = tl.broadcast_to(bias, [BLOCK_SIZE_M, BLOCK_SIZE_N])\n        accumulator += bias\n\n    c = accumulator.to(tl.bfloat16)\n\n    if ACTIVATION == \"GELU\":\n        c = gelu(c)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)", "encoded": [32, 354, 250, 254, 113, 255, 113, 256, 113, 257, 113, 258, 113, 259, 113, 260, 113, 261, 113, 262, 113, 263, 113, 264, 113, 265, 113, 266, 113, 267, 65, 6, 113, 268, 65, 6, 113, 269, 65, 6, 113, 270, 65, 6, 113, 271, 65, 6, 113, 272, 65, 6, 181, 65, 35, -1, 273, 200, 177, 250, 274, 200, 355, 181, 35, 275, 200, 67, 250, 258, 113, 268, 181, 35, 276, 200, 67, 250, 259, 113, 269, 181, 35, 277, 200, 271, 251, 276, 35, 278, 200, 273, 51, 277, 35, 279, 200, 278, 251, 271, 35, 280, 200, 42, 250, 275, 4, 279, 113, 271, 181, 35, 281, 200, 279, 77, 273, 232, 280, 35, 282, 200, 273, 232, 277, 51, 280, 35, 283, 200, 281, 251, 268, 35, 284, 200, 282, 251, 269, 35, 285, 200, 283, 77, 79, 250, 355, 113, 268, 181, 35, 286, 200, 284, 77, 79, 250, 355, 113, 269, 181, 35, 285, 200, 211, 250, 285, 1, 258, 113, 285, 113, 355, 181, 35, 286, 200, 211, 250, 286, 1, 259, 113, 286, 113, 355, 181, 35, 285, 200, 217, 250, 60, 250, 285, 113, 268, 181, 113, 268, 181, 35, 286, 200, 217, 250, 60, 250, 286, 113, 269, 181, 113, 269, 181, 35, 287, 200, 79, 250, 355, 113, 270, 181, 35, 288, 200, 254, 77, 250, 285, 236, 65, 113, 208, 30, 251, 261, 77, 287, 236, 208, 113, 65, 30, 251, 262, 181, 35, 289, 200, 255, 77, 250, 287, 236, 65, 113, 208, 30, 251, 263, 77, 286, 236, 208, 113, 65, 30, 251, 264, 181, 35, 290, 200, 182, 250, 250, 268, 113, 269, 181, 113, 95, 200, 151, 181, 35, 141, 291, 163, 5, 250, 355, 113, 67, 250, 260, 113, 270, 181, 181, 65, 35, 292, 200, 58, 250, 288, 113, 293, 200, 287, 236, 208, 113, 65, 30, 1, 260, 4, 291, 251, 270, 113, 294, 200, 355, 181, 35, 295, 200, 58, 250, 289, 113, 293, 200, 287, 236, 65, 113, 208, 30, 1, 260, 4, 291, 251, 270, 113, 294, 200, 355, 181, 35, 290, 200, 15, 250, 292, 113, 295, 113, 290, 181, 35, 288, 178, 270, 251, 262, 35, 289, 178, 270, 251, 263, 35, 81, 35, 188, 267, 65, 35, 296, 200, 257, 77, 286, 236, 208, 113, 65, 30, 35, 297, 200, 58, 250, 296, 181, 85, 298, 250, 151, 181, 35, 297, 200, 199, 250, 297, 113, 236, 268, 113, 269, 30, 181, 35, 290, 178, 297, 35, 191, 35, 299, 200, 290, 85, 298, 250, 239, 181, 35, 188, 272, 80, 356, 65, 35, 299, 200, 300, 250, 299, 181, 35, 191, 35, 301, 200, 281, 251, 268, 77, 79, 250, 355, 113, 268, 181, 35, 302, 200, 282, 251, 269, 77, 79, 250, 355, 113, 269, 181, 35, 303, 200, 256, 77, 265, 251, 301, 236, 65, 113, 208, 30, 77, 266, 251, 302, 236, 208, 113, 65, 30, 35, 304, 200, 250, 301, 236, 65, 113, 208, 30, 1, 258, 181, 179, 250, 302, 236, 208, 113, 65, 30, 1, 259, 181, 35, 10, 250, 303, 113, 299, 113, 293, 200, 304, 181, 35, 3, 35]}, {"code": "def add_kernel(\n    x_ptr, y_ptr, z_ptr, size, block_size: tl.constexpr, boundary_check: tl.constexpr\n):\n    offset = tl.program_id(0) * block_size\n\n    x_block_ptr = tl.make_block_ptr(\n        x_ptr,\n        shape=(size,),\n        strides=(1,),\n        offsets=(offset,),\n        block_shape=(block_size,),\n        order=(0,),\n    )\n    y_block_ptr = tl.make_block_ptr(\n        y_ptr,\n        shape=(size,),\n        strides=(1,),\n        offsets=(offset,),\n        block_shape=(block_size,),\n        order=(0,),\n    )\n\n    if boundary_check:\n        x = tl.load(x_block_ptr, boundary_check=(0,))\n        y = tl.load(y_block_ptr, boundary_check=(0,))\n    else:\n        x = tl.load(x_block_ptr)\n        y = tl.load(y_block_ptr)\n\n    z = x + y\n\n    z_block_ptr = tl.make_block_ptr(\n        z_ptr,\n        shape=(size,),\n        strides=(1,),\n        offsets=(offset,),\n        block_shape=(block_size,),\n        order=(0,),\n    )\n\n    if boundary_check:\n        tl.store(z_block_ptr, z, boundary_check=(0,))\n    else:\n        tl.store(z_block_ptr, z)", "encoded": [32, 354, 250, 254, 113, 255, 113, 256, 113, 221, 113, 257, 66, 6, 113, 258, 66, 6, 181, 66, 35, -1, 259, 200, 177, 250, 355, 181, 251, 257, 35, 260, 200, 225, 250, 254, 113, 128, 200, 250, 221, 113, 181, 113, 261, 200, 250, 356, 113, 181, 113, 262, 200, 250, 259, 113, 181, 113, 263, 200, 250, 257, 113, 181, 113, 264, 200, 250, 355, 113, 181, 181, 35, 265, 200, 225, 250, 255, 113, 128, 200, 250, 221, 113, 181, 113, 261, 200, 250, 356, 113, 181, 113, 262, 200, 250, 259, 113, 181, 113, 263, 200, 250, 257, 113, 181, 113, 264, 200, 250, 355, 113, 181, 181, 35, 188, 258, 66, 35, 266, 200, 60, 250, 260, 113, 258, 200, 250, 355, 113, 181, 181, 35, 267, 200, 60, 250, 265, 113, 258, 200, 250, 355, 113, 181, 181, 35, 191, 35, 33, 66, 35, 266, 200, 60, 250, 260, 181, 35, 267, 200, 60, 250, 265, 181, 35, 59, 35, 268, 200, 266, 77, 267, 35, 269, 200, 225, 250, 256, 113, 128, 200, 250, 221, 113, 181, 113, 261, 200, 250, 356, 113, 181, 113, 262, 200, 250, 259, 113, 181, 113, 263, 200, 250, 257, 113, 181, 113, 264, 200, 250, 355, 113, 181, 181, 35, 188, 258, 66, 35, 10, 250, 269, 113, 268, 113, 258, 200, 250, 355, 113, 181, 181, 35, 191, 35, 33, 66, 35, 10, 250, 269, 113, 268, 181, 35, 59, 35, 3, 35]}, {"code": "def _fwd_kernel(\n    Q,\n    K,\n    V,\n    sm_scale,\n    qkv_scale_ptr,\n    out_scale_ptr,\n    Out,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_on,\n    Z,\n    H,\n    N_CTX,\n    EVEN_CTX: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + qvk_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + qvk_offset,\n        shape=(BLOCK_DMODEL, N_CTX),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1),\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + qvk_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_vk, stride_vn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    qkv_scale = tl.load(qkv_scale_ptr)\n    qk_scale = qkv_scale * qkv_scale * sm_scale * 1.44269504\n\n    if EVEN_CTX:\n        q = tl.load(Q_block_ptr)\n    else:\n        q = tl.load(Q_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n    for start_n in range(0, N_CTX, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n\n        if EVEN_CTX:\n            k = tl.load(K_block_ptr)\n        else:\n            k = tl.load(K_block_ptr, boundary_check=(1,), padding_option=\"zero\")\n\n        qk = tl.dot(q, k, allow_tf32=False, out_dtype=tl.int32)\n        qk_fp32 = qk * qk_scale\n\n        m_ij = tl.maximum(m_i, tl.max(qk_fp32, 1))\n        p = tl.math.exp2(qk_fp32 - m_ij[:, None])\n\n        alpha = tl.math.exp2(m_i - m_ij)\n        m_i = m_ij\n\n        if EVEN_CTX:\n            v = tl.load(V_block_ptr)\n        else:\n            v = tl.load(V_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n        v = (v * qkv_scale).to(tl.bfloat16)\n        acc *= alpha[:, None]\n        acc += tl.dot(\n            p.to(tl.bfloat16),\n            v,\n            allow_tf32=True,\n        )\n        l_i = l_i * alpha + tl.sum(p, 1)\n\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n\n    out_scale = tl.load(out_scale_ptr)\n    acc = tl.math.llrint(acc / (l_i[:, None] * out_scale)).to(tl.int8)\n\n    O_block_ptr = tl.make_block_ptr(\n        base=Out + qvk_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_om, stride_on),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    if EVEN_CTX:\n        tl.store(O_block_ptr, acc)\n    else:\n        tl.store(O_block_ptr, acc, boundary_check=(0,))", "encoded": [32, 355, 251, 255, 114, 256, 114, 257, 114, 258, 114, 259, 114, 260, 114, 261, 114, 262, 114, 263, 114, 264, 114, 265, 114, 266, 114, 267, 114, 268, 114, 269, 114, 270, 114, 271, 114, 272, 114, 273, 114, 274, 114, 275, 114, 276, 114, 277, 114, 278, 114, 279, 114, 280, 114, 281, 65, 6, 114, 282, 65, 6, 114, 283, 65, 6, 114, 284, 65, 6, 182, 65, 35, -1, 285, 201, 178, 251, 356, 182, 35, 286, 201, 178, 251, 357, 182, 35, 287, 201, 286, 51, 279, 35, 288, 201, 286, 233, 279, 35, 289, 201, 287, 85, 290, 251, 188, 182, 252, 262, 77, 288, 85, 290, 251, 188, 182, 252, 263, 35, 291, 201, 226, 251, 292, 201, 255, 77, 289, 114, 129, 201, 251, 280, 114, 283, 182, 114, 293, 201, 251, 264, 114, 265, 182, 114, 294, 201, 251, 285, 252, 282, 114, 356, 182, 114, 295, 201, 251, 282, 114, 283, 182, 114, 296, 201, 251, 357, 114, 356, 182, 182, 35, 297, 201, 226, 251, 292, 201, 256, 77, 289, 114, 129, 201, 251, 283, 114, 280, 182, 114, 293, 201, 251, 269, 114, 268, 182, 114, 294, 201, 251, 356, 114, 356, 182, 114, 295, 201, 251, 283, 114, 284, 182, 114, 296, 201, 251, 356, 114, 357, 182, 182, 35, 298, 201, 226, 251, 292, 201, 257, 77, 289, 114, 129, 201, 251, 280, 114, 283, 182, 114, 293, 201, 251, 272, 114, 273, 182, 114, 294, 201, 251, 356, 114, 356, 182, 114, 295, 201, 251, 284, 114, 283, 182, 114, 296, 201, 251, 357, 114, 356, 182, 182, 35, 299, 201, 183, 251, 237, 282, 30, 114, 95, 201, 152, 182, 4, 300, 251, 358, 182, 35, 301, 201, 183, 251, 237, 282, 30, 114, 95, 201, 152, 182, 35, 302, 201, 183, 251, 237, 282, 114, 283, 30, 114, 95, 201, 152, 182, 35, 303, 201, 58, 251, 259, 182, 35, 304, 201, 303, 252, 303, 252, 258, 252, 359, 35, 189, 281, 65, 35, 305, 201, 58, 251, 291, 182, 35, 192, 35, 33, 65, 35, 305, 201, 58, 251, 291, 114, 306, 201, 251, 356, 114, 182, 114, 307, 201, 360, 182, 35, 59, 35, 142, 308, 164, 5, 251, 356, 114, 280, 114, 284, 182, 65, 35, 308, 201, 60, 251, 308, 114, 284, 182, 35, 189, 281, 65, 35, 309, 201, 58, 251, 297, 182, 35, 192, 35, 33, 65, 35, 309, 201, 58, 251, 297, 114, 306, 201, 251, 357, 114, 182, 114, 307, 201, 360, 182, 35, 59, 35, 310, 201, 15, 251, 305, 114, 309, 114, 311, 201, 62, 114, 312, 201, 254, 182, 35, 313, 201, 310, 252, 304, 35, 314, 201, 199, 251, 299, 114, 12, 251, 313, 114, 357, 182, 182, 35, 315, 201, 162, 251, 313, 4, 314, 237, 65, 114, 209, 30, 182, 35, 316, 201, 162, 251, 299, 4, 314, 182, 35, 299, 201, 314, 35, 189, 281, 65, 35, 317, 201, 58, 251, 298, 182, 35, 192, 35, 33, 65, 35, 317, 201, 58, 251, 298, 114, 306, 201, 251, 356, 114, 182, 114, 307, 201, 360, 182, 35, 59, 35, 317, 201, 251, 317, 252, 303, 182, 85, 290, 251, 240, 182, 35, 302, 26, 316, 237, 65, 114, 209, 30, 35, 302, 179, 15, 251, 315, 85, 290, 251, 240, 182, 114, 317, 114, 311, 201, 167, 182, 35, 301, 201, 301, 252, 316, 77, 228, 251, 315, 114, 357, 182, 35, 297, 201, 150, 251, 297, 114, 251, 356, 114, 284, 182, 182, 35, 298, 201, 150, 251, 298, 114, 251, 284, 114, 356, 182, 182, 35, 81, 35, 318, 201, 58, 251, 260, 182, 35, 302, 201, 100, 251, 302, 45, 251, 301, 237, 65, 114, 209, 30, 252, 318, 182, 182, 85, 290, 251, 147, 182, 35, 319, 201, 226, 251, 292, 201, 261, 77, 289, 114, 129, 201, 251, 280, 114, 283, 182, 114, 293, 201, 251, 276, 114, 277, 182, 114, 294, 201, 251, 285, 252, 282, 114, 356, 182, 114, 295, 201, 251, 282, 114, 283, 182, 114, 296, 201, 251, 357, 114, 356, 182, 182, 35, 189, 281, 65, 35, 10, 251, 319, 114, 302, 182, 35, 192, 35, 33, 65, 35, 10, 251, 319, 114, 302, 114, 306, 201, 251, 356, 114, 182, 182, 35, 59, 35, 3, 35]}, {"code": "def _kernel(\n    A,\n    B,\n    C,\n    bias,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    a_scale_ptr,\n    b_scale_ptr,\n    out_scale_ptr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    BIAS_ADD: tl.constexpr,\n    A_PER_CHANNEL: tl.constexpr,\n    B_PER_CHANNEL: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (pid % group_size)\n    pid_n = (pid % width) // (group_size)\n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            k_remaining = K - k * BLOCK_K\n            _0 = tl.zeros((1, 1), dtype=tl.int8)\n            a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n            b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n        acc += tl.dot(a, b, allow_tf32=True, out_dtype=tl.int32)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n    if A_PER_CHANNEL:\n        _0 = tl.zeros((1,), dtype=a_scale_ptr.dtype.element_ty)\n        mask = ram < M\n        a_scale = tl.load(a_scale_ptr + ram, mask=mask, other=_0)\n    else:\n        a_scale = tl.load(a_scale_ptr)\n    if B_PER_CHANNEL:\n        _0 = tl.zeros((1,), dtype=b_scale_ptr.dtype.element_ty)\n        mask = rbn < N\n        b_scale = tl.load(b_scale_ptr + rbn, mask=mask, other=_0)\n    else:\n        b_scale = tl.load(b_scale_ptr)\n    if BIAS_ADD:\n        bias = tl.load(bias + rn)\n        if A_PER_CHANNEL and B_PER_CHANNEL:\n            bias = tl.math.llrint(bias / (a_scale[:, None] * b_scale[None, :])).to(\n                tl.int32\n            )\n            acc = acc + bias\n        else:\n            bias = tl.math.llrint(bias / (a_scale * b_scale)).to(tl.int32)\n            acc = acc + bias[None, :]\n\n    if A_PER_CHANNEL and B_PER_CHANNEL:\n        mask = ram < M\n        _0 = tl.zeros((1,), dtype=out_scale_ptr.dtype.element_ty)\n        out_scale = tl.load(out_scale_ptr + ram, mask=mask, other=_0)\n        acc = tl.math.llrint(\n            (\n                acc.to(tl.float32)\n                * a_scale[:, None]\n                * b_scale[None, :]\n                * out_scale[:, None]\n            )\n        ).to(tl.int8)\n    else:\n        out_scale = tl.load(out_scale_ptr)\n        acc = tl.math.llrint((acc.to(tl.float32) * (a_scale * b_scale * out_scale))).to(\n            tl.int8\n        )\n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    tl.store(C, acc, mask=mask)", "encoded": [32, 355, 251, 255, 114, 256, 114, 257, 114, 258, 114, 259, 114, 260, 114, 261, 114, 262, 114, 263, 114, 264, 114, 265, 114, 266, 114, 267, 114, 268, 114, 269, 114, 270, 114, 271, 66, 6, 114, 272, 66, 6, 114, 273, 66, 6, 114, 274, 66, 6, 114, 275, 66, 6, 114, 276, 66, 6, 114, 277, 66, 6, 114, 278, 66, 6, 182, 66, 35, -1, 279, 201, 178, 251, 356, 182, 35, 280, 201, 178, 251, 357, 182, 35, 281, 201, 68, 251, 259, 114, 271, 182, 35, 282, 201, 68, 251, 260, 114, 272, 182, 35, 283, 201, 274, 252, 282, 35, 284, 201, 279, 51, 283, 35, 285, 201, 42, 251, 281, 4, 284, 252, 274, 114, 274, 182, 35, 286, 201, 284, 252, 274, 77, 279, 233, 285, 35, 287, 201, 279, 233, 283, 51, 285, 35, 288, 201, 286, 252, 271, 77, 79, 251, 356, 114, 271, 182, 35, 289, 201, 287, 252, 272, 77, 79, 251, 356, 114, 272, 182, 35, 290, 201, 218, 251, 58, 251, 288, 233, 259, 114, 271, 182, 114, 271, 182, 35, 291, 201, 218, 251, 58, 251, 289, 233, 260, 114, 272, 182, 114, 272, 182, 35, 292, 201, 280, 252, 273, 77, 79, 251, 356, 114, 273, 182, 35, 255, 201, 255, 77, 251, 290, 237, 66, 114, 209, 30, 252, 262, 77, 292, 237, 209, 114, 66, 30, 252, 263, 182, 35, 256, 201, 256, 77, 251, 292, 237, 66, 114, 209, 30, 252, 264, 77, 291, 237, 209, 114, 66, 30, 252, 265, 182, 35, 293, 201, 183, 251, 251, 271, 114, 272, 182, 114, 95, 201, 65, 182, 35, 142, 294, 164, 5, 251, 356, 114, 68, 251, 261, 114, 273, 182, 182, 66, 35, 189, 275, 66, 35, 295, 201, 60, 251, 255, 182, 35, 296, 201, 60, 251, 256, 182, 35, 192, 35, 33, 66, 35, 297, 201, 261, 4, 294, 252, 273, 35, 298, 201, 183, 251, 251, 357, 114, 357, 182, 114, 95, 201, 147, 182, 35, 295, 201, 60, 251, 255, 114, 299, 201, 292, 237, 209, 114, 66, 30, 1, 297, 114, 300, 201, 298, 182, 35, 296, 201, 60, 251, 256, 114, 299, 201, 292, 237, 66, 114, 209, 30, 1, 297, 114, 300, 201, 298, 182, 35, 59, 35, 293, 179, 15, 251, 295, 114, 296, 114, 301, 201, 167, 114, 302, 201, 65, 182, 35, 255, 179, 273, 252, 263, 35, 256, 179, 273, 252, 264, 35, 81, 35, 189, 277, 66, 35, 298, 201, 183, 251, 251, 357, 114, 182, 114, 95, 201, 268, 85, 95, 85, 120, 182, 35, 299, 201, 290, 1, 259, 35, 303, 201, 60, 251, 268, 77, 290, 114, 299, 201, 299, 114, 300, 201, 298, 182, 35, 192, 35, 33, 66, 35, 303, 201, 60, 251, 268, 182, 35, 59, 35, 189, 278, 66, 35, 298, 201, 183, 251, 251, 357, 114, 182, 114, 95, 201, 269, 85, 95, 85, 120, 182, 35, 299, 201, 291, 1, 260, 35, 304, 201, 60, 251, 269, 77, 291, 114, 299, 201, 299, 114, 300, 201, 298, 182, 35, 192, 35, 33, 66, 35, 304, 201, 60, 251, 269, 182, 35, 59, 35, 189, 276, 66, 35, 258, 201, 60, 251, 258, 77, 289, 182, 35, 189, 277, 107, 278, 66, 35, 258, 201, 100, 251, 258, 45, 251, 303, 237, 66, 114, 209, 30, 252, 304, 237, 209, 114, 66, 30, 182, 182, 85, 305, 251, 65, 182, 35, 293, 201, 293, 77, 258, 35, 192, 35, 33, 66, 35, 258, 201, 100, 251, 258, 45, 251, 303, 252, 304, 182, 182, 85, 305, 251, 65, 182, 35, 293, 201, 293, 77, 258, 237, 209, 114, 66, 30, 35, 59, 35, 192, 35, 189, 277, 107, 278, 66, 35, 299, 201, 290, 1, 259, 35, 298, 201, 183, 251, 251, 357, 114, 182, 114, 95, 201, 270, 85, 95, 85, 120, 182, 35, 306, 201, 60, 251, 270, 77, 290, 114, 299, 201, 299, 114, 300, 201, 298, 182, 35, 293, 201, 100, 251, 293, 85, 305, 251, 152, 182, 252, 303, 237, 66, 114, 209, 30, 252, 304, 237, 209, 114, 66, 30, 252, 306, 237, 66, 114, 209, 30, 182, 85, 305, 251, 147, 182, 35, 192, 35, 33, 66, 35, 306, 201, 60, 251, 270, 182, 35, 293, 201, 100, 251, 293, 85, 305, 251, 152, 182, 252, 251, 303, 252, 304, 252, 306, 182, 182, 85, 305, 251, 147, 182, 35, 59, 35, 288, 201, 286, 252, 271, 77, 79, 251, 356, 114, 271, 182, 35, 289, 201, 287, 252, 272, 77, 79, 251, 356, 114, 272, 182, 35, 257, 201, 257, 77, 251, 288, 237, 66, 114, 209, 30, 252, 266, 77, 289, 237, 209, 114, 66, 30, 252, 267, 182, 35, 299, 201, 251, 288, 1, 259, 182, 237, 66, 114, 209, 30, 180, 251, 289, 1, 260, 182, 237, 209, 114, 66, 30, 35, 10, 251, 257, 114, 293, 114, 299, 201, 299, 182, 35, 3, 35]}, {"code": "def _kernel(\n    A,\n    B,\n    C,\n    bias,\n    M,\n    N,\n    K,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    a_scale_ptr,\n    b_scale_ptr,\n    out_dtype: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    SPLIT_K: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    BIAS_ADD: tl.constexpr,\n    A_PER_CHANNEL: tl.constexpr,\n    B_PER_CHANNEL: tl.constexpr,\n):\n\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (pid % group_size)\n    pid_n = (pid % width) // (group_size)\n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n    if A_PER_CHANNEL:\n        a_scale = tl.load(a_scale_ptr + ram)\n    else:\n        a_scale = tl.load(a_scale_ptr)\n\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            k_remaining = K - k * BLOCK_K\n            _0 = tl.zeros((1, 1), dtype=tl.int8)\n            a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n            b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n        if A_PER_CHANNEL:\n            a = tl.math.llrint((a / a_scale[:, None])).to(tl.int8)\n        else:\n            a = tl.math.llrint((a / a_scale)).to(tl.int8)\n        acc += tl.dot(a, b, allow_tf32=True, out_dtype=tl.int32)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    if B_PER_CHANNEL:\n        b_scale = tl.load(b_scale_ptr + rbn)\n    else:\n        b_scale = tl.load(b_scale_ptr)\n    if A_PER_CHANNEL and B_PER_CHANNEL:\n        acc = (acc.to(tl.float32) * (a_scale[:, None] * b_scale[None, :])).to(out_dtype)\n    else:\n        acc = (acc.to(tl.float32) * (a_scale * b_scale)).to(out_dtype)\n    if BIAS_ADD:\n        bias = tl.load(bias + rn)\n        acc = acc + bias[None, :]\n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)", "encoded": [32, 355, 251, 255, 114, 256, 114, 257, 114, 258, 114, 259, 114, 260, 114, 261, 114, 262, 114, 263, 114, 264, 114, 265, 114, 266, 114, 267, 114, 268, 114, 269, 114, 270, 65, 6, 114, 271, 65, 6, 114, 272, 65, 6, 114, 273, 65, 6, 114, 274, 65, 6, 114, 275, 65, 6, 114, 276, 65, 6, 114, 277, 65, 6, 114, 278, 65, 6, 114, 279, 65, 6, 182, 65, 35, -1, 280, 201, 178, 251, 356, 182, 35, 281, 201, 178, 251, 357, 182, 35, 282, 201, 67, 251, 259, 114, 271, 182, 35, 283, 201, 67, 251, 260, 114, 272, 182, 35, 284, 201, 274, 252, 283, 35, 285, 201, 280, 51, 284, 35, 286, 201, 42, 251, 282, 4, 285, 252, 274, 114, 274, 182, 35, 287, 201, 285, 252, 274, 77, 280, 233, 286, 35, 288, 201, 280, 233, 284, 51, 286, 35, 289, 201, 287, 252, 271, 77, 79, 251, 356, 114, 271, 182, 35, 290, 201, 288, 252, 272, 77, 79, 251, 356, 114, 272, 182, 35, 291, 201, 218, 251, 60, 251, 289, 233, 259, 114, 271, 182, 114, 271, 182, 35, 292, 201, 218, 251, 60, 251, 290, 233, 260, 114, 272, 182, 114, 272, 182, 35, 293, 201, 281, 252, 273, 77, 79, 251, 356, 114, 273, 182, 35, 255, 201, 255, 77, 251, 291, 237, 65, 114, 209, 30, 252, 262, 77, 293, 237, 209, 114, 65, 30, 252, 263, 182, 35, 256, 201, 256, 77, 251, 293, 237, 65, 114, 209, 30, 252, 264, 77, 292, 237, 209, 114, 65, 30, 252, 265, 182, 35, 294, 201, 183, 251, 251, 271, 114, 272, 182, 114, 95, 201, 254, 182, 35, 189, 278, 65, 35, 295, 201, 58, 251, 268, 77, 291, 182, 35, 192, 35, 33, 65, 35, 295, 201, 58, 251, 268, 182, 35, 59, 35, 142, 296, 164, 5, 251, 356, 114, 67, 251, 261, 114, 273, 252, 275, 182, 182, 65, 35, 189, 276, 65, 35, 297, 201, 58, 251, 255, 182, 35, 298, 201, 58, 251, 256, 182, 35, 192, 35, 33, 65, 35, 299, 201, 261, 4, 296, 252, 273, 35, 300, 201, 183, 251, 251, 357, 114, 357, 182, 114, 95, 201, 147, 182, 35, 297, 201, 58, 251, 255, 114, 301, 201, 293, 237, 209, 114, 65, 30, 1, 299, 114, 302, 201, 300, 182, 35, 298, 201, 58, 251, 256, 114, 301, 201, 293, 237, 65, 114, 209, 30, 1, 299, 114, 302, 201, 300, 182, 35, 59, 35, 189, 278, 65, 35, 297, 201, 100, 251, 297, 45, 295, 237, 65, 114, 209, 30, 182, 85, 303, 251, 147, 182, 35, 192, 35, 33, 65, 35, 297, 201, 100, 251, 297, 45, 295, 182, 85, 303, 251, 147, 182, 35, 59, 35, 294, 179, 15, 251, 297, 114, 298, 114, 304, 201, 167, 114, 270, 201, 254, 182, 35, 255, 179, 273, 252, 275, 252, 263, 35, 256, 179, 273, 252, 275, 252, 264, 35, 81, 35, 189, 279, 65, 35, 305, 201, 58, 251, 269, 77, 292, 182, 35, 192, 35, 33, 65, 35, 305, 201, 58, 251, 269, 182, 35, 59, 35, 189, 278, 107, 279, 65, 35, 294, 201, 251, 294, 85, 303, 251, 152, 182, 252, 251, 295, 237, 65, 114, 209, 30, 252, 305, 237, 209, 114, 65, 30, 182, 182, 85, 303, 251, 270, 182, 35, 192, 35, 33, 65, 35, 294, 201, 251, 294, 85, 303, 251, 152, 182, 252, 251, 295, 252, 305, 182, 182, 85, 303, 251, 270, 182, 35, 59, 35, 189, 277, 65, 35, 258, 201, 58, 251, 258, 77, 290, 182, 35, 294, 201, 294, 77, 258, 237, 209, 114, 65, 30, 35, 192, 35, 289, 201, 287, 252, 271, 77, 79, 251, 356, 114, 271, 182, 35, 290, 201, 288, 252, 272, 77, 79, 251, 356, 114, 272, 182, 35, 257, 201, 257, 77, 251, 289, 237, 65, 114, 209, 30, 252, 266, 77, 290, 237, 209, 114, 65, 30, 252, 267, 182, 35, 301, 201, 251, 289, 1, 259, 182, 237, 65, 114, 209, 30, 180, 251, 290, 1, 260, 182, 237, 209, 114, 65, 30, 35, 189, 275, 80, 357, 65, 35, 10, 251, 257, 114, 294, 114, 301, 201, 301, 182, 35, 192, 35, 33, 65, 35, 203, 251, 257, 114, 294, 114, 301, 201, 301, 182, 35, 59, 35, 3, 35]}, {"code": "def _safe_softmax_forward_kernel(\n    output_ptr, input_ptr, M, N, K, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offset = tl.arange(0, BLOCK_N)\n    offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n    mask = m_offset[:, None] < M and n_offset[None, :] < N\n    input_ptrs = input_ptr + offset\n    inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\"))\n    row_minus_max = inp - tl.max(inp, axis=1)[:, None]\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=1)[:, None]\n    softmax_output = numerator / denominator\n    output_ptrs = output_ptr + offset\n    tl.store(output_ptrs, softmax_output, mask=mask)", "encoded": [32, 355, 251, 255, 114, 256, 114, 257, 114, 258, 114, 259, 114, 260, 66, 6, 114, 261, 66, 6, 182, 66, 35, -1, 262, 201, 178, 251, 356, 182, 35, 263, 201, 178, 251, 357, 182, 35, 264, 201, 262, 252, 260, 77, 79, 251, 356, 114, 260, 182, 35, 265, 201, 79, 251, 356, 114, 261, 182, 35, 266, 201, 264, 237, 66, 114, 209, 30, 252, 258, 252, 259, 77, 265, 237, 209, 114, 66, 30, 252, 259, 77, 263, 35, 267, 201, 264, 237, 66, 114, 209, 30, 1, 257, 107, 265, 237, 209, 114, 66, 30, 1, 258, 35, 268, 201, 256, 77, 266, 35, 269, 201, 60, 251, 268, 114, 267, 201, 267, 114, 270, 201, 4, 271, 251, 358, 182, 182, 35, 272, 201, 269, 4, 12, 251, 269, 114, 273, 201, 357, 182, 237, 66, 114, 209, 30, 35, 274, 201, 113, 251, 272, 182, 35, 275, 201, 228, 251, 274, 114, 273, 201, 357, 182, 237, 66, 114, 209, 30, 35, 276, 201, 274, 45, 275, 35, 277, 201, 255, 77, 266, 35, 10, 251, 277, 114, 276, 114, 267, 201, 267, 182, 35, 3, 35]}, {"code": "def _safe_softmax_backward_kernel(\n    out_ptr,\n    out_grad_ptr,\n    in_grad_ptr,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offset = tl.arange(0, BLOCK_N)\n    offsets = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n    mask = m_offset[:, None] < M and n_offset[None, :] < N\n    out_ptrs = out_ptr + offsets\n    out = tl.load(out_ptrs, mask=mask)\n    out_grad_ptrs = out_grad_ptr + offsets\n    out_grad = tl.load(out_grad_ptrs, mask=mask)\n\n    scale = tl.sum(out * out_grad, 1)\n    in_grad = out * (out_grad - scale[:, None])\n\n    in_grad_ptrs = in_grad_ptr + offsets\n    tl.store(in_grad_ptrs, in_grad, mask=mask)", "encoded": [32, 355, 251, 255, 114, 256, 114, 257, 114, 258, 114, 259, 114, 260, 114, 261, 65, 6, 114, 262, 65, 6, 182, 65, 35, -1, 263, 201, 178, 251, 356, 182, 35, 264, 201, 178, 251, 357, 182, 35, 265, 201, 263, 252, 261, 77, 79, 251, 356, 114, 261, 182, 35, 266, 201, 79, 251, 356, 114, 262, 182, 35, 267, 201, 265, 237, 65, 114, 209, 30, 252, 259, 252, 260, 77, 266, 237, 209, 114, 65, 30, 252, 260, 77, 264, 35, 268, 201, 265, 237, 65, 114, 209, 30, 1, 258, 107, 266, 237, 209, 114, 65, 30, 1, 259, 35, 269, 201, 255, 77, 267, 35, 14, 201, 58, 251, 269, 114, 268, 201, 268, 182, 35, 270, 201, 256, 77, 267, 35, 271, 201, 58, 251, 270, 114, 268, 201, 268, 182, 35, 272, 201, 228, 251, 14, 252, 271, 114, 357, 182, 35, 273, 201, 14, 252, 251, 271, 4, 272, 237, 65, 114, 209, 30, 182, 35, 274, 201, 257, 77, 267, 35, 10, 251, 274, 114, 273, 114, 268, 201, 268, 182, 35, 3, 35]}, {"code": "def _online_softmax_kernel_non_inner(\n    output_ptr,\n    input_ptr,\n    M,\n    N,\n    K,\n    TILE_N: tl.constexpr,\n    TILE_K: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_k = tl.program_id(1)\n    pid_m = tl.program_id(0)\n\n    k_offsets = pid_k * TILE_K + tl.arange(0, TILE_K)\n\n    if ONE_TILE_PER_CTA:\n        n_offsets = tl.arange(0, TILE_N)\n        offset = pid_m * N * K + n_offsets[:, None] * K + k_offsets\n        mask = (n_offsets[:, None] < N) & (k_offsets < K)\n        input_ptrs = input_ptr + offset\n        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\"))\n        m = tl.max(inp, 0)\n        e = tl.exp(inp - m[None, :])\n        z = tl.sum(e, 0)\n        out = e / z\n        output_ptrs = output_ptr + offset\n        tl.store(output_ptrs, out, mask=mask)\n    else:\n        m = tl.full([TILE_N, TILE_K], value=float(\"-inf\"), dtype=tl.float32)\n        z = tl.full([TILE_N, TILE_K], value=0.0, dtype=tl.float32)\n\n        for start_n in range(0, N, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)\n            offsets = pid_m * N * K + n_offsets[:, None] * K + k_offsets\n            mask = (n_offsets[:, None] < N) & (k_offsets < K)\n            inp = tl.load(input_ptr + offsets, mask=mask, other=-float(\"inf\"))\n            m_new = tl.maximum(m, inp)\n            all_neg_inf = m_new == float(\"-inf\")\n            z = tl.where(all_neg_inf, z, z * tl.exp(m - m_new) + tl.exp(inp - m_new))\n            m = m_new\n\n        m_reduced = tl.max(m, 0)\n        z = tl.sum(z * tl.exp(m - m_reduced[None, :]), 0)\n        m = m_reduced\n\n        previous_multiple = prev_multiple_of(N, TILE_N)\n        for start_n in range(0, N, TILE_N):\n            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)\n            offsets = pid_m * N * K + n_offsets[:, None] * K + k_offsets\n            mask = (n_offsets[:, None] < N) & (k_offsets[None, :] < K)\n            inp = tl.load(input_ptr + offsets, mask=mask, other=-float(\"inf\"))\n            o = tl.exp(inp - m[None, :]) / z[None, :]\n            tl.store(output_ptr + offsets, o, mask=mask)", "encoded": [32, 355, 251, 255, 114, 256, 114, 257, 114, 258, 114, 259, 114, 260, 66, 6, 114, 261, 66, 6, 114, 262, 66, 6, 182, 66, 35, -1, 263, 201, 178, 251, 356, 182, 35, 264, 201, 178, 251, 357, 182, 35, 265, 201, 263, 252, 261, 77, 79, 251, 357, 114, 261, 182, 35, 189, 262, 66, 35, 266, 201, 79, 251, 357, 114, 260, 182, 35, 267, 201, 264, 252, 258, 252, 259, 77, 266, 237, 66, 114, 209, 30, 252, 259, 77, 265, 35, 268, 201, 251, 266, 237, 66, 114, 209, 30, 1, 258, 182, 180, 251, 265, 1, 259, 182, 35, 269, 201, 256, 77, 267, 35, 270, 201, 60, 251, 269, 114, 268, 201, 268, 114, 271, 201, 4, 272, 251, 358, 182, 182, 35, 273, 201, 12, 251, 270, 114, 357, 182, 35, 274, 201, 113, 251, 270, 4, 273, 237, 209, 114, 66, 30, 182, 35, 275, 201, 228, 251, 274, 114, 357, 182, 35, 14, 201, 274, 45, 275, 35, 276, 201, 255, 77, 267, 35, 10, 251, 276, 114, 14, 114, 268, 201, 268, 182, 35, 192, 35, 33, 66, 35, 273, 201, 244, 251, 237, 260, 114, 261, 30, 114, 277, 201, 272, 251, 359, 182, 114, 95, 201, 152, 182, 35, 275, 201, 244, 251, 237, 260, 114, 261, 30, 114, 277, 201, 357, 114, 95, 201, 152, 182, 35, 142, 278, 164, 5, 251, 357, 114, 258, 114, 260, 182, 66, 35, 266, 201, 278, 77, 79, 251, 357, 114, 260, 182, 35, 279, 201, 264, 252, 258, 252, 259, 77, 266, 237, 66, 114, 209, 30, 252, 259, 77, 265, 35, 268, 201, 251, 266, 237, 66, 114, 209, 30, 1, 258, 182, 180, 251, 265, 1, 259, 182, 35, 270, 201, 60, 251, 256, 77, 279, 114, 268, 201, 268, 114, 271, 201, 4, 272, 251, 358, 182, 182, 35, 280, 201, 199, 251, 273, 114, 270, 182, 35, 281, 201, 280, 80, 272, 251, 359, 182, 35, 275, 201, 212, 251, 281, 114, 275, 114, 275, 252, 113, 251, 273, 4, 280, 182, 77, 113, 251, 270, 4, 280, 182, 182, 35, 273, 201, 280, 35, 81, 35, 282, 201, 12, 251, 273, 114, 357, 182, 35, 275, 201, 228, 251, 275, 252, 113, 251, 273, 4, 282, 237, 209, 114, 66, 30, 182, 114, 357, 182, 35, 273, 201, 282, 35, 283, 201, 284, 251, 258, 114, 260, 182, 35, 142, 278, 164, 5, 251, 357, 114, 258, 114, 260, 182, 66, 35, 266, 201, 283, 4, 278, 77, 79, 251, 357, 114, 260, 182, 35, 279, 201, 264, 252, 258, 252, 259, 77, 266, 237, 66, 114, 209, 30, 252, 259, 77, 265, 35, 268, 201, 251, 266, 237, 66, 114, 209, 30, 1, 258, 182, 180, 251, 265, 237, 209, 114, 66, 30, 1, 259, 182, 35, 270, 201, 60, 251, 256, 77, 279, 114, 268, 201, 268, 114, 271, 201, 4, 272, 251, 358, 182, 182, 35, 285, 201, 113, 251, 270, 4, 273, 237, 209, 114, 66, 30, 182, 45, 275, 237, 209, 114, 66, 30, 35, 10, 251, 255, 77, 279, 114, 285, 114, 268, 201, 268, 182, 35, 81, 35, 59, 35, 3, 35]}, {"code": "def _online_softmax_kernel_inner(\n    output_ptr,\n    input_ptr,\n    M,\n    N,\n    TILE_N: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    if ONE_TILE_PER_CTA:\n        n_offsets = tl.arange(0, TILE_N)\n        offset = pid_m * N + n_offsets\n        input_ptrs = input_ptr + offset\n        mask = n_offsets < N\n        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(\n            output_ptr.dtype.element_ty\n        )\n        m = tl.max(inp, 0)\n        e = tl.exp(inp - m)\n        z = tl.sum(e, 0)\n        out = e / z\n        output_ptrs = output_ptr + offset\n        tl.store(output_ptrs, out, mask=mask)\n    else:\n        m = tl.full([TILE_N], value=float(\"-inf\"), dtype=tl.float32)\n        z = tl.full([TILE_N], value=0.0, dtype=tl.float32)\n        input_ptr += pid_m * N\n        output_ptr += pid_m * N\n\n        previous_multiple = prev_multiple_of(N, TILE_N)\n        for start_n in range(0, previous_multiple, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)\n            inp = tl.load(input_ptr + n_offsets)\n            m_new = tl.maximum(m, inp)\n\n            all_neg_inf = m_new == float(\"-inf\")\n            z = tl.where(all_neg_inf, z, z * tl.exp(m - m_new) + tl.exp(inp - m_new))\n            m = m_new\n\n        for start_n in range(previous_multiple, N, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)\n            mask = n_offsets < N\n            inp = tl.load(input_ptr + n_offsets, mask=mask, other=-float(\"inf\"))\n            m_new = tl.maximum(m, inp)\n            all_neg_inf = m_new == float(\"-inf\")\n            z = tl.where(all_neg_inf, z, z * tl.exp(m - m_new) + tl.exp(inp - m_new))\n            m = m_new\n\n        m_reduced = tl.max(m, 0)\n        z = tl.sum(z * tl.exp(m - m_reduced), 0)\n        m = m_reduced\n\n        previous_multiple = prev_multiple_of(N, TILE_N)\n\n        for start_n in range(0, TILE_N, TILE_N):\n            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)\n            mask = n_offsets < N\n            inp = tl.load(\n                input_ptr + n_offsets,\n                mask=mask,\n                other=-float(\"inf\"),\n                eviction_policy=\"evict_first\",\n            )\n            o = tl.exp(inp - m) / z\n            tl.store(output_ptr + n_offsets, o, mask=mask)\n        for start_n in range(TILE_N, N, TILE_N):\n            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)\n            inp = tl.load(input_ptr + n_offsets, eviction_policy=\"evict_first\")\n            o = tl.exp(inp - m) / z\n            tl.store(output_ptr + n_offsets, o)", "encoded": [32, 355, 251, 255, 114, 256, 114, 257, 114, 258, 114, 259, 65, 6, 114, 260, 65, 6, 182, 65, 35, -1, 261, 201, 178, 251, 356, 182, 35, 189, 260, 65, 35, 262, 201, 79, 251, 356, 114, 259, 182, 35, 263, 201, 261, 252, 258, 77, 262, 35, 264, 201, 256, 77, 263, 35, 265, 201, 262, 1, 258, 35, 266, 201, 58, 251, 264, 114, 265, 201, 265, 114, 267, 201, 4, 268, 251, 357, 182, 182, 85, 269, 251, 255, 85, 95, 85, 120, 182, 35, 270, 201, 12, 251, 266, 114, 356, 182, 35, 271, 201, 113, 251, 266, 4, 270, 182, 35, 272, 201, 228, 251, 271, 114, 356, 182, 35, 14, 201, 271, 45, 272, 35, 273, 201, 255, 77, 263, 35, 10, 251, 273, 114, 14, 114, 265, 201, 265, 182, 35, 192, 35, 33, 65, 35, 270, 201, 244, 251, 237, 259, 30, 114, 274, 201, 268, 251, 358, 182, 114, 95, 201, 152, 182, 35, 272, 201, 244, 251, 237, 259, 30, 114, 274, 201, 356, 114, 95, 201, 152, 182, 35, 256, 179, 261, 252, 258, 35, 255, 179, 261, 252, 258, 35, 275, 201, 276, 251, 258, 114, 259, 182, 35, 142, 277, 164, 5, 251, 356, 114, 275, 114, 259, 182, 65, 35, 262, 201, 277, 77, 79, 251, 356, 114, 259, 182, 35, 266, 201, 58, 251, 256, 77, 262, 182, 35, 278, 201, 199, 251, 270, 114, 266, 182, 35, 279, 201, 278, 80, 268, 251, 358, 182, 35, 272, 201, 212, 251, 279, 114, 272, 114, 272, 252, 113, 251, 270, 4, 278, 182, 77, 113, 251, 266, 4, 278, 182, 182, 35, 270, 201, 278, 35, 81, 35, 142, 277, 164, 5, 251, 275, 114, 258, 114, 259, 182, 65, 35, 262, 201, 277, 77, 79, 251, 356, 114, 259, 182, 35, 265, 201, 262, 1, 258, 35, 266, 201, 58, 251, 256, 77, 262, 114, 265, 201, 265, 114, 267, 201, 4, 268, 251, 357, 182, 182, 35, 278, 201, 199, 251, 270, 114, 266, 182, 35, 279, 201, 278, 80, 268, 251, 358, 182, 35, 272, 201, 212, 251, 279, 114, 272, 114, 272, 252, 113, 251, 270, 4, 278, 182, 77, 113, 251, 266, 4, 278, 182, 182, 35, 270, 201, 278, 35, 81, 35, 280, 201, 12, 251, 270, 114, 356, 182, 35, 272, 201, 228, 251, 272, 252, 113, 251, 270, 4, 280, 182, 114, 356, 182, 35, 270, 201, 280, 35, 275, 201, 276, 251, 258, 114, 259, 182, 35, 142, 277, 164, 5, 251, 356, 114, 259, 114, 259, 182, 65, 35, 262, 201, 275, 4, 277, 77, 79, 251, 356, 114, 259, 182, 35, 265, 201, 262, 1, 258, 35, 266, 201, 58, 251, 256, 77, 262, 114, 265, 201, 265, 114, 267, 201, 4, 268, 251, 357, 182, 114, 281, 201, 359, 182, 35, 282, 201, 113, 251, 266, 4, 270, 182, 45, 272, 35, 10, 251, 255, 77, 262, 114, 282, 114, 265, 201, 265, 182, 35, 81, 35, 142, 277, 164, 5, 251, 259, 114, 258, 114, 259, 182, 65, 35, 262, 201, 275, 4, 277, 77, 79, 251, 356, 114, 259, 182, 35, 266, 201, 58, 251, 256, 77, 262, 114, 281, 201, 359, 182, 35, 282, 201, 113, 251, 266, 4, 270, 182, 45, 272, 35, 10, 251, 255, 77, 262, 114, 282, 182, 35, 81, 35, 59, 35, 3, 35]}, {"code": "def _online_softmax_backward_kernel_non_inner(\n    out_ptr,\n    out_grad_ptr,\n    in_grad_ptr,\n    M,\n    N,\n    K,\n    TILE_N: tl.constexpr,\n    TILE_K: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    offsets_k = pid_k * TILE_K + tl.arange(0, TILE_K)\n\n    if ONE_TILE_PER_CTA:\n        offsets_n = tl.arange(0, TILE_N)\n        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k\n        mask = (offsets_n < N)[:, None] & (offsets_k < K)\n        out_tile = tl.load(out_ptr + offsets, mask=mask)\n        out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n        scale = tl.sum(out_tile * out_grad_tile, axis=0)\n        in_grad_tile = out_tile * (out_grad_tile - scale[None, :])\n        tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n    else:\n        offsets_n = tl.arange(0, TILE_N)\n        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k\n        scale = tl.zeros([TILE_N, TILE_K], dtype=tl.float32)\n        for _ in range(0, N, TILE_N):\n            mask = (offsets_n < N)[:, None] & (offsets_k < K)\n            out_tile = tl.load(out_ptr + offsets, mask=mask)\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            scale += out_tile * out_grad_tile\n            offsets_n += TILE_N\n            offsets += TILE_N * K\n        scale = tl.sum(scale, axis=0)\n\n        offsets_n = tl.arange(0, TILE_N)\n        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k\n        for _ in range(0, N, TILE_N):\n            mask = (offsets_n < N)[:, None] & (offsets_k < K)\n            out_tile = tl.load(out_ptr + offsets, mask=mask)\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            in_grad_tile = out_tile * (out_grad_tile - scale[None, :])\n            tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n            offsets_n += TILE_N\n            offsets += TILE_N * K", "encoded": [32, 355, 251, 255, 114, 256, 114, 257, 114, 258, 114, 259, 114, 260, 114, 261, 66, 6, 114, 262, 66, 6, 114, 263, 66, 6, 182, 66, 35, -1, 264, 201, 178, 251, 356, 182, 35, 265, 201, 178, 251, 357, 182, 35, 266, 201, 265, 252, 262, 77, 79, 251, 356, 114, 262, 182, 35, 189, 263, 66, 35, 267, 201, 79, 251, 356, 114, 261, 182, 35, 268, 201, 264, 252, 259, 252, 260, 77, 267, 237, 66, 114, 209, 30, 252, 260, 77, 266, 35, 269, 201, 251, 267, 1, 259, 182, 237, 66, 114, 209, 30, 180, 251, 266, 1, 260, 182, 35, 270, 201, 60, 251, 255, 77, 268, 114, 269, 201, 269, 182, 35, 271, 201, 60, 251, 256, 77, 268, 114, 269, 201, 269, 182, 35, 272, 201, 228, 251, 270, 252, 271, 114, 273, 201, 356, 182, 35, 274, 201, 270, 252, 251, 271, 4, 272, 237, 209, 114, 66, 30, 182, 35, 10, 251, 257, 77, 268, 114, 274, 114, 269, 201, 269, 182, 35, 192, 35, 33, 66, 35, 267, 201, 79, 251, 356, 114, 261, 182, 35, 268, 201, 264, 252, 259, 252, 260, 77, 267, 237, 66, 114, 209, 30, 252, 260, 77, 266, 35, 272, 201, 183, 251, 237, 261, 114, 262, 30, 114, 95, 201, 152, 182, 35, 142, 275, 164, 5, 251, 356, 114, 259, 114, 261, 182, 66, 35, 269, 201, 251, 267, 1, 259, 182, 237, 66, 114, 209, 30, 180, 251, 266, 1, 260, 182, 35, 270, 201, 60, 251, 255, 77, 268, 114, 269, 201, 269, 182, 35, 271, 201, 60, 251, 256, 77, 268, 114, 269, 201, 269, 182, 35, 272, 179, 270, 252, 271, 35, 267, 179, 261, 35, 268, 179, 261, 252, 260, 35, 81, 35, 272, 201, 228, 251, 272, 114, 273, 201, 356, 182, 35, 267, 201, 79, 251, 356, 114, 261, 182, 35, 268, 201, 264, 252, 259, 252, 260, 77, 267, 237, 66, 114, 209, 30, 252, 260, 77, 266, 35, 142, 275, 164, 5, 251, 356, 114, 259, 114, 261, 182, 66, 35, 269, 201, 251, 267, 1, 259, 182, 237, 66, 114, 209, 30, 180, 251, 266, 1, 260, 182, 35, 270, 201, 60, 251, 255, 77, 268, 114, 269, 201, 269, 182, 35, 271, 201, 60, 251, 256, 77, 268, 114, 269, 201, 269, 182, 35, 274, 201, 270, 252, 251, 271, 4, 272, 237, 209, 114, 66, 30, 182, 35, 10, 251, 257, 77, 268, 114, 274, 114, 269, 201, 269, 182, 35, 267, 179, 261, 35, 268, 179, 261, 252, 260, 35, 81, 35, 59, 35, 3, 35]}, {"code": "def _online_softmax_backward_kernel_inner(\n    out_ptr,\n    out_grad_ptr,\n    in_grad_ptr,\n    M,\n    N,\n    TILE_M: tl.constexpr,\n    TILE_N: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    m_offsets = pid_m * TILE_M + tl.arange(0, TILE_M)\n    if ONE_TILE_PER_CTA:\n        n_offsets = tl.arange(0, TILE_N)\n        offsets = m_offsets[:, None] * N + n_offsets\n        mask = (m_offsets[:, None] < M) & (n_offsets < N)\n        out_tile = tl.load(out_ptr + offsets, mask=mask)\n        out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n        scale = tl.sum(out_tile * out_grad_tile, 1)\n        in_grad_tile = out_tile * (out_grad_tile - scale[:, None])\n        tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n    else:\n        scale = tl.zeros([TILE_M, TILE_N], dtype=tl.float32)\n\n        n_offsets = tl.arange(0, TILE_N)\n        offsets = m_offsets[:, None] * N + n_offsets\n        for _ in range(0, N, TILE_N):\n            mask = (m_offsets[:, None] < M) & (n_offsets < N)\n            out_tile = tl.load(\n                out_ptr + offsets, mask=mask, eviction_policy=\"evict_last\"\n            )\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            scale += out_tile * out_grad_tile\n            n_offsets += TILE_N\n            offsets += TILE_N\n        scale = tl.sum(scale, 1)\n\n        n_offsets = tl.arange(0, TILE_N)\n        offsets = m_offsets[:, None] * N + n_offsets\n        for _ in range(0, N, TILE_N):\n            mask = (m_offsets[:, None] < M) & (n_offsets < N)\n            out_tile = tl.load(\n                out_ptr + offsets, mask=mask, eviction_policy=\"evict_first\"\n            )\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            in_grad_tile = out_tile * (out_grad_tile - scale[:, None])\n            tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n            n_offsets += TILE_N\n            offsets += TILE_N", "encoded": [32, 355, 251, 255, 114, 256, 114, 257, 114, 258, 114, 259, 114, 260, 65, 6, 114, 261, 65, 6, 114, 262, 65, 6, 182, 65, 35, -1, 263, 201, 178, 251, 356, 182, 35, 264, 201, 263, 252, 260, 77, 79, 251, 356, 114, 260, 182, 35, 189, 262, 65, 35, 265, 201, 79, 251, 356, 114, 261, 182, 35, 266, 201, 264, 237, 65, 114, 209, 30, 252, 259, 77, 265, 35, 267, 201, 251, 264, 237, 65, 114, 209, 30, 1, 258, 182, 180, 251, 265, 1, 259, 182, 35, 268, 201, 58, 251, 255, 77, 266, 114, 267, 201, 267, 182, 35, 269, 201, 58, 251, 256, 77, 266, 114, 267, 201, 267, 182, 35, 270, 201, 228, 251, 268, 252, 269, 114, 357, 182, 35, 271, 201, 268, 252, 251, 269, 4, 270, 237, 65, 114, 209, 30, 182, 35, 10, 251, 257, 77, 266, 114, 271, 114, 267, 201, 267, 182, 35, 192, 35, 33, 65, 35, 270, 201, 183, 251, 237, 260, 114, 261, 30, 114, 95, 201, 152, 182, 35, 265, 201, 79, 251, 356, 114, 261, 182, 35, 266, 201, 264, 237, 65, 114, 209, 30, 252, 259, 77, 265, 35, 142, 272, 164, 5, 251, 356, 114, 259, 114, 261, 182, 65, 35, 267, 201, 251, 264, 237, 65, 114, 209, 30, 1, 258, 182, 180, 251, 265, 1, 259, 182, 35, 268, 201, 58, 251, 255, 77, 266, 114, 267, 201, 267, 114, 273, 201, 358, 182, 35, 269, 201, 58, 251, 256, 77, 266, 114, 267, 201, 267, 182, 35, 270, 179, 268, 252, 269, 35, 265, 179, 261, 35, 266, 179, 261, 35, 81, 35, 270, 201, 228, 251, 270, 114, 357, 182, 35, 265, 201, 79, 251, 356, 114, 261, 182, 35, 266, 201, 264, 237, 65, 114, 209, 30, 252, 259, 77, 265, 35, 142, 272, 164, 5, 251, 356, 114, 259, 114, 261, 182, 65, 35, 267, 201, 251, 264, 237, 65, 114, 209, 30, 1, 258, 182, 180, 251, 265, 1, 259, 182, 35, 268, 201, 58, 251, 255, 77, 266, 114, 267, 201, 267, 114, 273, 201, 359, 182, 35, 269, 201, 58, 251, 256, 77, 266, 114, 267, 201, 267, 182, 35, 271, 201, 268, 252, 251, 269, 4, 270, 237, 65, 114, 209, 30, 182, 35, 10, 251, 257, 77, 266, 114, 271, 114, 267, 201, 267, 182, 35, 265, 179, 261, 35, 266, 179, 261, 35, 81, 35, 59, 35, 3, 35]}, {"code": "def kernel_fma(\n    C,\n    A,\n    B,\n    bias,\n    dtype: tl.constexpr,\n    M,\n    N,\n    K,\n    CACHE_KEY_M,\n    CACHE_KEY_N,\n    CACHE_KEY_K,\n    output_m_stride,\n    output_n_stride,\n    a_m_stride,\n    a_k_stride,\n    b_n_stride,\n    b_k_stride,\n    BLOCK_M: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    SPLIT_K: tl.constexpr,\n    K_LOAD_MASK_NEEDED: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n):\n\n    program_idx = tl.program_id(axis=0)\n\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n\n    width = GROUP_M * grid_n\n    group_idx = program_idx // width\n    group_size = min(grid_m - group_idx * GROUP_M, GROUP_M)\n    block_m_idx = group_idx * GROUP_M + (program_idx % group_size)\n    block_n_idx = (program_idx % width) // group_size\n\n    m_offs_untagged = block_m_idx * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offs_untagged = block_n_idx * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    m_offs = tl.max_contiguous(tl.multiple_of(m_offs_untagged % M, BLOCK_M), BLOCK_M)\n    n_offs = tl.max_contiguous(tl.multiple_of(n_offs_untagged % N, BLOCK_N), BLOCK_N)\n\n    k_range_offs = tl.arange(0, BLOCK_K)\n\n    A = A + (m_offs[:, None] * a_m_stride + k_range_offs[None, :] * a_k_stride)\n    B = B + (k_range_offs[:, None] * b_k_stride + n_offs[None, :] * b_n_stride)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    if HAS_BIAS:\n        bias = tl.load(bias + n_offs, mask=n_offs < N, other=0.0).to(tl.float32)\n        acc += bias[None, :]\n\n    for k in range(K, 0, -BLOCK_K):\n        if K_LOAD_MASK_NEEDED:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=k_range_offs[None, :] < k, other=0.0)\n            b = tl.load(B, mask=k_range_offs[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n\n        A += BLOCK_K * a_k_stride\n        B += BLOCK_K * b_k_stride\n\n    if ACTIVATION:\n        acc = silu(acc)\n    acc = acc.to(dtype)\n\n    C = C + m_offs[:, None] * output_m_stride + n_offs[None, :] * output_n_stride\n    c_ptr_mask = (m_offs < M)[:, None] & (n_offs < N)[None, :]\n    tl.store(C, acc, mask=c_ptr_mask)", "encoded": [32, 355, 251, 255, 114, 256, 114, 257, 114, 258, 114, 95, 66, 6, 114, 259, 114, 260, 114, 261, 114, 262, 114, 263, 114, 264, 114, 265, 114, 266, 114, 267, 114, 268, 114, 269, 114, 270, 114, 271, 66, 6, 114, 272, 66, 6, 114, 273, 66, 6, 114, 274, 66, 6, 114, 275, 66, 6, 114, 276, 66, 6, 114, 277, 66, 6, 114, 278, 66, 6, 182, 66, 35, -1, 279, 201, 178, 251, 280, 201, 356, 182, 35, 281, 201, 251, 259, 77, 271, 4, 357, 182, 51, 271, 35, 282, 201, 251, 260, 77, 273, 4, 357, 182, 51, 273, 35, 283, 201, 272, 252, 282, 35, 284, 201, 279, 51, 283, 35, 285, 201, 42, 251, 281, 4, 284, 252, 272, 114, 272, 182, 35, 286, 201, 284, 252, 272, 77, 279, 233, 285, 35, 287, 201, 279, 233, 283, 51, 285, 35, 288, 201, 286, 252, 271, 77, 79, 251, 356, 114, 271, 182, 35, 289, 201, 287, 252, 273, 77, 79, 251, 356, 114, 273, 182, 35, 290, 201, 218, 251, 58, 251, 288, 233, 259, 114, 271, 182, 114, 271, 182, 35, 291, 201, 218, 251, 58, 251, 289, 233, 260, 114, 273, 182, 114, 273, 182, 35, 292, 201, 79, 251, 356, 114, 274, 182, 35, 256, 201, 256, 77, 251, 290, 237, 66, 114, 209, 30, 252, 267, 77, 292, 237, 209, 114, 66, 30, 252, 268, 182, 35, 257, 201, 257, 77, 251, 292, 237, 66, 114, 209, 30, 252, 270, 77, 291, 237, 209, 114, 66, 30, 252, 269, 182, 35, 293, 201, 183, 251, 251, 271, 114, 273, 182, 114, 95, 201, 152, 182, 35, 189, 277, 66, 35, 258, 201, 60, 251, 258, 77, 291, 114, 294, 201, 291, 1, 260, 114, 295, 201, 356, 182, 85, 296, 251, 152, 182, 35, 293, 179, 258, 237, 209, 114, 66, 30, 35, 192, 35, 142, 297, 164, 5, 251, 261, 114, 356, 114, 4, 274, 182, 66, 35, 189, 276, 66, 35, 298, 201, 60, 251, 256, 182, 35, 299, 201, 60, 251, 257, 182, 35, 192, 35, 33, 66, 35, 298, 201, 60, 251, 256, 114, 294, 201, 292, 237, 209, 114, 66, 30, 1, 297, 114, 295, 201, 356, 182, 35, 299, 201, 60, 251, 257, 114, 294, 201, 292, 237, 66, 114, 209, 30, 1, 297, 114, 295, 201, 356, 182, 35, 59, 35, 293, 179, 15, 251, 298, 114, 299, 182, 35, 256, 179, 274, 252, 268, 35, 257, 179, 274, 252, 270, 35, 81, 35, 189, 278, 66, 35, 293, 201, 300, 251, 293, 182, 35, 192, 35, 293, 201, 293, 85, 296, 251, 95, 182, 35, 255, 201, 255, 77, 290, 237, 66, 114, 209, 30, 252, 265, 77, 291, 237, 209, 114, 66, 30, 252, 266, 35, 301, 201, 251, 290, 1, 259, 182, 237, 66, 114, 209, 30, 180, 251, 291, 1, 260, 182, 237, 209, 114, 66, 30, 35, 10, 251, 255, 114, 293, 114, 294, 201, 301, 182, 35, 3, 35]}, {"code": "def attn_fwd(\n    Q,\n    K,\n    V,\n    bias,\n    SM_SCALE: tl.constexpr,\n    L,\n    Out,\n    stride_qz: tl.constexpr,\n    stride_qh: tl.constexpr,\n    stride_qm: tl.constexpr,\n    stride_qk: tl.constexpr,\n    stride_kz: tl.constexpr,\n    stride_kh: tl.constexpr,\n    stride_kn: tl.constexpr,\n    stride_kk: tl.constexpr,\n    stride_vz: tl.constexpr,\n    stride_vh: tl.constexpr,\n    stride_vk: tl.constexpr,\n    stride_vn: tl.constexpr,\n    stride_oz: tl.constexpr,\n    stride_oh: tl.constexpr,\n    stride_om: tl.constexpr,\n    stride_on: tl.constexpr,\n    stride_bz: tl.constexpr,\n    stride_bh: tl.constexpr,\n    stride_bm: tl.constexpr,\n    stride_bn: tl.constexpr,\n    stride_az: tl.constexpr,\n    stride_ah: tl.constexpr,\n    cu_seqlens_q,\n    cu_seqlens_k,\n    dropout_p,\n    philox_seed,\n    PERSISTENT: tl.constexpr,\n    PERSISTENT_DYNAMIC: tl.constexpr,\n    atomic_counter,\n    NUM_CU: tl.constexpr,\n    B: tl.constexpr,\n    philox_offset_base,\n    encoded_softmax,\n    alibi_slopes,\n    HQ: tl.constexpr,\n    HK: tl.constexpr,\n    ACTUAL_BLOCK_DMODEL: tl.constexpr,\n    MAX_SEQLENS_Q: tl.constexpr,\n    MAX_SEQLENS_K: tl.constexpr,\n    VARLEN: tl.constexpr,\n    IS_CAUSAL: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    USE_BIAS: tl.constexpr,\n    ENABLE_DROPOUT: tl.constexpr,\n    RETURN_ENCODED_SOFTMAX: tl.constexpr,\n    USE_ALIBI: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    PRE_LOAD_V: tl.constexpr,\n    GRID_CU_MULTIP: tl.constexpr,\n):\n\n    if PERSISTENT:\n        NUM_WG = NUM_CU * GRID_CU_MULTIP\n        num_tiles_per_head = tl.cdiv(MAX_SEQLENS_Q, BLOCK_M)\n        num_tiles_per_sample = num_tiles_per_head * HQ\n        num_tiles_total = num_tiles_per_sample * B\n        if PERSISTENT_DYNAMIC:\n            tile_id = atomic_counter.atomic_add(1)\n        else:\n            tile_id = tl.program_id(0)\n    else:\n        tile_id = 0\n        num_tiles_total = 1\n\n    while tile_id < num_tiles_total:\n        if PERSISTENT:\n\n            off_z = tile_id // num_tiles_per_sample\n\n            off_h_q = tile_id % num_tiles_per_sample // num_tiles_per_head\n\n            start_m = tile_id % num_tiles_per_sample % num_tiles_per_head\n        else:\n            start_m = tl.program_id(0)\n            off_h_q = tl.program_id(1)\n            off_z = tl.program_id(2)\n\n        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_n = tl.arange(0, BLOCK_N)\n        offs_d = tl.arange(0, BLOCK_DMODEL)\n\n        continue_condition = True\n\n        if VARLEN:\n            cu_seqlens_q_start = tl.load(cu_seqlens_q + off_z)\n            cu_seqlens_q_end = tl.load(cu_seqlens_q + off_z + 1)\n            seqlen_q = cu_seqlens_q_end - cu_seqlens_q_start\n\n            if start_m * BLOCK_M > seqlen_q:\n                continue_condition = False\n\n            cu_seqlens_k_start = tl.load(cu_seqlens_k + off_z)\n            cu_seqlens_k_end = tl.load(cu_seqlens_k + off_z + 1)\n            seqlen_k = cu_seqlens_k_end - cu_seqlens_k_start\n        else:\n            cu_seqlens_q_start = 0\n            cu_seqlens_k_start = 0\n            seqlen_q = MAX_SEQLENS_Q\n            seqlen_k = MAX_SEQLENS_K\n\n        if continue_condition:\n\n            n_blocks = cdiv_fn(seqlen_k, BLOCK_N)\n            if IS_CAUSAL:\n\n                n_blocks_seqlen = cdiv_fn(\n                    (start_m + 1) * BLOCK_M + seqlen_k - seqlen_q, BLOCK_N\n                )\n\n                n_blocks = min(n_blocks, n_blocks_seqlen)\n\n                if n_blocks <= 0:\n                    o_offset = (\n                        Out\n                        + off_z * stride_oz\n                        + off_h_q * stride_oh\n                        + cu_seqlens_q_start * stride_om\n                    )\n                    o_ptrs = (\n                        o_offset\n                        + offs_m[:, None] * stride_om\n                        + offs_d[None, :] * stride_on\n                    )\n                    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=Out.type.element_ty)\n                    o_ptrs_mask = (offs_m[:, None] < seqlen_q).broadcast_to(\n                        [BLOCK_M, BLOCK_DMODEL]\n                    )\n\n                    tl.store(o_ptrs, acc, mask=o_ptrs_mask)\n\n                    l_ptrs = (\n                        L\n                        + off_z * HQ * MAX_SEQLENS_Q\n                        + off_h_q * MAX_SEQLENS_Q\n                        + offs_m\n                    )\n\n                    l_value = tl.full([BLOCK_M], value=float(\"inf\"), dtype=tl.float32)\n                    l_ptrs_mask = offs_m < MAX_SEQLENS_Q\n                    tl.store(l_ptrs, l_value, mask=l_ptrs_mask)\n\n                    continue_condition = False\n\n            if continue_condition:\n\n                GROUP_SIZE: tl.constexpr = HQ // HK\n                off_h_k = off_h_q // GROUP_SIZE if GROUP_SIZE != 1 else off_h_q\n\n                n_extra_tokens = 0\n                if seqlen_k < BLOCK_N:\n                    n_extra_tokens = BLOCK_N - seqlen_k\n                elif seqlen_k % BLOCK_N:\n                    n_extra_tokens = seqlen_k % BLOCK_N\n                PADDED_HEAD: tl.constexpr = ACTUAL_BLOCK_DMODEL != BLOCK_DMODEL\n\n                q_offset = (\n                    Q\n                    + off_z * stride_qz\n                    + off_h_q * stride_qh\n                    + cu_seqlens_q_start * stride_qm\n                )\n                q_ptrs = (\n                    q_offset + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n                )\n                k_offset = (\n                    K\n                    + off_z * stride_kz\n                    + off_h_k * stride_kh\n                    + cu_seqlens_k_start * stride_kn\n                )\n                k_ptrs = (\n                    k_offset + offs_d[:, None] * stride_kk + offs_n[None, :] * stride_kn\n                )\n                v_offset = (\n                    V\n                    + off_z * stride_vz\n                    + off_h_k * stride_vh\n                    + cu_seqlens_k_start * stride_vk\n                )\n                v_ptrs = (\n                    v_offset + offs_n[:, None] * stride_vk + offs_d[None, :] * stride_vn\n                )\n\n                if USE_BIAS:\n\n                    bias_offset = off_h_q * stride_bh\n                    bias_ptrs = (\n                        bias\n                        + bias_offset\n                        + offs_m[:, None] * stride_bm\n                        + offs_n[None, :] * stride_bn\n                    )\n                else:\n                    bias_ptrs = None\n\n                if USE_ALIBI:\n                    a_offset = off_z * stride_az + off_h_q * stride_ah\n                    alibi_slope = tl.load(alibi_slopes + a_offset)\n                else:\n                    alibi_slope = None\n\n                if ENABLE_DROPOUT:\n                    off_hz = off_z * HQ + off_h_q\n                    batch_philox_offset = (\n                        philox_offset_base + off_hz * seqlen_q * seqlen_k\n                    )\n                else:\n                    batch_philox_offset = 0\n\n                if RETURN_ENCODED_SOFTMAX:\n                    encoded_sm_base = encoded_softmax + off_h_q * seqlen_q * seqlen_k\n                    encoded_sm_ptrs = (\n                        encoded_sm_base + offs_m[:, None] * seqlen_k + offs_n[None, :]\n                    )\n                else:\n                    encoded_sm_ptrs = None\n\n                m_i = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)\n                l_i = tl.full([BLOCK_M], 1.0, dtype=tl.float32)\n                acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n                QK_SCALE: tl.constexpr = SM_SCALE * 1.44269504089\n\n                q_ptrs_mask = offs_m[:, None] < seqlen_q\n                if PADDED_HEAD:\n                    q_ptrs_mask = q_ptrs_mask & (offs_d[None, :] < ACTUAL_BLOCK_DMODEL)\n                q = tl.load(q_ptrs, mask=q_ptrs_mask, other=0.0)\n\n                padded_block_k = n_extra_tokens != 0\n                is_modulo_mn = not padded_block_k and (seqlen_q % BLOCK_M == 0)\n                if IS_CAUSAL:\n\n                    masked_blocks = BLOCK_M // BLOCK_N + (not is_modulo_mn)\n                else:\n\n                    masked_blocks = padded_block_k\n\n                masked_blocks = min(masked_blocks, n_blocks)\n                n_full_blocks = n_blocks - masked_blocks\n                block_min = 0\n                block_max = n_blocks * BLOCK_N\n\n                if n_full_blocks > 0:\n                    block_max = (n_blocks - masked_blocks) * BLOCK_N\n                    acc, l_i, m_i = _attn_fwd_inner(\n                        acc,\n                        l_i,\n                        m_i,\n                        q,\n                        k_ptrs,\n                        v_ptrs,\n                        bias_ptrs,\n                        stride_kn,\n                        stride_vk,\n                        stride_bn,\n                        start_m,\n                        seqlen_k,\n                        seqlen_q,\n                        dropout_p,\n                        philox_seed,\n                        batch_philox_offset,\n                        encoded_sm_ptrs,\n                        block_min,\n                        block_max,\n                        0,\n                        0,\n                        0,\n                        alibi_slope,\n                        False,\n                        BLOCK_M,\n                        BLOCK_DMODEL,\n                        BLOCK_N,\n                        offs_m,\n                        offs_n,\n                        PRE_LOAD_V,\n                        False,\n                        ENABLE_DROPOUT,\n                        RETURN_ENCODED_SOFTMAX,\n                        PADDED_HEAD,\n                        ACTUAL_BLOCK_DMODEL,\n                        QK_SCALE,\n                    )\n\n                    block_min = block_max\n                    block_max = n_blocks * BLOCK_N\n\n                tl.debug_barrier()\n\n                if masked_blocks > 0:\n                    if IS_CAUSAL:\n                        offs_n_causal = offs_n + (seqlen_q - seqlen_k)\n                    else:\n                        offs_n_causal = 0\n                    k_ptrs += n_full_blocks * BLOCK_N * stride_kn\n                    v_ptrs += n_full_blocks * BLOCK_N * stride_vk\n                    if USE_BIAS:\n                        bias_ptrs += n_full_blocks * BLOCK_N * stride_bn\n                    if RETURN_ENCODED_SOFTMAX:\n                        encoded_sm_ptrs += n_full_blocks * BLOCK_N\n                    acc, l_i, m_i = _attn_fwd_inner(\n                        acc,\n                        l_i,\n                        m_i,\n                        q,\n                        k_ptrs,\n                        v_ptrs,\n                        bias_ptrs,\n                        stride_kn,\n                        stride_vk,\n                        stride_bn,\n                        start_m,\n                        seqlen_k,\n                        seqlen_q,\n                        dropout_p,\n                        philox_seed,\n                        batch_philox_offset,\n                        encoded_sm_ptrs,\n                        block_min,\n                        block_max,\n                        offs_n_causal,\n                        masked_blocks,\n                        n_extra_tokens,\n                        alibi_slope,\n                        IS_CAUSAL,\n                        BLOCK_M,\n                        BLOCK_DMODEL,\n                        BLOCK_N,\n                        offs_m,\n                        offs_n,\n                        PRE_LOAD_V,\n                        True,\n                        ENABLE_DROPOUT,\n                        RETURN_ENCODED_SOFTMAX,\n                        PADDED_HEAD,\n                        ACTUAL_BLOCK_DMODEL,\n                        QK_SCALE,\n                    )\n\n                l_recip = 1 / l_i[:, None]\n                acc = acc * l_recip\n\n                if ENABLE_DROPOUT:\n                    acc = acc / (1 - dropout_p)\n\n                end_m_idx = (start_m + 1) * BLOCK_M\n                start_m_idx = start_m * BLOCK_M\n                causal_start_idx = seqlen_q - seqlen_k\n                acc = acc.to(Out.type.element_ty)\n                if IS_CAUSAL:\n                    if causal_start_idx > start_m_idx and causal_start_idx < end_m_idx:\n                        out_mask_boundary = tl.full(\n                            (BLOCK_DMODEL,), causal_start_idx, dtype=tl.int32\n                        )\n                        mask_m_offsets = start_m_idx + tl.arange(0, BLOCK_M)\n                        out_ptrs_mask = (\n                            mask_m_offsets[:, None] >= out_mask_boundary[None, :]\n                        )\n                        z = 0.0\n                        acc = tl.where(out_ptrs_mask, acc, z.to(acc.type.element_ty))\n\n                l_ptrs = (\n                    L + off_z * HQ * MAX_SEQLENS_Q + off_h_q * MAX_SEQLENS_Q + offs_m\n                )\n\n                overflow_size = end_m_idx - seqlen_q\n                if overflow_size > 0:\n                    boundary = tl.full(\n                        (BLOCK_M,), BLOCK_M - overflow_size, dtype=tl.int32\n                    )\n                    l_ptrs_mask = tl.arange(0, BLOCK_M) < boundary\n                    tl.store(l_ptrs, m_i + tl.math.log2(l_i), mask=l_ptrs_mask)\n                else:\n                    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n\n                o_offset = (\n                    Out\n                    + off_z * stride_oz\n                    + off_h_q * stride_oh\n                    + cu_seqlens_q_start * stride_om\n                )\n                o_ptrs = (\n                    o_offset + offs_m[:, None] * stride_om + offs_d[None, :] * stride_on\n                )\n                o_ptrs_mask = tl.full([BLOCK_M, BLOCK_DMODEL], 1, dtype=tl.int1)\n                if overflow_size > 0:\n                    o_ptrs_mask = o_ptrs_mask & (offs_m[:, None] < seqlen_q)\n                if PADDED_HEAD:\n                    o_ptrs_mask = o_ptrs_mask & (offs_d[None, :] < ACTUAL_BLOCK_DMODEL)\n                tl.store(o_ptrs, acc.to(Out.dtype.element_ty), mask=o_ptrs_mask)\n\n        if PERSISTENT:\n            if PERSISTENT_DYNAMIC:\n                tile_id = atomic_counter.atomic_add(1)\n            else:\n                tile_id += NUM_WG\n        else:\n            tile_id = num_tiles_total", "encoded": [32, 355, 251, 255, 114, 256, 114, 257, 114, 258, 114, 259, 65, 6, 114, 260, 114, 261, 114, 262, 65, 6, 114, 263, 65, 6, 114, 264, 65, 6, 114, 265, 65, 6, 114, 266, 65, 6, 114, 267, 65, 6, 114, 268, 65, 6, 114, 269, 65, 6, 114, 270, 65, 6, 114, 271, 65, 6, 114, 272, 65, 6, 114, 273, 65, 6, 114, 274, 65, 6, 114, 275, 65, 6, 114, 276, 65, 6, 114, 277, 65, 6, 114, 278, 65, 6, 114, 279, 65, 6, 114, 280, 65, 6, 114, 281, 65, 6, 114, 282, 65, 6, 114, 283, 65, 6, 114, 284, 114, 285, 114, 286, 114, 287, 114, 288, 65, 6, 114, 289, 65, 6, 114, 290, 114, 291, 65, 6, 114, 292, 65, 6, 114, 293, 114, 294, 114, 295, 114, 296, 65, 6, 114, 297, 65, 6, 114, 298, 65, 6, 114, 299, 65, 6, 114, 300, 65, 6, 114, 301, 65, 6, 114, 302, 65, 6, 114, 303, 65, 6, 114, 304, 65, 6, 114, 305, 65, 6, 114, 306, 65, 6, 114, 307, 65, 6, 114, 308, 65, 6, 114, 309, 65, 6, 114, 310, 65, 6, 114, 311, 65, 6, 182, 65, 35, -1, 189, 288, 65, 35, 312, 201, 291, 252, 311, 35, 313, 201, 67, 251, 299, 114, 309, 182, 35, 314, 201, 313, 252, 296, 35, 315, 201, 314, 252, 292, 35, 189, 289, 65, 35, 316, 201, 290, 85, 96, 251, 356, 182, 35, 192, 35, 33, 65, 35, 316, 201, 178, 251, 357, 182, 35, 59, 35, 192, 35, 33, 65, 35, 316, 201, 357, 35, 315, 201, 356, 35, 59, 35, 61, 316, 1, 315, 65, 35, 189, 288, 65, 35, 317, 201, 316, 51, 314, 35, 318, 201, 316, 233, 314, 51, 313, 35, 319, 201, 316, 233, 314, 233, 313, 35, 192, 35, 33, 65, 35, 319, 201, 178, 251, 357, 182, 35, 318, 201, 178, 251, 356, 182, 35, 317, 201, 178, 251, 358, 182, 35, 59, 35, 320, 201, 319, 252, 309, 77, 79, 251, 357, 114, 309, 182, 35, 321, 201, 79, 251, 357, 114, 308, 182, 35, 322, 201, 79, 251, 357, 114, 303, 182, 35, 323, 201, 167, 35, 189, 301, 65, 35, 324, 201, 58, 251, 284, 77, 317, 182, 35, 325, 201, 58, 251, 284, 77, 317, 77, 356, 182, 35, 326, 201, 325, 4, 324, 35, 189, 319, 252, 309, 130, 326, 65, 35, 323, 201, 62, 35, 192, 35, 327, 201, 58, 251, 285, 77, 317, 182, 35, 328, 201, 58, 251, 285, 77, 317, 77, 356, 182, 35, 329, 201, 328, 4, 327, 35, 192, 35, 33, 65, 35, 324, 201, 357, 35, 327, 201, 357, 35, 326, 201, 299, 35, 329, 201, 300, 35, 59, 35, 189, 323, 65, 35, 330, 201, 331, 251, 329, 114, 308, 182, 35, 189, 302, 65, 35, 332, 201, 331, 251, 251, 319, 77, 356, 182, 252, 309, 77, 329, 4, 326, 114, 308, 182, 35, 330, 201, 42, 251, 330, 114, 332, 182, 35, 189, 330, 225, 357, 65, 35, 333, 201, 261, 77, 317, 252, 274, 77, 318, 252, 275, 77, 324, 252, 276, 35, 334, 201, 333, 77, 320, 237, 65, 114, 209, 30, 252, 276, 77, 322, 237, 209, 114, 65, 30, 252, 277, 35, 335, 201, 183, 251, 237, 309, 114, 303, 30, 114, 95, 201, 261, 85, 194, 85, 120, 182, 35, 336, 201, 251, 320, 237, 65, 114, 209, 30, 1, 326, 182, 85, 337, 251, 237, 309, 114, 303, 30, 182, 35, 10, 251, 334, 114, 335, 114, 338, 201, 336, 182, 35, 339, 201, 260, 77, 317, 252, 296, 252, 299, 77, 318, 252, 299, 77, 320, 35, 340, 201, 244, 251, 237, 309, 30, 114, 341, 201, 342, 251, 359, 182, 114, 95, 201, 152, 182, 35, 343, 201, 320, 1, 299, 35, 10, 251, 339, 114, 340, 114, 338, 201, 343, 182, 35, 323, 201, 62, 35, 192, 35, 192, 35, 189, 323, 65, 35, 344, 65, 6, 201, 296, 51, 297, 35, 345, 201, 318, 51, 344, 189, 344, 193, 356, 33, 318, 35, 346, 201, 357, 35, 189, 329, 1, 308, 65, 35, 346, 201, 308, 4, 329, 35, 68, 35, 40, 329, 233, 308, 65, 35, 346, 201, 329, 233, 308, 35, 192, 35, 347, 65, 6, 201, 298, 193, 303, 35, 348, 201, 255, 77, 317, 252, 262, 77, 318, 252, 263, 77, 324, 252, 264, 35, 349, 201, 348, 77, 320, 237, 65, 114, 209, 30, 252, 264, 77, 322, 237, 209, 114, 65, 30, 252, 265, 35, 350, 201, 256, 77, 317, 252, 266, 77, 345, 252, 267, 77, 327, 252, 268, 35, 351, 201, 350, 77, 322, 237, 65, 114, 209, 30, 252, 269, 77, 321, 237, 209, 114, 65, 30, 252, 268, 35, 352, 201, 257, 77, 317, 252, 270, 77, 345, 252, 271, 77, 327, 252, 272, 35, 353, 201, 352, 77, 321, 237, 65, 114, 209, 30, 252, 272, 77, 322, 237, 209, 114, 65, 30, 252, 273, 35, 189, 304, 65, 35, 354, 201, 318, 252, 279, 35, 360, 201, 258, 77, 354, 77, 320, 237, 65, 114, 209, 30, 252, 280, 77, 321, 237, 209, 114, 65, 30, 252, 281, 35, 192, 35, 33, 65, 35, 360, 201, 209, 35, 59, 35, 189, 307, 65, 35, 361, 201, 317, 252, 282, 77, 318, 252, 283, 35, 362, 201, 58, 251, 295, 77, 361, 182, 35, 192, 35, 33, 65, 35, 362, 201, 209, 35, 59, 35, 189, 305, 65, 35, 363, 201, 317, 252, 296, 77, 318, 35, 364, 201, 293, 77, 363, 252, 326, 252, 329, 35, 192, 35, 33, 65, 35, 364, 201, 357, 35, 59, 35, 189, 306, 65, 35, 365, 201, 294, 77, 318, 252, 326, 252, 329, 35, 366, 201, 365, 77, 320, 237, 65, 114, 209, 30, 252, 329, 77, 321, 237, 209, 114, 65, 30, 35, 192, 35, 33, 65, 35, 366, 201, 209, 35, 59, 35, 367, 201, 244, 251, 237, 309, 30, 114, 342, 251, 368, 182, 114, 95, 201, 152, 182, 35, 369, 201, 244, 251, 237, 309, 30, 114, 356, 114, 95, 201, 152, 182, 35, 335, 201, 183, 251, 237, 309, 114, 303, 30, 114, 95, 201, 152, 182, 35, 370, 65, 6, 201, 259, 252, 371, 35, 372, 201, 320, 237, 65, 114, 209, 30, 1, 326, 35, 189, 347, 65, 35, 372, 201, 372, 180, 251, 322, 237, 209, 114, 65, 30, 1, 298, 182, 35, 192, 35, 373, 201, 58, 251, 349, 114, 338, 201, 372, 114, 374, 201, 357, 182, 35, 375, 201, 346, 193, 357, 35, 376, 201, 66, 375, 107, 326, 233, 309, 80, 357, 35, 189, 302, 65, 35, 377, 201, 309, 51, 308, 77, 251, 66, 376, 182, 35, 192, 35, 33, 65, 35, 377, 201, 375, 35, 59, 35, 377, 201, 42, 251, 377, 114, 330, 182, 35, 378, 201, 330, 4, 377, 35, 379, 201, 357, 35, 380, 201, 330, 252, 308, 35, 189, 378, 130, 357, 65, 35, 380, 201, 251, 330, 4, 377, 182, 252, 308, 35, 335, 114, 369, 114, 367, 201, 381, 251, 335, 114, 369, 114, 367, 114, 373, 114, 351, 114, 353, 114, 360, 114, 268, 114, 272, 114, 281, 114, 319, 114, 329, 114, 326, 114, 286, 114, 287, 114, 364, 114, 366, 114, 379, 114, 380, 114, 357, 114, 357, 114, 357, 114, 362, 114, 62, 114, 309, 114, 303, 114, 308, 114, 320, 114, 321, 114, 310, 114, 62, 114, 305, 114, 306, 114, 347, 114, 298, 114, 370, 182, 35, 379, 201, 380, 35, 380, 201, 330, 252, 308, 35, 192, 35, 54, 251, 182, 35, 189, 377, 130, 357, 65, 35, 189, 302, 65, 35, 382, 201, 321, 77, 251, 326, 4, 329, 182, 35, 192, 35, 33, 65, 35, 382, 201, 357, 35, 59, 35, 351, 179, 378, 252, 308, 252, 268, 35, 353, 179, 378, 252, 308, 252, 272, 35, 189, 304, 65, 35, 360, 179, 378, 252, 308, 252, 281, 35, 192, 35, 189, 306, 65, 35, 366, 179, 378, 252, 308, 35, 192, 35, 335, 114, 369, 114, 367, 201, 381, 251, 335, 114, 369, 114, 367, 114, 373, 114, 351, 114, 353, 114, 360, 114, 268, 114, 272, 114, 281, 114, 319, 114, 329, 114, 326, 114, 286, 114, 287, 114, 364, 114, 366, 114, 379, 114, 380, 114, 382, 114, 377, 114, 346, 114, 362, 114, 302, 114, 309, 114, 303, 114, 308, 114, 320, 114, 321, 114, 310, 114, 167, 114, 305, 114, 306, 114, 347, 114, 298, 114, 370, 182, 35, 192, 35, 383, 201, 356, 45, 369, 237, 65, 114, 209, 30, 35, 335, 201, 335, 252, 383, 35, 189, 305, 65, 35, 335, 201, 335, 45, 251, 356, 4, 286, 182, 35, 192, 35, 384, 201, 251, 319, 77, 356, 182, 252, 309, 35, 385, 201, 319, 252, 309, 35, 386, 201, 326, 4, 329, 35, 335, 201, 335, 85, 387, 251, 261, 85, 194, 85, 120, 182, 35, 189, 302, 65, 35, 189, 386, 130, 385, 107, 386, 1, 384, 65, 35, 388, 201, 244, 251, 251, 303, 114, 182, 114, 386, 114, 95, 201, 254, 182, 35, 389, 201, 385, 77, 79, 251, 357, 114, 309, 182, 35, 390, 201, 389, 237, 65, 114, 209, 30, 151, 388, 237, 209, 114, 65, 30, 35, 391, 201, 357, 35, 335, 201, 212, 251, 390, 114, 335, 114, 391, 85, 387, 251, 335, 85, 194, 85, 120, 182, 182, 35, 192, 35, 192, 35, 339, 201, 260, 77, 317, 252, 296, 252, 299, 77, 318, 252, 299, 77, 320, 35, 392, 201, 384, 4, 326, 35, 189, 392, 130, 357, 65, 35, 393, 201, 244, 251, 251, 309, 114, 182, 114, 309, 4, 392, 114, 95, 201, 254, 182, 35, 343, 201, 79, 251, 357, 114, 309, 182, 1, 393, 35, 10, 251, 339, 114, 367, 77, 117, 251, 369, 182, 114, 338, 201, 343, 182, 35, 192, 35, 33, 65, 35, 10, 251, 339, 114, 367, 77, 117, 251, 369, 182, 182, 35, 59, 35, 333, 201, 261, 77, 317, 252, 274, 77, 318, 252, 275, 77, 324, 252, 276, 35, 334, 201, 333, 77, 320, 237, 65, 114, 209, 30, 252, 276, 77, 322, 237, 209, 114, 65, 30, 252, 277, 35, 336, 201, 244, 251, 237, 309, 114, 303, 30, 114, 356, 114, 95, 201, 126, 182, 35, 189, 392, 130, 357, 65, 35, 336, 201, 336, 180, 251, 320, 237, 65, 114, 209, 30, 1, 326, 182, 35, 192, 35, 189, 347, 65, 35, 336, 201, 336, 180, 251, 322, 237, 209, 114, 65, 30, 1, 298, 182, 35, 192, 35, 10, 251, 334, 114, 335, 85, 387, 251, 261, 85, 95, 85, 120, 182, 114, 338, 201, 336, 182, 35, 192, 35, 192, 35, 189, 288, 65, 35, 189, 289, 65, 35, 316, 201, 290, 85, 96, 251, 356, 182, 35, 192, 35, 33, 65, 35, 316, 179, 312, 35, 59, 35, 192, 35, 33, 65, 35, 316, 201, 315, 35, 59, 35, 187, 35, 3, 35]}, {"code": "def kernel_paged_attention_2d(\n    output_ptr,\n    query_ptr,\n    key_cache_ptr,\n    value_cache_ptr,\n    block_tables_ptr,\n    seq_lens_ptr,\n    alibi_slopes_ptr,\n    scale,\n    k_scale,\n    v_scale,\n    num_query_heads: tl.constexpr,\n    num_queries_per_kv: tl.constexpr,\n    num_queries_per_kv_padded: tl.constexpr,\n    block_table_stride: tl.constexpr,\n    query_stride_0: tl.constexpr,\n    query_stride_1: tl.constexpr,\n    output_stride_0: tl.constexpr,\n    output_stride_1: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    HEAD_SIZE: tl.constexpr,\n    HEAD_SIZE_PADDED: tl.constexpr,\n    USE_ALIBI_SLOPES: tl.constexpr,\n    SLIDING_WINDOW: tl.constexpr,\n    x: tl.constexpr,\n    stride_k_cache_0: tl.constexpr,\n    stride_k_cache_1: tl.constexpr,\n    stride_k_cache_2: tl.constexpr,\n    stride_k_cache_3: tl.constexpr,\n    stride_k_cache_4: tl.constexpr,\n    stride_v_cache_0: tl.constexpr,\n    stride_v_cache_1: tl.constexpr,\n    stride_v_cache_2: tl.constexpr,\n    stride_v_cache_3: tl.constexpr,\n    filter_by_query_len: tl.constexpr,\n    query_start_len_ptr,\n):\n    seq_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n\n    if filter_by_query_len:\n        cur_batch_in_all_start_index = tl.load(query_start_len_ptr + seq_idx)\n        cur_batch_in_all_stop_index = tl.load(query_start_len_ptr + seq_idx + 1)\n        cur_batch_query_len = cur_batch_in_all_stop_index - cur_batch_in_all_start_index\n        if cur_batch_query_len > 1:\n            return\n    else:\n        cur_batch_in_all_start_index = seq_idx\n\n    query_head_idx = kv_head_idx * num_queries_per_kv + tl.arange(\n        0, num_queries_per_kv_padded\n    )\n\n    query_offset = (\n        cur_batch_in_all_start_index * query_stride_0\n        + query_head_idx[:, None] * query_stride_1\n    )\n\n    head_mask = query_head_idx < (kv_head_idx + 1) * num_queries_per_kv\n    head_mask = head_mask & (query_head_idx < num_query_heads)\n\n    dim_mask = tl.where(tl.arange(0, HEAD_SIZE_PADDED) < HEAD_SIZE, 1, 0).to(tl.int1)\n\n    Q = tl.load(\n        query_ptr + query_offset + tl.arange(0, HEAD_SIZE_PADDED)[None, :],\n        mask=dim_mask[None, :] & head_mask[:, None],\n        other=0.0,\n    )\n\n    block_table_offset = seq_idx * block_table_stride\n\n    M = tl.full([num_queries_per_kv_padded], float(\"-inf\"), dtype=tl.float32)\n    L = tl.full([num_queries_per_kv_padded], 1.0, dtype=tl.float32)\n    acc = tl.zeros([num_queries_per_kv_padded, HEAD_SIZE_PADDED], dtype=tl.float32)\n\n    seq_len = tl.load(seq_lens_ptr + seq_idx)\n\n    if USE_ALIBI_SLOPES:\n        alibi_slope = tl.load(\n            alibi_slopes_ptr + query_head_idx, mask=head_mask, other=0.0\n        )\n\n    num_blocks = cdiv_fn(seq_len, BLOCK_SIZE)\n\n    for j in range(0, num_blocks):\n\n        physical_block_idx = tl.load(block_tables_ptr + block_table_offset + j)\n\n        offs_n = tl.arange(0, BLOCK_SIZE)\n        offs_d = tl.arange(0, HEAD_SIZE_PADDED)\n\n        v_offset = (\n            physical_block_idx * stride_v_cache_0\n            + kv_head_idx * stride_v_cache_1\n            + offs_d[None, :] * stride_v_cache_2\n            + offs_n[:, None] * stride_v_cache_3\n        )\n\n        k_offset = (\n            physical_block_idx * stride_k_cache_0\n            + kv_head_idx * stride_k_cache_1\n            + (offs_d[:, None] // x) * stride_k_cache_2\n            + offs_n[None, :] * stride_k_cache_3\n            + (offs_d[:, None] % x) * stride_k_cache_4\n        )\n\n        K_load = tl.load(key_cache_ptr + k_offset, mask=dim_mask[:, None], other=0.0)\n\n        if K_load.dtype.is_fp8():\n            K = (K_load.to(tl.float32) * tl.load(k_scale)).to(Q.dtype)\n        else:\n            K = K_load\n\n        V_load = tl.load(value_cache_ptr + v_offset, mask=dim_mask[None, :], other=0.0)\n\n        if V_load.dtype.is_fp8():\n            V = (V_load.to(tl.float32) * tl.load(v_scale)).to(Q.dtype)\n        else:\n            V = V_load\n\n        seq_offset = j * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        boundary = tl.full([BLOCK_SIZE], seq_len, dtype=tl.int32)\n        seq_mask = seq_offset[None, :] < boundary\n\n        S = tl.where(head_mask[:, None] & seq_mask, 0.0, float(\"-inf\")).to(tl.float32)\n        S += scale * tl.dot(Q, K)\n\n        context_len = seq_len - 1\n\n        if SLIDING_WINDOW > 0:\n            S = tl.where((context_len - seq_offset) < SLIDING_WINDOW, S, -10000)\n\n        if USE_ALIBI_SLOPES:\n            S += alibi_slope[:, None] * (seq_offset - context_len)\n\n        m_j = tl.maximum(M, tl.max(S, axis=1))\n\n        P = tl.exp(S - m_j[:, None])\n\n        l_j = tl.sum(P, axis=1)\n\n        alpha = tl.exp(M - m_j)\n\n        acc = acc * alpha[:, None]\n\n        L = L * alpha + l_j\n        M = m_j\n\n        acc += tl.dot(P.to(V.dtype), V)\n\n    acc = acc / L[:, None]\n\n    output_offset = (\n        cur_batch_in_all_start_index * output_stride_0\n        + query_head_idx * output_stride_1\n    )\n\n    tl.store(\n        output_ptr + output_offset[:, None] + tl.arange(0, HEAD_SIZE_PADDED)[None, :],\n        acc,\n        mask=dim_mask[None, :] & head_mask[:, None],\n    )", "encoded": [32, 355, 251, 255, 114, 256, 114, 257, 114, 258, 114, 259, 114, 260, 114, 261, 114, 262, 114, 263, 114, 264, 114, 265, 66, 6, 114, 266, 66, 6, 114, 267, 66, 6, 114, 268, 66, 6, 114, 269, 66, 6, 114, 270, 66, 6, 114, 271, 66, 6, 114, 272, 66, 6, 114, 273, 66, 6, 114, 274, 66, 6, 114, 275, 66, 6, 114, 276, 66, 6, 114, 277, 66, 6, 114, 278, 66, 6, 114, 279, 66, 6, 114, 280, 66, 6, 114, 281, 66, 6, 114, 282, 66, 6, 114, 283, 66, 6, 114, 284, 66, 6, 114, 285, 66, 6, 114, 286, 66, 6, 114, 287, 66, 6, 114, 288, 66, 6, 114, 289, 182, 66, 35, -1, 290, 201, 178, 251, 356, 182, 35, 291, 201, 178, 251, 357, 182, 35, 189, 288, 66, 35, 292, 201, 60, 251, 289, 77, 290, 182, 35, 293, 201, 60, 251, 289, 77, 290, 77, 357, 182, 35, 294, 201, 293, 4, 292, 35, 189, 294, 130, 357, 66, 35, 238, 35, 192, 35, 192, 35, 33, 66, 35, 292, 201, 290, 35, 59, 35, 295, 201, 291, 252, 266, 77, 79, 251, 356, 114, 267, 182, 35, 296, 201, 292, 252, 269, 77, 295, 237, 66, 114, 209, 30, 252, 270, 35, 297, 201, 295, 1, 251, 291, 77, 357, 182, 252, 266, 35, 297, 201, 297, 180, 251, 295, 1, 265, 182, 35, 298, 201, 212, 251, 79, 251, 356, 114, 275, 182, 1, 274, 114, 357, 114, 356, 182, 85, 299, 251, 126, 182, 35, 300, 201, 60, 251, 256, 77, 296, 77, 79, 251, 356, 114, 275, 182, 237, 209, 114, 66, 30, 114, 301, 201, 298, 237, 209, 114, 66, 30, 180, 297, 237, 66, 114, 209, 30, 114, 302, 201, 356, 182, 35, 303, 201, 290, 252, 268, 35, 304, 201, 244, 251, 237, 267, 30, 114, 305, 251, 358, 182, 114, 95, 201, 152, 182, 35, 306, 201, 244, 251, 237, 267, 30, 114, 357, 114, 95, 201, 152, 182, 35, 307, 201, 183, 251, 237, 267, 114, 275, 30, 114, 95, 201, 152, 182, 35, 308, 201, 60, 251, 260, 77, 290, 182, 35, 189, 276, 66, 35, 309, 201, 60, 251, 261, 77, 295, 114, 301, 201, 297, 114, 302, 201, 356, 182, 35, 192, 35, 310, 201, 311, 251, 308, 114, 273, 182, 35, 142, 312, 164, 5, 251, 356, 114, 310, 182, 66, 35, 313, 201, 60, 251, 259, 77, 303, 77, 312, 182, 35, 314, 201, 79, 251, 356, 114, 273, 182, 35, 315, 201, 79, 251, 356, 114, 275, 182, 35, 316, 201, 313, 252, 284, 77, 291, 252, 285, 77, 315, 237, 209, 114, 66, 30, 252, 286, 77, 314, 237, 66, 114, 209, 30, 252, 287, 35, 317, 201, 313, 252, 279, 77, 291, 252, 280, 77, 315, 237, 66, 114, 209, 30, 51, 278, 252, 281, 77, 314, 237, 209, 114, 66, 30, 252, 282, 77, 315, 237, 66, 114, 209, 30, 233, 278, 252, 283, 35, 318, 201, 60, 251, 257, 77, 317, 114, 301, 201, 298, 237, 66, 114, 209, 30, 114, 302, 201, 356, 182, 35, 189, 318, 85, 95, 85, 319, 251, 182, 66, 35, 320, 201, 251, 318, 85, 299, 251, 152, 182, 252, 60, 251, 263, 182, 182, 85, 299, 251, 300, 85, 95, 182, 35, 192, 35, 33, 66, 35, 320, 201, 318, 35, 59, 35, 321, 201, 60, 251, 258, 77, 316, 114, 301, 201, 298, 237, 209, 114, 66, 30, 114, 302, 201, 356, 182, 35, 189, 321, 85, 95, 85, 319, 251, 182, 66, 35, 322, 201, 251, 321, 85, 299, 251, 152, 182, 252, 60, 251, 264, 182, 182, 85, 299, 251, 300, 85, 95, 182, 35, 192, 35, 33, 66, 35, 322, 201, 321, 35, 59, 35, 323, 201, 312, 252, 273, 77, 79, 251, 356, 114, 273, 182, 35, 324, 201, 244, 251, 237, 273, 30, 114, 308, 114, 95, 201, 65, 182, 35, 325, 201, 323, 237, 209, 114, 66, 30, 1, 324, 35, 326, 201, 212, 251, 297, 237, 66, 114, 209, 30, 180, 325, 114, 356, 114, 305, 251, 358, 182, 182, 85, 299, 251, 152, 182, 35, 326, 179, 262, 252, 15, 251, 300, 114, 320, 182, 35, 327, 201, 308, 4, 357, 35, 189, 277, 130, 356, 66, 35, 326, 201, 212, 251, 327, 4, 323, 1, 277, 114, 326, 114, 4, 357, 356, 356, 356, 356, 182, 35, 192, 35, 189, 276, 66, 35, 326, 179, 309, 237, 66, 114, 209, 30, 252, 251, 323, 4, 327, 182, 35, 192, 35, 328, 201, 199, 251, 304, 114, 12, 251, 326, 114, 329, 201, 357, 182, 182, 35, 330, 201, 113, 251, 326, 4, 328, 237, 66, 114, 209, 30, 182, 35, 331, 201, 228, 251, 330, 114, 329, 201, 357, 182, 35, 332, 201, 113, 251, 304, 4, 328, 182, 35, 307, 201, 307, 252, 332, 237, 66, 114, 209, 30, 35, 306, 201, 306, 252, 332, 77, 331, 35, 304, 201, 328, 35, 307, 179, 15, 251, 330, 85, 299, 251, 322, 85, 95, 182, 114, 322, 182, 35, 81, 35, 307, 201, 307, 45, 306, 237, 66, 114, 209, 30, 35, 333, 201, 292, 252, 271, 77, 295, 252, 272, 35, 10, 251, 255, 77, 333, 237, 66, 114, 209, 30, 77, 79, 251, 356, 114, 275, 182, 237, 209, 114, 66, 30, 114, 307, 114, 301, 201, 298, 237, 209, 114, 66, 30, 180, 297, 237, 66, 114, 209, 30, 182, 35, 3, 35]}, {"code": "def kernel_paged_attention_3d(\n    segm_output_ptr,\n    segm_max_ptr,\n    segm_expsum_ptr,\n    query_ptr,\n    key_cache_ptr,\n    value_cache_ptr,\n    block_tables_ptr,\n    seq_lens_ptr,\n    alibi_slopes_ptr,\n    scale,\n    k_scale,\n    v_scale,\n    num_query_heads: tl.constexpr,\n    num_queries_per_kv: tl.constexpr,\n    num_queries_per_kv_padded: tl.constexpr,\n    block_table_stride: tl.constexpr,\n    query_stride_0: tl.constexpr,\n    query_stride_1: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    HEAD_SIZE: tl.constexpr,\n    HEAD_SIZE_PADDED: tl.constexpr,\n    USE_ALIBI_SLOPES: tl.constexpr,\n    SLIDING_WINDOW: tl.constexpr,\n    x: tl.constexpr,\n    stride_k_cache_0: tl.constexpr,\n    stride_k_cache_1: tl.constexpr,\n    stride_k_cache_2: tl.constexpr,\n    stride_k_cache_3: tl.constexpr,\n    stride_k_cache_4: tl.constexpr,\n    stride_v_cache_0: tl.constexpr,\n    stride_v_cache_1: tl.constexpr,\n    stride_v_cache_2: tl.constexpr,\n    stride_v_cache_3: tl.constexpr,\n    filter_by_query_len: tl.constexpr,\n    query_start_len_ptr,\n    MAX_NUM_SEGMENTS_PER_SEQ: tl.constexpr,\n):\n    seq_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n    segm_idx = tl.program_id(2)\n\n    if filter_by_query_len:\n        cur_batch_in_all_start_index = tl.load(query_start_len_ptr + seq_idx)\n        cur_batch_in_all_stop_index = tl.load(query_start_len_ptr + seq_idx + 1)\n        cur_batch_query_len = cur_batch_in_all_stop_index - cur_batch_in_all_start_index\n        if cur_batch_query_len > 1:\n            return\n    else:\n        cur_batch_in_all_start_index = seq_idx\n\n    seq_len = tl.load(seq_lens_ptr + seq_idx)\n\n    blocks_per_segment = cdiv_fn(seq_len, MAX_NUM_SEGMENTS_PER_SEQ * BLOCK_SIZE)\n    if segm_idx * blocks_per_segment * BLOCK_SIZE >= seq_len:\n        return\n\n    query_head_idx = kv_head_idx * num_queries_per_kv + tl.arange(\n        0, num_queries_per_kv_padded\n    )\n\n    query_offset = (\n        cur_batch_in_all_start_index * query_stride_0\n        + query_head_idx[:, None] * query_stride_1\n    )\n\n    head_mask = query_head_idx < (kv_head_idx + 1) * num_queries_per_kv\n    head_mask = head_mask & (query_head_idx < num_query_heads)\n\n    dim_mask = tl.where(tl.arange(0, HEAD_SIZE_PADDED) < HEAD_SIZE, 1, 0).to(tl.int1)\n\n    Q = tl.load(\n        query_ptr + query_offset + tl.arange(0, HEAD_SIZE_PADDED)[None, :],\n        mask=dim_mask[None, :] & head_mask[:, None],\n        other=0.0,\n    )\n\n    block_table_offset = seq_idx * block_table_stride\n\n    M = tl.full([num_queries_per_kv_padded], float(\"-inf\"), dtype=tl.float32)\n    L = tl.full([num_queries_per_kv_padded], 1.0, dtype=tl.float32)\n    acc = tl.zeros([num_queries_per_kv_padded, HEAD_SIZE_PADDED], dtype=tl.float32)\n\n    if USE_ALIBI_SLOPES:\n        alibi_slope = tl.load(\n            alibi_slopes_ptr + query_head_idx, mask=head_mask, other=0.0\n        )\n\n    num_blocks = cdiv_fn(seq_len, BLOCK_SIZE)\n\n    for j in range(\n        segm_idx * blocks_per_segment,\n        min((segm_idx + 1) * blocks_per_segment, num_blocks),\n    ):\n        physical_block_idx = tl.load(block_tables_ptr + block_table_offset + j)\n\n        offs_n = tl.arange(0, BLOCK_SIZE)\n        offs_d = tl.arange(0, HEAD_SIZE_PADDED)\n\n        v_offset = (\n            physical_block_idx * stride_v_cache_0\n            + kv_head_idx * stride_v_cache_1\n            + offs_d[None, :] * stride_v_cache_2\n            + offs_n[:, None] * stride_v_cache_3\n        )\n\n        k_offset = (\n            physical_block_idx * stride_k_cache_0\n            + kv_head_idx * stride_k_cache_1\n            + (offs_d[:, None] // x) * stride_k_cache_2\n            + offs_n[None, :] * stride_k_cache_3\n            + (offs_d[:, None] % x) * stride_k_cache_4\n        )\n\n        K_load = tl.load(key_cache_ptr + k_offset, mask=dim_mask[:, None], other=0.0)\n\n        if K_load.dtype.is_fp8():\n            K = (K_load.to(tl.float32) * tl.load(k_scale)).to(Q.dtype)\n        else:\n            K = K_load\n\n        V_load = tl.load(value_cache_ptr + v_offset, mask=dim_mask[None, :], other=0.0)\n\n        if V_load.dtype.is_fp8():\n            V = (V_load.to(tl.float32) * tl.load(v_scale)).to(Q.dtype)\n        else:\n            V = V_load\n\n        seq_offset = j * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        boundary = tl.full([BLOCK_SIZE], seq_len, dtype=tl.int32)\n        seq_mask = seq_offset[None, :] < boundary\n\n        S = tl.where(head_mask[:, None] & seq_mask, 0.0, float(\"-inf\")).to(tl.float32)\n        S += scale * tl.dot(Q, K)\n\n        context_len = seq_len - 1\n\n        if SLIDING_WINDOW > 0:\n            S = tl.where((context_len - seq_offset) < SLIDING_WINDOW, S, -10000)\n\n        if USE_ALIBI_SLOPES:\n            S += alibi_slope[:, None] * (seq_offset - context_len)\n\n        m_j = tl.maximum(M, tl.max(S, axis=1))\n\n        P = tl.exp(S - m_j[:, None])\n\n        l_j = tl.sum(P, axis=1)\n\n        alpha = tl.exp(M - m_j)\n\n        acc = acc * alpha[:, None]\n\n        L = L * alpha + l_j\n        M = m_j\n\n        acc += tl.dot(P.to(V.dtype), V)\n\n    segm_output_offset = (\n        seq_idx * (num_query_heads * MAX_NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED)\n        + query_head_idx[:, None] * (MAX_NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED)\n        + segm_idx * HEAD_SIZE_PADDED\n        + tl.arange(0, HEAD_SIZE_PADDED)[None, :]\n    )\n    tl.store(\n        segm_output_ptr + segm_output_offset,\n        acc,\n        mask=dim_mask[None, :] & head_mask[:, None],\n    )\n\n    segm_offset = (\n        seq_idx * (num_query_heads * MAX_NUM_SEGMENTS_PER_SEQ)\n        + query_head_idx * MAX_NUM_SEGMENTS_PER_SEQ\n        + segm_idx\n    )\n    tl.store(segm_max_ptr + segm_offset, M, mask=head_mask)\n    tl.store(segm_expsum_ptr + segm_offset, L, mask=head_mask)", "encoded": [32, 355, 251, 255, 114, 256, 114, 257, 114, 258, 114, 259, 114, 260, 114, 261, 114, 262, 114, 263, 114, 264, 114, 265, 114, 266, 114, 267, 65, 6, 114, 268, 65, 6, 114, 269, 65, 6, 114, 270, 65, 6, 114, 271, 65, 6, 114, 272, 65, 6, 114, 273, 65, 6, 114, 274, 65, 6, 114, 275, 65, 6, 114, 276, 65, 6, 114, 277, 65, 6, 114, 278, 65, 6, 114, 279, 65, 6, 114, 280, 65, 6, 114, 281, 65, 6, 114, 282, 65, 6, 114, 283, 65, 6, 114, 284, 65, 6, 114, 285, 65, 6, 114, 286, 65, 6, 114, 287, 65, 6, 114, 288, 65, 6, 114, 289, 114, 290, 65, 6, 182, 65, 35, -1, 291, 201, 178, 251, 356, 182, 35, 292, 201, 178, 251, 357, 182, 35, 293, 201, 178, 251, 358, 182, 35, 189, 288, 65, 35, 294, 201, 58, 251, 289, 77, 291, 182, 35, 295, 201, 58, 251, 289, 77, 291, 77, 357, 182, 35, 296, 201, 295, 4, 294, 35, 189, 296, 130, 357, 65, 35, 238, 35, 192, 35, 192, 35, 33, 65, 35, 294, 201, 291, 35, 59, 35, 297, 201, 58, 251, 262, 77, 291, 182, 35, 298, 201, 299, 251, 297, 114, 290, 252, 273, 182, 35, 189, 293, 252, 298, 252, 273, 151, 297, 65, 35, 238, 35, 192, 35, 300, 201, 292, 252, 268, 77, 79, 251, 356, 114, 269, 182, 35, 301, 201, 294, 252, 271, 77, 300, 237, 65, 114, 209, 30, 252, 272, 35, 302, 201, 300, 1, 251, 292, 77, 357, 182, 252, 268, 35, 302, 201, 302, 180, 251, 300, 1, 267, 182, 35, 303, 201, 212, 251, 79, 251, 356, 114, 275, 182, 1, 274, 114, 357, 114, 356, 182, 85, 304, 251, 126, 182, 35, 305, 201, 58, 251, 258, 77, 301, 77, 79, 251, 356, 114, 275, 182, 237, 209, 114, 65, 30, 114, 306, 201, 303, 237, 209, 114, 65, 30, 180, 302, 237, 65, 114, 209, 30, 114, 307, 201, 356, 182, 35, 308, 201, 291, 252, 270, 35, 309, 201, 244, 251, 237, 269, 30, 114, 310, 251, 359, 182, 114, 95, 201, 152, 182, 35, 311, 201, 244, 251, 237, 269, 30, 114, 357, 114, 95, 201, 152, 182, 35, 312, 201, 183, 251, 237, 269, 114, 275, 30, 114, 95, 201, 152, 182, 35, 189, 276, 65, 35, 313, 201, 58, 251, 263, 77, 300, 114, 306, 201, 302, 114, 307, 201, 356, 182, 35, 192, 35, 314, 201, 299, 251, 297, 114, 273, 182, 35, 142, 315, 164, 5, 251, 293, 252, 298, 114, 42, 251, 251, 293, 77, 357, 182, 252, 298, 114, 314, 182, 182, 65, 35, 316, 201, 58, 251, 261, 77, 308, 77, 315, 182, 35, 317, 201, 79, 251, 356, 114, 273, 182, 35, 318, 201, 79, 251, 356, 114, 275, 182, 35, 319, 201, 316, 252, 284, 77, 292, 252, 285, 77, 318, 237, 209, 114, 65, 30, 252, 286, 77, 317, 237, 65, 114, 209, 30, 252, 287, 35, 320, 201, 316, 252, 279, 77, 292, 252, 280, 77, 318, 237, 65, 114, 209, 30, 51, 278, 252, 281, 77, 317, 237, 209, 114, 65, 30, 252, 282, 77, 318, 237, 65, 114, 209, 30, 233, 278, 252, 283, 35, 321, 201, 58, 251, 259, 77, 320, 114, 306, 201, 303, 237, 65, 114, 209, 30, 114, 307, 201, 356, 182, 35, 189, 321, 85, 95, 85, 322, 251, 182, 65, 35, 323, 201, 251, 321, 85, 304, 251, 152, 182, 252, 58, 251, 265, 182, 182, 85, 304, 251, 305, 85, 95, 182, 35, 192, 35, 33, 65, 35, 323, 201, 321, 35, 59, 35, 324, 201, 58, 251, 260, 77, 319, 114, 306, 201, 303, 237, 209, 114, 65, 30, 114, 307, 201, 356, 182, 35, 189, 324, 85, 95, 85, 322, 251, 182, 65, 35, 325, 201, 251, 324, 85, 304, 251, 152, 182, 252, 58, 251, 266, 182, 182, 85, 304, 251, 305, 85, 95, 182, 35, 192, 35, 33, 65, 35, 325, 201, 324, 35, 59, 35, 326, 201, 315, 252, 273, 77, 79, 251, 356, 114, 273, 182, 35, 327, 201, 244, 251, 237, 273, 30, 114, 297, 114, 95, 201, 254, 182, 35, 328, 201, 326, 237, 209, 114, 65, 30, 1, 327, 35, 329, 201, 212, 251, 302, 237, 65, 114, 209, 30, 180, 328, 114, 356, 114, 310, 251, 359, 182, 182, 85, 304, 251, 152, 182, 35, 329, 179, 264, 252, 15, 251, 305, 114, 323, 182, 35, 330, 201, 297, 4, 357, 35, 189, 277, 130, 356, 65, 35, 329, 201, 212, 251, 330, 4, 326, 1, 277, 114, 329, 114, 4, 357, 356, 356, 356, 356, 182, 35, 192, 35, 189, 276, 65, 35, 329, 179, 313, 237, 65, 114, 209, 30, 252, 251, 326, 4, 330, 182, 35, 192, 35, 331, 201, 199, 251, 309, 114, 12, 251, 329, 114, 332, 201, 357, 182, 182, 35, 333, 201, 113, 251, 329, 4, 331, 237, 65, 114, 209, 30, 182, 35, 334, 201, 228, 251, 333, 114, 332, 201, 357, 182, 35, 335, 201, 113, 251, 309, 4, 331, 182, 35, 312, 201, 312, 252, 335, 237, 65, 114, 209, 30, 35, 311, 201, 311, 252, 335, 77, 334, 35, 309, 201, 331, 35, 312, 179, 15, 251, 333, 85, 304, 251, 325, 85, 95, 182, 114, 325, 182, 35, 81, 35, 336, 201, 291, 252, 251, 267, 252, 290, 252, 275, 182, 77, 300, 237, 65, 114, 209, 30, 252, 251, 290, 252, 275, 182, 77, 293, 252, 275, 77, 79, 251, 356, 114, 275, 182, 237, 209, 114, 65, 30, 35, 10, 251, 255, 77, 336, 114, 312, 114, 306, 201, 303, 237, 209, 114, 65, 30, 180, 302, 237, 65, 114, 209, 30, 182, 35, 337, 201, 291, 252, 251, 267, 252, 290, 182, 77, 300, 252, 290, 77, 293, 35, 10, 251, 256, 77, 337, 114, 309, 114, 306, 201, 302, 182, 35, 10, 251, 257, 77, 337, 114, 311, 114, 306, 201, 302, 182, 35, 3, 35]}], "mapping": {"LT": 1, "MINUSEQ": 2, "ENDDEF": 3, "MINUS": 4, "RANGE": 5, "TL_CONSTEXPR": 6, "POW": 7, "DIVEQ": 8, "NDIM": 9, "TL_STORE": 10, "LOG": 11, "TL_MAX": 12, "STR": 13, "OUTPUT": 14, "TL_DOT": 15, "TL_EXTRA_CUDA_LIBDEVICE_LLRINT": 16, "TL_PHILOX": 17, "LIBDEVICE": 18, "TL_INT16": 19, "ISINSTANCE": 20, "SHFL_DOWN_SYNC_I32": 21, "TRITON": 22, "TL_FLOAT16": 23, "BOREQ": 24, "TL_RESHAPE": 25, "TIMESEQ": 26, "ABS": 27, "FLOAT64_STR": 28, "TL_MATH_POW": 29, "RBRACKET": 30, "COMPLEX64_STR": 31, "DEF": 32, "ELSE": 33, "TL_CEIL": 34, "ENTER": 35, "TRY": 36, "DIVMOD": 37, "PRINT": 38, "BOOL_STR": 39, "ELIF": 40, "TILDEEQ": 41, "MIN": 42, "DICT": 43, "PIN_MEMORY": 44, "DIVIDE": 45, "TL_MATH_MAX": 46, "SYNCTHREADS": 47, "EXCEPT": 48, "SHFL_UP_SYNC_I32": 49, "HPU_STR": 50, "DDVIDE": 51, "MAX": 52, "TL_LOG": 53, "TL_DEBUG_BARRIER": 54, "ROUND": 55, "TL_JOIN": 56, "FLOAT16_STR": 57, "TL_LOAD": 58, "ENDELSE": 59, "TL_MULTIPLE_OF": 60, "WHILE": 61, "FALSE": 62, "CONTINUE": 63, "TL_MATH_RSQRT": 64, "COLON": 65, "NOT": 66, "TL_CDIV": 67, "ENDIF": 68, "TL_MINIMUM": 69, "TILDE": 70, "TL_RAND": 71, "TL_SWIZZLE2D": 72, "GRAD_FN": 73, "RBRACE": 74, "TL_TRANS": 75, "ENDEXCEPT": 76, "PLUS": 77, "TL_ATOMIC_XCHG": 78, "TL_ARANGE": 79, "EQ": 80, "ENDFOR": 81, "BFLOAT16_STR": 82, "MEMORY_FORMAT": 83, "REQUIRES_GRAD": 84, "DOT": 85, "TL_STATIC_ASSERT": 86, "TL_FDIV": 87, "TL_EXTRA_CUDA_LIBDEVICE_POW": 88, "TL_CUMSUM": 89, "STRING": 90, "LD": 91, "ELEMENT_TX": 92, "COMPLEX128_STR": 93, "RSHIFTEQ": 94, "DTYPE": 95, "ATOMIC_ADD": 96, "ATOMIC_CAS": 97, "LBRACE": 98, "DEVICE": 99, "TL_MATH_LLRINT": 100, "BANDEQ": 101, "TL_TENSOR": 102, "ENDWITH": 103, "NON_BLOCKING": 104, "TL_ABS": 105, "EQEQ": 106, "AND": 107, "GRAD": 108, "TL_LOG2": 109, "SUM": 110, "BXOREQ": 111, "TID": 112, "TL_EXP": 113, "COMMA": 114, "INT8_STR": 115, "IS": 116, "TL_MATH_LOG2": 117, "XPU_STR": 118, "SQUEEZE": 119, "ELEMENT_TY": 120, "GPU_STR": 121, "TL_VIEW": 122, "BREAK": 123, "TL_RSQRT": 124, "ID": 125, "TL_INT1": 126, "LIST": 127, "KEEP_DIM": 128, "SHAPE": 129, "GT": 130, "UINT8_STR": 131, "TRITON_HELPERS": 132, "RSHIFT": 133, "TL_MATH_FAST_EXPF": 134, "TL_MAKE_TENSOR_DESCRIPTOR": 135, "TL_SQRT": 136, "DDVIDEEQ": 137, "SET": 138, "ENDTRY": 139, "TL_MIN": 140, "EXCLAMATION": 141, "FOR": 142, "LSHIFT": 143, "MPS_STR": 144, "TL_NUM_PROGRAMS": 145, "TL_FLIP": 146, "TL_INT8": 147, "OR": 148, "BALLOT_SYNC": 149, "TL_ADVANCE": 150, "GTEQ": 151, "TL_FLOAT32": 152, "TL_MATH_EXP": 153, "SHFL_SYNC_I32": 154, "CEIL_DIV": 155, "TL__EXPERIMENTAL_DESCRIPTOR_LOAD": 156, "TL_ATOMIC_CAS": 157, "FLOAT": 158, "BOR": 159, "MTIA_STR": 160, "INT": 161, "TL_MATH_EXP2": 162, "TL_EXTRA_CUDA_LIBDEVICE_ROUND": 163, "IN": 164, "FP_UPCAST_ROUNDING": 165, "INT32_STR": 166, "TRUE": 167, "TL_MATH_SQRT": 168, "TL_ASSUME": 169, "TL__EXPERIMENTAL_DESCRIPTOR_STORE": 170, "IPU_STR": 171, "TL_UINT32": 172, "ALL": 173, "TORCH_FLOAT8_E5M2": 174, "FP_DOWNCAST_ROUNDING": 175, "LANGUAGE": 176, "DIM": 177, "TL_PROGRAM_ID": 178, "PLUSEQ": 179, "BAND": 180, "NUMEL": 181, "RPAREN": 182, "TL_ZEROS": 183, "YIELD": 184, "MODEQ": 185, "TL_DEVICE_ASSERT": 186, "ENDWHILE": 187, "TL_INT64": 188, "IF": 189, "TL_STANDARD__LOG2": 190, "LSHIFTEQ": 191, "ENDELIF": 192, "NEQ": 193, "TYPE": 194, "INT16_STR": 195, "TL_STATIC_PRINT": 196, "TL_ARGMAX": 197, "BXOR": 198, "TL_MAXIMUM": 199, "TL_BROADCAST_TO": 200, "ASSIGN": 201, "ATOMIC_XCHG": 202, "TL_ATOMIC_ADD": 203, "CUDA_STR": 204, "FLOAT32_STR": 205, "TL_REDUCE": 206, "UNSQUEEZE": 207, "TL_FLOAT8E5": 208, "NONE": 209, "LEN": 210, "ATOMIC_MIN": 211, "TL_WHERE": 212, "TL_SIGMOID": 213, "EXP": 214, "CPU_STR": 215, "TL_MATH_LOG": 216, "TL_STATIC_RANGE": 217, "TL_MAX_CONTIGUOUS": 218, "TL_EXP2": 219, "STRIDE": 220, "FFS": 221, "SIZE": 222, "ANY": 223, "TUPLE": 224, "LTEQ": 225, "TL_MAKE_BLOCK_PTR": 226, "TL_RANGE": 227, "TL_SUM": 228, "TL_FLOAT8E4NV": 229, "TL_CAST": 230, "POWEQ": 231, "TL_POINTER_TYPE": 232, "MOD": 233, "LAYOUT": 234, "DL": 235, "TL_FMA": 236, "LBRACKET": 237, "RETURN": 238, "ST": 239, "TL_BFLOAT16": 240, "ZIP": 241, "LIBSHMEM_DEVICE": 242, "WITH": 243, "TL_FULL": 244, "FINALLY": 245, "PASS": 246, "SLICE": 247, "ENDFINALLY": 248, "TL_MATH_MIN": 249, "TL_FLOOR": 250, "LPAREN": 251, "TIMES": 252, "INT64_STR": 253, "TL_INT32": 254, "v0": 255, "v1": 256, "v2": 257, "v3": 258, "v4": 259, "v5": 260, "v6": 261, "v7": 262, "v8": 263, "v9": 264, "v10": 265, "v11": 266, "v12": 267, "v13": 268, "v14": 269, "v15": 270, "v16": 271, "v17": 272, "v18": 273, "v19": 274, "v20": 275, "v21": 276, "v22": 277, "v23": 278, "v24": 279, "v25": 280, "v26": 281, "v27": 282, "v28": 283, "v29": 284, "v30": 285, "v31": 286, "v32": 287, "v33": 288, "v34": 289, "v35": 290, "v36": 291, "v37": 292, "v38": 293, "v39": 294, "v40": 295, "v41": 296, "v42": 297, "v43": 298, "v44": 299, "v45": 300, "v46": 301, "v47": 302, "v48": 303, "v49": 304, "v50": 305, "v51": 306, "v52": 307, "v53": 308, "v54": 309, "v55": 310, "v56": 311, "v57": 312, "v58": 313, "v59": 314, "v60": 315, "v61": 316, "v62": 317, "v63": 318, "v64": 319, "v65": 320, "v66": 321, "v67": 322, "v68": 323, "v69": 324, "v70": 325, "v71": 326, "v72": 327, "v73": 328, "v74": 329, "v75": 330, "v76": 331, "v77": 332, "v78": 333, "v79": 334, "v80": 335, "v81": 336, "v82": 337, "v83": 338, "v84": 339, "v85": 340, "v86": 341, "v87": 342, "v88": 343, "v89": 344, "v90": 345, "v91": 346, "v92": 347, "v93": 348, "v94": 349, "v95": 350, "v96": 351, "v97": 352, "v98": 353, "v99": 354, "f0": 355, "0": 356, "1": 357, "2": 358, "'-inf'": 359}}