import torch
import torch.distributed
import triton
import triton.language as tl
from triton_dist.utils import (
    perf_func,
    get_torch_prof_ctx,
)
from functools import partial

from triton_dist import pynvshmem

import argparse
import random
import os
import datetime
import numpy as np

from triton_dist.layers.nvidia import EPAll2AllLayer

EP_GROUP = None
RANK = int(os.environ.get("RANK", 0))
LOCAL_RANK = int(os.environ.get("LOCAL_RANK", 0))
WORLD_SIZE = int(os.environ.get("WORLD_SIZE", 1))
LOCAL_WORLD_SIZE = int(os.environ.get("LOCAL_WORLD_SIZE", 1))


def _check(out: torch.Tensor, ref: torch.Tensor, msg: str = "Triton"):
    try:
        torch.testing.assert_close(out, ref, rtol=0, atol=0)
        print(f"✅ RANK[{RANK}] check {msg} passed")
    except Exception as e:
        print(f"❌ RANK[{RANK}] check {msg} failed")
        raise e


def generate_random_exp_indices(token_num, total_num_experts, topk):
    exp_indices = []
    exp_list = list(range(total_num_experts))
    for tid in range(token_num):
        top_selected = random.sample(exp_list, topk)
        exp_indices.append(top_selected)
    return torch.Tensor(exp_indices).int()


def splits_to_cumsum(splits: torch.Tensor):
    out = torch.zeros(splits.shape[0] + 1, dtype=splits.dtype, device=splits.device)

    _ = torch.cumsum(splits, 0, out=out[1:])
    return out


def sort_by_vectors(x):
    M, K = x.shape
    current_order = torch.arange(M, device=x.device)
    for k in reversed(range(K)):
        current_col = x[current_order, k]
        _, sorted_indices = torch.sort(current_col, stable=True)
        current_order = current_order[sorted_indices]
    sorted_x = x[current_order]
    return sorted_x


def calc_gather_index(
    scatter_index: torch.Tensor,
    row_start: int,
    row_end: int,
    BLOCK_SIZE: int = 1024,
):

    @triton.jit
    def _kernel(
        scatter_index: torch.Tensor,
        gather_index: torch.Tensor,
        topk_index: torch.Tensor,
        ntokens: int,
        topk: int,
        row_start: int,
        row_end: int,
        BLOCK_SIZE: tl.constexpr,
    ):
        pid = tl.program_id(axis=0)
        offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
        mask = offset < ntokens * topk
        scatter_idx = tl.load(scatter_index + offset, mask=mask, other=-1)
        token_idx = offset // topk
        topk_idx = offset % topk
        token_idx_mask = (scatter_idx >= row_start) & (scatter_idx < row_end)
        tl.store(gather_index + scatter_idx - row_start, token_idx, mask=token_idx_mask)
        tl.store(topk_index + scatter_idx - row_start, topk_idx, mask=token_idx_mask)

    ntokens, topk = scatter_index.shape
    gather_index = torch.zeros(
        row_end - row_start, dtype=torch.int32, device=scatter_index.device
    )
    topk_index = torch.zeros(
        row_end - row_start, dtype=torch.int32, device=scatter_index.device
    )
    grid = lambda META: (triton.cdiv(ntokens * topk, META["BLOCK_SIZE"]),)
    _kernel[grid](
        scatter_index,
        gather_index,
        topk_index,
        ntokens,
        topk,
        row_start,
        row_end,
        BLOCK_SIZE=BLOCK_SIZE,
        num_warps=BLOCK_SIZE // 32,
    )
    return gather_index, topk_index


def calc_scatter_index_stable(choosed_experts: torch.Tensor):
    return (
        choosed_experts.flatten()
        .argsort(stable=True)
        .argsort()
        .int()
        .view(choosed_experts.shape)
    )


DTYPE_MAP = {
    "bfloat16": torch.bfloat16,
    "float16": torch.float16,
    "float8_e4m3fn": torch.float8_e4m3fn,
    "float8_e5m2": torch.float8_e5m2,
    "s8": torch.int8,
    "s32": torch.int32,
    "float32": torch.float32,
}


def init_seed(seed=0):
    os.environ["NCCL_DEBUG"] = os.getenv("NCCL_DEBUG", "ERROR")
    os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":16:8"
    torch.use_deterministic_algorithms(False, warn_only=True)
    torch.set_printoptions(precision=5, profile="full")
    torch.manual_seed(3 + seed)
    torch.cuda.manual_seed_all(3 + seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.backends.cuda.matmul.allow_tf32 = False
    torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False
    torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False
    np.random.seed(3 + seed)
    random.seed(3 + seed)


def initialize_distributed():
    global EP_GROUP
    assert EP_GROUP is None, "EP_GROUP has already been initialized"

    torch.cuda.set_device(LOCAL_RANK)
    torch.distributed.init_process_group(
        backend="nccl",
        world_size=WORLD_SIZE,
        rank=RANK,
        timeout=datetime.timedelta(seconds=1800),
    )
    assert torch.distributed.is_initialized()

    EP_GROUP = torch.distributed.new_group(
        ranks=list(range(WORLD_SIZE)), backend="nccl"
    )
    torch.distributed.barrier(EP_GROUP)

    init_seed(seed=RANK)

    torch.cuda.synchronize()
    pynvshmem.init_nvshmem_by_uniqueid(EP_GROUP)
    return EP_GROUP


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-M", type=int, default=4096)
    parser.add_argument("-N", type=int, default=7168)
    parser.add_argument("-G", type=int, default=256)
    parser.add_argument("--topk", type=int, default=8)
    parser.add_argument("--iters", default=3, type=int, help="perf iterations")
    parser.add_argument("--verify-iters", default=5, type=int)
    parser.add_argument("--bench_iters", default=1, type=int, help="perf iterations")
    parser.add_argument("--rounds", default=1, type=int, help="random data round")
    parser.add_argument("--sm_margin", default=16, type=int, help="sm margin")
    parser.add_argument(
        "--dtype", default="bfloat16", help="data type", choices=list(DTYPE_MAP.keys())
    )
    parser.add_argument("--profile", action="store_true")
    parser.add_argument("--check", action="store_true")
    parser.add_argument("--with_scale", action="store_true")
    return parser.parse_args()


def perf_torch(args, input, scale_tensor, exp_indices):

    token_num, _ = input.shape
    splits_gpu_cur_rank = torch.bincount(exp_indices.view(-1), minlength=args.G).to(
        torch.int32
    )
    splits_cpu_cur_rank = splits_gpu_cur_rank.cpu()

    scatter_idx_cur_rank = calc_scatter_index_stable(exp_indices)

    gather_idx_cur_rank, _ = calc_gather_index(
        scatter_idx_cur_rank, 0, token_num * args.topk
    )

    scattered_input = torch.empty(
        input.size(0) * args.topk, input.size(1), dtype=input.dtype, device=input.device
    )
    scattered_scale_tensor = torch.empty(
        (scale_tensor.size(0) * args.topk),
        dtype=scale_tensor.dtype,
        device=scale_tensor.device,
    )
    scattered_scale_tensor.copy_(
        torch.index_select(scale_tensor, dim=0, index=gather_idx_cur_rank)
    )
    scattered_input.copy_(torch.index_select(input, dim=0, index=gather_idx_cur_rank))

    a2a_splits = torch.empty_like(splits_gpu_cur_rank)
    torch.distributed.all_to_all_single(a2a_splits, splits_gpu_cur_rank, group=EP_GROUP)
    a2a_splits_cpu = a2a_splits.cpu()
    ep_size = EP_GROUP.size()
    a2a_dispatch_output = torch.empty(
        [a2a_splits_cpu.sum(), input.size(1)], dtype=input.dtype, device=input.device
    )
    a2a_dispatch_scale = torch.empty(
        [a2a_splits_cpu.sum()], dtype=scale_tensor.dtype, device=scale_tensor.device
    )
    torch.cuda.synchronize()

    def fwd():
        torch.distributed.all_to_all_single(
            output=a2a_dispatch_output,
            input=scattered_input,
            output_split_sizes=a2a_splits_cpu.reshape(ep_size, -1).sum(dim=-1).tolist(),
            input_split_sizes=splits_cpu_cur_rank.reshape(ep_size, -1).sum(-1).tolist(),
            group=EP_GROUP,
        )
        if args.with_scale:
            torch.distributed.all_to_all_single(
                output=a2a_dispatch_scale,
                input=scattered_scale_tensor,
                output_split_sizes=a2a_splits_cpu.reshape(ep_size, -1)
                .sum(dim=-1)
                .tolist(),
                input_split_sizes=splits_cpu_cur_rank.reshape(ep_size, -1)
                .sum(-1)
                .tolist(),
                group=EP_GROUP,
            )

    for _ in range(10):
        fwd()
    torch.cuda.synchronize()

    st = torch.cuda.Event(enable_timing=True)
    ed = torch.cuda.Event(enable_timing=True)

    st.record()
    for _ in range(args.bench_iters):
        fwd()
    ed.record()
    torch.cuda.synchronize()
    avg_time = st.elapsed_time(ed) / args.bench_iters
    return a2a_dispatch_output, a2a_dispatch_scale, avg_time


if __name__ == "__main__":
    args = parse_args()
    EP_GROUP = initialize_distributed()
    assert (
        args.G % WORLD_SIZE == 0
    ), f"args.G:{args.G} should be divisible by WORLD_SIZE:{WORLD_SIZE}"

    experts_per_rank = args.G // WORLD_SIZE
    input_dtype = DTYPE_MAP[args.dtype]
    triton_a2a_op = EPAll2AllLayer(
        EP_GROUP,
        args.M,
        args.N,
        args.topk,
        RANK,
        args.G,
        LOCAL_WORLD_SIZE,
        WORLD_SIZE,
        input_dtype,
    )

    def _make_data(token_num):
        exp_indices = generate_random_exp_indices(token_num, args.G, args.topk)
        assert exp_indices.size(0) == token_num and exp_indices.size(1) == args.topk
        exp_indices = exp_indices.to("cuda")
        input = (
            torch.rand(token_num, args.N, dtype=torch.float32)
            .to(DTYPE_MAP[args.dtype])
            .to("cuda")
        )
        scale_tensor = torch.rand(token_num, dtype=torch.float32).to("cuda")
        return input, scale_tensor, exp_indices

    if args.check:
        for n in range(args.iters):
            torch.cuda.empty_cache()

            input_list = [
                _make_data(random.randint(args.M // 2, args.M))
                for _ in range(args.verify_iters)
            ]
            combine_out_list, dispatch_out_list, torch_input_list, torch_out_list = (
                [],
                [],
                [],
                [],
            )

            for input, scale_tensor, exp_indices in input_list:
                ref_out, ref_scale, ref_time = perf_torch(
                    args, input, scale_tensor, exp_indices
                )
                torch_out_list.append(ref_out)
                torch_input_list.append(input * args.topk)

            for input, scale_tensor, exp_indices in input_list:
                dispatch_out = triton_a2a_op.dispatch(input, exp_indices)
                dispatch_out_list.append(dispatch_out)
                torch.cuda.synchronize()
                combined_out = triton_a2a_op.combine(dispatch_out)
                combine_out_list.append(combined_out)

            for idx, (torch_out, dist_out) in enumerate(
                zip(torch_out_list, dispatch_out_list)
            ):
                if RANK == 0:
                    print(f"dispatch: shape = {torch_out.shape}, {dist_out.shape}")
                try:
                    sorted_dist_out = sort_by_vectors(dist_out)
                    sorted_ref_out = sort_by_vectors(torch_out)
                    torch.testing.assert_close(
                        sorted_dist_out, sorted_ref_out, atol=0, rtol=0
                    )
                except Exception as e:
                    raise e

            for idx, (input, triton_combine_out) in enumerate(
                zip(torch_input_list, combine_out_list)
            ):
                if RANK == 0:
                    print(f"combine: shape = {input.shape}, {triton_combine_out.shape}")
                try:
                    torch.testing.assert_close(
                        input, triton_combine_out, atol=1e-2, rtol=1e-2
                    )
                except Exception as e:
                    raise e

        print(f"RANK[{RANK}]: pass.")
        torch.distributed.destroy_process_group(EP_GROUP)
        exit(0)

    for rid in range(args.rounds):

        L = args.M // 2 if not args.profile else args.M

        token_num = random.randint(L, args.M)

        print(f"Rank-{RANK}: Received {token_num} tokens")

        exp_indices = generate_random_exp_indices(token_num, args.G, args.topk)

        assert exp_indices.size(0) == token_num and exp_indices.size(1) == args.topk
        exp_indices = exp_indices.to("cuda")
        input = (
            torch.rand(token_num, args.N, dtype=torch.float32)
            .to(DTYPE_MAP[args.dtype])
            .to("cuda")
        )
        scale_tensor = torch.rand(token_num, dtype=torch.float32).to("cuda")

        ctx = get_torch_prof_ctx(args.profile)
        with ctx:
            ref_out, ref_scale, ref_time = perf_torch(
                args, input, scale_tensor, exp_indices
            )
            triton_dispatch_out, triton_perf = perf_func(
                partial(triton_a2a_op.dispatch, input, exp_indices),
                iters=100,
                warmup_iters=20,
            )
            combined_out, triton_combine_perf = perf_func(
                partial(triton_a2a_op.combine, triton_dispatch_out),
                iters=100,
                warmup_iters=20,
            )

        torch.cuda.synchronize()
        torch.distributed.barrier()

        torch.distributed.barrier()
        sorted_triton_dispatch_out = sort_by_vectors(triton_dispatch_out)
        sorted_ref_out = sort_by_vectors(ref_out)
        torch.cuda.synchronize()
        torch.distributed.barrier()

        if args.profile:
            run_id = os.environ["TORCHELASTIC_RUN_ID"]
            prof_dir = f"prof/{run_id}"
            os.makedirs(prof_dir, exist_ok=True)
            ctx.export_chrome_trace(f"{prof_dir}/trace_rank{EP_GROUP.rank()}.json.gz")

        _check(sorted_triton_dispatch_out, sorted_ref_out)

        combined_out, triton_combine_perf = perf_func(
            partial(triton_a2a_op.combine, triton_dispatch_out),
            iters=100,
            warmup_iters=20,
        )
        torch.cuda.synchronize()
        torch.distributed.barrier()

        torch.testing.assert_close(
            combined_out, input * args.topk, rtol=1e-2, atol=1e-2
        )

        print(
            f"RANK {RANK}: triton dispatch perf = {triton_perf}ms, triton_combine_perf = {triton_combine_perf}ms"
        )

    torch.distributed.destroy_process_group(EP_GROUP)
